{"case_index": 1, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"Empirical results on six tasks, including reasoning and long-form generation, demonstrate that SELF- RAGsignificantly outperforms pre-trained and [MASK] LLMs that have more parameters and widely adopted RAG approaches with higher citation accuracy.\"", "gold": "instruction-tuned", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.25, "hit_rank": 4, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.402, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.71949476, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 27, "text_snippet": "n (reasoning path consistency) and without retrieval. Recent work on LLM refinement (Dhuliawala et al., 2023; Madaan et al., 2023; Paul et al., 2023) prompts a model to generate task output, natural language feedback and refined task output"}, {"rank": 2, "score": 0.7084943, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 96, "text_snippet": "ains an LM to learn to retrieve, generate, and critique text passages and its own generation by predicting the next tokens from its original vocabulary as well as newly added special tokens, called reflection tokens. SELF-RAGfurther enables"}, {"rank": 3, "score": 0.68485886, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 3, "text_snippet": "e phase, enabling it to tailor its behavior to diverse task requirements. Experiments show that SELF- RAG(7B and 13B parameters) significantly outperforms state-of-the-art LLMs and retrieval-augmented models on a diverse set of tasks. Speci"}, {"rank": 4, "score": 0.68470067, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 17, "text_snippet": "el beam search using the weighted linear sum of the reflection token probabilities as segment score. Empirical results on six tasks, including reasoning and long-form generation, demonstrate that SELF- RAGsignificantly outperforms pre-train"}, {"rank": 5, "score": 0.683317, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 0, "text_snippet": "Preprint. SELF-RAG: LEARNING TO RETRIEVE , GENERATE ,AND CRITIQUE THROUGH SELF-REFLECTION Akari Asai†, Zeqiu Wu†, Yizhong Wang†§, Avirup Sil‡, Hannaneh Hajishirzi†§ †University of Washington§Allen Institute for AI‡IBM Research AI {akari,zeq"}]}
{"case_index": 2, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"In real [MASK] (e.g., RAG), the context is the dominating part of the input (i.e., s≫q) and hence the overall input to the decod\"", "gold": "applications", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 5.53, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.5666261, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 12, "text_snippet": "is information is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-a"}, {"rank": 2, "score": 0.55670905, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 0, "text_snippet": "REFRAG: Rethinking RAG based Decoding Xiaoqiang Lin1,2,∗,Aritra Ghosh1,Bryan Kian Hsiang Low2,Anshumali Shrivastava1,3,Vijai Mohan1 1Meta Superintelligence Labs,2National University of Singapore,3Rice University ∗Work done at Meta Large Lan"}, {"rank": 3, "score": 0.5525099, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 95, "text_snippet": "oducedREFRAG, a novel and efficient decoding framework tailored for RAG applications. By leveraging the inherent sparsity and block-diagonal attention patterns present in RAG contexts,REFRAG compresses, senses, and expands context represent"}, {"rank": 4, "score": 0.5408962, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 17, "text_snippet": "FRAG achieves better performance than LLaMA without incurring higher latency in the downstream applications. 2 Model Architecture We denote the decoder model as Mdecand the encoder model as Menc. Given an input with Ttokens x1, x2, . . . , "}, {"rank": 5, "score": 0.53742474, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 3, "text_snippet": " on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30 .85×the time-t"}]}
{"case_index": 3, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"[MASK] language models (Khandelwal et al., 2020; Borgeaud et al., 2022; Izacard et al., 2022b; Yasunaga et al., 2022), in con- trast, can retrieve knowledge from an external datastore when needed, potentially reducing hallucination and increas- ing coverage.\"", "gold": "retrieval-augmented", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.2, "hit_rank": 5, "matched_by": "doc_id+chunk_id", "retrieval_ms": 5.732, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.62056524, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 83, "text_snippet": "sser, S., and Leahy, C. The Pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027 , 2020.  REPLUG: Retrieval-Augmented Black-Box Language Models Goyal, A., Friesen, A., Banino, A., Weber, T., Ke, N. R"}, {"rank": 2, "score": 0.6152239, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 206, "text_snippet": "ational Linguistics. doi: 10.18653/v1/2020.emnlp-main.346. URL https://aclanthology.org/2020.emnlp-main.346 . 8 Kurt Shuster, Spencer Poﬀ, Moya Chen, Douwe Kiela, and Jason Weston. Retrieval augmentation reduces hallucination in conversatio"}, {"rank": 3, "score": 0.60617924, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 72, "text_snippet": "it from retrieval. However, REPLUG lacks interpretability as it is unclear when the model relies on retrieved knowledge or parametric knowledge. Future research could focus on developing more interpretable retrieval-augmented language model"}, {"rank": 4, "score": 0.60416377, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 0, "text_snippet": "REPLUG: Retrieval-Augmented Black-Box Language Models Weijia Shi,1 *Sewon Min,1Michihiro Yasunaga,2Minjoon Seo,3Rich James,4Mike Lewis,4 Luke Zettlemoyer1 4Wen-tau Yih4 Abstract We introduce REPLUG, a retrieval-augmented lan- guage modeling"}, {"rank": 5, "score": 0.6035683, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 3, "text_snippet": "itly in their parameters. However, they are also prone to hallucination and cannot represent the full long tail of knowledge from the training corpus. Retrieval-augmented language models (Khandelwal et al., 2020; Borgeaud et al., 2022; Izac"}]}
{"case_index": 4, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"(2019): •[MASK] (Kwiatkowski et al., 2019) contains questions corresponding to Google search queries.\"", "gold": "naturalquestions", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 10.6, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.6485207, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 2, "score": 0.6018027, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 86, "text_snippet": "models for open domain question an- swering. In Proceedings of the 16th Conference of the European Chapter of the Association for Compu- tational Linguistics: Main Volume , pp. 874–880, On- line, April 2021a. Association for Computational L"}, {"rank": 3, "score": 0.5925586, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 0, "text_snippet": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering Gautier Izacard1,2,3Edouard Grave1 1Facebook AI Research, Paris 2ENS, PSL University, Paris 3Inria, Paris gizacard|egrave@fb.com Abstract Generative mode"}, {"rank": 4, "score": 0.5919659, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 5, "score": 0.59074295, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}]}
{"case_index": 5, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"2 Model [MASK] We denote the decoder model as Mdecand the encoder model as Menc.\"", "gold": "architecture", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 5.641, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.5824978, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 17, "text_snippet": "FRAG achieves better performance than LLaMA without incurring higher latency in the downstream applications. 2 Model Architecture We denote the decoder model as Mdecand the encoder model as Menc. Given an input with Ttokens x1, x2, . . . , "}, {"rank": 2, "score": 0.5557881, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 34, "text_snippet": "s the target answer. Output: An updated encdec θ 1:T ← ∅ 2:fori∈ {1, . . . , T }do 3: vr← −∞ 4: forj∈ {1, . . . , n }do 5: sj=Decode (Mt,[pj;xi;Di]) 6: vj=Score (M,yi,[sj;xi]) 7: ifvj> vrthen 8: st←sj,vr←vj 9: vd=Score (M,yi,[xi]) 10: ifvr<"}, {"rank": 3, "score": 0.55504346, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 59, "text_snippet": "decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B as decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance varies with different encoder and decoder sizes. Figure 11 presents results for"}, {"rank": 4, "score": 0.5121393, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 18, "text_snippet": "f question and retrieval in this section. Model overview.Figure 1 shows the main architecture ofREFRAG. This model consists of a decoder-only foundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Rober"}, {"rank": 5, "score": 0.50371563, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}]}
{"case_index": 6, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"This [MASK] is implemented to broaden the spectrum of retrieved information, harnessing the expansive and dynamic nature of the web to complement and enrich the initially obtained documents.\"", "gold": "augmentation", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.557, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.538652, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 2, "score": 0.5256603, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 12, "text_snippet": " are integrated as a strategic extension, since retrieval from static and limited corpora can only return sub-optimal documents in terms of scope and diversity. This augmentation is implemented to broaden the spectrum of retrieved informati"}, {"rank": 3, "score": 0.5153351, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 4, "score": 0.5063887, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 1, "text_snippet": "evitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Al- though retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heav"}, {"rank": 5, "score": 0.5011475, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 79, "text_snippet": "ges are provided as input (correct, incorrect and ambiguous content). All the data in the table only represents a rough estimate of the generation phase, the retrieval and data-processing stages are not included. anism, rather than solely f"}]}
{"case_index": 7, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"Our extractive compressor is trained with a [MASK] learning objective to identify sentences that lead to target outputs, and our abstractive compressor is distilled (West et a\"", "gold": "contrastive", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.2, "hit_rank": 5, "matched_by": "doc_id+chunk_id", "retrieval_ms": 5.971, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6374484, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 23, "text_snippet": "odels in the output should be faithful to the original input, yet the main goal is different. Instead of capturing salient information for humans readers, compressors aim to produce a concise text that are useful for a LM on an end task. In"}, {"rank": 2, "score": 0.61929494, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 19, "text_snippet": "ining compressors for conciseness and effectiveness. We summarize the key ideas for our two compressors, extractive compressors and abstractive compressor here, and discuss their training schemes formally in Section 3. 2Improving retriever "}, {"rank": 3, "score": 0.61149937, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 24, "text_snippet": "TRACTIVE COMPRESSION As we formulate extractive compression as a ranking problem, training extractive compressor re- sembles training a reranker for the retrieved documents4with two differences. First, our compressor considers a different g"}, {"rank": 4, "score": 0.6073624, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 82, "text_snippet": " and allowing the compression models to perform selective augmentation. Our experiments show that our compressors can improve the efficiency of retrieval augmented LMs significantly with minimal drop in performances. ACKNOWLEDGEMENT We than"}, {"rank": 5, "score": 0.6059494, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 12, "text_snippet": "ompressors implement multi-document query-focused summarization (Xu & Lapata, 2020), where we summarize retrieved evidence document set with respect to the input query. As we aim to enable RALM to generate correct output when summary is pre"}]}
{"case_index": 8, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"We propose two models that marginalize over the latent documents in different ways to produce a [MASK] over generated text.\"", "gold": "distribution", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.2, "hit_rank": 5, "matched_by": "doc_id+chunk_id", "retrieval_ms": 21.752, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.6410593, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 2, "score": 0.6077053, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 37, "text_snippet": " To cast language modeling in the text-to-text framework, we consider a chunk of Nwords, and split this chunk in two sub-sequences of equal length N/2. Then, the ﬁrst sub-sequence is used as the query, and the second corresponds to the outp"}, {"rank": 3, "score": 0.60634017, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 83, "text_snippet": ".49 28.22 5 20.18 17.3728.2419.6518.7127.08 820.5217.60 28.17 16.87 18.05 25.36 10 19.67 17.41 27.62 15.72 17.42 23.60 6 Related Works Retrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi- tec"}, {"rank": 4, "score": 0.5952387, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 5, "score": 0.56390727, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 17, "text_snippet": "rator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a distribution over generated text. In one approach, RAG-Sequence , the mo"}]}
{"case_index": 9, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"The retrieved documents are processed, along with the current context, by a [MASK] model using the Fusion-in-Decoder architecture (Izacard & Grave, 2020) that generates the corresponding output.\"", "gold": "sequence-to-sequence", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.3333333333333333, "hit_rank": 3, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.276, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.64433515, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 2, "score": 0.59657294, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 111, "text_snippet": " arXiv:2208.03299, 2022. https://arxiv.org/abs/ 2208.03299. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Edouard Grave, and Sebastian Riedel. Atlas: Few-shot learn"}, {"rank": 3, "score": 0.59537506, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 9, "text_snippet": " strong few-shot learning, despite having lower parameter counts than other powerful recent few-shot learners. Atlasretrieves relevant documents based on the current context by using a general-purpose dense retriever using a dual-encoder ar"}, {"rank": 4, "score": 0.5898131, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 83, "text_snippet": ".49 28.22 5 20.18 17.3728.2419.6518.7127.08 820.5217.60 28.17 16.87 18.05 25.36 10 19.67 17.41 27.62 15.72 17.42 23.60 6 Related Works Retrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi- tec"}, {"rank": 5, "score": 0.5885403, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}]}
{"case_index": 10, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"Though our work also studies [MASK] critique on retrieval and generation, we train our target LM on task examples augmented with reflection tokens from a critic model offline, with a far lower training cost compared to RLHF.\"", "gold": "fine-grained", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 14.428, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.6588437, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 14, "text_snippet": "t with reflection tokens by unifying them as the next token prediction from the expanded model vocabulary. We train our generator LM on a diverse collection of text interleaved with reflection tokens and retrieved passages. Reflection token"}, {"rank": 2, "score": 0.65472084, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 25, "text_snippet": " has proven effective in aligning LLMs with human preferences (Ouyang et al., 2022). Wu et al. (2023) introduce fine-grained RLHF with multiple reward models. Though our work also studies fine-grained critique on retrieval and generation, w"}, {"rank": 3, "score": 0.6517083, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 15, "text_snippet": "st a critic model during training, reducing overhead. The critic model, in part, is supervised on a dataset of input, output, and corresponding reflection tokens collected by prompting a propriety LM (i.e., GPT-4; OpenAI 2023). While we dra"}, {"rank": 4, "score": 0.6424526, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 37, "text_snippet": "e quality of a given task output (Section 3.2.1). Using the critic model, we update the training corpus by inserting reflection tokens into task outputs offline. Subsequently, we train the final generator model ( M) using the conventional L"}, {"rank": 5, "score": 0.6385506, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 2, "text_snippet": "ew framework called Self-Reflective Retrieval-Augmented Gen- eration ( SELF-RAG)that enhances an LM’s quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passage"}]}
{"case_index": 11, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"[MASK], our approach uses the LLM to infer the potential users would use the RAG\"", "gold": "specifically", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 13.86, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6168477, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 0, "text_snippet": "From Local to Global: A GraphRAG Approach to Query-Focused Summarization Darren Edge1†Ha Trinh1†Newman Cheng2Joshua Bradley2Alex Chao3 Apurva Mody3Steven Truitt2Dasha Metropolitansky1Robert Osazuwa Ness1 Jonathan Larson1 1Microsoft Research"}, {"rank": 2, "score": 0.5861995, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 1, "text_snippet": "ibuted equally to this work Abstract The use of retrieval-augmented generation (RAG) to retrieve relevant informa- tion from an external knowledge source enables large language models (LLMs) to answer questions over private and/or previousl"}, {"rank": 3, "score": 0.5792678, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 22, "text_snippet": "gly global summaries by using the LLM to create summaries spanning this community hierarchy. 2.3 Adaptive benchmarking for RAG Evaluation Many benchmark datasets for open-domain question answering exist, including HotPotQA (Yang et al., 201"}, {"rank": 4, "score": 0.56642413, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 15, "text_snippet": "e LLM’s context window. In canonical RAG approaches, the retrieval process returns a set number of records that are seman- tically similar to the query and the generated answer uses only the information in those retrieved records. A common "}, {"rank": 5, "score": 0.56494534, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 8, "text_snippet": "nce analysis (Ranade and Joshi, 2023). Given a sensemaking query and a text with an implicit and interconnected set of concepts, an LLM can generate a summary that answers the query. The challenge, however, arises when the volume of data re"}]}
{"case_index": 12, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"3 L EARNING THE COMPRESSORS Our compressor resembles text [MASK] models in the output should be faithful to the original input, ye\"", "gold": "summarization", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.25, "hit_rank": 4, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.091, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.61535925, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 23, "text_snippet": "odels in the output should be faithful to the original input, yet the main goal is different. Instead of capturing salient information for humans readers, compressors aim to produce a concise text that are useful for a LM on an end task. In"}, {"rank": 2, "score": 0.60046786, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 82, "text_snippet": " and allowing the compression models to perform selective augmentation. Our experiments show that our compressors can improve the efficiency of retrieval augmented LMs significantly with minimal drop in performances. ACKNOWLEDGEMENT We than"}, {"rank": 3, "score": 0.599463, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 123, "text_snippet": "//huggingface.co/facebook/contriever 10https://huggingface.co/facebook/contriever-msmarco 15  Input: Base LM M, Compressor encoder encθ, Training data {xi,Si,yi}T 1where xiis input, Si={sj}n 1 is a set of candidate sentences from the retrie"}, {"rank": 4, "score": 0.59241164, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 22, "text_snippet": "y. Yet, using an extreme-scale model as the compressor is not desirable as we want the compressor to be substantially smaller than the LMs. Thus, we perform distillation (Hinton et al., 2015) of extreme-scale LMs to build a lightweight abst"}, {"rank": 5, "score": 0.5797328, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 0, "text_snippet": "RECOMP: I MPROVING RETRIEVAL -AUGMENTED LM S WITH COMPRESSION AND SELECTIVE AUGMENTATION Fangyuan Xu1, Weijia Shi2, Eunsol Choi1 Department of Computer Science 1The University of Texas at Austin 2University of Washington {fangyuan,eunsol }@"}]}
{"case_index": 13, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"Ablation studies in section 4 demonstrate thatthis [MASK] achieving strong CPT performance.\"", "gold": "recipeiscrucialfor", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 6.673, "llm_ms": 0.004, "top_contexts": [{"rank": 1, "score": 0.52498436, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 174, "text_snippet": "when usingk= 8andk= 32compression rate forREFRAGrespectively. Ablation study result for curriculum learning.Table 11 shows the necessity of curriculum learning to the success of reconstruction task. Ablation study result for reconstruction "}, {"rank": 2, "score": 0.52298594, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 3, "text_snippet": " on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30 .85×the time-t"}, {"rank": 3, "score": 0.5216468, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 12, "text_snippet": "is information is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-a"}, {"rank": 4, "score": 0.5211668, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 95, "text_snippet": "oducedREFRAG, a novel and efficient decoding framework tailored for RAG applications. By leveraging the inherent sparsity and block-diagonal attention patterns present in RAG contexts,REFRAG compresses, senses, and expands context represent"}, {"rank": 5, "score": 0.5210981, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 26, "text_snippet": "e detailed discussion 1REFRAGwithout cache means that we recompute the chunk embedding for the context and take this latency into account. 3  on empirical evaluation is in section A. 3 Methodology To align the encoder and decoder, we follow"}]}
{"case_index": 14, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"We then minimize the [MASK] between pattn(dk), and the distribution pretrfrom the retriever deﬁned in Equation 1: KL(pattn∥pretr) =K∑ k=1pa\"", "gold": "kl-divergence", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 9.319, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.5891707, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 2, "score": 0.58384633, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 29, "text_snippet": "om the retriever deﬁned in Equation 1: KL(pattn∥pretr) =K∑ k=1pattn(dk) log(pattn(dk) pretr(dk)) . Here, this loss is only used to optimize the parameters of the retriever, and not the language model. When using recent deep learning framewo"}, {"rank": 3, "score": 0.5592053, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 4, "score": 0.5496721, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 83, "text_snippet": ".49 28.22 5 20.18 17.3728.2419.6518.7127.08 820.5217.60 28.17 16.87 18.05 25.36 10 19.67 17.41 27.62 15.72 17.42 23.60 6 Related Works Retrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi- tec"}, {"rank": 5, "score": 0.5398628, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 111, "text_snippet": " arXiv:2208.03299, 2022. https://arxiv.org/abs/ 2208.03299. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Edouard Grave, and Sebastian Riedel. Atlas: Few-shot learn"}]}
{"case_index": 15, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"Retrieval methods have [MASK] from traditional term-based techniques like TF-IDF (Sp¨arck Jones, 1972) and BM25 (Robertson et al., 1995; Roberts et al., 2020) to deep learning–based strategies (Karpukhin et al., 2020; Khattab & Zaharia, 2020; Sachan et al., 2023).\"", "gold": "transitioned", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.502, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.586318, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 0, "text_snippet": "Published as a conference paper at ICLR 2024 RAPTOR: R ECURSIVE ABSTRACTIVE PROCESSING FOR TREE-ORGANIZED RETRIEVAL Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D. Manning Stanford University psarthi@cs."}, {"rank": 2, "score": 0.56989455, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 14, "text_snippet": "omponents: the retriever, the reader, and end-to-end system training. Retrieval methods have transitioned from traditional term-based techniques like TF-IDF (Sp¨arck Jones, 1972) and BM25 (Robertson et al., 1995; Roberts et al., 2020) to de"}, {"rank": 3, "score": 0.5581287, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 4, "score": 0.5564669, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 5, "score": 0.5516548, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 22, "text_snippet": " RAPTOR addresses the issue of semantic depth and connection in reading by building a recursive tree structure that balances broader thematic comprehension with granular details and which allows nodes to be grouped based on semantic sim- il"}]}
{"case_index": 16, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"In addition to their memorisation abilities, such architectures are attractive due to a number of other established advantages in terms of adaptability, [MASK] and eﬃciency (Guu et al., 2020; Lewis et al., 2020; Yogatama et al., 2021; Borgeaud et al., 2021, inter alia).\"", "gold": "interpretability", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 6.674, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.63855755, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 111, "text_snippet": " arXiv:2208.03299, 2022. https://arxiv.org/abs/ 2208.03299. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Edouard Grave, and Sebastian Riedel. Atlas: Few-shot learn"}, {"rank": 2, "score": 0.6189881, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 83, "text_snippet": ".49 28.22 5 20.18 17.3728.2419.6518.7127.08 820.5217.60 28.17 16.87 18.05 25.36 10 19.67 17.41 27.62 15.72 17.42 23.60 6 Related Works Retrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi- tec"}, {"rank": 3, "score": 0.605934, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 4, "score": 0.59327185, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 0, "text_snippet": "Atlas: Few-shot Learning with Retrieval Augmented Language Models Gautier Izacard∗ ∗,♦,♣,♥gizacard@fb.com Patrick Lewis∗,♦plewis@fb.com Maria Lomeli♦marialomeli@fb.com Lucas Hosseini♦hoss@fb.com Fabio Petroni♦fabiopetroni@fb.com Timo Schick"}, {"rank": 5, "score": 0.5849711, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 2, "text_snippet": "ings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including"}]}
{"case_index": 17, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"It also adds to a growing body of RAG [MASK] that use a knowledge graph as an index (Gao et al., 2023).\"", "gold": "approaches", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.25, "hit_rank": 4, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.295, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.623653, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 0, "text_snippet": "From Local to Global: A GraphRAG Approach to Query-Focused Summarization Darren Edge1†Ha Trinh1†Newman Cheng2Joshua Bradley2Alex Chao3 Apurva Mody3Steven Truitt2Dasha Metropolitansky1Robert Osazuwa Ness1 Jonathan Larson1 1Microsoft Research"}, {"rank": 2, "score": 0.6049925, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 96, "text_snippet": "tigate these downstream risks for questions of a global nature, which might otherwise be answered by samples of retrieved facts falsely presented as global summaries. 7 Conclusion We have presented GraphRAG, a RAG approach that combines kno"}, {"rank": 3, "score": 0.5959135, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 17, "text_snippet": " are later used to answer queries as in Mao et al. 2020). These summaries are generated in parallel and iteratively aggregated into global summaries, similar to prior techniques (Feng et al., 2023; Gao et al., 2023; Khattab et al., 2022; Sh"}, {"rank": 4, "score": 0.5719497, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 19, "text_snippet": "2016; Mooney and Bunescu, 2005; Yates et al., 2007). GraphRAG falls into a more recent body of research that use of LLMs for knowledge graph extraction (Ban et al., 2023; Melnyk et al., 2022; OpenAI, 2023; Tan et al., 2017; Trajanoska et al"}, {"rank": 5, "score": 0.5649735, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 16, "text_snippet": " may use alternative retrieval mechanisms, we collectively refer to the family of conventional approaches as vector RAG . GraphRAG contrasts with vector RAG in its ability to answer queries that require global sensemaking over the entire da"}]}
{"case_index": 18, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"Given an input query and the retrieved documents from any retriever, a [MASK] retrieval evaluator is constructed to estimate the relevance score of retrieved d\"", "gold": "lightweight", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 6.935, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.58527863, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 10, "text_snippet": " should not have been equally referred to and involved in RAG. On account of the above issues, this paper particularly studies the scenarios where the retriever returns inaccurate results. A method named Corrective Retrieval- Augmented Gene"}, {"rank": 2, "score": 0.5783404, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 31, "text_snippet": "neratorkex+xkex+ Figure 2: An overview of the proposed CRAG at inference. A retrieval evaluator is constructed to evaluate the relevance of the retrieved documents to the input, and estimate a confidence degree based on which different know"}, {"rank": 3, "score": 0.56387633, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 29, "text_snippet": "bitrary generative model can be adopted. 4.2 Retrieval Evaluator It is natural to wonder whether the retrieved docu- ments are accurate or not before using them, which is significant since irrelevant or misleading mes- sages can be identifi"}, {"rank": 4, "score": 0.56116676, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 5, "score": 0.5489358, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 1, "text_snippet": "evitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Al- though retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heav"}]}
{"case_index": 19, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG with k= 16achieves16 .53×TTFT [MASK] with cache and8 .59×without cache1, both surpassing CEPE (\"", "gold": "acceleration", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 5.984, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.64436567, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 199, "text_snippet": "RAG 16+RL). Arxiv Book PG19 ProofPile Compression Rate P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048↓ Context Length=2048 REFRAG 8 8 1.124 1.091 1.062 1.905 1.868 1.844 1.996 1.9561.9270.997 0.952 0.916 REFRAG 16+RL 8."}, {"rank": 2, "score": 0.64159364, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 3, "text_snippet": " on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30 .85×the time-t"}, {"rank": 3, "score": 0.63941085, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 136, "text_snippet": " CEPE in the long context scenario. Acceleration/SaveShortsLongs KV cache memoryks+ko s+ko1∼k×k× TTFTk2(6ds+s2) 6dsk+s2 k×k2× TTIT2dlbsk+nk+2dlbok 2dlbs+nk+2dlbok1×k× Throughputk∗TTFT+k∗TTIT TTFT+kTTIT∼k2∗TTFT+k2∗TTIT TTFT+k∗TTIT1∼k×k∼k2× T"}, {"rank": 4, "score": 0.63922906, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 134, "text_snippet": "ntext),REFRAG achieves16 .53×acceleration in TTFT with cache and8 .59×without cache. Both higher than CEPE (i.e., 2.01×and1 .04×acceleration respectively) while having better model performance (see table 1). With longer context, we are able"}, {"rank": 5, "score": 0.62811154, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 26, "text_snippet": "e detailed discussion 1REFRAGwithout cache means that we recompute the chunk embedding for the context and take this latency into account. 3  on empirical evaluation is in section A. 3 Methodology To align the encoder and decoder, we follow"}]}
{"case_index": 20, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"triever inREPLUG by using the LM itself to provide [MASK] about which documents should be retrieved.\"", "gold": "supervision", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 7.099, "llm_ms": 0.004, "top_contexts": [{"rank": 1, "score": 0.63435155, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 0, "text_snippet": "REPLUG: Retrieval-Augmented Black-Box Language Models Weijia Shi,1 *Sewon Min,1Michihiro Yasunaga,2Minjoon Seo,3Rich James,4Mike Lewis,4 Luke Zettlemoyer1 4Wen-tau Yih4 Abstract We introduce REPLUG, a retrieval-augmented lan- guage modeling"}, {"rank": 2, "score": 0.62585354, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 28, "text_snippet": "ethod of prepending all the retrieved docu-  REPLUG: Retrieval-Augmented Black-Box Language Models ments, our ensemble methods do not incur additional com- putational cost overhead. 4. R EPLUG LSR: Training the Dense Retriever Instead of re"}, {"rank": 3, "score": 0.6240536, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 7, "text_snippet": " retrieved documents are prepended to the input context and fed into the black-box LM to make the final prediction. Because the LM context length limits the number of documents that can be prepended, we also introduce a new ensemble scheme "}, {"rank": 4, "score": 0.6222744, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 71, "text_snippet": "a black box and augments it with a tuneable retrieval model. Our evaluation shows that REPLUG can be integrated with any existing language model to improve their performance  REPLUG: Retrieval-Augmented Black-Box Language Models on language"}, {"rank": 5, "score": 0.617444, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 21, "text_snippet": " (§3.1 Document Retrieval ). Then it prepends each document separately to the input context and ensembles output probabilities from different passes (§3.2 Input Reformulation ). and a training scheme to further adapt the retriever to large "}]}
{"case_index": 21, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"ve models, and multiple [MASK] have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a).\"", "gold": "techniques", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.185, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.7016485, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}, {"rank": 2, "score": 0.67657137, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 0, "text_snippet": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering Gautier Izacard1,2,3Edouard Grave1 1Facebook AI Research, Paris 2ENS, PSL University, Paris 3Inria, Paris gizacard|egrave@fb.com Abstract Generative mode"}, {"rank": 3, "score": 0.67494214, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 86, "text_snippet": "models for open domain question an- swering. In Proceedings of the 16th Conference of the European Chapter of the Association for Compu- tational Linguistics: Main Volume , pp. 874–880, On- line, April 2021a. Association for Computational L"}, {"rank": 4, "score": 0.63560426, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 5, "score": 0.6335786, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 15, "text_snippet": " that answers do not correspond to spans in support documents, thus requiring ab- stractive models. Raffel et al. (2019) showed that generative models are competitive for reading com- prehension tasks such as SQuAD (Rajpurkar et al., 2016),"}]}
{"case_index": 22, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"Together, these community summaries provide global [MASK] and insights over the corpus.\"", "gold": "descriptions", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.3333333333333333, "hit_rank": 3, "matched_by": "doc_id+chunk_id", "retrieval_ms": 9.67, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.5945276, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 97, "text_snippet": "omprehensive- ness and diversity of answers, as well as favorable comparisons to a global but graph-free approach using map-reduce source text summarization. For situations requiring many global queries over the same dataset, summaries of r"}, {"rank": 2, "score": 0.59218407, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 0, "text_snippet": "From Local to Global: A GraphRAG Approach to Query-Focused Summarization Darren Edge1†Ha Trinh1†Newman Cheng2Joshua Bradley2Alex Chao3 Apurva Mody3Steven Truitt2Dasha Metropolitansky1Robert Osazuwa Ness1 Jonathan Larson1 1Microsoft Research"}, {"rank": 3, "score": 0.5690613, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 10, "text_snippet": "cted communities, with summaries at higher levels of the hierarchy recursively incorporating lower-level summaries. Together, these community summaries provide global descriptions and insights over the corpus. Finally, GraphRAG answers quer"}, {"rank": 4, "score": 0.5559821, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 5, "score": 0.5408728, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 47, "text_snippet": "t within the context window. 3.1.6 Community Summaries →Community Answers →Global Answer Given a user query, the community summaries generated in the previous step can be used to generate a final answer in a multi-stage process. The hierarc"}]}
{"case_index": 23, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"Computing Retrieval [MASK] We retrieve kdocuments D′⊂ D with the highest simi- larity scores from a corpus Dgiven a\"", "gold": "likelihood", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 9.087, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.5791786, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 83, "text_snippet": "sser, S., and Leahy, C. The Pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027 , 2020.  REPLUG: Retrieval-Augmented Black-Box Language Models Goyal, A., Friesen, A., Banino, A., Weber, T., Ke, N. R"}, {"rank": 2, "score": 0.5786083, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 7, "text_snippet": " retrieved documents are prepended to the input context and fed into the black-box LM to make the final prediction. Because the LM context length limits the number of documents that can be prepended, we also introduce a new ensemble scheme "}, {"rank": 3, "score": 0.5764897, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 4, "score": 0.57579994, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 5, "score": 0.5741969, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 50, "text_snippet": "odel, which we consider a white-box retrieval LM setting. Our model We add REPLUG andREPLUG LSR only to Codex because other models such as PaLM and Flan-PaLM are not accessible to the public. We use the test question as the query to retriev"}]}
{"case_index": 24, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"The critic model, in part, is supervised on a dataset of input, output, and [MASK] reflection tokens collected by prompting a propriety LM (i.e., GPT-4; OpenAI 2023).\"", "gold": "corresponding", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.345, "llm_ms": 0.008, "top_contexts": [{"rank": 1, "score": 0.7161796, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 15, "text_snippet": "st a critic model during training, reducing overhead. The critic model, in part, is supervised on a dataset of input, output, and corresponding reflection tokens collected by prompting a propriety LM (i.e., GPT-4; OpenAI 2023). While we dra"}, {"rank": 2, "score": 0.6323149, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 14, "text_snippet": "t with reflection tokens by unifying them as the next token prediction from the expanded model vocabulary. We train our generator LM on a diverse collection of text interleaved with reflection tokens and retrieved passages. Reflection token"}, {"rank": 3, "score": 0.61952794, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 37, "text_snippet": "e quality of a given task output (Section 3.2.1). Using the critic model, we update the training corpus by inserting reflection tokens into task outputs offline. Subsequently, we train the final generator model ( M) using the conventional L"}, {"rank": 4, "score": 0.6181686, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 38, "text_snippet": "ing of two models, the critic C(Section 3.2.1) and the generator M(Section 3.2.2). 3.2.1 T RAINING THE CRITIC MODEL Data collection for critic model. Manual annotation of reflection tokens for each segment is expensive (Wu et al., 2023). A "}, {"rank": 5, "score": 0.6115104, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 48, "text_snippet": "gmented with reflection tokens Dgenusing the standard next token objective: max ME(x,y,r )∼DgenlogpM(y, r|x). (2) Unlike Ctraining (Eq. 1), Mlearns to predict the target output as well as the reflection tokens. During training, we mask out "}]}
{"case_index": 25, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"On the other hand, even open sourced language models such as OPT-175B (Zhang et al., 2022a) and BLOOM-176B (Scao et al., 2022) require significant [MASK] resourcesto run and finetune locally.\"", "gold": "computational", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.221, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.64277613, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 13, "text_snippet": " (Wu et al., 2021), are not open-sourced due to commercial considerations and are only available as black-box APIs, through which users can send queries and receive responses. On the other hand, even open sourced language models such as OPT"}, {"rank": 2, "score": 0.6207275, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 104, "text_snippet": "2a. Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V ., et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 , 2022b. Zhong, Z., Lei, T., and Chen"}, {"rank": 3, "score": 0.61730635, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 83, "text_snippet": "sser, S., and Leahy, C. The Pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027 , 2020.  REPLUG: Retrieval-Augmented Black-Box Language Models Goyal, A., Friesen, A., Banino, A., Weber, T., Ke, N. R"}, {"rank": 4, "score": 0.6023412, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 12, "text_snippet": "gnals, resulting in improved retrieval quality. •Evaluations on language modeling (§6), open-domain QA and MMLU demonstrate that REPLUG can im- prove the performance of various language models such as GPT, OPT and BLOOM, including very larg"}, {"rank": 5, "score": 0.59367585, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 0, "text_snippet": "REPLUG: Retrieval-Augmented Black-Box Language Models Weijia Shi,1 *Sewon Min,1Michihiro Yasunaga,2Minjoon Seo,3Rich James,4Mike Lewis,4 Luke Zettlemoyer1 4Wen-tau Yih4 Abstract We introduce REPLUG, a retrieval-augmented lan- guage modeling"}]}
{"case_index": 26, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"On the other hand, pro- cessing passages jointly in the decoder allows to better [MASK] evidence from multiple passages.\"", "gold": "aggregate", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 6.435, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6723956, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}, {"rank": 2, "score": 0.65412444, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 9, "text_snippet": "rticular, we show that the performance of our method signiﬁcantly improves when the number of retrieved passages increases. We believe that this is evidence that generative mod- els are good at combining evidence from multiple passages, com"}, {"rank": 3, "score": 0.63580966, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 16, "text_snippet": "0) and Lewis et al. (2020) in- troduced retrieval augmented generative models for open domain question answering. Our approach differs from these works by how the generative model processes the retrieved passages. This al- lows to scale to "}, {"rank": 4, "score": 0.6265356, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 5, "score": 0.6258209, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 86, "text_snippet": "models for open domain question an- swering. In Proceedings of the 16th Conference of the European Chapter of the Association for Compu- tational Linguistics: Main Volume , pp. 874–880, On- line, April 2021a. Association for Computational L"}]}
{"case_index": 27, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"(2021) employs task [MASK] to summarize smaller text chunks, which are later integrated to form summaries of larger sections.\"", "gold": "decomposition", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.211, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.5699158, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 20, "text_snippet": "ets but can sometimes be a lossy means of compression. The recursive-abstractive summarization model by Wu et al. (2021) employs task decomposition to summarize smaller text chunks, which are later integrated to form summaries of larger sec"}, {"rank": 2, "score": 0.5614679, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 1, "text_snippet": "rom a retrieval corpus, limiting holistic under- standing of the overall document context. We introduce the novel approach of recursively embedding, clustering, and summarizing chunks of text, constructing a tree with differing levels of su"}, {"rank": 3, "score": 0.5484437, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 4, "score": 0.5459445, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 5, "score": 0.5456503, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 8, "text_snippet": " indexing and retrieval system that uses a tree structure to capture both high-level and low-level details about a text. As shown in Figure 1, our system, RAPTOR, clusters chunks of text, generates text summaries of those clusters, and then"}]}
{"case_index": 28, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"The models which extract the answers are often based on contextualized word [MASK] such as ELMo or BERT (Peters et al., 2018; De- vlin et al., 2019), and predict a span as answer.\"", "gold": "representations", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 7.895, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.69519246, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 86, "text_snippet": "models for open domain question an- swering. In Proceedings of the 16th Conference of the European Chapter of the Association for Compu- tational Linguistics: Main Volume , pp. 874–880, On- line, April 2021a. Association for Computational L"}, {"rank": 2, "score": 0.6665927, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 0, "text_snippet": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering Gautier Izacard1,2,3Edouard Grave1 1Facebook AI Research, Paris 2ENS, PSL University, Paris 3Inria, Paris gizacard|egrave@fb.com Abstract Generative mode"}, {"rank": 3, "score": 0.6623676, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}, {"rank": 4, "score": 0.6543389, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 5, "score": 0.64745224, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 15, "text_snippet": " that answers do not correspond to spans in support documents, thus requiring ab- stractive models. Raffel et al. (2019) showed that generative models are competitive for reading com- prehension tasks such as SQuAD (Rajpurkar et al., 2016),"}]}
{"case_index": 29, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"For a class of global sensemaking questions over datasets in the 1 million token range, we show that GraphRAG leads to substantial [MASK] over a conventional RAG baseline for\"", "gold": "improvements", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.3333333333333333, "hit_rank": 3, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.271, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.65913343, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 96, "text_snippet": "tigate these downstream risks for questions of a global nature, which might otherwise be answered by samples of retrieved facts falsely presented as global summaries. 7 Conclusion We have presented GraphRAG, a RAG approach that combines kno"}, {"rank": 2, "score": 0.6529272, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 0, "text_snippet": "From Local to Global: A GraphRAG Approach to Query-Focused Summarization Darren Edge1†Ha Trinh1†Newman Cheng2Joshua Bradley2Alex Chao3 Apurva Mody3Steven Truitt2Dasha Metropolitansky1Robert Osazuwa Ness1 Jonathan Larson1 1Microsoft Research"}, {"rank": 3, "score": 0.64306426, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 3, "text_snippet": "wo stages: first, to derive an entity knowledge graph from the source documents, then to pre- generate community summaries for all groups of closely related entities. Given a question, each community summary is used to generate a partial re"}, {"rank": 4, "score": 0.6344366, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 22, "text_snippet": "gly global summaries by using the LLM to create summaries spanning this community hierarchy. 2.3 Adaptive benchmarking for RAG Evaluation Many benchmark datasets for open-domain question answering exist, including HotPotQA (Yang et al., 201"}, {"rank": 5, "score": 0.6262553, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 81, "text_snippet": "ng. Tuning element extraction prompts may help to retain more of these details in the GraphRAG index. Community summaries vs. source texts . When comparing community summaries to source texts using GraphRAG, community summaries generally pr"}]}
{"case_index": 30, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"Black-Box Language Models Figure 1, REPLUG is [MASK] flexible and can be used with any existing black-box LM and retrieval model.\"", "gold": "extremely", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.124, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.7232563, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 8, "text_snippet": "Black-Box Language Models Figure 1, REPLUG is extremely flexible and can be used with any existing black-box LM and retrieval model. We also introduce REPLUG LSR (REPLUG with LM- Supervised Retrieval), a training scheme that can further imp"}, {"rank": 2, "score": 0.72083503, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 0, "text_snippet": "REPLUG: Retrieval-Augmented Black-Box Language Models Weijia Shi,1 *Sewon Min,1Michihiro Yasunaga,2Minjoon Seo,3Rich James,4Mike Lewis,4 Luke Zettlemoyer1 4Wen-tau Yih4 Abstract We introduce REPLUG, a retrieval-augmented lan- guage modeling"}, {"rank": 3, "score": 0.7107645, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 71, "text_snippet": "a black box and augments it with a tuneable retrieval model. Our evaluation shows that REPLUG can be integrated with any existing language model to improve their performance  REPLUG: Retrieval-Augmented Black-Box Language Models on language"}, {"rank": 4, "score": 0.66876066, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 83, "text_snippet": "sser, S., and Leahy, C. The Pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027 , 2020.  REPLUG: Retrieval-Augmented Black-Box Language Models Goyal, A., Friesen, A., Banino, A., Weber, T., Ke, N. R"}, {"rank": 5, "score": 0.655622, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 7, "text_snippet": " retrieved documents are prepended to the input context and fed into the black-box LM to make the final prediction. Because the LM context length limits the number of documents that can be prepended, we also introduce a new ensemble scheme "}]}
{"case_index": 31, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"Eleven states are named after an individual person (e.g, California was named after [MASK] Columbus).\"", "gold": "christopher", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.326, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.60488886, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 9, "text_snippet": "io/ . 1arXiv:2310.11511v1 [cs.CL] 17 Oct 2023  Preprint. Step 1: Retrieve K documentsCalifornia was named after a ﬁctional island in a Spanish book. Prompt How did US states get their names?  US states got their names from a variety of sour"}, {"rank": 2, "score": 0.5725069, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 10, "text_snippet": " Generation (RAG)Ours: Self-reﬂective Retrieval-Augmented Generation (Self-RAG)  Popular names by states. In Texas, Emma is a popular baby name. Of the ﬁfty states, eleven are named after an individual person.  Prompt How did US states get "}, {"rank": 3, "score": 0.49886072, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 11, "text_snippet": "+  11 of 50 state namesRelevant Step 2: Generate segment in parallel  come from persons.SupportedIrrelevantTexas is namedafter a Native American tribe. Step 3: Critique outputs and select best segmentorigins in a 16th-century novel Las Serg"}, {"rank": 4, "score": 0.49035573, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 40, "text_snippet": "p>LOUISIANA: Named in<p>Of the ﬁfty states, eleven are named after an individual person</p>. 11 of 50 states’ names come from person. RelevantSupportedhonor of Louis XIV of France.</p>. RelevantFor instance, Louisiana was named after King L"}, {"rank": 5, "score": 0.48541093, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 39, "text_snippet": "tates get their names? Input: Write an essay of your best summer vacationOutput: My best summer vacation was a magical escape to the coastal town of Santorini. The azure waters, charming white-washed building are unforgettable.  Critic LMOu"}]}
{"case_index": 32, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"On [MASK] tasks that involve complex, multi-step reasoning, we show state-of-the-art results; for example, by coupling RAPTOR retrieval with the use of GPT-4, we can improve the best performance on the QuALITY benchmark by 20% in absolute accuracy.\"", "gold": "question-answering", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 24.929, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.59953135, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 0, "text_snippet": "Published as a conference paper at ICLR 2024 RAPTOR: R ECURSIVE ABSTRACTIVE PROCESSING FOR TREE-ORGANIZED RETRIEVAL Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D. Manning Stanford University psarthi@cs."}, {"rank": 2, "score": 0.5910639, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 74, "text_snippet": "employing recursive clustering and summarization techniques, RAPTOR creates a hierarchical tree structure that is capable of synthesizing information across various sections of the retrieval corpora. During the query phase, RAPTOR leverages"}, {"rank": 3, "score": 0.5826209, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 55, "text_snippet": "ly or within higher-layer summaries. our results demonstrate that RAPTOR, when combined with any retriever, consistently outperforms the respective retriever across all datasets.2 Since RAPTOR with SBERT has the best performance, we use it "}, {"rank": 4, "score": 0.58179855, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 48, "text_snippet": "ntext to RAPTOR and to the baselines. Qualitative Study We conduct a qualitative analysis to understand the benefits of RAP- TOR’s retrieval process compared to Dense Passage Retrieval (DPR) methods. Our study focuses on thematic, multi-hop"}, {"rank": 5, "score": 0.57654625, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 22, "text_snippet": " RAPTOR addresses the issue of semantic depth and connection in reading by building a recursive tree structure that balances broader thematic comprehension with granular details and which allows nodes to be grouped based on semantic sim- il"}]}
{"case_index": 33, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"Results show that we achieve30 .75×TTFT [MASK] without loss in perplexity which is3 .75×than previous method.\"", "gold": "acceleration", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 11.519, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.5728401, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 3, "text_snippet": " on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30 .85×the time-t"}, {"rank": 2, "score": 0.54903793, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 58, "text_snippet": "ning task. We observe a performance regression as the compression rate increases; however, even at a compression rate of32, our model remains competitive (as shown in table 1). In contrast, a compression rate of64appears to be overly aggres"}, {"rank": 3, "score": 0.5378163, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 96, "text_snippet": "ong document summarization, demonstrate thatREFRAG achieves up to30 .85×TTFT acceleration (3 .75×over previous state-of-the-art methods) without any loss in perplexity or downstream accuracy. Our results highlight the importance of speciali"}, {"rank": 4, "score": 0.5359184, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 95, "text_snippet": "oducedREFRAG, a novel and efficient decoding framework tailored for RAG applications. By leveraging the inherent sparsity and block-diagonal attention patterns present in RAG contexts,REFRAG compresses, senses, and expands context represent"}, {"rank": 5, "score": 0.5321513, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 177, "text_snippet": "re 11 shows the performance of CPT with different combination of encoder and decoder models. Table 14 shows the performance on LLaMA-3.1-8B and LLaMA-3.2-3B model. Additional results in RAG.Table 16 shows the performance of different baseli"}]}
{"case_index": 34, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"In particular, GraphRAG is similar to other approaches that use [MASK] indexing to create summaries (similar to Kim et al.\"", "gold": "hierarchical", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.983, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.63555515, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 0, "text_snippet": "From Local to Global: A GraphRAG Approach to Query-Focused Summarization Darren Edge1†Ha Trinh1†Newman Cheng2Joshua Bradley2Alex Chao3 Apurva Mody3Steven Truitt2Dasha Metropolitansky1Robert Osazuwa Ness1 Jonathan Larson1 1Microsoft Research"}, {"rank": 2, "score": 0.6257734, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 17, "text_snippet": " are later used to answer queries as in Mao et al. 2020). These summaries are generated in parallel and iteratively aggregated into global summaries, similar to prior techniques (Feng et al., 2023; Gao et al., 2023; Khattab et al., 2022; Sh"}, {"rank": 3, "score": 0.5908922, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 97, "text_snippet": "omprehensive- ness and diversity of answers, as well as favorable comparisons to a global but graph-free approach using map-reduce source text summarization. For situations requiring many global queries over the same dataset, summaries of r"}, {"rank": 4, "score": 0.58771193, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 96, "text_snippet": "tigate these downstream risks for questions of a global nature, which might otherwise be answered by samples of retrieved facts falsely presented as global summaries. 7 Conclusion We have presented GraphRAG, a RAG approach that combines kno"}, {"rank": 5, "score": 0.56326085, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 81, "text_snippet": "ng. Tuning element extraction prompts may help to retain more of these details in the GraphRAG index. Community summaries vs. source texts . When comparing community summaries to source texts using GraphRAG, community summaries generally pr"}]}
{"case_index": 35, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"[MASK] Models Augmenting language models with relevant information retrieved from various knowledge stores has shown to be effective in improving performance on various NLP tasks, including language mod- eling (Min et al., 2022;\"", "gold": "retrieval-augmented", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.25, "hit_rank": 4, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.983, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6314714, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 83, "text_snippet": "sser, S., and Leahy, C. The Pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027 , 2020.  REPLUG: Retrieval-Augmented Black-Box Language Models Goyal, A., Friesen, A., Banino, A., Weber, T., Ke, N. R"}, {"rank": 2, "score": 0.6262501, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 104, "text_snippet": "2a. Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V ., et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 , 2022b. Zhong, Z., Lei, T., and Chen"}, {"rank": 3, "score": 0.619205, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 0, "text_snippet": "REPLUG: Retrieval-Augmented Black-Box Language Models Weijia Shi,1 *Sewon Min,1Michihiro Yasunaga,2Minjoon Seo,3Rich James,4Mike Lewis,4 Luke Zettlemoyer1 4Wen-tau Yih4 Abstract We introduce REPLUG, a retrieval-augmented lan- guage modeling"}, {"rank": 4, "score": 0.60795844, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 15, "text_snippet": "anguage models makes this approach infeasible. To ad- dress the challenges posed by large language models, we investigate retrieval-augmentation in the black-box setting , where users only have access to the model predictions and cannot acc"}, {"rank": 5, "score": 0.6019522, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 71, "text_snippet": "a black box and augments it with a tuneable retrieval model. Our evaluation shows that REPLUG can be integrated with any existing language model to improve their performance  REPLUG: Retrieval-Augmented Black-Box Language Models on language"}]}
{"case_index": 36, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"Internal [MASK] of such models are not exposed and fine-tuning is not supported.\"", "gold": "representations", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 10.963, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.5850172, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 0, "text_snippet": "REPLUG: Retrieval-Augmented Black-Box Language Models Weijia Shi,1 *Sewon Min,1Michihiro Yasunaga,2Minjoon Seo,3Rich James,4Mike Lewis,4 Luke Zettlemoyer1 4Wen-tau Yih4 Abstract We introduce REPLUG, a retrieval-augmented lan- guage modeling"}, {"rank": 2, "score": 0.5786656, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 83, "text_snippet": "sser, S., and Leahy, C. The Pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027 , 2020.  REPLUG: Retrieval-Augmented Black-Box Language Models Goyal, A., Friesen, A., Banino, A., Weber, T., Ke, N. R"}, {"rank": 3, "score": 0.57155526, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 104, "text_snippet": "2a. Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V ., et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 , 2022b. Zhong, Z., Lei, T., and Chen"}, {"rank": 4, "score": 0.57109475, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 71, "text_snippet": "a black box and augments it with a tuneable retrieval model. Our evaluation shows that REPLUG can be integrated with any existing language model to improve their performance  REPLUG: Retrieval-Augmented Black-Box Language Models on language"}, {"rank": 5, "score": 0.5634551, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}]}
{"case_index": 37, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"This approach first uses one LLM to generate a diverse set of global [MASK] questions based on corpus-sp\"", "gold": "sensemaking", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.3333333333333333, "hit_rank": 3, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.265, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.5982974, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 2, "score": 0.586902, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 0, "text_snippet": "From Local to Global: A GraphRAG Approach to Query-Focused Summarization Darren Edge1†Ha Trinh1†Newman Cheng2Joshua Bradley2Alex Chao3 Apurva Mody3Steven Truitt2Dasha Metropolitansky1Robert Osazuwa Ness1 Jonathan Larson1 1Microsoft Research"}, {"rank": 3, "score": 0.57451594, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 11, "text_snippet": "swers are combined and used to generate a final global answer. The GraphRAG method and its ability to perform global sensemaking over an entire corpus form the main contribution of this work. To demonstrate this ability, we developed a nove"}, {"rank": 4, "score": 0.568221, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 29, "text_snippet": "lements (nodes, edges, covariates) that the LLM can summarize in parallel at both indexing time and query time. The “global answer” to a given query is produced using a final round of query-focused summarization over all community summaries"}, {"rank": 5, "score": 0.5670034, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 97, "text_snippet": "omprehensive- ness and diversity of answers, as well as favorable comparisons to a global but graph-free approach using map-reduce source text summarization. For situations requiring many global queries over the same dataset, summaries of r"}]}
{"case_index": 38, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"For clarity, we focus on a single turn of [MASK] and retrieval in this section.\"", "gold": "question", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 6.42, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.53840077, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 12, "text_snippet": "is information is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-a"}, {"rank": 2, "score": 0.5221302, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 2, "text_snippet": "ated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns th"}, {"rank": 3, "score": 0.5033767, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 95, "text_snippet": "oducedREFRAG, a novel and efficient decoding framework tailored for RAG applications. By leveraging the inherent sparsity and block-diagonal attention patterns present in RAG contexts,REFRAG compresses, senses, and expands context represent"}, {"rank": 4, "score": 0.5018167, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 5, "score": 0.50108844, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 0, "text_snippet": "REFRAG: Rethinking RAG based Decoding Xiaoqiang Lin1,2,∗,Aritra Ghosh1,Bryan Kian Hsiang Low2,Anshumali Shrivastava1,3,Vijai Mohan1 1Meta Superintelligence Labs,2National University of Singapore,3Rice University ∗Work done at Meta Large Lan"}]}
{"case_index": 39, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"This style of retrieval can be added to both encoder- decoder (Yu, 2022; Izacard et al., 2022b) and [MASK] models (Khandelwal et al., 2020; Borgeaud et al., 2022; Shi et al., 2022; Rubin et al., 2022).\"", "gold": "decoder-only", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 6.935, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.61212605, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 83, "text_snippet": "sser, S., and Leahy, C. The Pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027 , 2020.  REPLUG: Retrieval-Augmented Black-Box Language Models Goyal, A., Friesen, A., Banino, A., Weber, T., Ke, N. R"}, {"rank": 2, "score": 0.60418594, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 0, "text_snippet": "REPLUG: Retrieval-Augmented Black-Box Language Models Weijia Shi,1 *Sewon Min,1Michihiro Yasunaga,2Minjoon Seo,3Rich James,4Mike Lewis,4 Luke Zettlemoyer1 4Wen-tau Yih4 Abstract We introduce REPLUG, a retrieval-augmented lan- guage modeling"}, {"rank": 3, "score": 0.5829494, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 104, "text_snippet": "2a. Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V ., et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 , 2022b. Zhong, Z., Lei, T., and Chen"}, {"rank": 4, "score": 0.57612205, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 94, "text_snippet": "iedel, S. Question and an- swer test-train overlap in open-domain question answer- ing datasets. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume , pp. 1000–1008, 20"}, {"rank": 5, "score": 0.5748527, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}]}
{"case_index": 40, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"A more recent work (Luo et al., 2023) [MASK] an LM with a fixed number 2 Preprint.\"", "gold": "instruction-tunes", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 5.487, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.5766718, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 86, "text_snippet": "M. Dai, Orhan Firat, Melvin John- son, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica"}, {"rank": 2, "score": 0.57618624, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 0, "text_snippet": "Preprint. SELF-RAG: LEARNING TO RETRIEVE , GENERATE ,AND CRITIQUE THROUGH SELF-REFLECTION Akari Asai†, Zeqiu Wu†, Yizhong Wang†§, Avirup Sil‡, Hannaneh Hajishirzi†§ †University of Washington§Allen Institute for AI‡IBM Research AI {akari,zeq"}, {"rank": 3, "score": 0.5279181, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 83, "text_snippet": ".49 28.22 5 20.18 17.3728.2419.6518.7127.08 820.5217.60 28.17 16.87 18.05 25.36 10 19.67 17.41 27.62 15.72 17.42 23.60 6 Related Works Retrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi- tec"}, {"rank": 4, "score": 0.5278938, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 27, "text_snippet": "n (reasoning path consistency) and without retrieval. Recent work on LLM refinement (Dhuliawala et al., 2023; Madaan et al., 2023; Paul et al., 2023) prompts a model to generate task output, natural language feedback and refined task output"}, {"rank": 5, "score": 0.5240897, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 2, "text_snippet": "ew framework called Self-Reflective Retrieval-Augmented Gen- eration ( SELF-RAG)that enhances an LM’s quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passage"}]}
{"case_index": 41, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"This avoids the need to run [MASK] forward passes once the candidate set Yhas been generated.\"", "gold": "additional", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 5.592, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.589538, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 2, "score": 0.5804949, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 3, "score": 0.5103246, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 83, "text_snippet": ".49 28.22 5 20.18 17.3728.2419.6518.7127.08 820.5217.60 28.17 16.87 18.05 25.36 10 19.67 17.41 27.62 15.72 17.42 23.60 6 Related Works Retrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi- tec"}, {"rank": 4, "score": 0.50836325, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 37, "text_snippet": " To cast language modeling in the text-to-text framework, we consider a chunk of Nwords, and split this chunk in two sub-sequences of equal length N/2. Then, the ﬁrst sub-sequence is used as the query, and the second corresponds to the outp"}, {"rank": 5, "score": 0.4980687, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 189, "text_snippet": " knowledge- intensive nlp tasks. arXiv preprint arXiv:2005.11401 , 2020. 1, 7, 14 Xiang Lisa Li and Percy Liang. Preﬁx-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Co"}]}
{"case_index": 42, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"(2023) highlights a potential [MASK]: contiguous seg- mentation might not capture the complete semantic depth of the text.\"", "gold": "shortcoming", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.2, "hit_rank": 5, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.142, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.5617202, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 22, "text_snippet": " RAPTOR addresses the issue of semantic depth and connection in reading by building a recursive tree structure that balances broader thematic comprehension with granular details and which allows nodes to be grouped based on semantic sim- il"}, {"rank": 2, "score": 0.536539, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 3, "score": 0.5233966, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 4, "score": 0.5116953, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 0, "text_snippet": "Published as a conference paper at ICLR 2024 RAPTOR: R ECURSIVE ABSTRACTIVE PROCESSING FOR TREE-ORGANIZED RETRIEVAL Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D. Manning Stanford University psarthi@cs."}, {"rank": 5, "score": 0.49759012, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 18, "text_snippet": " ICLR 2024 Despite a diversity in methods, the retrieving components of models predominantly rely on stan- dard approaches, i.e., chunking corpora and encoding with BERT-based retrievers. Although this approach is widely adopted, Nair et al"}]}
{"case_index": 43, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"3 [MASK] To align the encoder and decoder, we follow the work of Yen et al.\"", "gold": "methodology", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 17.429, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.52893525, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 26, "text_snippet": "e detailed discussion 1REFRAGwithout cache means that we recompute the chunk embedding for the context and take this latency into account. 3  on empirical evaluation is in section A. 3 Methodology To align the encoder and decoder, we follow"}, {"rank": 2, "score": 0.5273004, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 12, "text_snippet": "is information is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-a"}, {"rank": 3, "score": 0.51496005, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 17, "text_snippet": "FRAG achieves better performance than LLaMA without incurring higher latency in the downstream applications. 2 Model Architecture We denote the decoder model as Mdecand the encoder model as Menc. Given an input with Ttokens x1, x2, . . . , "}, {"rank": 4, "score": 0.51178795, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 0, "text_snippet": "REFRAG: Rethinking RAG based Decoding Xiaoqiang Lin1,2,∗,Aritra Ghosh1,Bryan Kian Hsiang Low2,Anshumali Shrivastava1,3,Vijai Mohan1 1Meta Superintelligence Labs,2National University of Singapore,3Rice University ∗Work done at Meta Large Lan"}, {"rank": 5, "score": 0.5097213, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 28, "text_snippet": "decoding procedure as “Thorough Decoding.” For longer output sequences,|Y|can become large, requiring many forward passes. For more efﬁcient decoding, we can make a further approximation that pθ(y|x,zi)≈0whereywas not generated during beam "}]}
{"case_index": 44, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"with many retrieved passages being [MASK] and reused across multiple inferences.\"", "gold": "uninformative", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.422, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.56299746, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 11, "text_snippet": "with many retrieved passages being uninformative and reused across multiple inferences. Allocating memory/computation for all the tokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The ret"}, {"rank": 2, "score": 0.5533325, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 2, "text_snippet": "ated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns th"}, {"rank": 3, "score": 0.53555834, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 4, "score": 0.53493047, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 76, "text_snippet": "mber of retrieved passages for RAG under the strong retriever scenario. GenerationNQ FEVER TQA WebQA FreebaseQA GSM8K StrategyQA BoolQ↑(1/ # tokens) Short context with the same latency LLaMA FT+ 1 passage23.9662.04 9.64 37.3375.187.38 64.44"}, {"rank": 5, "score": 0.5337888, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 95, "text_snippet": "oducedREFRAG, a novel and efficient decoding framework tailored for RAG applications. By leveraging the inherent sparsity and block-diagonal attention patterns present in RAG contexts,REFRAG compresses, senses, and expands context represent"}]}
{"case_index": 45, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"Despite these being extractive tasks, we ﬁnd that [MASK] generation outperforms previous extractive approaches.\"", "gold": "unconstrained", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.2, "hit_rank": 5, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.179, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.5939506, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 2, "score": 0.59380364, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 3, "score": 0.5638832, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 83, "text_snippet": ".49 28.22 5 20.18 17.3728.2419.6518.7127.08 820.5217.60 28.17 16.87 18.05 25.36 10 19.67 17.41 27.62 15.72 17.42 23.60 6 Related Works Retrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi- tec"}, {"rank": 4, "score": 0.5456842, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 0, "text_snippet": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks Patrick Lewis†‡, Ethan Perez⋆, Aleksandra Piktus†, Fabio Petroni†, Vladimir Karpukhin†, Naman Goyal†, Heinrich Küttler†, Mike Lewis†, Wen-tau Yih†, Tim Rocktäschel†‡, Sebastia"}, {"rank": 5, "score": 0.53794485, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 13, "text_snippet": "sive tasks —tasks that humans could not reasonably be expected to perform without access to an external knowledge source. Our RAG models achieve state-of-the-art results on open Natural Questions [ 29], WebQuestions [ 3] and CuratedTrec [ 2"}]}
{"case_index": 46, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"2.5 Decoding At test time, [MASK] and RAG-Token require different ways to approximate arg maxyp(y|x).\"", "gold": "rag-sequence", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.3333333333333333, "hit_rank": 3, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.645, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.6059854, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 2, "score": 0.5983909, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 3, "score": 0.58188146, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 25, "text_snippet": "escent with Adam [ 28]. Updating the document encoder BERTdduring training is costly as it requires the document index to be periodically updated as REALM does during pre-training [ 20]. We do not ﬁnd this step necessary for strong performa"}, {"rank": 4, "score": 0.5502281, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 83, "text_snippet": ".49 28.22 5 20.18 17.3728.2419.6518.7127.08 820.5217.60 28.17 16.87 18.05 25.36 10 19.67 17.41 27.62 15.72 17.42 23.60 6 Related Works Retrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi- tec"}, {"rank": 5, "score": 0.54291993, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 235, "text_snippet": "ull results for the cyclic-permutation-de-biased Atlas-XXL can be found in Table 18. A.2 Question answering A.2.1 Training Details For question answering, similarly to the MMLU experiments, we format the input using the following template: "}]}
{"case_index": 47, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"Encoder Encoder Encoder Context Text Decoder-only Foundation Model Sequence [MASK] Light-weight Encoder Who is the President of USA?\"", "gold": "precomputable", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 5.507, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6440212, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 20, "text_snippet": " the 2  Donald Trump is  the President of  the United States . He assumed office  on January 20,  2025, making him the 47th  President of the  United States. Encoder Encoder Encoder  Context Text Decoder-only Foundation Model  Sequence  Pre"}, {"rank": 2, "score": 0.5679047, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 19, "text_snippet": "1 }. The encoder model then processes all the chunks to obtain a chunk embedding for each chunkc i=Menc(Ci). This chunk embedding is then projected with a projection layer ϕto match the size of the token embedding of the decoder model, ecnk"}, {"rank": 3, "score": 0.49749213, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 146, "text_snippet": "works (Bello et al., 2017) to constrain the action space. 5Unless specified, we use the pre-trained checkpoint. The reason of choosing this model is that existing baselines (Yen et al., 2024; Shi et al., 2024) adapts LLaMA-2-7B. If we use o"}, {"rank": 4, "score": 0.4908204, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 147, "text_snippet": "ecoder Tokenizer &  Embedding  Decoder Input Text Token Embedding  Chunk  Embedding  RL-trained chunk expansion policy  Reward = - Log(Perplexity)  Donald Trump  Answer Figure 5A demonstration of selective token compression. For all chunks,"}, {"rank": 5, "score": 0.451092, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 21, "text_snippet": "uery  Encoder Figure 1The main design ofREFRAG. The input context is chunked and processed by the light-weight encoder to produce chunk embeddings, which are precomputable for efficient reuse. A light-weight RL policy decide few chunks to e"}]}
{"case_index": 48, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"be trained using weak supervision in the form of [MASK] pairs (Karpukhin et al., 2020), or pretrained using a cloze task and ﬁnetuned end-to- end (Guu et al., 2020; Lee et al., 2019).\"", "gold": "question-answer", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.699, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.723627, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 14, "text_snippet": " be trained using weak supervision in the form of question-answer pairs (Karpukhin et al., 2020), or pretrained using a cloze task and ﬁnetuned end-to- end (Guu et al., 2020; Lee et al., 2019). Generative question answering was mostly consi"}, {"rank": 2, "score": 0.6731661, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}, {"rank": 3, "score": 0.6517424, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 86, "text_snippet": "models for open domain question an- swering. In Proceedings of the 16th Conference of the European Chapter of the Association for Compu- tational Linguistics: Main Volume , pp. 874–880, On- line, April 2021a. Association for Computational L"}, {"rank": 4, "score": 0.6470695, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 5, "score": 0.6466864, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 0, "text_snippet": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering Gautier Izacard1,2,3Edouard Grave1 1Facebook AI Research, Paris 2ENS, PSL University, Paris 3Inria, Paris gizacard|egrave@fb.com Abstract Generative mode"}]}
{"case_index": 49, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"As shown in Figure 1, our system, RAPTOR, clusters chunks of text, generates text summaries of those clusters, and then repeats, [MASK] a tree from the bottom up.\"", "gold": "generating", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 11.62, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6658707, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 8, "text_snippet": " indexing and retrieval system that uses a tree structure to capture both high-level and low-level details about a text. As shown in Figure 1, our system, RAPTOR, clusters chunks of text, generates text summaries of those clusters, and then"}, {"rank": 2, "score": 0.63712096, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 24, "text_snippet": "r text chunks, we employ a clustering algorithm. Once clustered, a Language Model is used to summarize the grouped texts. These summarized texts are then re-embedded, and the cycle of embedding, clustering, and summarization continues until"}, {"rank": 3, "score": 0.63184255, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 22, "text_snippet": " RAPTOR addresses the issue of semantic depth and connection in reading by building a recursive tree structure that balances broader thematic comprehension with granular details and which allows nodes to be grouped based on semantic sim- il"}, {"rank": 4, "score": 0.61688685, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 1, "text_snippet": "rom a retrieval corpus, limiting holistic under- standing of the overall document context. We introduce the novel approach of recursively embedding, clustering, and summarizing chunks of text, constructing a tree with differing levels of su"}, {"rank": 5, "score": 0.60391355, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 26, "text_snippet": "d tree method evaluates nodes collectively across all layers to find the most relevant ones. Clustering Algorithm Clustering plays a key role in building the RAPTOR tree, organizing text segments into cohesive groups. This step groups relat"}]}
{"case_index": 50, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"More precisely, each retrieved passage and its title are [MASK] with the question, and processed in- dependently from other passages by the encoder.\"", "gold": "concatenated", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 5.65, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.70297134, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}, {"rank": 2, "score": 0.67567366, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 21, "text_snippet": "Raffel et al., 2019; Lewis et al., 2019). The model takes as input the question, as well as the support passages, and generates the answer. More precisely, each retrieved passage and its title are concatenated with the question, and process"}, {"rank": 3, "score": 0.65820235, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 86, "text_snippet": "models for open domain question an- swering. In Proceedings of the 16th Conference of the European Chapter of the Association for Compu- tational Linguistics: Main Volume , pp. 874–880, On- line, April 2021a. Association for Computational L"}, {"rank": 4, "score": 0.64327186, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 5, "score": 0.6379312, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 0, "text_snippet": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering Gautier Izacard1,2,3Edouard Grave1 1Facebook AI Research, Paris 2ENS, PSL University, Paris 3Inria, Paris gizacard|egrave@fb.com Abstract Generative mode"}]}
{"case_index": 51, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"Building on that [MASK] and the advances in pretrain- ing of natural language processing models, Roberts et al.\"", "gold": "observation", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 8.347, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6319622, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 86, "text_snippet": "models for open domain question an- swering. In Proceedings of the 16th Conference of the European Chapter of the Association for Compu- tational Linguistics: Main Volume , pp. 874–880, On- line, April 2021a. Association for Computational L"}, {"rank": 2, "score": 0.6275807, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 0, "text_snippet": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering Gautier Izacard1,2,3Edouard Grave1 1Facebook AI Research, Paris 2ENS, PSL University, Paris 3Inria, Paris gizacard|egrave@fb.com Abstract Generative mode"}, {"rank": 3, "score": 0.6269449, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}, {"rank": 4, "score": 0.6258503, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 5, "score": 0.6079298, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 101, "text_snippet": "ov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906 , 2020. URL https://arxiv.org/abs/2004.04906 . [27] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, "}]}
{"case_index": 52, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"The retriever Raims to retrieve the top- [MASK] D={dr1, ..., d rk}that are relevant to the input Xfrom the corpus C.\"", "gold": "kdocuments", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 5.844, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6049369, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 25, "text_snippet": "rator G. The retriever Raims to retrieve the top- Kdocuments D={dr1, ..., d rk}that are relevant to the input Xfrom the corpus C. Based on the input Xand the retrieved results D, the generator Gis responsible for generating the output Y. Th"}, {"rank": 2, "score": 0.55056393, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 3, "score": 0.54996514, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 4, "score": 0.5230355, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 56, "text_snippet": " the reader and the retriever by leveraging the perplexity of the output generated by the reader. Sachan et al. (2021) and Lee et al. (2021a) both employ salient span masking to pre-train retrievers, leveraging the perplexity and attention "}, {"rank": 5, "score": 0.5190244, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 24, "text_snippet": " knowledge, this paper makes the first attempt to explore and design corrective strategies for RAG to improve its robustness of generation. 3 Task Formulation Following previous work (Lewis et al., 2020; Asai et al., 2024), given input Xand"}]}
{"case_index": 53, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"(2023) fine-tune both the retriever and LM on [MASK] datasets in two step\"", "gold": "instruction-tuning", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.2, "hit_rank": 5, "matched_by": "doc_id+chunk_id", "retrieval_ms": 5.667, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6072523, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 22, "text_snippet": " the retriever and LM on instruction-tuning datasets in two steps. While we also train our model on diverse instruction-following datasets, SELF-RAGenables retrieval on demand and selection of the best possible model output via fine-grained"}, {"rank": 2, "score": 0.580171, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 3, "score": 0.5537417, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 27, "text_snippet": "n (reasoning path consistency) and without retrieval. Recent work on LLM refinement (Dhuliawala et al., 2023; Madaan et al., 2023; Paul et al., 2023) prompts a model to generate task output, natural language feedback and refined task output"}, {"rank": 4, "score": 0.55115354, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 0, "text_snippet": "Preprint. SELF-RAG: LEARNING TO RETRIEVE , GENERATE ,AND CRITIQUE THROUGH SELF-REFLECTION Akari Asai†, Zeqiu Wu†, Yizhong Wang†§, Avirup Sil‡, Hannaneh Hajishirzi†§ †University of Washington§Allen Institute for AI‡IBM Research AI {akari,zeq"}, {"rank": 5, "score": 0.5508657, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 21, "text_snippet": " al., 2023a; Gao et al., 2023). We introduce a method to train an arbitrary LM to learn to use retrieval on-demand for diverse instruction-following queries and introduce controlled generation guided by reflections tokens to further improve"}]}
{"case_index": 54, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"We train an arbitrary LM in an end-to-end manner to learn to reflect on its own generation process given a task input by generating both task output and [MASK] special tokens (i.e., reflection tokens ).\"", "gold": "intermittent", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.277, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6429286, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 35, "text_snippet": "parallel and uses its own generated reflection tokens to enforce soft constraints (Section 3.3) or hard control (Algorithm 1) over the generated task output. For instance, in Figure 1 (right), the retrieved passages d1is selected at the fir"}, {"rank": 2, "score": 0.64284253, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 6, "text_snippet": "y trained to leverage and follow facts from provided passages. This work introduces Self-Reflective Retrieval-augmented Generation ( SELF-RAG)to improve an LLM’s generation quality, including its factual accuracy without hurting its versati"}, {"rank": 3, "score": 0.63343704, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 2, "text_snippet": "ew framework called Self-Reflective Retrieval-Augmented Gen- eration ( SELF-RAG)that enhances an LM’s quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passage"}, {"rank": 4, "score": 0.6325674, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 14, "text_snippet": "t with reflection tokens by unifying them as the next token prediction from the expanded model vocabulary. We train our generator LM on a diverse collection of text interleaved with reflection tokens and retrieved passages. Reflection token"}, {"rank": 5, "score": 0.6241996, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 48, "text_snippet": "gmented with reflection tokens Dgenusing the standard next token objective: max ME(x,y,r )∼DgenlogpM(y, r|x). (2) Unlike Ctraining (Eq. 1), Mlearns to predict the target output as well as the reflection tokens. During training, we mask out "}]}
{"case_index": 55, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"[MASK] Izacard & Grave (2021), we average these scores over all attention heads, layers, and tokens to obtain a score for each document.\"", "gold": "following", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 9.933, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.6302333, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 2, "score": 0.5642238, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 111, "text_snippet": " arXiv:2208.03299, 2022. https://arxiv.org/abs/ 2208.03299. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Edouard Grave, and Sebastian Riedel. Atlas: Few-shot learn"}, {"rank": 3, "score": 0.55787873, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 4, "score": 0.5476373, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 235, "text_snippet": "ull results for the cyclic-permutation-de-biased Atlas-XXL can be found in Table 18. A.2 Question answering A.2.1 Training Details For question answering, similarly to the MMLU experiments, we format the input using the following template: "}, {"rank": 5, "score": 0.5335158, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 83, "text_snippet": ".49 28.22 5 20.18 17.3728.2419.6518.7127.08 820.5217.60 28.17 16.87 18.05 25.36 10 19.67 17.41 27.62 15.72 17.42 23.60 6 Related Works Retrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi- tec"}]}
{"case_index": 56, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"For query x, we use Maximum Inner Product Search (MIPS) to ﬁnd the top-K [MASK] zi.\"", "gold": "documents", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 6.582, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.5989883, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 2, "score": 0.59509677, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 3, "score": 0.52996594, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 37, "text_snippet": " To cast language modeling in the text-to-text framework, we consider a chunk of Nwords, and split this chunk in two sub-sequences of equal length N/2. Then, the ﬁrst sub-sequence is used as the query, and the second corresponds to the outp"}, {"rank": 4, "score": 0.5288738, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 22, "text_snippet": "t prior probability pη(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [ 23]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index."}, {"rank": 5, "score": 0.52338153, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 98, "text_snippet": "hology.org/Q19-1026 . Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen tau Yih, Tim Rockt ¨aschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented gener"}]}
{"case_index": 57, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"This graph index spans nodes (e.g., entities), edges (e.g., [MASK]), and covariates (e.g., claims) that have been detected, extracted, and summarized by LLM prompts tailored to the domain of the dataset.\"", "gold": "relationships", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 7.028, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6193956, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 0, "text_snippet": "From Local to Global: A GraphRAG Approach to Query-Focused Summarization Darren Edge1†Ha Trinh1†Newman Cheng2Joshua Bradley2Alex Chao3 Apurva Mody3Steven Truitt2Dasha Metropolitansky1Robert Osazuwa Ness1 Jonathan Larson1 1Microsoft Research"}, {"rank": 2, "score": 0.5898241, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 97, "text_snippet": "omprehensive- ness and diversity of answers, as well as favorable comparisons to a global but graph-free approach using map-reduce source text summarization. For situations requiring many global queries over the same dataset, summaries of r"}, {"rank": 3, "score": 0.58395445, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 29, "text_snippet": "lements (nodes, edges, covariates) that the LLM can summarize in parallel at both indexing time and query time. The “global answer” to a given query is produced using a final round of query-focused summarization over all community summaries"}, {"rank": 4, "score": 0.5758647, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 27, "text_snippet": "ns (Wang et al., 2023a; Zheng et al., 2024). Some prior work proposes criteria for having LLMs quantify the quality of 3  Source Documents Text Chunkstext extraction and chunking Entities & Relationshipsdomain-tailored summarization Knowled"}, {"rank": 5, "score": 0.5521835, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}]}
{"case_index": 58, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"This [MASK] the need to host a critic model during training, reducing overhead.\"", "gold": "eliminates", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 6.647, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.6189275, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 15, "text_snippet": "st a critic model during training, reducing overhead. The critic model, in part, is supervised on a dataset of input, output, and corresponding reflection tokens collected by prompting a propriety LM (i.e., GPT-4; OpenAI 2023). While we dra"}, {"rank": 2, "score": 0.55970496, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 48, "text_snippet": "gmented with reflection tokens Dgenusing the standard next token objective: max ME(x,y,r )∼DgenlogpM(y, r|x). (2) Unlike Ctraining (Eq. 1), Mlearns to predict the target output as well as the reflection tokens. During training, we mask out "}, {"rank": 3, "score": 0.5580301, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 37, "text_snippet": "e quality of a given task output (Section 3.2.1). Using the critic model, we update the training corpus by inserting reflection tokens into task outputs offline. Subsequently, we train the final generator model ( M) using the conventional L"}, {"rank": 4, "score": 0.55453867, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 5, "score": 0.54554224, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 0, "text_snippet": "Preprint. SELF-RAG: LEARNING TO RETRIEVE , GENERATE ,AND CRITIQUE THROUGH SELF-REFLECTION Akari Asai†, Zeqiu Wu†, Yizhong Wang†§, Avirup Sil‡, Hannaneh Hajishirzi†§ †University of Washington§Allen Institute for AI‡IBM Research AI {akari,zeq"}]}
{"case_index": 59, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"This approach scales well with the number of retrieved passages, as the [MASK] keeps improving when retrieving up to one hundred passages.\"", "gold": "performance", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 6.282, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.685737, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}, {"rank": 2, "score": 0.65209556, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 3, "score": 0.6498663, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 0, "text_snippet": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering Gautier Izacard1,2,3Edouard Grave1 1Facebook AI Research, Paris 2ENS, PSL University, Paris 3Inria, Paris gizacard|egrave@fb.com Abstract Generative mode"}, {"rank": 4, "score": 0.6341245, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 86, "text_snippet": "models for open domain question an- swering. In Proceedings of the 16th Conference of the European Chapter of the Association for Compu- tational Linguistics: Main Volume , pp. 874–880, On- line, April 2021a. Association for Computational L"}, {"rank": 5, "score": 0.6317762, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 16, "text_snippet": "0) and Lewis et al. (2020) in- troduced retrieval augmented generative models for open domain question answering. Our approach differs from these works by how the generative model processes the retrieved passages. This al- lows to scale to "}]}
{"case_index": 60, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"For instance, REPLUG can improve Codex (175B) performance on MMLU by 4.5%, achieving compa- rable results to the 540B, [MASK] Flan-PaLM.\"", "gold": "instruction-finetuned", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 4.847, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6395521, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 51, "text_snippet": ", and the output probabilities are ensemble together. 1Code-Davinci-002Results Table 2 presents the results from the baselines, REPLUG, and REPLUG LSR on the MMLU dataset. We observe that both the REPLUG andREPLUG LSR improve the original C"}, {"rank": 2, "score": 0.61466444, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 0, "text_snippet": "REPLUG: Retrieval-Augmented Black-Box Language Models Weijia Shi,1 *Sewon Min,1Michihiro Yasunaga,2Minjoon Seo,3Rich James,4Mike Lewis,4 Luke Zettlemoyer1 4Wen-tau Yih4 Abstract We introduce REPLUG, a retrieval-augmented lan- guage modeling"}, {"rank": 3, "score": 0.6109324, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 83, "text_snippet": "sser, S., and Leahy, C. The Pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027 , 2020.  REPLUG: Retrieval-Augmented Black-Box Language Models Goyal, A., Friesen, A., Banino, A., Weber, T., Ke, N. R"}, {"rank": 4, "score": 0.6073092, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 44, "text_snippet": "uage modeling tasks. Fur- thermore, REPLUG LSR consistently performs better than REPLUG by a large margin. Specifically, REPLUG LSR results in 7.7% improvement over baselines compared to 4.7% improvement of REPLUG averaged over the 8 models"}, {"rank": 5, "score": 0.6072379, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 63, "text_snippet": " the performance of REPLUG andREPLUG LSR improved monotonically. How- ever, a small number of documents (e.g., 10) was sufficient to achieve large performance gains.  REPLUG: Retrieval-Augmented Black-Box Language Models Perplexity 14.0016."}]}
{"case_index": 61, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"RetrieveStep 1: Retrieve on demand Prompt + 11 of 50 state [MASK] Step 2: Generate segment in para\"", "gold": "namesrelevant", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 7.291, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.5470175, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 7, "text_snippet": "rmittent special tokens (i.e., reflection tokens ). Reflection tokens are categorized into retrieval andcritique tokens to indicate the need for retrieval and its generation quality respectively (Figure 1 right). In particular, given an inp"}, {"rank": 2, "score": 0.5359406, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 34, "text_snippet": "l predicts the next output segment, as it does in a standard LM. If retrieval is needed, the model generates: a critique token to evaluate the retrieved passage’s relevance, the next response segment, and a critique token to evaluate if the"}, {"rank": 3, "score": 0.5347135, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 174, "text_snippet": "rd example, while the generation is mostly correct, SELF-RAGpredicts Partially Support to the statement listing the name of the songs, as they were not explicitly mentioned. D F ULL LIST OF INSTRUCTIONS AND DEMONSTRATIONS FOR GPT-4 Here, we"}, {"rank": 4, "score": 0.5314348, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 27, "text_snippet": "n (reasoning path consistency) and without retrieval. Recent work on LLM refinement (Dhuliawala et al., 2023; Madaan et al., 2023; Paul et al., 2023) prompts a model to generate task output, natural language feedback and refined task output"}, {"rank": 5, "score": 0.53093266, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 8, "text_snippet": "on demand (Step 1). Subsequently, SELF-RAGconcurrently processes multiple retrieved passages, evaluating their relevance and then generating corresponding task outputs (Step 2). It then generates critique tokens to criticize its own output "}]}
{"case_index": 62, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"Advanced RAG Many advanced [MASK] have been developed from the original RAG in recent years (Zhang et al., 2024; Kim et al., 2024; Wa\"", "gold": "approaches", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.25, "hit_rank": 4, "matched_by": "doc_id+chunk_id", "retrieval_ms": 4.945, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.52194893, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 7, "text_snippet": "ieval-augmented generation (RAG) (Lewis et al., 2020). In this framework, the input to models is augmented by prepending relevant documents that are retrieved from an external knowledge corpus (Guu et al., 2020). While RAG serves as a pract"}, {"rank": 2, "score": 0.5219173, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 3, "score": 0.5139698, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 1, "text_snippet": "evitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Al- though retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heav"}, {"rank": 4, "score": 0.50756913, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 20, "text_snippet": "model that specializes in response generation. Despite this, the methods above usually ignore a question, what if the retrieval goes wrong? Since the purpose of introducing a retrieval is to secure that generative LMs can obtain relevant an"}, {"rank": 5, "score": 0.50487506, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 79, "text_snippet": "ges are provided as input (correct, incorrect and ambiguous content). All the data in the table only represents a rough estimate of the generation phase, the retrieval and data-processing stages are not included. anism, rather than solely f"}]}
{"case_index": 63, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"The examples show that a low-quality retriever is prone to introducing a [MASK] amount of irrelevant information, impeding the generators from acquiring accurate knowledge and potentially misleading them.\"", "gold": "substantial", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.545, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6361761, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 6, "text_snippet": " The examples show that a low-quality retriever is prone to introducing a substantial amount of irrelevant information, impeding the generators from acquiring accurate knowledge and potentially misleading them. the parametric knowledge they"}, {"rank": 2, "score": 0.56797254, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 3, "score": 0.567196, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 1, "text_snippet": "evitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Al- though retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heav"}, {"rank": 4, "score": 0.56443477, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 56, "text_snippet": " the reader and the retriever by leveraging the perplexity of the output generated by the reader. Sachan et al. (2021) and Lee et al. (2021a) both employ salient span masking to pre-train retrievers, leveraging the perplexity and attention "}, {"rank": 5, "score": 0.5631509, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 83, "text_snippet": " in Table 6. The findings indicate that the self-correction mecha- nism incurs only modest computational overheadwhile significantly enhancing performance, thereby validating its lightweight nature. 6 Conclusion & Limitation This paper stud"}]}
{"case_index": 64, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"In summary, our [MASK] in this paper are three-fold: 1) This paper studies the scenarios where the retriever returns inaccurate results and, to the best of our knowledge, makes the first attempt to design corrective strategies for RAG to improve its robustness.\"", "gold": "contributions", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 5.468, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.644688, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 15, "text_snippet": "hers to reproduce our results, we will publish all source code later. In summary, our contributions in this paper are three-fold: 1) This paper studies the scenarios where the retriever returns inaccurate results and, to the best of our kno"}, {"rank": 2, "score": 0.6416527, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 10, "text_snippet": " should not have been equally referred to and involved in RAG. On account of the above issues, this paper particularly studies the scenarios where the retriever returns inaccurate results. A method named Corrective Retrieval- Augmented Gene"}, {"rank": 3, "score": 0.6338638, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 83, "text_snippet": " in Table 6. The findings indicate that the self-correction mecha- nism incurs only modest computational overheadwhile significantly enhancing performance, thereby validating its lightweight nature. 6 Conclusion & Limitation This paper stud"}, {"rank": 4, "score": 0.597044, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 1, "text_snippet": "evitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Al- though retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heav"}, {"rank": 5, "score": 0.58461773, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 24, "text_snippet": " knowledge, this paper makes the first attempt to explore and design corrective strategies for RAG to improve its robustness of generation. 3 Task Formulation Following previous work (Lewis et al., 2020; Asai et al., 2024), given input Xand"}]}
{"case_index": 65, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"For example, in the case of question answering, the query [MASK] to the question and the model needs to generate the answer.\"", "gold": "corresponds", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 17.098, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.640007, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 2, "score": 0.621312, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 235, "text_snippet": "ull results for the cyclic-permutation-de-biased Atlas-XXL can be found in Table 18. A.2 Question answering A.2.1 Training Details For question answering, similarly to the MMLU experiments, we format the input using the following template: "}, {"rank": 3, "score": 0.5761744, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 83, "text_snippet": ".49 28.22 5 20.18 17.3728.2419.6518.7127.08 820.5217.60 28.17 16.87 18.05 25.36 10 19.67 17.41 27.62 15.72 17.42 23.60 6 Related Works Retrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi- tec"}, {"rank": 4, "score": 0.57508326, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 2, "text_snippet": "ings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including"}, {"rank": 5, "score": 0.568172, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 60, "text_snippet": "ut. Retrieval-Augmentation with Search Engines. Recently, diﬀerent works have proposed to train large language models to interact with a search engine, by generating text queries, and using the retrieved documents as additional context (Nak"}]}
{"case_index": 66, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"3 Score (M,yi,[sj;xi]) = log pM(y|[sj;xi]), log [MASK] assigned to target output accord\"", "gold": "likelihood", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 6.711, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.56230634, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 29, "text_snippet": "g pM(y|[sj;xi]), log likelihood assigned to target output according to LM Mwhen candidate sentence is prepended to the input. We consider the sentence with the highest log likelihood as a positive example pi(line 3). To construct negative e"}, {"rank": 2, "score": 0.5608294, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 3, "score": 0.5539645, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 123, "text_snippet": "//huggingface.co/facebook/contriever 10https://huggingface.co/facebook/contriever-msmarco 15  Input: Base LM M, Compressor encoder encθ, Training data {xi,Si,yi}T 1where xiis input, Si={sj}n 1 is a set of candidate sentences from the retrie"}, {"rank": 4, "score": 0.5527655, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 219, "text_snippet": "4.6 38.8 52.8 A Training details and additional results A.1 MMLU A.1.1 Training Details Featurization MMLU consists of multiple choice questions with four possible lexicalized answer options. We represent the input using the following templ"}, {"rank": 5, "score": 0.54779303, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 220, "text_snippet": "f the correct answer: [MASK_0] {correct answer option letter} This format closely matches the format of MLM pre-training objective, aiding few-shot learning. When training, we permute the order of the answer options, i.e. shuﬄing which answ"}]}
{"case_index": 67, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"We evaluate our approach on language modeling task and open domain question [MASK] task.\"", "gold": "answering", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 8.241, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.620968, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 2, "score": 0.59885657, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 0, "text_snippet": "RECOMP: I MPROVING RETRIEVAL -AUGMENTED LM S WITH COMPRESSION AND SELECTIVE AUGMENTATION Fangyuan Xu1, Weijia Shi2, Eunsol Choi1 Department of Computer Science 1The University of Texas at Austin 2University of Washington {fangyuan,eunsol }@"}, {"rank": 3, "score": 0.59335184, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 83, "text_snippet": ".49 28.22 5 20.18 17.3728.2419.6518.7127.08 820.5217.60 28.17 16.87 18.05 25.36 10 19.67 17.41 27.62 15.72 17.42 23.60 6 Related Works Retrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi- tec"}, {"rank": 4, "score": 0.58891106, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 135, "text_snippet": "arXiv preprint arXiv:2305.00633 , 2023. URL https://arxiv.org/abs/2305.00633 . Fangyuan Xu, Weijia Shi, and Eunsol Choi. Recomp: Improving retrieval-augmented lms with compression and selective augmentation, 2023. URL https://arxiv.org/abs/"}, {"rank": 5, "score": 0.5781373, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 82, "text_snippet": " and allowing the compression models to perform selective augmentation. Our experiments show that our compressors can improve the efficiency of retrieval augmented LMs significantly with minimal drop in performances. ACKNOWLEDGEMENT We than"}]}
{"case_index": 68, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"In this paper, we investigate whether few-shot learning requires models to store a large amount of information in their parameters, and if memorisation can be decoupled from [MASK].\"", "gold": "generalisation", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.385, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.61961526, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 5, "text_snippet": "ally, it is unclear to what extent eﬀective few-shot learning requires vast knowledge in the parameters of the model. In this paper, we investigate whether few-shot learning requires models to store a large amount of information in their pa"}, {"rank": 2, "score": 0.61707747, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 111, "text_snippet": " arXiv:2208.03299, 2022. https://arxiv.org/abs/ 2208.03299. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Edouard Grave, and Sebastian Riedel. Atlas: Few-shot learn"}, {"rank": 3, "score": 0.6031407, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 1, "text_snippet": " Abstract Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter count"}, {"rank": 4, "score": 0.5957279, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 5, "score": 0.59558856, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 8, "text_snippet": "ingFew-shotQuestion answering:Where is the Bermuda Triangle?Western part of the North Atlantic Ocean……Figure 1: We introduce Atlas, a retrieval-augmented language model that exhibits strong few-shot perfor- mance on knowledge tasks, and use"}]}
{"case_index": 69, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"3.1 P ROBLEM [MASK] AND OVERVIEW Formally, given input x, we train Mto sequentially generate textual outputs yconsisting of multiple segments y= [y1, .\"", "gold": "formalization", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.58, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.6467918, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 29, "text_snippet": "teness. In contrast, common RAG approaches retrieve passages indiscriminately, without ensuring complete support from cited sources. 3.1 P ROBLEM FORMALIZATION AND OVERVIEW Formally, given input x, we train Mto sequentially generate textual"}, {"rank": 2, "score": 0.6042248, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 48, "text_snippet": "gmented with reflection tokens Dgenusing the standard next token objective: max ME(x,y,r )∼DgenlogpM(y, r|x). (2) Unlike Ctraining (Eq. 1), Mlearns to predict the target output as well as the reflection tokens. During training, we mask out "}, {"rank": 3, "score": 0.5646318, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 32, "text_snippet": ", respectively. Algorithm 1 SELF-RAGInference Require: Generator LM M, Retriever R, Large-scale passage collections {d1, . . . , d N} 1:Input: input prompt xand preceding generation y<t,Output: next output segment yt 2:Mpredicts Retrieve gi"}, {"rank": 4, "score": 0.55080295, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 35, "text_snippet": "parallel and uses its own generated reflection tokens to enforce soft constraints (Section 3.3) or hard control (Algorithm 1) over the generated task output. For instance, in Figure 1 (right), the retrieved passages d1is selected at the fir"}, {"rank": 5, "score": 0.54469895, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 34, "text_snippet": "l predicts the next output segment, as it does in a standard LM. If retrieval is needed, the model generates: a critique token to evaluate the retrieved passage’s relevance, the next response segment, and a critique token to evaluate if the"}]}
{"case_index": 70, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"wledge raises [MASK] concerns about the model’s behavior and performance in scenarios where retrieval may fail or return inaccu- rate results (Shi et al., 2023).\"", "gold": "significant", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.803, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.59107244, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 8, "text_snippet": "wledge raises significant concerns about the model’s behavior and performance in scenarios where retrieval may fail or return inaccu- rate results (Shi et al., 2023). As Figure 1 shows that a low-quality retriever is prone to introducingarX"}, {"rank": 2, "score": 0.57920897, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 3, "score": 0.5623779, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 83, "text_snippet": " in Table 6. The findings indicate that the self-correction mecha- nism incurs only modest computational overheadwhile significantly enhancing performance, thereby validating its lightweight nature. 6 Conclusion & Limitation This paper stud"}, {"rank": 4, "score": 0.5580854, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 1, "text_snippet": "evitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Al- though retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heav"}, {"rank": 5, "score": 0.54651785, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 83, "text_snippet": ".49 28.22 5 20.18 17.3728.2419.6518.7127.08 820.5217.60 28.17 16.87 18.05 25.36 10 19.67 17.41 27.62 15.72 17.42 23.60 6 Related Works Retrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi- tec"}]}
{"case_index": 71, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"In contrast, we explore a setting where both parametric and [MASK] memory components are pre-trained and pre-loaded with extensive knowledge.\"", "gold": "non-parametric", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.131, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6068756, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 2, "score": 0.5924603, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 12, "text_snippet": "ck- augmented networks [ 25] and memory layers [ 30]. In contrast, we explore a setting where both parametric and non-parametric memory components are pre-trained and pre-loaded with extensive knowledge. Crucially, by using pre-trained acce"}, {"rank": 3, "score": 0.58232564, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 4, "score": 0.55918145, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 83, "text_snippet": ".49 28.22 5 20.18 17.3728.2419.6518.7127.08 820.5217.60 28.17 16.87 18.05 25.36 10 19.67 17.41 27.62 15.72 17.42 23.60 6 Related Works Retrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi- tec"}, {"rank": 5, "score": 0.54673696, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 0, "text_snippet": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks Patrick Lewis†‡, Ethan Perez⋆, Aleksandra Piktus†, Fabio Petroni†, Vladimir Karpukhin†, Naman Goyal†, Heinrich Küttler†, Mike Lewis†, Wen-tau Yih†, Tim Rocktäschel†‡, Sebastia"}]}
{"case_index": 72, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"However, it requires models containing billions of parameters, since all the [MASK] needs to be stored in the weights.\"", "gold": "information", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 1.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 5.934, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.64751786, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 0, "text_snippet": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering Gautier Izacard1,2,3Edouard Grave1 1Facebook AI Research, Paris 2ENS, PSL University, Paris 3Inria, Paris gizacard|egrave@fb.com Abstract Generative mode"}, {"rank": 2, "score": 0.64360154, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 3, "score": 0.63124985, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}, {"rank": 4, "score": 0.6200896, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 86, "text_snippet": "models for open domain question an- swering. In Proceedings of the 16th Conference of the European Chapter of the Association for Compu- tational Linguistics: Main Volume , pp. 874–880, On- line, April 2021a. Association for Computational L"}, {"rank": 5, "score": 0.6049335, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 235, "text_snippet": "ull results for the cyclic-permutation-de-biased Atlas-XXL can be found in Table 18. A.2 Question answering A.2.1 Training Details For question answering, similarly to the MMLU experiments, we format the input using the following template: "}]}
{"case_index": 73, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"When performing a task, from question answering to generating Wikipedia articles, our model starts by [MASK] the top-k relevant documents from a large corpus of\"", "gold": "retrieving", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 5.312, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.67400014, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 2, "score": 0.61505485, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 2, "text_snippet": "ings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including"}, {"rank": 3, "score": 0.60987854, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 4, "score": 0.5915165, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 235, "text_snippet": "ull results for the cyclic-permutation-de-biased Atlas-XXL can be found in Table 18. A.2 Question answering A.2.1 Training Details For question answering, similarly to the MMLU experiments, we format the input using the following template: "}, {"rank": 5, "score": 0.58856875, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 37, "text_snippet": " To cast language modeling in the text-to-text framework, we consider a chunk of Nwords, and split this chunk in two sub-sequences of equal length N/2. Then, the ﬁrst sub-sequence is used as the query, and the second corresponds to the outp"}]}
{"case_index": 74, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"o substantial improvements over a conventional RAG baseline for both the [MASK] and diversity of generated answers.\"", "gold": "comprehensiveness", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.3333333333333333, "hit_rank": 3, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.254, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6119648, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 97, "text_snippet": "omprehensive- ness and diversity of answers, as well as favorable comparisons to a global but graph-free approach using map-reduce source text summarization. For situations requiring many global queries over the same dataset, summaries of r"}, {"rank": 2, "score": 0.601303, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 0, "text_snippet": "From Local to Global: A GraphRAG Approach to Query-Focused Summarization Darren Edge1†Ha Trinh1†Newman Cheng2Joshua Bradley2Alex Chao3 Apurva Mody3Steven Truitt2Dasha Metropolitansky1Robert Osazuwa Ness1 Jonathan Larson1 1Microsoft Research"}, {"rank": 3, "score": 0.5975031, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 4, "text_snippet": "o substantial improvements over a conventional RAG baseline for both the comprehensiveness and diversity of generated answers. 1 Introduction Retrieval augmented generation (RAG) (Lewis et al., 2020) is an established approach to using LLMs"}, {"rank": 4, "score": 0.58419913, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 22, "text_snippet": "gly global summaries by using the LLM to create summaries spanning this community hierarchy. 2.3 Adaptive benchmarking for RAG Evaluation Many benchmark datasets for open-domain question answering exist, including HotPotQA (Yang et al., 201"}, {"rank": 5, "score": 0.5798169, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 96, "text_snippet": "tigate these downstream risks for questions of a global nature, which might otherwise be answered by samples of retrieved facts falsely presented as global summaries. 7 Conclusion We have presented GraphRAG, a RAG approach that combines kno"}]}
{"case_index": 75, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"[MASK] Large language models (LLMs) such as GPT-3 (Brown et al., 2020a) and Codex (Chen et al., 2021a), have demonstrated impressive performance on a wide range of language tasks.\"", "gold": "introduction", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.959, "llm_ms": 0.008, "top_contexts": [{"rank": 1, "score": 0.67025256, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 83, "text_snippet": "sser, S., and Leahy, C. The Pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027 , 2020.  REPLUG: Retrieval-Augmented Black-Box Language Models Goyal, A., Friesen, A., Banino, A., Weber, T., Ke, N. R"}, {"rank": 2, "score": 0.6545725, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 2, "text_snippet": "mproves the performance of GPT-3 (175B) on language modeling by 6.3%, as well as the perfor- mance of Codex on five-shot MMLU by 5.1%. 1. Introduction Large language models (LLMs) such as GPT-3 (Brown et al., 2020a) and Codex (Chen et al., "}, {"rank": 3, "score": 0.6439297, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 12, "text_snippet": "gnals, resulting in improved retrieval quality. •Evaluations on language modeling (§6), open-domain QA and MMLU demonstrate that REPLUG can im- prove the performance of various language models such as GPT, OPT and BLOOM, including very larg"}, {"rank": 4, "score": 0.62553096, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 45, "text_snippet": "t that covers exam questions from 57 tasks includ- ing mathematics, computer science, law, US history and etc. The 57 tasks are grouped into 4 categories: humani- ties, STEM, social sciences and other. Following Chung  REPLUG: Retrieval-Aug"}, {"rank": 5, "score": 0.62434506, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 0, "text_snippet": "REPLUG: Retrieval-Augmented Black-Box Language Models Weijia Shi,1 *Sewon Min,1Michihiro Yasunaga,2Minjoon Seo,3Rich James,4Mike Lewis,4 Luke Zettlemoyer1 4Wen-tau Yih4 Abstract We introduce REPLUG, a retrieval-augmented lan- guage modeling"}]}
{"case_index": 76, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"Reading extracted snippets from technical or [MASK] documents may lack important context making them difficult to read or even misleading.\"", "gold": "scientific", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 37.537, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.54408836, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 22, "text_snippet": " RAPTOR addresses the issue of semantic depth and connection in reading by building a recursive tree structure that balances broader thematic comprehension with granular details and which allows nodes to be grouped based on semantic sim- il"}, {"rank": 2, "score": 0.5321212, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 18, "text_snippet": " ICLR 2024 Despite a diversity in methods, the retrieving components of models predominantly rely on stan- dard approaches, i.e., chunking corpora and encoding with BERT-based retrievers. Although this approach is widely adopted, Nair et al"}, {"rank": 3, "score": 0.5219059, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 4, "score": 0.5106944, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 5, "score": 0.5051818, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 0, "text_snippet": "Published as a conference paper at ICLR 2024 RAPTOR: R ECURSIVE ABSTRACTIVE PROCESSING FOR TREE-ORGANIZED RETRIEVAL Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D. Manning Stanford University psarthi@cs."}]}
{"case_index": 77, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"1 [MASK] Large language models (LLMs) are impressive few-shot learners (Brown et al., 2020; Rae et al., 2021; Hoﬀmann et al., 2022; Chowdhery et al., 2022).\"", "gold": "introduction", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 9.951, "llm_ms": 0.008, "top_contexts": [{"rank": 1, "score": 0.64316666, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 111, "text_snippet": " arXiv:2208.03299, 2022. https://arxiv.org/abs/ 2208.03299. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Edouard Grave, and Sebastian Riedel. Atlas: Few-shot learn"}, {"rank": 2, "score": 0.6235647, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 110, "text_snippet": "s of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 874–880, Online, April 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.eacl-main.74. https://aclant"}, {"rank": 3, "score": 0.62013936, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 0, "text_snippet": "Atlas: Few-shot Learning with Retrieval Augmented Language Models Gautier Izacard∗ ∗,♦,♣,♥gizacard@fb.com Patrick Lewis∗,♦plewis@fb.com Maria Lomeli♦marialomeli@fb.com Lucas Hosseini♦hoss@fb.com Fabio Petroni♦fabiopetroni@fb.com Timo Schick"}, {"rank": 4, "score": 0.61401075, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 83, "text_snippet": ".49 28.22 5 20.18 17.3728.2419.6518.7127.08 820.5217.60 28.17 16.87 18.05 25.36 10 19.67 17.41 27.62 15.72 17.42 23.60 6 Related Works Retrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi- tec"}, {"rank": 5, "score": 0.5966266, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}]}
{"case_index": 78, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"As shown in the following section, an advantage of dense retrievers is that both query and document encoders can be trained without document annotation, using standard techniques such as gradient descent and [MASK].\"", "gold": "distillation", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 8.73, "llm_ms": 0.01, "top_contexts": [{"rank": 1, "score": 0.6301877, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 2, "score": 0.6200196, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 56, "text_snippet": " the reader and the retriever by leveraging the perplexity of the output generated by the reader. Sachan et al. (2021) and Lee et al. (2021a) both employ salient span masking to pre-train retrievers, leveraging the perplexity and attention "}, {"rank": 3, "score": 0.6131534, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 2, "text_snippet": "ings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including"}, {"rank": 4, "score": 0.60406864, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 111, "text_snippet": " arXiv:2208.03299, 2022. https://arxiv.org/abs/ 2208.03299. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Edouard Grave, and Sebastian Riedel. Atlas: Few-shot learn"}, {"rank": 5, "score": 0.58318675, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}]}
{"case_index": 79, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"In canonical RAG approaches, the retrieval process returns a set number of records that are seman- tically similar to the query and the generated answer uses only the [MASK] in those retrieved records.\"", "gold": "information", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 1.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.748, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.62890416, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 0, "text_snippet": "From Local to Global: A GraphRAG Approach to Query-Focused Summarization Darren Edge1†Ha Trinh1†Newman Cheng2Joshua Bradley2Alex Chao3 Apurva Mody3Steven Truitt2Dasha Metropolitansky1Robert Osazuwa Ness1 Jonathan Larson1 1Microsoft Research"}, {"rank": 2, "score": 0.6275254, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 15, "text_snippet": "e LLM’s context window. In canonical RAG approaches, the retrieval process returns a set number of records that are seman- tically similar to the query and the generated answer uses only the information in those retrieved records. A common "}, {"rank": 3, "score": 0.59453905, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 1, "text_snippet": "ibuted equally to this work Abstract The use of retrieval-augmented generation (RAG) to retrieve relevant informa- tion from an external knowledge source enables large language models (LLMs) to answer questions over private and/or previousl"}, {"rank": 4, "score": 0.5829928, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 97, "text_snippet": "omprehensive- ness and diversity of answers, as well as favorable comparisons to a global but graph-free approach using map-reduce source text summarization. For situations requiring many global queries over the same dataset, summaries of r"}, {"rank": 5, "score": 0.5699415, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 29, "text_snippet": "lements (nodes, edges, covariates) that the LLM can summarize in parallel at both indexing time and query time. The “global answer” to a given query is produced using a final round of query-focused summarization over all community summaries"}]}
{"case_index": 80, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"We introduce a method to train an arbitrary LM to learn to use retrieval on-demand for diverse [MASK] queries and introduce controlled generation guided by reflections tokens to further improve generation quality and attributions.\"", "gold": "instruction-following", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.3333333333333333, "hit_rank": 3, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.394, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6870033, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 6, "text_snippet": "y trained to leverage and follow facts from provided passages. This work introduces Self-Reflective Retrieval-augmented Generation ( SELF-RAG)to improve an LLM’s generation quality, including its factual accuracy without hurting its versati"}, {"rank": 2, "score": 0.68330777, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 2, "text_snippet": "ew framework called Self-Reflective Retrieval-Augmented Gen- eration ( SELF-RAG)that enhances an LM’s quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passage"}, {"rank": 3, "score": 0.678731, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 21, "text_snippet": " al., 2023a; Gao et al., 2023). We introduce a method to train an arbitrary LM to learn to use retrieval on-demand for diverse instruction-following queries and introduce controlled generation guided by reflections tokens to further improve"}, {"rank": 4, "score": 0.6642055, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 27, "text_snippet": "n (reasoning path consistency) and without retrieval. Recent work on LLM refinement (Dhuliawala et al., 2023; Madaan et al., 2023; Paul et al., 2023) prompts a model to generate task output, natural language feedback and refined task output"}, {"rank": 5, "score": 0.6413191, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 96, "text_snippet": "ains an LM to learn to retrieve, generate, and critique text passages and its own generation by predicting the next tokens from its original vocabulary as well as newly added special tokens, called reflection tokens. SELF-RAGfurther enables"}]}
{"case_index": 81, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"When applied to QA datasets, our best model compresses the documents to 5 - 10% of the original tokens with at most less than 10% relative [MASK] drop.\"", "gold": "performance", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.006, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.63001657, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 0, "text_snippet": "RECOMP: I MPROVING RETRIEVAL -AUGMENTED LM S WITH COMPRESSION AND SELECTIVE AUGMENTATION Fangyuan Xu1, Weijia Shi2, Eunsol Choi1 Department of Computer Science 1The University of Texas at Austin 2University of Washington {fangyuan,eunsol }@"}, {"rank": 2, "score": 0.61880344, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 15, "text_snippet": "6% and significantly outperforms prepending full documents. Our trained compressors also show promising results. For language modelling, both trained compressors achieve a compression ratio of 25% with minimal performance drop. When applied"}, {"rank": 3, "score": 0.6162396, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 82, "text_snippet": " and allowing the compression models to perform selective augmentation. Our experiments show that our compressors can improve the efficiency of retrieval augmented LMs significantly with minimal drop in performances. ACKNOWLEDGEMENT We than"}, {"rank": 4, "score": 0.60949636, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 5, "score": 0.5922439, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 57, "text_snippet": "ewer tokens. Comparing to prepending one document, we achieve a compression ratio of 25% at minimum performance drop. Our trained abstractive compressor performs the best across the board, achieving the lowest perplexity and the highest com"}]}
{"case_index": 82, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"Eventually, when it cannot [MASK] make a correct or incorrect judgment, a soft and balanced action Ambiguous which combines both of them is triggered.\"", "gold": "confidently", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.037, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.57072574, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 28, "text_snippet": "sition, filter, and recomposition (Section 4.4). If the action Incorrect is triggered, the retrieved documents will be discarded. Instead, web searches are resorted to and regarded as complementary knowledge sources for corrections (Section"}, {"rank": 2, "score": 0.5083566, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 1, "text_snippet": "evitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Al- though retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heav"}, {"rank": 3, "score": 0.50162584, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 83, "text_snippet": " in Table 6. The findings indicate that the self-correction mecha- nism incurs only modest computational overheadwhile significantly enhancing performance, thereby validating its lightweight nature. 6 Conclusion & Limitation This paper stud"}, {"rank": 4, "score": 0.49801266, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 42, "text_snippet": " fabricated facts. Therefore, we need to seek new sources of knowledge for correction. Here, web search is introduced to search from the Internet as elaborated in Section 4.5. This corrective action helps overcome the embarrassing challenge"}, {"rank": 5, "score": 0.4955982, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 39, "text_snippet": "ngly where the upper and lower thresholds are set. If the confidence score is higher than the upper threshold, the retrieved document is identified as Correct , while identified as Incorrect if below the lower threshold. Otherwise, a more s"}]}
{"case_index": 83, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"Some [MASK] use subgraphs, elements of the graph, or properties of the graph structure dire\"", "gold": "techniques", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 7.046, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.60733724, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 0, "text_snippet": "From Local to Global: A GraphRAG Approach to Query-Focused Summarization Darren Edge1†Ha Trinh1†Newman Cheng2Joshua Bradley2Alex Chao3 Apurva Mody3Steven Truitt2Dasha Metropolitansky1Robert Osazuwa Ness1 Jonathan Larson1 1Microsoft Research"}, {"rank": 2, "score": 0.544832, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 97, "text_snippet": "omprehensive- ness and diversity of answers, as well as favorable comparisons to a global but graph-free approach using map-reduce source text summarization. For situations requiring many global queries over the same dataset, summaries of r"}, {"rank": 3, "score": 0.5417597, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 4, "score": 0.52639115, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 27, "text_snippet": "ns (Wang et al., 2023a; Zheng et al., 2024). Some prior work proposes criteria for having LLMs quantify the quality of 3  Source Documents Text Chunkstext extraction and chunking Entities & Relationshipsdomain-tailored summarization Knowled"}, {"rank": 5, "score": 0.52086675, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 96, "text_snippet": "tigate these downstream risks for questions of a global nature, which might otherwise be answered by samples of retrieved facts falsely presented as global summaries. 7 Conclusion We have presented GraphRAG, a RAG approach that combines kno"}]}
{"case_index": 84, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"In the following, we formally introduce both models and then describe the pηandpθ[MASK], as well as the training and\"", "gold": "components", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 13.815, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.59838295, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 2, "score": 0.5911019, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 3, "score": 0.5471688, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 37, "text_snippet": " To cast language modeling in the text-to-text framework, we consider a chunk of Nwords, and split this chunk in two sub-sequences of equal length N/2. Then, the ﬁrst sub-sequence is used as the query, and the second corresponds to the outp"}, {"rank": 4, "score": 0.54693234, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 83, "text_snippet": ".49 28.22 5 20.18 17.3728.2419.6518.7127.08 820.5217.60 28.17 16.87 18.05 25.36 10 19.67 17.41 27.62 15.72 17.42 23.60 6 Related Works Retrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi- tec"}, {"rank": 5, "score": 0.52980214, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 98, "text_snippet": "hology.org/Q19-1026 . Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen tau Yih, Tim Rockt ¨aschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented gener"}]}
{"case_index": 85, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"The training [MASK] is to minimize −logesim (xi,pi) esim (xi,pi)+P nj∈Niesim (xi,nj).\"", "gold": "objective", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.25, "hit_rank": 4, "matched_by": "doc_id+chunk_id", "retrieval_ms": 5.773, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.58038044, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 123, "text_snippet": "//huggingface.co/facebook/contriever 10https://huggingface.co/facebook/contriever-msmarco 15  Input: Base LM M, Compressor encoder encθ, Training data {xi,Si,yi}T 1where xiis input, Si={sj}n 1 is a set of candidate sentences from the retrie"}, {"rank": 2, "score": 0.56888264, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 3, "score": 0.55479515, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 0, "text_snippet": "RECOMP: I MPROVING RETRIEVAL -AUGMENTED LM S WITH COMPRESSION AND SELECTIVE AUGMENTATION Fangyuan Xu1, Weijia Shi2, Eunsol Choi1 Department of Computer Science 1The University of Texas at Austin 2University of Washington {fangyuan,eunsol }@"}, {"rank": 4, "score": 0.5495262, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 30, "text_snippet": "ilarly, but scoring will evaluate whether the LM will generate the correct answer with summary prepended (change in line 6). Pseudo code for the QA tasks is in Figure 6 the Appendix. We train our encoder with a contrastive loss (Karpukhin e"}, {"rank": 5, "score": 0.54625994, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 26, "text_snippet": " 7: L ← L ∪ si 8: if|L|>0then 9: Ni←argTop5 sj∈L(⟨encθ(sj),encθ(xi)⟩) 10: T ← T ∪ { (xi,pi,Ni)} 11:encθ=Finetune (encθ,T) Figure 2: Learning an extractive compressor for lan- guage modeling task.Model We train a dual-encoder model encθwhich"}]}
{"case_index": 86, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"rsively clusters chunks of text based on their vector embeddings and generates text summaries of those clusters, [MASK] a tree from the bottom up.\"", "gold": "constructing", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.2, "hit_rank": 5, "matched_by": "doc_id+chunk_id", "retrieval_ms": 5.663, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.62966084, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 8, "text_snippet": " indexing and retrieval system that uses a tree structure to capture both high-level and low-level details about a text. As shown in Figure 1, our system, RAPTOR, clusters chunks of text, generates text summaries of those clusters, and then"}, {"rank": 2, "score": 0.6185919, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 9, "text_snippet": "uestions at different levels. 1arXiv:2401.18059v1 [cs.CL] 31 Jan 2024  Published as a conference paper at ICLR 2024 2 3 4 5  1 1  2  3  3  4 5  5 6  8  7 Index #8  Text: summary of  nodes 2 and 3  Child Nodes: 2, 3  Text Embedding  Text chu"}, {"rank": 3, "score": 0.61155206, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 24, "text_snippet": "r text chunks, we employ a clustering algorithm. Once clustered, a Language Model is used to summarize the grouped texts. These summarized texts are then re-embedded, and the cycle of embedding, clustering, and summarization continues until"}, {"rank": 4, "score": 0.6058029, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 1, "text_snippet": "rom a retrieval corpus, limiting holistic under- standing of the overall document context. We introduce the novel approach of recursively embedding, clustering, and summarizing chunks of text, constructing a tree with differing levels of su"}, {"rank": 5, "score": 0.59592474, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 10, "text_snippet": "rsively clusters chunks of text based on their vector embeddings and generates text summaries of those clusters, constructing a tree from the bottom up. Nodes clustered together are siblings; a parent node contains the text summary of that "}]}
{"case_index": 87, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"However, they are also prone to [MASK] and cannot represent the full long tail of knowledge from the training corpus.\"", "gold": "hallucination", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 14.047, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.5696281, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 2, "score": 0.56288564, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 94, "text_snippet": "iedel, S. Question and an- swer test-train overlap in open-domain question answer- ing datasets. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume , pp. 1000–1008, 20"}, {"rank": 3, "score": 0.5584643, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 104, "text_snippet": "2a. Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V ., et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 , 2022b. Zhong, Z., Lei, T., and Chen"}, {"rank": 4, "score": 0.5561905, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 83, "text_snippet": "sser, S., and Leahy, C. The Pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027 , 2020.  REPLUG: Retrieval-Augmented Black-Box Language Models Goyal, A., Friesen, A., Banino, A., Weber, T., Ke, N. R"}, {"rank": 5, "score": 0.55297345, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 37, "text_snippet": " To cast language modeling in the text-to-text framework, we consider a chunk of Nwords, and split this chunk in two sub-sequences of equal length N/2. Then, the ﬁrst sub-sequence is used as the query, and the second corresponds to the outp"}]}
{"case_index": 88, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"This structure enables RAPTOR to load into an LLM’s context chunks [MASK] the text at different levels so that it can effectively and efficiently answer questions at different levels.\"", "gold": "representing", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 5.392, "llm_ms": 0.004, "top_contexts": [{"rank": 1, "score": 0.61641854, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 22, "text_snippet": " RAPTOR addresses the issue of semantic depth and connection in reading by building a recursive tree structure that balances broader thematic comprehension with granular details and which allows nodes to be grouped based on semantic sim- il"}, {"rank": 2, "score": 0.6151655, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 8, "text_snippet": " indexing and retrieval system that uses a tree structure to capture both high-level and low-level details about a text. As shown in Figure 1, our system, RAPTOR, clusters chunks of text, generates text summaries of those clusters, and then"}, {"rank": 3, "score": 0.5837256, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 67, "text_snippet": "ring approaches, which allows it to capture a range of information, from general themes to specific details, contributing to its overall strong performance. 4.1 C ONTRIBUTION OF THE TREE STRUCTURE We examine the contribution of each layer o"}, {"rank": 4, "score": 0.5792222, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 24, "text_snippet": "r text chunks, we employ a clustering algorithm. Once clustered, a Language Model is used to summarize the grouped texts. These summarized texts are then re-embedded, and the cycle of embedding, clustering, and summarization continues until"}, {"rank": 5, "score": 0.5668502, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 0, "text_snippet": "Published as a conference paper at ICLR 2024 RAPTOR: R ECURSIVE ABSTRACTIVE PROCESSING FOR TREE-ORGANIZED RETRIEVAL Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D. Manning Stanford University psarthi@cs."}]}
{"case_index": 89, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"[MASK] methods were pro- posed to tackle the setting where no gold spans are given to the system, but only the correct answer.\"", "gold": "different", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 4.473, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.62177646, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 2, "score": 0.6183924, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}, {"rank": 3, "score": 0.6179651, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 86, "text_snippet": "models for open domain question an- swering. In Proceedings of the 16th Conference of the European Chapter of the Association for Compu- tational Linguistics: Main Volume , pp. 874–880, On- line, April 2021a. Association for Computational L"}, {"rank": 4, "score": 0.60824186, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 0, "text_snippet": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering Gautier Izacard1,2,3Edouard Grave1 1Facebook AI Research, Paris 2ENS, PSL University, Paris 3Inria, Paris gizacard|egrave@fb.com Abstract Generative mode"}, {"rank": 5, "score": 0.60053533, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 235, "text_snippet": "ull results for the cyclic-permutation-de-biased Atlas-XXL can be found in Table 18. A.2 Question answering A.2.1 Training Details For question answering, similarly to the MMLU experiments, we format the input using the following template: "}]}
{"case_index": 90, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"Here, we bring hybrid parametric and [MASK] memory to the “workhorse of NLP,” i.e.\"", "gold": "non-parametric", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 5.173, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.57642066, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 2, "score": 0.5757915, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 3, "score": 0.5704945, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 9, "text_snippet": "y to the “workhorse of NLP,” i.e. sequence-to-sequence (seq2seq) models. We endow pre-trained, parametric-memory generation models with a non-parametric memory through a general-purpose ﬁne-tuning approach which we refer to as retrieval-aug"}, {"rank": 4, "score": 0.5531627, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 98, "text_snippet": "hology.org/Q19-1026 . Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen tau Yih, Tim Rockt ¨aschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented gener"}, {"rank": 5, "score": 0.5502753, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 0, "text_snippet": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks Patrick Lewis†‡, Ethan Perez⋆, Aleksandra Piktus†, Fabio Petroni†, Vladimir Karpukhin†, Naman Goyal†, Heinrich Küttler†, Mike Lewis†, Wen-tau Yih†, Tim Rocktäschel†‡, Sebastia"}]}
{"case_index": 91, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"1arXiv:2310.04408v1 [cs.CL] 6 Oct 2023 RECOMP during inference moved from Smyrna, [MASK], to Nissan's facility in Canton, M\"", "gold": "tennessee", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 5.815, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.57743627, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 7, "text_snippet": "rieved evidence documents, and guide RALM to generate desired outputs when prepended to the input. To satisfy both efficiency and effectiveness constraints, our compressor strategically performs selective augmentation by generating an empty"}, {"rank": 2, "score": 0.54598314, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 3, "score": 0.5192574, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 0, "text_snippet": "RECOMP: I MPROVING RETRIEVAL -AUGMENTED LM S WITH COMPRESSION AND SELECTIVE AUGMENTATION Fangyuan Xu1, Weijia Shi2, Eunsol Choi1 Department of Computer Science 1The University of Texas at Austin 2University of Washington {fangyuan,eunsol }@"}, {"rank": 4, "score": 0.502766, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 135, "text_snippet": "arXiv preprint arXiv:2305.00633 , 2023. URL https://arxiv.org/abs/2305.00633 . Fangyuan Xu, Weijia Shi, and Eunsol Choi. Recomp: Improving retrieval-augmented lms with compression and selective augmentation, 2023. URL https://arxiv.org/abs/"}, {"rank": 5, "score": 0.49792153, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 82, "text_snippet": " and allowing the compression models to perform selective augmentation. Our experiments show that our compressors can improve the efficiency of retrieval augmented LMs significantly with minimal drop in performances. ACKNOWLEDGEMENT We than"}]}
{"case_index": 92, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"1 [MASK] Recently, several works have shown that factual information can be extracted from large scale language models trained on vast quantities of data (Radford et al., 2019; Petroni et al., 2019; Jiang et al., 2019; Talmor et al., 2019).\"", "gold": "introduction", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 9.763, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.69042456, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 86, "text_snippet": "models for open domain question an- swering. In Proceedings of the 16th Conference of the European Chapter of the Association for Compu- tational Linguistics: Main Volume , pp. 874–880, On- line, April 2021a. Association for Computational L"}, {"rank": 2, "score": 0.68647397, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 0, "text_snippet": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering Gautier Izacard1,2,3Edouard Grave1 1Facebook AI Research, Paris 2ENS, PSL University, Paris 3Inria, Paris gizacard|egrave@fb.com Abstract Generative mode"}, {"rank": 3, "score": 0.6682613, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 4, "score": 0.6677174, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}, {"rank": 5, "score": 0.6599762, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 109, "text_snippet": "ng. InInternational conference on machine learning, pages 3929–3938. PMLR, 2020. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. InPr"}]}
{"case_index": 93, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"2 Extractive Compressor Given [MASK] [s1,s2...sn]in the input document set ( [d1, d2, ...dN]), we train a dual encoder model encθwhich embeds sentence siand the input sequence xint\"", "gold": "nsentences", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.25, "hit_rank": 4, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.076, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.67445743, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 123, "text_snippet": "//huggingface.co/facebook/contriever 10https://huggingface.co/facebook/contriever-msmarco 15  Input: Base LM M, Compressor encoder encθ, Training data {xi,Si,yi}T 1where xiis input, Si={sj}n 1 is a set of candidate sentences from the retrie"}, {"rank": 2, "score": 0.67259765, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 20, "text_snippet": "er model encθwhich embeds sentence siand the input sequence xinto fixed- dimensional embeddings respectively. Their inner product represents how helpful it would be for the LMMto prepend sito the input xto generate y. The final summary sfro"}, {"rank": 3, "score": 0.6689197, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 26, "text_snippet": " 7: L ← L ∪ si 8: if|L|>0then 9: Ni←argTop5 sj∈L(⟨encθ(sj),encθ(xi)⟩) 10: T ← T ∪ { (xi,pi,Ni)} 11:encθ=Finetune (encθ,T) Figure 2: Learning an extractive compressor for lan- guage modeling task.Model We train a dual-encoder model encθwhich"}, {"rank": 4, "score": 0.65337, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 19, "text_snippet": "ining compressors for conciseness and effectiveness. We summarize the key ideas for our two compressors, extractive compressors and abstractive compressor here, and discuss their training schemes formally in Section 3. 2Improving retriever "}, {"rank": 5, "score": 0.65199894, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 25, "text_snippet": "b; Ram et al., 2023). Input: Base LM M, Compressor encθ, Training data {xi,Si,yi}T 1where xiis input, Si={sj}n 1is a set of candidate sentences from the retrieved documents for xi, yiis the target answer, and score threshold ϵ. Output: An u"}]}
{"case_index": 94, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"How- ever, the increasing scale and black-box nature of large language models makes this approach [MASK].\"", "gold": "infeasible", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 5.876, "llm_ms": 0.004, "top_contexts": [{"rank": 1, "score": 0.61020744, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 83, "text_snippet": "sser, S., and Leahy, C. The Pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027 , 2020.  REPLUG: Retrieval-Augmented Black-Box Language Models Goyal, A., Friesen, A., Banino, A., Weber, T., Ke, N. R"}, {"rank": 2, "score": 0.60888284, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 0, "text_snippet": "REPLUG: Retrieval-Augmented Black-Box Language Models Weijia Shi,1 *Sewon Min,1Michihiro Yasunaga,2Minjoon Seo,3Rich James,4Mike Lewis,4 Luke Zettlemoyer1 4Wen-tau Yih4 Abstract We introduce REPLUG, a retrieval-augmented lan- guage modeling"}, {"rank": 3, "score": 0.6007266, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 71, "text_snippet": "a black box and augments it with a tuneable retrieval model. Our evaluation shows that REPLUG can be integrated with any existing language model to improve their performance  REPLUG: Retrieval-Augmented Black-Box Language Models on language"}, {"rank": 4, "score": 0.5831914, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 37, "text_snippet": " To cast language modeling in the text-to-text framework, we consider a chunk of Nwords, and split this chunk in two sub-sequences of equal length N/2. Then, the ﬁrst sub-sequence is used as the query, and the second corresponds to the outp"}, {"rank": 5, "score": 0.5816673, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 104, "text_snippet": "2a. Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V ., et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 , 2022b. Zhong, Z., Lei, T., and Chen"}]}
{"case_index": 95, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"Ideally, the retrieval likelihood is computed by [MASK] over all the documents in the corpus D, which is intractable in practice.\"", "gold": "marginalizing", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 7.079, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.61153793, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 7, "text_snippet": " retrieved documents are prepended to the input context and fed into the black-box LM to make the final prediction. Because the LM context length limits the number of documents that can be prepended, we also introduce a new ensemble scheme "}, {"rank": 2, "score": 0.6045303, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 83, "text_snippet": "sser, S., and Leahy, C. The Pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027 , 2020.  REPLUG: Retrieval-Augmented Black-Box Language Models Goyal, A., Friesen, A., Banino, A., Weber, T., Ke, N. R"}, {"rank": 3, "score": 0.59888023, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 20, "text_snippet": "n improve GPT- 3 performance on open-domain question answering, we approach the problem in a more general setting, including language modeling and understanding tasks. We also pro- pose an ensemble method to incorporate more documents  REPL"}, {"rank": 4, "score": 0.5968979, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 28, "text_snippet": "ethod of prepending all the retrieved docu-  REPLUG: Retrieval-Augmented Black-Box Language Models ments, our ensemble methods do not incur additional com- putational cost overhead. 4. R EPLUG LSR: Training the Dense Retriever Instead of re"}, {"rank": 5, "score": 0.58919036, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 50, "text_snippet": "odel, which we consider a white-box retrieval LM setting. Our model We add REPLUG andREPLUG LSR only to Codex because other models such as PaLM and Flan-PaLM are not accessible to the public. We use the test question as the query to retriev"}]}
{"case_index": 96, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"2020) augment the input of LLMs with relevant retrieved passages, reducing factual errors in [MASK] tasks (Ram et al., 2023; Asai et al., 2023a).\"", "gold": "knowledge-intensive", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 6.931, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6144427, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 27, "text_snippet": "n (reasoning path consistency) and without retrieval. Recent work on LLM refinement (Dhuliawala et al., 2023; Madaan et al., 2023; Paul et al., 2023) prompts a model to generate task output, natural language feedback and refined task output"}, {"rank": 2, "score": 0.61125374, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 23, "text_snippet": "ess retrieved passages before using them to prompt the LM to generate the output. SELF-RAGprocesses passages in parallel and filters out irrelevant ones through self-reflection, without relying on external models at inference. Moreover, our"}, {"rank": 3, "score": 0.60612345, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 6, "text_snippet": "y trained to leverage and follow facts from provided passages. This work introduces Self-Reflective Retrieval-augmented Generation ( SELF-RAG)to improve an LLM’s generation quality, including its factual accuracy without hurting its versati"}, {"rank": 4, "score": 0.6048014, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 0, "text_snippet": "Preprint. SELF-RAG: LEARNING TO RETRIEVE , GENERATE ,AND CRITIQUE THROUGH SELF-REFLECTION Akari Asai†, Zeqiu Wu†, Yizhong Wang†§, Avirup Sil‡, Hannaneh Hajishirzi†§ †University of Washington§Allen Institute for AI‡IBM Research AI {akari,zeq"}, {"rank": 5, "score": 0.598078, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 2, "text_snippet": "ew framework called Self-Reflective Retrieval-Augmented Gen- eration ( SELF-RAG)that enhances an LM’s quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passage"}]}
{"case_index": 97, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"1 I NTRODUCTION Large Language Models (LLMs) have emerged as [MASK] tools showing impressive perfor- mance on m\"", "gold": "transformative", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 5.434, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.5743725, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 2, "score": 0.57377636, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 2, "text_snippet": "hat retrieval with recursive summaries offers significant improvements over tra- ditional retrieval-augmented LMs on several tasks. On question-answering tasks that involve complex, multi-step reasoning, we show state-of-the-art results; fo"}, {"rank": 3, "score": 0.5639614, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 83, "text_snippet": ".49 28.22 5 20.18 17.3728.2419.6518.7127.08 820.5217.60 28.17 16.87 18.05 25.36 10 19.67 17.41 27.62 15.72 17.42 23.60 6 Related Works Retrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi- tec"}, {"rank": 4, "score": 0.5609288, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 104, "text_snippet": "uage Processing , pp. 6997–7008, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021. emnlp-main.560. URL https://aclanthology.org/2021.emnlp-main.560 . Sewon Min, Weiji"}, {"rank": 5, "score": 0.55778384, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 37, "text_snippet": " To cast language modeling in the text-to-text framework, we consider a chunk of Nwords, and split this chunk in two sub-sequences of equal length N/2. Then, the ﬁrst sub-sequence is used as the query, and the second corresponds to the outp"}]}
{"case_index": 98, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"Then, a [MASK] model generates the answer, taking as input the re- trieved passages in addition to the question.\"", "gold": "sequence-to-sequence", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 13.691, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6960494, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}, {"rank": 2, "score": 0.68686783, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 0, "text_snippet": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering Gautier Izacard1,2,3Edouard Grave1 1Facebook AI Research, Paris 2ENS, PSL University, Paris 3Inria, Paris gizacard|egrave@fb.com Abstract Generative mode"}, {"rank": 3, "score": 0.68309355, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 86, "text_snippet": "models for open domain question an- swering. In Proceedings of the 16th Conference of the European Chapter of the Association for Compu- tational Linguistics: Main Volume , pp. 874–880, On- line, April 2021a. Association for Computational L"}, {"rank": 4, "score": 0.66169715, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 5, "score": 0.63668734, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 16, "text_snippet": "0) and Lewis et al. (2020) in- troduced retrieval augmented generative models for open domain question answering. Our approach differs from these works by how the generative model processes the retrieved passages. This al- lows to scale to "}]}
{"case_index": 99, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"As shown in Figure 2, given an input context, REPLUG first [MASK] a small set of relevant documents from an external corpus using a retriever (§3.1).\"", "gold": "retrieves", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 5.487, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.644641, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 7, "text_snippet": " retrieved documents are prepended to the input context and fed into the black-box LM to make the final prediction. Because the LM context length limits the number of documents that can be prepended, we also introduce a new ensemble scheme "}, {"rank": 2, "score": 0.6406947, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 20, "text_snippet": "n improve GPT- 3 performance on open-domain question answering, we approach the problem in a more general setting, including language modeling and understanding tasks. We also pro- pose an ensemble method to incorporate more documents  REPL"}, {"rank": 3, "score": 0.6394299, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 21, "text_snippet": " (§3.1 Document Retrieval ). Then it prepends each document separately to the input context and ensembles output probabilities from different passes (§3.2 Input Reformulation ). and a training scheme to further adapt the retriever to large "}, {"rank": 4, "score": 0.6349623, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 0, "text_snippet": "REPLUG: Retrieval-Augmented Black-Box Language Models Weijia Shi,1 *Sewon Min,1Michihiro Yasunaga,2Minjoon Seo,3Rich James,4Mike Lewis,4 Luke Zettlemoyer1 4Wen-tau Yih4 Abstract We introduce REPLUG, a retrieval-augmented lan- guage modeling"}, {"rank": 5, "score": 0.6310983, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 71, "text_snippet": "a black box and augments it with a tuneable retrieval model. Our evaluation shows that REPLUG can be integrated with any existing language model to improve their performance  REPLUG: Retrieval-Augmented Black-Box Language Models on language"}]}
{"case_index": 100, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"model parameters), for both reducing LM perplexity and and im- proving in-context learning [MASK].\"", "gold": "performance", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.234, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.62201524, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 0, "text_snippet": "REPLUG: Retrieval-Augmented Black-Box Language Models Weijia Shi,1 *Sewon Min,1Michihiro Yasunaga,2Minjoon Seo,3Rich James,4Mike Lewis,4 Luke Zettlemoyer1 4Wen-tau Yih4 Abstract We introduce REPLUG, a retrieval-augmented lan- guage modeling"}, {"rank": 2, "score": 0.62177646, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 11, "text_snippet": " model parameters), for both reducing LM perplexity and and im- proving in-context learning performance. We summarize our contributions as follows: •We introduce REPLUG (§3), the first retrieval- augmented language modeling framework for en"}, {"rank": 3, "score": 0.5975045, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 7, "text_snippet": " retrieved documents are prepended to the input context and fed into the black-box LM to make the final prediction. Because the LM context length limits the number of documents that can be prepended, we also introduce a new ensemble scheme "}, {"rank": 4, "score": 0.5873177, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 5, "score": 0.58576477, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 71, "text_snippet": "a black box and augments it with a tuneable retrieval model. Our evaluation shows that REPLUG can be integrated with any existing language model to improve their performance  REPLUG: Retrieval-Augmented Black-Box Language Models on language"}]}
{"case_index": 101, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"In this work we address this gap, and present Atlas, a [MASK] language model capable of strong few-shot learning, despite having lower parameter counts\"", "gold": "retrieval-augmented", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 4.751, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6686675, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 111, "text_snippet": " arXiv:2208.03299, 2022. https://arxiv.org/abs/ 2208.03299. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Edouard Grave, and Sebastian Riedel. Atlas: Few-shot learn"}, {"rank": 2, "score": 0.65495974, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 8, "text_snippet": "ingFew-shotQuestion answering:Where is the Bermuda Triangle?Western part of the North Atlantic Ocean……Figure 1: We introduce Atlas, a retrieval-augmented language model that exhibits strong few-shot perfor- mance on knowledge tasks, and use"}, {"rank": 3, "score": 0.63946736, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 0, "text_snippet": "Atlas: Few-shot Learning with Retrieval Augmented Language Models Gautier Izacard∗ ∗,♦,♣,♥gizacard@fb.com Patrick Lewis∗,♦plewis@fb.com Maria Lomeli♦marialomeli@fb.com Lucas Hosseini♦hoss@fb.com Fabio Petroni♦fabiopetroni@fb.com Timo Schick"}, {"rank": 4, "score": 0.62903976, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 2, "text_snippet": "ings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including"}, {"rank": 5, "score": 0.62231445, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 154, "text_snippet": "odel. By jointly pre-training the retriever module and the language model, we show that Atlashas strong few-shot learning capabilities on a wide range of knowledge intensive tasks, including NaturalQuestions, TriviaQA, FEVER, 8 KILT tasks a"}]}
{"case_index": 102, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"(Wu et al., 2021), are not open-sourced due to commercial [MASK] and are only available as black-box APIs, through which users can send queries and receive responses.\"", "gold": "considerations", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.3333333333333333, "hit_rank": 3, "matched_by": "doc_id+chunk_id", "retrieval_ms": 4.673, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.5890769, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 83, "text_snippet": "sser, S., and Leahy, C. The Pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027 , 2020.  REPLUG: Retrieval-Augmented Black-Box Language Models Goyal, A., Friesen, A., Banino, A., Weber, T., Ke, N. R"}, {"rank": 2, "score": 0.57903636, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 0, "text_snippet": "REPLUG: Retrieval-Augmented Black-Box Language Models Weijia Shi,1 *Sewon Min,1Michihiro Yasunaga,2Minjoon Seo,3Rich James,4Mike Lewis,4 Luke Zettlemoyer1 4Wen-tau Yih4 Abstract We introduce REPLUG, a retrieval-augmented lan- guage modeling"}, {"rank": 3, "score": 0.57732487, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 13, "text_snippet": " (Wu et al., 2021), are not open-sourced due to commercial considerations and are only available as black-box APIs, through which users can send queries and receive responses. On the other hand, even open sourced language models such as OPT"}, {"rank": 4, "score": 0.57392883, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 125, "text_snippet": "ematical reasoning in open language models.arXiv preprint arXiv:2402.03300, 2024. Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Richard James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. REPLUG: Retrieval-augmented black-box la"}, {"rank": 5, "score": 0.56540394, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 94, "text_snippet": "iedel, S. Question and an- swer test-train overlap in open-domain question answer- ing datasets. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume , pp. 1000–1008, 20"}]}
{"case_index": 103, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"A common approach to [MASK] RAG is to use text embeddings, retrieving records closest to the query in vector space where closeness corresponds to semantic similarity (Gao et al., 2023).\"", "gold": "conventional", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.917, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.63716584, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 0, "text_snippet": "From Local to Global: A GraphRAG Approach to Query-Focused Summarization Darren Edge1†Ha Trinh1†Newman Cheng2Joshua Bradley2Alex Chao3 Apurva Mody3Steven Truitt2Dasha Metropolitansky1Robert Osazuwa Ness1 Jonathan Larson1 1Microsoft Research"}, {"rank": 2, "score": 0.6097325, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 15, "text_snippet": "e LLM’s context window. In canonical RAG approaches, the retrieval process returns a set number of records that are seman- tically similar to the query and the generated answer uses only the information in those retrieved records. A common "}, {"rank": 3, "score": 0.5867717, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 97, "text_snippet": "omprehensive- ness and diversity of answers, as well as favorable comparisons to a global but graph-free approach using map-reduce source text summarization. For situations requiring many global queries over the same dataset, summaries of r"}, {"rank": 4, "score": 0.5769338, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 17, "text_snippet": " are later used to answer queries as in Mao et al. 2020). These summaries are generated in parallel and iteratively aggregated into global summaries, similar to prior techniques (Feng et al., 2023; Gao et al., 2023; Khattab et al., 2022; Sh"}, {"rank": 5, "score": 0.5717833, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 16, "text_snippet": " may use alternative retrieval mechanisms, we collectively refer to the family of conventional approaches as vector RAG . GraphRAG contrasts with vector RAG in its ability to answer queries that require global sensemaking over the entire da"}]}
{"case_index": 104, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"It has obtained [MASK] results on a diverse set of generation tasks and outperforms comparably-sized T5 models [32].\"", "gold": "state-of-the-art", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 9.046, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6312603, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 2, "score": 0.5898687, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 3, "score": 0.58913845, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 83, "text_snippet": ".49 28.22 5 20.18 17.3728.2419.6518.7127.08 820.5217.60 28.17 16.87 18.05 25.36 10 19.67 17.41 27.62 15.72 17.42 23.60 6 Related Works Retrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi- tec"}, {"rank": 4, "score": 0.55297935, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 0, "text_snippet": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks Patrick Lewis†‡, Ethan Perez⋆, Aleksandra Piktus†, Fabio Petroni†, Vladimir Karpukhin†, Naman Goyal†, Heinrich Küttler†, Mike Lewis†, Wen-tau Yih†, Tim Rocktäschel†‡, Sebastia"}, {"rank": 5, "score": 0.5430666, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 100, "text_snippet": ", 6:317–328, 2018. URL https://arxiv. org/abs/1712.07040 . 12  Published as a conference paper at ICLR 2024 Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K ¨uttler, Mike Lewis, Wen-t"}]}
{"case_index": 105, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"[MASK], for each data data point, it contains s+o=Tnumber of tokens, which we use for CPT to prepare the model for downstream tasks utilizing chunk embeddings\"", "gold": "specifically", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.047, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.59042335, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 26, "text_snippet": "e detailed discussion 1REFRAGwithout cache means that we recompute the chunk embedding for the context and take this latency into account. 3  on empirical evaluation is in section A. 3 Methodology To align the encoder and decoder, we follow"}, {"rank": 2, "score": 0.57513094, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 27, "text_snippet": "repare the model for downstream tasks utilizing chunk embeddings. To further enhance performance, we introduce selective compression via RL. After aligning the encoder and decoder through CPT, we apply supervised fine-tuning (SFT) to adapt "}, {"rank": 3, "score": 0.5594343, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 147, "text_snippet": "ecoder Tokenizer &  Embedding  Decoder Input Text Token Embedding  Chunk  Embedding  RL-trained chunk expansion policy  Reward = - Log(Perplexity)  Donald Trump  Answer Figure 5A demonstration of selective token compression. For all chunks,"}, {"rank": 4, "score": 0.5580495, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 5, "score": 0.5434019, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}]}
{"case_index": 106, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"tion, one can use a subset of the corpus held out from [MASK] graph extraction and answer evaluation steps).\"", "gold": "subsequent", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 6.653, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.5980719, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 0, "text_snippet": "From Local to Global: A GraphRAG Approach to Query-Focused Summarization Darren Edge1†Ha Trinh1†Newman Cheng2Joshua Bradley2Alex Chao3 Apurva Mody3Steven Truitt2Dasha Metropolitansky1Robert Osazuwa Ness1 Jonathan Larson1 1Microsoft Research"}, {"rank": 2, "score": 0.5818577, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 3, "score": 0.5731324, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 4, "score": 0.55926466, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 97, "text_snippet": "omprehensive- ness and diversity of answers, as well as favorable comparisons to a global but graph-free approach using map-reduce source text summarization. For situations requiring many global queries over the same dataset, summaries of r"}, {"rank": 5, "score": 0.5463705, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 27, "text_snippet": "ns (Wang et al., 2023a; Zheng et al., 2024). Some prior work proposes criteria for having LLMs quantify the quality of 3  Source Documents Text Chunkstext extraction and chunking Entities & Relationshipsdomain-tailored summarization Knowled"}]}
{"case_index": 107, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"We summarize the key ideas for our two [MASK], extractive compressors and abstractive compressor here, and discuss their training schemes formally in Section 3.\"", "gold": "compressors", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 9.345, "llm_ms": 0.008, "top_contexts": [{"rank": 1, "score": 0.64957535, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 19, "text_snippet": "ining compressors for conciseness and effectiveness. We summarize the key ideas for our two compressors, extractive compressors and abstractive compressor here, and discuss their training schemes formally in Section 3. 2Improving retriever "}, {"rank": 2, "score": 0.6230463, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 23, "text_snippet": "odels in the output should be faithful to the original input, yet the main goal is different. Instead of capturing salient information for humans readers, compressors aim to produce a concise text that are useful for a LM on an end task. In"}, {"rank": 3, "score": 0.6136184, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 2, "text_snippet": "d an abstractive compressor which generates summaries by synthesizing information from multiple documents. Both compressors are trained to improve LMs’ performance on end tasks when the generated summaries are prepended to the LMs’ input, w"}, {"rank": 4, "score": 0.6104888, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 82, "text_snippet": " and allowing the compression models to perform selective augmentation. Our experiments show that our compressors can improve the efficiency of retrieval augmented LMs significantly with minimal drop in performances. ACKNOWLEDGEMENT We than"}, {"rank": 5, "score": 0.60778844, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 0, "text_snippet": "RECOMP: I MPROVING RETRIEVAL -AUGMENTED LM S WITH COMPRESSION AND SELECTIVE AUGMENTATION Fangyuan Xu1, Weijia Shi2, Eunsol Choi1 Department of Computer Science 1The University of Texas at Austin 2University of Washington {fangyuan,eunsol }@"}]}
{"case_index": 108, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"rieved evidence documents, and guide RALM to generate desired outputs when [MASK] to the input.\"", "gold": "prepended", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.368, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.6237228, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 7, "text_snippet": "rieved evidence documents, and guide RALM to generate desired outputs when prepended to the input. To satisfy both efficiency and effectiveness constraints, our compressor strategically performs selective augmentation by generating an empty"}, {"rank": 2, "score": 0.6231649, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 6, "text_snippet": "further confuse LMs with irrelevant information, degrading model performances (Mallen et al., 2022; Shi et al., 2023a). To overcome such limitations, we propose RECOMP (Retrieve, Com press, Prepend), an inter- mediate step for RALMs which c"}, {"rank": 3, "score": 0.59632814, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 0, "text_snippet": "RECOMP: I MPROVING RETRIEVAL -AUGMENTED LM S WITH COMPRESSION AND SELECTIVE AUGMENTATION Fangyuan Xu1, Weijia Shi2, Eunsol Choi1 Department of Computer Science 1The University of Texas at Austin 2University of Washington {fangyuan,eunsol }@"}, {"rank": 4, "score": 0.58524275, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 5, "score": 0.5446523, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 4, "text_snippet": "trieval-augmented language models (RALMs) (Khandelwal et al., 2019; Izacard et al., 2022; Lewis et al., 2020; Borgeaud et al., 2022) have shown impressive performance on knowledge-intensive tasks (Kwiatkowski et al., 2019; Petroni et al., 2"}]}
{"case_index": 109, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"(y) Question Answering: Answer [MASK] pη (Non-Parametric) z 4 z3 z2 z 1d(z) Jeopardy Question Generation: Answer QueryFigure 1: Overview of our approach.\"", "gold": "generationretriever", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.25, "hit_rank": 4, "matched_by": "doc_id+chunk_id", "retrieval_ms": 13.598, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6178288, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 2, "score": 0.6046838, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 3, "score": 0.5804412, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 235, "text_snippet": "ull results for the cyclic-permutation-de-biased Atlas-XXL can be found in Table 18. A.2 Question answering A.2.1 Training Details For question answering, similarly to the MMLU experiments, we format the input using the following template: "}, {"rank": 4, "score": 0.5789424, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 7, "text_snippet": " born in Hawaii. (x) Fact V eriﬁcation: Fact Querysupports (y) Question GenerationFact V eriﬁcation: Label GenerationDocument IndexDefine \"middle ear\" (x) Question Answering: Question QueryThe middle ear includes the tympanic cavity and the"}, {"rank": 5, "score": 0.5722905, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 37, "text_snippet": "(line 9). This allows for selective augmentation and mitigates the risk of prepending irrelevant documents. 5https://huggingface.co/facebook/contriever-msmarco 6The exact prompts can be found in Table 6 in A.2. 7We use gpt-3.5-turbo in all "}]}
{"case_index": 110, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"We use a training objective which prefers [MASK] documents that improve language model perplexity, while treating the LM as a frozen, black-box scoring function.\"", "gold": "retrieving", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.651, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6390178, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 9, "text_snippet": "22) that adapts language models to the retriever. We use a training objective which prefers retrieving documents that improve language model perplexity, while treating the LM as a frozen, black-box scoring function. Our experiments show tha"}, {"rank": 2, "score": 0.6190475, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 7, "text_snippet": " retrieved documents are prepended to the input context and fed into the black-box LM to make the final prediction. Because the LM context length limits the number of documents that can be prepended, we also introduce a new ensemble scheme "}, {"rank": 3, "score": 0.6117169, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 0, "text_snippet": "REPLUG: Retrieval-Augmented Black-Box Language Models Weijia Shi,1 *Sewon Min,1Michihiro Yasunaga,2Minjoon Seo,3Rich James,4Mike Lewis,4 Luke Zettlemoyer1 4Wen-tau Yih4 Abstract We introduce REPLUG, a retrieval-augmented lan- guage modeling"}, {"rank": 4, "score": 0.6061517, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 11, "text_snippet": " model parameters), for both reducing LM perplexity and and im- proving in-context learning performance. We summarize our contributions as follows: •We introduce REPLUG (§3), the first retrieval- augmented language modeling framework for en"}, {"rank": 5, "score": 0.5969143, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 37, "text_snippet": "vision LM to com- pute the LM likelihood. Training data We use 800K sequences of 256 tokens each, sampled from the Pile training data (Gao et al., 2020), as our training queries. Each query is split into two parts: the first 128 tokens are "}]}
{"case_index": 111, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"This preserves the [MASK] and semantic coherence of the text within each chunk.\"", "gold": "contextual", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 5.576, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.5332687, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 2, "score": 0.5116904, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 23, "text_snippet": "es. If a sentence exceeds the 100-token limit, we move the entire sentence to the next chunk, rather than cutting it mid-sentence. This preserves the contextual and semantic coherence of the text within each chunk. These texts are then embe"}, {"rank": 3, "score": 0.510173, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 22, "text_snippet": " RAPTOR addresses the issue of semantic depth and connection in reading by building a recursive tree structure that balances broader thematic comprehension with granular details and which allows nodes to be grouped based on semantic sim- il"}, {"rank": 4, "score": 0.49471706, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 5, "score": 0.48318166, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 8, "text_snippet": " indexing and retrieval system that uses a tree structure to capture both high-level and low-level details about a text. As shown in Figure 1, our system, RAPTOR, clusters chunks of text, generates text summaries of those clusters, and then"}]}
{"case_index": 112, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"retrieved documents are prepended to the input context and fed into the black-box LM to make the final [MASK].\"", "gold": "prediction", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 12.029, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.6471791, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 7, "text_snippet": " retrieved documents are prepended to the input context and fed into the black-box LM to make the final prediction. Because the LM context length limits the number of documents that can be prepended, we also introduce a new ensemble scheme "}, {"rank": 2, "score": 0.60160005, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 0, "text_snippet": "REPLUG: Retrieval-Augmented Black-Box Language Models Weijia Shi,1 *Sewon Min,1Michihiro Yasunaga,2Minjoon Seo,3Rich James,4Mike Lewis,4 Luke Zettlemoyer1 4Wen-tau Yih4 Abstract We introduce REPLUG, a retrieval-augmented lan- guage modeling"}, {"rank": 3, "score": 0.57063794, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 83, "text_snippet": "sser, S., and Leahy, C. The Pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027 , 2020.  REPLUG: Retrieval-Augmented Black-Box Language Models Goyal, A., Friesen, A., Banino, A., Weber, T., Ke, N. R"}, {"rank": 4, "score": 0.56910276, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 28, "text_snippet": "ethod of prepending all the retrieved docu-  REPLUG: Retrieval-Augmented Black-Box Language Models ments, our ensemble methods do not incur additional com- putational cost overhead. 4. R EPLUG LSR: Training the Dense Retriever Instead of re"}, {"rank": 5, "score": 0.56878364, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 21, "text_snippet": " (§3.1 Document Retrieval ). Then it prepends each document separately to the input context and ensembles output probabilities from different passes (§3.2 Input Reformulation ). and a training scheme to further adapt the retriever to large "}]}
{"case_index": 113, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"We rely on the Fusion-in-Decoder modiﬁcation of [MASK] models, and process each document independently in the encoder (Izacard & Grave, 2020).\"", "gold": "sequence-to-sequence", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.3333333333333333, "hit_rank": 3, "matched_by": "doc_id+chunk_id", "retrieval_ms": 9.719, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.6355878, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 2, "score": 0.5786318, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 111, "text_snippet": " arXiv:2208.03299, 2022. https://arxiv.org/abs/ 2208.03299. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Edouard Grave, and Sebastian Riedel. Atlas: Few-shot learn"}, {"rank": 3, "score": 0.5713044, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 21, "text_snippet": "ce architecture (Raﬀel et al., 2019). We rely on the Fusion-in-Decoder modiﬁcation of sequence-to-sequence models, and process each document independently in the encoder (Izacard & Grave, 2020). We then concatenate the outputs of the encode"}, {"rank": 4, "score": 0.56828123, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 9, "text_snippet": " strong few-shot learning, despite having lower parameter counts than other powerful recent few-shot learners. Atlasretrieves relevant documents based on the current context by using a general-purpose dense retriever using a dual-encoder ar"}, {"rank": 5, "score": 0.5644988, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}]}
{"case_index": 114, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"” Sensemaking tasks require reasoning over “ connections (which can be among people, places, and events) in order to anticipate their [MASK] and act effectively ” (Klein et al., 2006).\"", "gold": "trajectories", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 5.413, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.60330343, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 0, "text_snippet": "From Local to Global: A GraphRAG Approach to Query-Focused Summarization Darren Edge1†Ha Trinh1†Newman Cheng2Joshua Bradley2Alex Chao3 Apurva Mody3Steven Truitt2Dasha Metropolitansky1Robert Osazuwa Ness1 Jonathan Larson1 1Microsoft Research"}, {"rank": 2, "score": 0.55812573, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 29, "text_snippet": "lements (nodes, edges, covariates) that the LLM can summarize in parallel at both indexing time and query time. The “global answer” to a given query is produced using a final round of query-focused summarization over all community summaries"}, {"rank": 3, "score": 0.55074, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 8, "text_snippet": "nce analysis (Ranade and Joshi, 2023). Given a sensemaking query and a text with an implicit and interconnected set of concepts, an LLM can generate a summary that answers the query. The challenge, however, arises when the volume of data re"}, {"rank": 4, "score": 0.53436804, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 27, "text_snippet": "ns (Wang et al., 2023a; Zheng et al., 2024). Some prior work proposes criteria for having LLMs quantify the quality of 3  Source Documents Text Chunkstext extraction and chunking Entities & Relationshipsdomain-tailored summarization Knowled"}, {"rank": 5, "score": 0.5336547, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 97, "text_snippet": "omprehensive- ness and diversity of answers, as well as favorable comparisons to a global but graph-free approach using map-reduce source text summarization. For situations requiring many global queries over the same dataset, summaries of r"}]}
{"case_index": 115, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"To train the retriever and generator [MASK], we treat the retrieved document as a latent va\"", "gold": "end-to-end", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 6.234, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.60791856, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 2, "score": 0.59709346, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 57, "text_snippet": "nted generators using a second “informed” retriever with access to the output, which the test-time retriever can be distilled from, and Hofstätter et al. (2022) recently proposed a training set ﬁltering/weighting approach to train stronger "}, {"rank": 3, "score": 0.58625555, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 4, "score": 0.5806712, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 56, "text_snippet": " the reader and the retriever by leveraging the perplexity of the output generated by the reader. Sachan et al. (2021) and Lee et al. (2021a) both employ salient span masking to pre-train retrievers, leveraging the perplexity and attention "}, {"rank": 5, "score": 0.5780542, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 83, "text_snippet": ".49 28.22 5 20.18 17.3728.2419.6518.7127.08 820.5217.60 28.17 16.87 18.05 25.36 10 19.67 17.41 27.62 15.72 17.42 23.60 6 Related Works Retrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi- tec"}]}
{"case_index": 116, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"If the retrieved documents are irrelevant to the input or offer no additional information to LM, our compressor can return an empty string, [MASK] selective augmentation.\"", "gold": "implementing", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 8.648, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.65526664, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 0, "text_snippet": "RECOMP: I MPROVING RETRIEVAL -AUGMENTED LM S WITH COMPRESSION AND SELECTIVE AUGMENTATION Fangyuan Xu1, Weijia Shi2, Eunsol Choi1 Department of Computer Science 1The University of Texas at Austin 2University of Washington {fangyuan,eunsol }@"}, {"rank": 2, "score": 0.60526234, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 82, "text_snippet": " and allowing the compression models to perform selective augmentation. Our experiments show that our compressors can improve the efficiency of retrieval augmented LMs significantly with minimal drop in performances. ACKNOWLEDGEMENT We than"}, {"rank": 3, "score": 0.59673494, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 135, "text_snippet": "arXiv preprint arXiv:2305.00633 , 2023. URL https://arxiv.org/abs/2305.00633 . Fangyuan Xu, Weijia Shi, and Eunsol Choi. Recomp: Improving retrieval-augmented lms with compression and selective augmentation, 2023. URL https://arxiv.org/abs/"}, {"rank": 4, "score": 0.59582543, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 18, "text_snippet": "cy. If the retrieved documents do not contain relevant information or retrieval augmentation is not necessary, scan be an empty sequence. (2) Effecive : when sis prepended to input sequence xand provided to LM Mas a prompt, LM should genera"}, {"rank": 5, "score": 0.57505476, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 77, "text_snippet": "by compressing retrieved documents into a concise summary or an empty sequence, facilitating selective retrieval augmentation. Prompt Compression Recent work (Wingate et al., 2022; Chevalier et al., 2023; Mu et al., 2023) proposes compressi"}]}
{"case_index": 117, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"Although we do not have human annotations to train this model, prior work (Goyal et al., 2022; Chen et al., 2023; Potluri et al., 2023) suggests that the extreme- scale LMs can generate good [MASK] summaries when prompted carefully.\"", "gold": "query-focused", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 9.579, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.6038574, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 135, "text_snippet": "arXiv preprint arXiv:2305.00633 , 2023. URL https://arxiv.org/abs/2305.00633 . Fangyuan Xu, Weijia Shi, and Eunsol Choi. Recomp: Improving retrieval-augmented lms with compression and selective augmentation, 2023. URL https://arxiv.org/abs/"}, {"rank": 2, "score": 0.60153687, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 0, "text_snippet": "RECOMP: I MPROVING RETRIEVAL -AUGMENTED LM S WITH COMPRESSION AND SELECTIVE AUGMENTATION Fangyuan Xu1, Weijia Shi2, Eunsol Choi1 Department of Computer Science 1The University of Texas at Austin 2University of Washington {fangyuan,eunsol }@"}, {"rank": 3, "score": 0.5980785, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 4, "score": 0.5974605, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 6, "text_snippet": "further confuse LMs with irrelevant information, degrading model performances (Mallen et al., 2022; Shi et al., 2023a). To overcome such limitations, we propose RECOMP (Retrieve, Com press, Prepend), an inter- mediate step for RALMs which c"}, {"rank": 5, "score": 0.59425426, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 83, "text_snippet": ".49 28.22 5 20.18 17.3728.2419.6518.7127.08 820.5217.60 28.17 16.87 18.05 25.36 10 19.67 17.41 27.62 15.72 17.42 23.60 6 Related Works Retrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi- tec"}]}
{"case_index": 118, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"In this framework, the input to models is augmented by [MASK] relevant documents that are retrieved from an external knowledge corpus (Guu et al., 2020).\"", "gold": "prepending", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.256, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.5844331, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 7, "text_snippet": "ieval-augmented generation (RAG) (Lewis et al., 2020). In this framework, the input to models is augmented by prepending relevant documents that are retrieved from an external knowledge corpus (Guu et al., 2020). While RAG serves as a pract"}, {"rank": 2, "score": 0.5841604, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 83, "text_snippet": ".49 28.22 5 20.18 17.3728.2419.6518.7127.08 820.5217.60 28.17 16.87 18.05 25.36 10 19.67 17.41 27.62 15.72 17.42 23.60 6 Related Works Retrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi- tec"}, {"rank": 3, "score": 0.57114977, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 4, "score": 0.5638472, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 5, "score": 0.5500196, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 15, "text_snippet": " that answers do not correspond to spans in support documents, thus requiring ab- stractive models. Raffel et al. (2019) showed that generative models are competitive for reading com- prehension tasks such as SQuAD (Rajpurkar et al., 2016),"}]}
{"case_index": 119, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"The second approach, [MASK] , can predict each target token based on a different document.\"", "gold": "rag-token", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 8.926, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6109528, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 2, "score": 0.58396757, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 3, "score": 0.57102036, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 37, "text_snippet": " To cast language modeling in the text-to-text framework, we consider a chunk of Nwords, and split this chunk in two sub-sequences of equal length N/2. Then, the ﬁrst sub-sequence is used as the query, and the second corresponds to the outp"}, {"rank": 4, "score": 0.5450891, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 83, "text_snippet": ".49 28.22 5 20.18 17.3728.2419.6518.7127.08 820.5217.60 28.17 16.87 18.05 25.36 10 19.67 17.41 27.62 15.72 17.42 23.60 6 Related Works Retrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi- tec"}, {"rank": 5, "score": 0.52479684, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 56, "text_snippet": " the reader and the retriever by leveraging the perplexity of the output generated by the reader. Sachan et al. (2021) and Lee et al. (2021a) both employ salient span masking to pre-train retrievers, leveraging the perplexity and attention "}]}
{"case_index": 120, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"These projected chunk [MASK] are then fed to the decoder model along with the token embeddings for the question to generate the answer y∼ M dec({e1, .\"", "gold": "embeddings", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.25, "hit_rank": 4, "matched_by": "doc_id+chunk_id", "retrieval_ms": 36.607, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.6045237, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 18, "text_snippet": "f question and retrieval in this section. Model overview.Figure 1 shows the main architecture ofREFRAG. This model consists of a decoder-only foundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Rober"}, {"rank": 2, "score": 0.60000074, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 17, "text_snippet": "FRAG achieves better performance than LLaMA without incurring higher latency in the downstream applications. 2 Model Architecture We denote the decoder model as Mdecand the encoder model as Menc. Given an input with Ttokens x1, x2, . . . , "}, {"rank": 3, "score": 0.589362, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 4, "score": 0.5889317, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 19, "text_snippet": "1 }. The encoder model then processes all the chunks to obtain a chunk embedding for each chunkc i=Menc(Ci). This chunk embedding is then projected with a projection layer ϕto match the size of the token embedding of the decoder model, ecnk"}, {"rank": 5, "score": 0.5849511, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 21, "text_snippet": "uery  Encoder Figure 1The main design ofREFRAG. The input context is chunked and processed by the light-weight encoder to produce chunk embeddings, which are precomputable for efficient reuse. A light-weight RL policy decide few chunks to e"}]}
{"case_index": 121, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"[MASK] Generation (RAG) augments the input space of LMs with retrieved text passages (Guu et al., 2020; Lewis et al., 2020), lea\"", "gold": "retrieval-augmented", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 5.603, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6200361, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 27, "text_snippet": "n (reasoning path consistency) and without retrieval. Recent work on LLM refinement (Dhuliawala et al., 2023; Madaan et al., 2023; Paul et al., 2023) prompts a model to generate task output, natural language feedback and refined task output"}, {"rank": 2, "score": 0.61622965, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 2, "text_snippet": "ew framework called Self-Reflective Retrieval-Augmented Gen- eration ( SELF-RAG)that enhances an LM’s quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passage"}, {"rank": 3, "score": 0.6114205, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 96, "text_snippet": "ains an LM to learn to retrieve, generate, and critique text passages and its own generation by predicting the next tokens from its original vocabulary as well as newly added special tokens, called reflection tokens. SELF-RAGfurther enables"}, {"rank": 4, "score": 0.6066146, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 6, "text_snippet": "y trained to leverage and follow facts from provided passages. This work introduces Self-Reflective Retrieval-augmented Generation ( SELF-RAG)to improve an LLM’s generation quality, including its factual accuracy without hurting its versati"}, {"rank": 5, "score": 0.5805534, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 8, "text_snippet": "on demand (Step 1). Subsequently, SELF-RAGconcurrently processes multiple retrieved passages, evaluating their relevance and then generating corresponding task outputs (Step 2). It then generates critique tokens to criticize its own output "}]}
{"case_index": 122, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"While being a [MASK] problem in natural lan- guage processing (V oorhees et al., 199\"", "gold": "longstanding", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 10.077, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.6213271, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 86, "text_snippet": "models for open domain question an- swering. In Proceedings of the 16th Conference of the European Chapter of the Association for Compu- tational Linguistics: Main Volume , pp. 874–880, On- line, April 2021a. Association for Computational L"}, {"rank": 2, "score": 0.6134529, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 3, "score": 0.60496217, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 0, "text_snippet": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering Gautier Izacard1,2,3Edouard Grave1 1Facebook AI Research, Paris 2ENS, PSL University, Paris 3Inria, Paris gizacard|egrave@fb.com Abstract Generative mode"}, {"rank": 4, "score": 0.59939474, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}, {"rank": 5, "score": 0.58412707, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 10, "text_snippet": " problem in natural lan- guage processing (V oorhees et al., 1999), this task has recently regained interest following the work by Chen et al. (2017). In that version of the prob- lem, strong supervision is available to the learning system,"}]}
{"case_index": 123, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"These texts are then embedded using SBERT, a BERT-based encoder ( [MASK]1 ) (Reimers & Gurevych, 2019).\"", "gold": "multi-qa-mpnet-base-cos-v", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.772, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.5800453, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 2, "score": 0.57441545, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 23, "text_snippet": "es. If a sentence exceeds the 100-token limit, we move the entire sentence to the next chunk, rather than cutting it mid-sentence. This preserves the contextual and semantic coherence of the text within each chunk. These texts are then embe"}, {"rank": 3, "score": 0.55633414, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 4, "score": 0.54818726, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 25, "text_snippet": "escent with Adam [ 28]. Updating the document encoder BERTdduring training is costly as it requires the document index to be periodically updated as REALM does during pre-training [ 20]. We do not ﬁnd this step necessary for strong performa"}, {"rank": 5, "score": 0.5332395, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 22, "text_snippet": " RAPTOR addresses the issue of semantic depth and connection in reading by building a recursive tree structure that balances broader thematic comprehension with granular details and which allows nodes to be grouped based on semantic sim- il"}]}
{"case_index": 124, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"A confidence degree is [MASK] based on which different knowledge retrieval actions of { Correct , Incorrect ,Ambiguous } can be triggered.\"", "gold": "quantified", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 4.586, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.54987544, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 44, "text_snippet": "ce. Discussion Preliminary experiments of employ- ing only the Correct andIncorrect actions show that the efficacy of CRAG was easily affected by the accuracy of the retrieval evaluator. The reason might be the distinct knowledge switch for"}, {"rank": 2, "score": 0.54644316, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 28, "text_snippet": "sition, filter, and recomposition (Section 4.4). If the action Incorrect is triggered, the retrieved documents will be discarded. Instead, web searches are resorted to and regarded as complementary knowledge sources for corrections (Section"}, {"rank": 3, "score": 0.54092777, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 1, "text_snippet": "evitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Al- though retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heav"}, {"rank": 4, "score": 0.53784716, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 31, "text_snippet": "neratorkex+xkex+ Figure 2: An overview of the proposed CRAG at inference. A retrieval evaluator is constructed to evaluate the relevance of the retrieved documents to the input, and estimate a confidence degree based on which different know"}, {"rank": 5, "score": 0.5346651, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 83, "text_snippet": " in Table 6. The findings indicate that the self-correction mecha- nism incurs only modest computational overheadwhile significantly enhancing performance, thereby validating its lightweight nature. 6 Conclusion & Limitation This paper stud"}]}
{"case_index": 125, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x)via a top-K [MASK].\"", "gold": "approximation", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 7.593, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.64012, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 2, "score": 0.5769024, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 37, "text_snippet": " To cast language modeling in the text-to-text framework, we consider a chunk of Nwords, and split this chunk in two sub-sequences of equal length N/2. Then, the ﬁrst sub-sequence is used as the query, and the second corresponds to the outp"}, {"rank": 3, "score": 0.5752493, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 4, "score": 0.5548042, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 19, "text_snippet": "r produces the output sequence probability for each document, which are then marginalized, pRAG-Sequence (y|x)≈∑ z∈top-k(p(·|x))pη(z|x)pθ(y|x,z) =∑ z∈top-k(p(·|x))pη(z|x)N∏ ipθ(yi|x,z,y 1:i−1) RAG-Token Model In the RAG-Token model we can d"}, {"rank": 5, "score": 0.5504949, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 10, "text_snippet": "eural retriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The retriever (Dense Passage Retriever [ 26], henceforth DPR) provides latent documents conditioned on the input, and the seq2seq model (BART"}]}
{"case_index": 126, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"As shown inarXiv:2301.12652v4 [cs.CL] 24 May 2023 REPLUG: [MASK] Black-Box Language Models Figure 1, REPLUG is extremely flexible\"", "gold": "retrieval-augmented", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.25, "hit_rank": 4, "matched_by": "doc_id+chunk_id", "retrieval_ms": 5.41, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6522746, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 0, "text_snippet": "REPLUG: Retrieval-Augmented Black-Box Language Models Weijia Shi,1 *Sewon Min,1Michihiro Yasunaga,2Minjoon Seo,3Rich James,4Mike Lewis,4 Luke Zettlemoyer1 4Wen-tau Yih4 Abstract We introduce REPLUG, a retrieval-augmented lan- guage modeling"}, {"rank": 2, "score": 0.6466895, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 71, "text_snippet": "a black box and augments it with a tuneable retrieval model. Our evaluation shows that REPLUG can be integrated with any existing language model to improve their performance  REPLUG: Retrieval-Augmented Black-Box Language Models on language"}, {"rank": 3, "score": 0.6438322, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 83, "text_snippet": "sser, S., and Leahy, C. The Pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027 , 2020.  REPLUG: Retrieval-Augmented Black-Box Language Models Goyal, A., Friesen, A., Banino, A., Weber, T., Ke, N. R"}, {"rank": 4, "score": 0.63015103, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 7, "text_snippet": " retrieved documents are prepended to the input context and fed into the black-box LM to make the final prediction. Because the LM context length limits the number of documents that can be prepended, we also introduce a new ensemble scheme "}, {"rank": 5, "score": 0.617151, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 8, "text_snippet": "Black-Box Language Models Figure 1, REPLUG is extremely flexible and can be used with any existing black-box LM and retrieval model. We also introduce REPLUG LSR (REPLUG with LM- Supervised Retrieval), a training scheme that can further imp"}]}
{"case_index": 127, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"These reflection tokens (Table 1) signal the need for retrieval or confirm the output’s relevance, support, or [MASK].\"", "gold": "completeness", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 7.871, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.62069875, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 7, "text_snippet": "rmittent special tokens (i.e., reflection tokens ). Reflection tokens are categorized into retrieval andcritique tokens to indicate the need for retrieval and its generation quality respectively (Figure 1 right). In particular, given an inp"}, {"rank": 2, "score": 0.5699774, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 140, "text_snippet": " for GPT-4 21 16  Preprint. A S ELF-RAGDETAILS A.1 R EFLECTION TOKENS . Definitions of reflection tokens. Below, we provide a detailed definition of reflection type and output tokens. The first three aspects will be provided at each segment"}, {"rank": 3, "score": 0.5692092, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 35, "text_snippet": "parallel and uses its own generated reflection tokens to enforce soft constraints (Section 3.3) or hard control (Algorithm 1) over the generated task output. For instance, in Figure 1 (right), the retrieved passages d1is selected at the fir"}, {"rank": 4, "score": 0.56168836, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 27, "text_snippet": "n (reasoning path consistency) and without retrieval. Recent work on LLM refinement (Dhuliawala et al., 2023; Madaan et al., 2023; Paul et al., 2023) prompts a model to generate task output, natural language feedback and refined task output"}, {"rank": 5, "score": 0.55323064, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 48, "text_snippet": "gmented with reflection tokens Dgenusing the standard next token objective: max ME(x,y,r )∼DgenlogpM(y, r|x). (2) Unlike Ctraining (Eq. 1), Mlearns to predict the target output as well as the reflection tokens. During training, we mask out "}]}
{"case_index": 128, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"Born in Maida Vale, London…Where was Alan Turing born?[MASK]2seq modelMaida Vale, LondonFigure 1: A simple approach to open domain question answering.\"", "gold": "generativeseq", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.983, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.62834704, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 4, "text_snippet": "external source of knowledge, such as Wikipedia. Retrieval based approaches were previously con- sidered in the context of open domain question answering with extractive models (Chen et al., 2017). In that case, systems start by retrieving "}, {"rank": 2, "score": 0.6175769, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 86, "text_snippet": "models for open domain question an- swering. In Proceedings of the 16th Conference of the European Chapter of the Association for Compu- tational Linguistics: Main Volume , pp. 874–880, On- line, April 2021a. Association for Computational L"}, {"rank": 3, "score": 0.59507895, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 0, "text_snippet": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering Gautier Izacard1,2,3Edouard Grave1 1Facebook AI Research, Paris 2ENS, PSL University, Paris 3Inria, Paris gizacard|egrave@fb.com Abstract Generative mode"}, {"rank": 4, "score": 0.5908435, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 235, "text_snippet": "ull results for the cyclic-permutation-de-biased Atlas-XXL can be found in Table 18. A.2 Question answering A.2.1 Training Details For question answering, similarly to the MMLU experiments, we format the input using the following template: "}, {"rank": 5, "score": 0.57297355, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 101, "text_snippet": "ov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906 , 2020. URL https://arxiv.org/abs/2004.04906 . [27] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, "}]}
{"case_index": 129, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"Background and Related Work Black-box Language Models Large language models (i.e., >100B), such as GPT-3 (Brown et al., 2020a), Codex (Chen et al., 2021a), and Yuan 1.0 (Wu et al., 2021), are not [MASK] due to commercial consi\"", "gold": "open-sourced", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.25, "hit_rank": 4, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.957, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.6362852, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 83, "text_snippet": "sser, S., and Leahy, C. The Pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027 , 2020.  REPLUG: Retrieval-Augmented Black-Box Language Models Goyal, A., Friesen, A., Banino, A., Weber, T., Ke, N. R"}, {"rank": 2, "score": 0.60470796, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 104, "text_snippet": "2a. Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V ., et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 , 2022b. Zhong, Z., Lei, T., and Chen"}, {"rank": 3, "score": 0.5955019, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 0, "text_snippet": "REPLUG: Retrieval-Augmented Black-Box Language Models Weijia Shi,1 *Sewon Min,1Michihiro Yasunaga,2Minjoon Seo,3Rich James,4Mike Lewis,4 Luke Zettlemoyer1 4Wen-tau Yih4 Abstract We introduce REPLUG, a retrieval-augmented lan- guage modeling"}, {"rank": 4, "score": 0.5952163, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 12, "text_snippet": "gnals, resulting in improved retrieval quality. •Evaluations on language modeling (§6), open-domain QA and MMLU demonstrate that REPLUG can im- prove the performance of various language models such as GPT, OPT and BLOOM, including very larg"}, {"rank": 5, "score": 0.587994, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 20, "text_snippet": "n improve GPT- 3 performance on open-domain question answering, we approach the problem in a more general setting, including language modeling and understanding tasks. We also pro- pose an ensemble method to incorporate more documents  REPL"}]}
{"case_index": 130, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"Considering that retrieval is sometimes [MASK] for some queries, conversely, responses without retrieval are even more accurate in many situations.\"", "gold": "unnecessary", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 7.201, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.56273204, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 1, "text_snippet": "evitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Al- though retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heav"}, {"rank": 2, "score": 0.5543847, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 83, "text_snippet": " in Table 6. The findings indicate that the self-correction mecha- nism incurs only modest computational overheadwhile significantly enhancing performance, thereby validating its lightweight nature. 6 Conclusion & Limitation This paper stud"}, {"rank": 3, "score": 0.54551035, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 10, "text_snippet": " should not have been equally referred to and involved in RAG. On account of the above issues, this paper particularly studies the scenarios where the retriever returns inaccurate results. A method named Corrective Retrieval- Augmented Gene"}, {"rank": 4, "score": 0.5387838, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 43, "text_snippet": "of the retrieval is hard to distinguish and the evaluator gives an intermediate score. Since the retrieval evaluator is not confident in its judgment, both types of processed knowledge in Correct andIncorrect are combined to comple- ment ea"}, {"rank": 5, "score": 0.5355871, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 56, "text_snippet": " the reader and the retriever by leveraging the perplexity of the output generated by the reader. Sachan et al. (2021) and Lee et al. (2021a) both employ salient span masking to pre-train retrievers, leveraging the perplexity and attention "}]}
{"case_index": 131, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"For the retrieval of support passages, we consider two methods: BM25 ([MASK] et al., 1995) and DPR (Karpukhin et al., 2020).\"", "gold": "robertson", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 17.871, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.68743604, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}, {"rank": 2, "score": 0.6650088, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 19, "text_snippet": "den test set (right), competitions.codalab.org/competitions/17208#results ). Retrieval. For the retrieval of support passages, we consider two methods: BM25 (Robertson et al., 1995) and DPR (Karpukhin et al., 2020). In BM25, passages are re"}, {"rank": 3, "score": 0.65359545, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 86, "text_snippet": "models for open domain question an- swering. In Proceedings of the 16th Conference of the European Chapter of the Association for Compu- tational Linguistics: Main Volume , pp. 874–880, On- line, April 2021a. Association for Computational L"}, {"rank": 4, "score": 0.6460642, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 0, "text_snippet": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering Gautier Izacard1,2,3Edouard Grave1 1Facebook AI Research, Paris 2ENS, PSL University, Paris 3Inria, Paris gizacard|egrave@fb.com Abstract Generative mode"}, {"rank": 5, "score": 0.64012146, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}]}
{"case_index": 132, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"uestion answering systems (Chen et al., 2017; Yu et al., 2018), is to index large quantities of text, after splitting it into chunks (paragraphs), in a separate [MASK] retrieval system.\"", "gold": "information", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 1.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 5.877, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.6597161, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 5, "text_snippet": "uestion answering systems (Chen et al., 2017; Yu et al., 2018), is to index large quantities of text, after splitting it into chunks (paragraphs), in a separate information retrieval system. Retrieved information is then presented to the LL"}, {"rank": 2, "score": 0.59220576, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 3, "score": 0.5860116, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 8, "text_snippet": " indexing and retrieval system that uses a tree structure to capture both high-level and low-level details about a text. As shown in Figure 1, our system, RAPTOR, clusters chunks of text, generates text summaries of those clusters, and then"}, {"rank": 4, "score": 0.57708985, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 235, "text_snippet": "ull results for the cyclic-permutation-de-biased Atlas-XXL can be found in Table 18. A.2 Question answering A.2.1 Training Details For question answering, similarly to the MMLU experiments, we format the input using the following template: "}, {"rank": 5, "score": 0.5700224, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 12, "text_snippet": "018b) described a technique to aggregate answers from different paragraphs, using conﬁdence and coverage scores. Passage retrieval is an important step in open domain question answering, and is an active area of research to improve QA syste"}]}
{"case_index": 133, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"3 2.2 Training [MASK] for the retriever In this section, we discuss four diﬀerent loss functions to train the retriever jointly with the langu\"", "gold": "objectives", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.2, "hit_rank": 5, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.061, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.59821415, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 2, "score": 0.5861462, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 23, "text_snippet": "ent loss functions to train the retriever jointly with the language model. We consider loss functions that leverage the language model to provide supervisory signal to train the retriever. In other words, if the language model ﬁnds a docume"}, {"rank": 3, "score": 0.58328307, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 56, "text_snippet": " the reader and the retriever by leveraging the perplexity of the output generated by the reader. Sachan et al. (2021) and Lee et al. (2021a) both employ salient span masking to pre-train retrievers, leveraging the perplexity and attention "}, {"rank": 4, "score": 0.58275425, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 111, "text_snippet": " arXiv:2208.03299, 2022. https://arxiv.org/abs/ 2208.03299. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Edouard Grave, and Sebastian Riedel. Atlas: Few-shot learn"}, {"rank": 5, "score": 0.58238196, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 22, "text_snippet": "ther way to process the retrieved documents in the language model would be to concatenate the query and all the documents, and to use this long sequence as input of the model. Unfortunately, this approach does not scale with the number of d"}]}
{"case_index": 134, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"But a [MASK] portion of the text within these retrieved documents is often non- essential for generation, which should not have been equally referred to and involved in RAG.\"", "gold": "considerable", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.25, "hit_rank": 4, "matched_by": "doc_id+chunk_id", "retrieval_ms": 4.888, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.59570724, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 1, "text_snippet": "evitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Al- though retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heav"}, {"rank": 2, "score": 0.5722518, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 10, "text_snippet": " should not have been equally referred to and involved in RAG. On account of the above issues, this paper particularly studies the scenarios where the retriever returns inaccurate results. A method named Corrective Retrieval- Augmented Gene"}, {"rank": 3, "score": 0.5599678, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 83, "text_snippet": " in Table 6. The findings indicate that the self-correction mecha- nism incurs only modest computational overheadwhile significantly enhancing performance, thereby validating its lightweight nature. 6 Conclusion & Limitation This paper stud"}, {"rank": 4, "score": 0.5522268, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 9, "text_snippet": "nations (Zhang et al., 2023b). However, most conventional RAG ap- proaches indiscriminately incorporate the retrieved documents, regardless of whether these documents are relevant or not (Rony et al., 2022). Furthermore, current methods mos"}, {"rank": 5, "score": 0.55032134, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 79, "text_snippet": "ges are provided as input (correct, incorrect and ambiguous content). All the data in the table only represents a rough estimate of the generation phase, the retrieval and data-processing stages are not included. anism, rather than solely f"}]}
{"case_index": 135, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"Therefore, com- pared with the method of prepending all the retrieved docu- REPLUG: [MASK]\"", "gold": "retrieval-a", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 5.301, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.5901023, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 28, "text_snippet": "ethod of prepending all the retrieved docu-  REPLUG: Retrieval-Augmented Black-Box Language Models ments, our ensemble methods do not incur additional com- putational cost overhead. 4. R EPLUG LSR: Training the Dense Retriever Instead of re"}, {"rank": 2, "score": 0.57873374, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 0, "text_snippet": "REPLUG: Retrieval-Augmented Black-Box Language Models Weijia Shi,1 *Sewon Min,1Michihiro Yasunaga,2Minjoon Seo,3Rich James,4Mike Lewis,4 Luke Zettlemoyer1 4Wen-tau Yih4 Abstract We introduce REPLUG, a retrieval-augmented lan- guage modeling"}, {"rank": 3, "score": 0.5786108, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 7, "text_snippet": " retrieved documents are prepended to the input context and fed into the black-box LM to make the final prediction. Because the LM context length limits the number of documents that can be prepended, we also introduce a new ensemble scheme "}, {"rank": 4, "score": 0.5583041, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 20, "text_snippet": "n improve GPT- 3 performance on open-domain question answering, we approach the problem in a more general setting, including language modeling and understanding tasks. We also pro- pose an ensemble method to incorporate more documents  REPL"}, {"rank": 5, "score": 0.55796313, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 71, "text_snippet": "a black box and augments it with a tuneable retrieval model. Our evaluation shows that REPLUG can be integrated with any existing language model to improve their performance  REPLUG: Retrieval-Augmented Black-Box Language Models on language"}]}
{"case_index": 136, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"Early US models include X, S and PRO-4X, with a choice of 6-speed manual or 5-speed automatic [MASK], a choice of [...] moved from Smyrna, Tennessee, to Nissan's facility in Canton, Mississippi.\"", "gold": "transmissions", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.091, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.68121755, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 8, "text_snippet": " moved from Smyrna, Tennessee, to Nissan's facility in Canton, Mississippi. Early US models include X, S and PRO-4X, with a choice of 6-speed manual or 5-speed automatic transmissions, a choice of [...] moved from Smyrna, Tennessee, to Niss"}, {"rank": 2, "score": 0.67016196, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 9, "text_snippet": "acility in Canton, Mississippi. Early US models include X, S and PRO-4X, with a choice of 6-speed manual or 5-speed automatic transmissions, a choice of [...] moved from Smyrna, Tennessee, to Nissan's facility in Canton, Mississippi. Early "}, {"rank": 3, "score": 0.5522467, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 10, "text_snippet": "els include X, S and PRO-4X, with a choice of 6-speed manual…Retrieved documents DRECOMP (58 tokens)RetrieveCompressPrependNo retrieval (0 tokens)RALM (749 tokens)2010 ❌ ✅2015SummaryInput query xwhen did they stop making the nissan xterra?B"}, {"rank": 4, "score": 0.4585326, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 5, "score": 0.45177418, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 7, "text_snippet": "rieved evidence documents, and guide RALM to generate desired outputs when prepended to the input. To satisfy both efficiency and effectiveness constraints, our compressor strategically performs selective augmentation by generating an empty"}]}
{"case_index": 137, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"We marginalize the latent documents with a top-K [MASK], either on a per-output basis (assuming the same document is responsible for all tokens) or a per-token basis (\"", "gold": "approximation", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.2, "hit_rank": 5, "matched_by": "doc_id+chunk_id", "retrieval_ms": 9.161, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.6742729, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 2, "score": 0.6063764, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 37, "text_snippet": " To cast language modeling in the text-to-text framework, we consider a chunk of Nwords, and split this chunk in two sub-sequences of equal length N/2. Then, the ﬁrst sub-sequence is used as the query, and the second corresponds to the outp"}, {"rank": 3, "score": 0.6028271, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 4, "score": 0.5557476, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 83, "text_snippet": ".49 28.22 5 20.18 17.3728.2419.6518.7127.08 820.5217.60 28.17 16.87 18.05 25.36 10 19.67 17.41 27.62 15.72 17.42 23.60 6 Related Works Retrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi- tec"}, {"rank": 5, "score": 0.5486174, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 10, "text_snippet": "eural retriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The retriever (Dense Passage Retriever [ 26], henceforth DPR) provides latent documents conditioned on the input, and the seq2seq model (BART"}]}
{"case_index": 138, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"Both methods require updating the model [MASK] through gradient descent, which cannot be applied to black-box LMs.\"", "gold": "parameters", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 11.713, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.63618565, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 18, "text_snippet": "e to incorporate retrieved texts and pretrains the language model from scratch. Both methods require updating the model parameters through gradient descent, which cannot be applied to black-box LMs. Another line of retrieval-augmented LMs s"}, {"rank": 2, "score": 0.6048305, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 0, "text_snippet": "REPLUG: Retrieval-Augmented Black-Box Language Models Weijia Shi,1 *Sewon Min,1Michihiro Yasunaga,2Minjoon Seo,3Rich James,4Mike Lewis,4 Luke Zettlemoyer1 4Wen-tau Yih4 Abstract We introduce REPLUG, a retrieval-augmented lan- guage modeling"}, {"rank": 3, "score": 0.58247375, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 8, "text_snippet": "Black-Box Language Models Figure 1, REPLUG is extremely flexible and can be used with any existing black-box LM and retrieval model. We also introduce REPLUG LSR (REPLUG with LM- Supervised Retrieval), a training scheme that can further imp"}, {"rank": 4, "score": 0.5760401, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 71, "text_snippet": "a black box and augments it with a tuneable retrieval model. Our evaluation shows that REPLUG can be integrated with any existing language model to improve their performance  REPLUG: Retrieval-Augmented Black-Box Language Models on language"}, {"rank": 5, "score": 0.5759357, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 83, "text_snippet": "sser, S., and Leahy, C. The Pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027 , 2020.  REPLUG: Retrieval-Augmented Black-Box Language Models Goyal, A., Friesen, A., Banino, A., Weber, T., Ke, N. R"}]}
{"case_index": 139, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"By processing passages [MASK] in the en- coder, but jointly in the decoder, this method dif- fers from Min et al.\"", "gold": "independently", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 9.939, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.6841914, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}, {"rank": 2, "score": 0.6475118, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 86, "text_snippet": "models for open domain question an- swering. In Proceedings of the 16th Conference of the European Chapter of the Association for Compu- tational Linguistics: Main Volume , pp. 874–880, On- line, April 2021a. Association for Computational L"}, {"rank": 3, "score": 0.6408663, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 0, "text_snippet": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering Gautier Izacard1,2,3Edouard Grave1 1Facebook AI Research, Paris 2ENS, PSL University, Paris 3Inria, Paris gizacard|egrave@fb.com Abstract Generative mode"}, {"rank": 4, "score": 0.63279146, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 5, "score": 0.6246041, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 16, "text_snippet": "las (Izacard et al., 2022), which fine-tunes an encoder- decoder model in conjunction with the retriever; REALM (Guu et al., 2020), a bidirectional, masked LM fine-tuned for open-domain question answering; and RAG (Retrieval-Augmented Gener"}]}
{"case_index": 140, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"2021), and [MASK] questions based on medium-length passages (QuALITY , Pang et al.\"", "gold": "multiple-choice", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.3333333333333333, "hit_rank": 3, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.273, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.5906403, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 2, "score": 0.5525006, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 235, "text_snippet": "ull results for the cyclic-permutation-de-biased Atlas-XXL can be found in Table 18. A.2 Question answering A.2.1 Training Details For question answering, similarly to the MMLU experiments, we format the input using the following template: "}, {"rank": 3, "score": 0.5497403, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 12, "text_snippet": "SPER, Dasigi et al. 2021), and multiple-choice questions based on medium-length passages (QuALITY , Pang et al. 2022).1 2 R ELATED WORK Why Retrieval? Recent advances in hardware and algorithms have indeed expanded the con- text lengths tha"}, {"rank": 4, "score": 0.54216564, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 90, "text_snippet": "age Processing (EMNLP) , pp. 6769–6781, Online, November 2020a. Association for Computational Lin- guistics. doi: 10.18653/v1/2020.emnlp-main.550. URL https://aclanthology.org/2020.emnlp-main.550 . Karpukhin, V ., Oguz, B., Min, S., Lewis, "}, {"rank": 5, "score": 0.5412041, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 139, "text_snippet": ". . .∪Sk end function G Q UALITATIVE ANALYSIS To qualitatively examine RAPTOR’s retrieval process, we test it on thematic, multi-hop questions about a 1500-word version of the fairytale Cinderella. We compare the context retrieved by RAP- T"}]}
{"case_index": 141, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"er model encθwhich embeds sentence siand the input sequence xinto fixed- dimensional embeddings [MASK].\"", "gold": "respectively", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.941, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.6184557, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 20, "text_snippet": "er model encθwhich embeds sentence siand the input sequence xinto fixed- dimensional embeddings respectively. Their inner product represents how helpful it would be for the LMMto prepend sito the input xto generate y. The final summary sfro"}, {"rank": 2, "score": 0.5971366, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 3, "score": 0.5852417, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 4, "score": 0.5835495, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 26, "text_snippet": " 7: L ← L ∪ si 8: if|L|>0then 9: Ni←argTop5 sj∈L(⟨encθ(sj),encθ(xi)⟩) 10: T ← T ∪ { (xi,pi,Ni)} 11:encθ=Finetune (encθ,T) Figure 2: Learning an extractive compressor for lan- guage modeling task.Model We train a dual-encoder model encθwhich"}, {"rank": 5, "score": 0.5825091, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 123, "text_snippet": "//huggingface.co/facebook/contriever 10https://huggingface.co/facebook/contriever-msmarco 15  Input: Base LM M, Compressor encoder encθ, Training data {xi,Si,yi}T 1where xiis input, Si={sj}n 1 is a set of candidate sentences from the retrie"}]}
{"case_index": 142, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"(2019a) introduced a method based on hard expectation- [MASK] to tackle noisy supervision from this setting.\"", "gold": "maximization", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 14.28, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.64010316, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}, {"rank": 2, "score": 0.6310924, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 3, "score": 0.6261621, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 0, "text_snippet": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering Gautier Izacard1,2,3Edouard Grave1 1Facebook AI Research, Paris 2ENS, PSL University, Paris 3Inria, Paris gizacard|egrave@fb.com Abstract Generative mode"}, {"rank": 4, "score": 0.62141854, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 86, "text_snippet": "models for open domain question an- swering. In Proceedings of the 16th Conference of the European Chapter of the Association for Compu- tational Linguistics: Main Volume , pp. 874–880, On- line, April 2021a. Association for Computational L"}, {"rank": 5, "score": 0.6021702, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 101, "text_snippet": "ov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906 , 2020. URL https://arxiv.org/abs/2004.04906 . [27] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, "}]}
{"case_index": 143, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"Without relying on external knowledge, this method obtained compet- itive results on several [MASK].\"", "gold": "benchmarks", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 5.106, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6517469, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 0, "text_snippet": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering Gautier Izacard1,2,3Edouard Grave1 1Facebook AI Research, Paris 2ENS, PSL University, Paris 3Inria, Paris gizacard|egrave@fb.com Abstract Generative mode"}, {"rank": 2, "score": 0.6431191, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 3, "score": 0.63769585, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}, {"rank": 4, "score": 0.6298038, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 86, "text_snippet": "models for open domain question an- swering. In Proceedings of the 16th Conference of the European Chapter of the Association for Compu- tational Linguistics: Main Volume , pp. 874–880, On- line, April 2021a. Association for Computational L"}, {"rank": 5, "score": 0.596381, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 235, "text_snippet": "ull results for the cyclic-permutation-de-biased Atlas-XXL can be found in Table 18. A.2 Question answering A.2.1 Training Details For question answering, similarly to the MMLU experiments, we format the input using the following template: "}]}
{"case_index": 144, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"In this work, we in- [MASK] ways to improve large black-box language models with retrieval.\"", "gold": "vestigate", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 6.619, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.66779375, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 0, "text_snippet": "REPLUG: Retrieval-Augmented Black-Box Language Models Weijia Shi,1 *Sewon Min,1Michihiro Yasunaga,2Minjoon Seo,3Rich James,4Mike Lewis,4 Luke Zettlemoyer1 4Wen-tau Yih4 Abstract We introduce REPLUG, a retrieval-augmented lan- guage modeling"}, {"rank": 2, "score": 0.65996224, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 71, "text_snippet": "a black box and augments it with a tuneable retrieval model. Our evaluation shows that REPLUG can be integrated with any existing language model to improve their performance  REPLUG: Retrieval-Augmented Black-Box Language Models on language"}, {"rank": 3, "score": 0.6442632, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 83, "text_snippet": "sser, S., and Leahy, C. The Pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027 , 2020.  REPLUG: Retrieval-Augmented Black-Box Language Models Goyal, A., Friesen, A., Banino, A., Weber, T., Ke, N. R"}, {"rank": 4, "score": 0.6301293, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 20, "text_snippet": "n improve GPT- 3 performance on open-domain question answering, we approach the problem in a more general setting, including language modeling and understanding tasks. We also pro- pose an ensemble method to incorporate more documents  REPL"}, {"rank": 5, "score": 0.6178046, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 15, "text_snippet": "anguage models makes this approach infeasible. To ad- dress the challenges posed by large language models, we investigate retrieval-augmentation in the black-box setting , where users only have access to the model predictions and cannot acc"}]}
{"case_index": 145, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"Moreover, [MASK] citations for each segment with its self-assessment of whether the output is supported by the passage, leading to easier fact verification.\"", "gold": "self-ragprovides", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.2, "hit_rank": 5, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.3, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.6059549, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 34, "text_snippet": "l predicts the next output segment, as it does in a standard LM. If retrieval is needed, the model generates: a critique token to evaluate the retrieved passage’s relevance, the next response segment, and a critique token to evaluate if the"}, {"rank": 2, "score": 0.5820715, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 8, "text_snippet": "on demand (Step 1). Subsequently, SELF-RAGconcurrently processes multiple retrieved passages, evaluating their relevance and then generating corresponding task outputs (Step 2). It then generates critique tokens to criticize its own output "}, {"rank": 3, "score": 0.5664342, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 7, "text_snippet": "rmittent special tokens (i.e., reflection tokens ). Reflection tokens are categorized into retrieval andcritique tokens to indicate the need for retrieval and its generation quality respectively (Figure 1 right). In particular, given an inp"}, {"rank": 4, "score": 0.5651238, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 23, "text_snippet": "ess retrieved passages before using them to prompt the LM to generate the output. SELF-RAGprocesses passages in parallel and filters out irrelevant ones through self-reflection, without relying on external models at inference. Moreover, our"}, {"rank": 5, "score": 0.5643146, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 13, "text_snippet": "erifiability. consistently retrieves a fixed number of documents for generation regardless of the retrieval necessity (e.g., the bottom figure example does not require factual knowledge) and never second visits the generation quality. Moreo"}]}
{"case_index": 146, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"These models are typically trained on very large datasets and store a [MASK] amount of world or domain knowledge implicitly in their parameters.\"", "gold": "substantial", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 7.715, "llm_ms": 0.01, "top_contexts": [{"rank": 1, "score": 0.6137267, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 104, "text_snippet": "2a. Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V ., et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 , 2022b. Zhong, Z., Lei, T., and Chen"}, {"rank": 2, "score": 0.6007015, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 3, "score": 0.5948653, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 83, "text_snippet": "sser, S., and Leahy, C. The Pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027 , 2020.  REPLUG: Retrieval-Augmented Black-Box Language Models Goyal, A., Friesen, A., Banino, A., Weber, T., Ke, N. R"}, {"rank": 4, "score": 0.5769233, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 0, "text_snippet": "REPLUG: Retrieval-Augmented Black-Box Language Models Weijia Shi,1 *Sewon Min,1Michihiro Yasunaga,2Minjoon Seo,3Rich James,4Mike Lewis,4 Luke Zettlemoyer1 4Wen-tau Yih4 Abstract We introduce REPLUG, a retrieval-augmented lan- guage modeling"}, {"rank": 5, "score": 0.5748894, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 72, "text_snippet": "it from retrieval. However, REPLUG lacks interpretability as it is unclear when the model relies on retrieved knowledge or parametric knowledge. Future research could focus on developing more interpretable retrieval-augmented language model"}]}
{"case_index": 147, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"Both compressors are trained to improve LMs’ [MASK] on end tasks when the generated summaries are prepended to the LMs’ input, while keeping the summary concise.\"", "gold": "performance", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 18.441, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.67090607, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 2, "text_snippet": "d an abstractive compressor which generates summaries by synthesizing information from multiple documents. Both compressors are trained to improve LMs’ performance on end tasks when the generated summaries are prepended to the LMs’ input, w"}, {"rank": 2, "score": 0.6253542, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 0, "text_snippet": "RECOMP: I MPROVING RETRIEVAL -AUGMENTED LM S WITH COMPRESSION AND SELECTIVE AUGMENTATION Fangyuan Xu1, Weijia Shi2, Eunsol Choi1 Department of Computer Science 1The University of Texas at Austin 2University of Washington {fangyuan,eunsol }@"}, {"rank": 3, "score": 0.6177813, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 11, "text_snippet": " prepending it as input to a language model at inference time. The compressed summary guides the LM to generate the correct answer, while significantly reducing the computation costs required to encode the documents. We propose compressors:"}, {"rank": 4, "score": 0.6175704, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 77, "text_snippet": "by compressing retrieved documents into a concise summary or an empty sequence, facilitating selective retrieval augmentation. Prompt Compression Recent work (Wingate et al., 2022; Chevalier et al., 2023; Mu et al., 2023) proposes compressi"}, {"rank": 5, "score": 0.61147237, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 3, "text_snippet": "n. We evaluate our approach on language modeling task and open domain question answering task. We achieve a compression rate of as low as 6% with minimal loss in performance for both tasks, significantly outperforming the off-the-shelf summ"}]}
{"case_index": 148, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"ting the LM’s [MASK], REPLUG treats the language model as a black box and augments it with a frozen or tunable retriever.\"", "gold": "parameters", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 7.267, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.70482457, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 0, "text_snippet": "REPLUG: Retrieval-Augmented Black-Box Language Models Weijia Shi,1 *Sewon Min,1Michihiro Yasunaga,2Minjoon Seo,3Rich James,4Mike Lewis,4 Luke Zettlemoyer1 4Wen-tau Yih4 Abstract We introduce REPLUG, a retrieval-augmented lan- guage modeling"}, {"rank": 2, "score": 0.6922151, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 71, "text_snippet": "a black box and augments it with a tuneable retrieval model. Our evaluation shows that REPLUG can be integrated with any existing language model to improve their performance  REPLUG: Retrieval-Augmented Black-Box Language Models on language"}, {"rank": 3, "score": 0.6617087, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 8, "text_snippet": "Black-Box Language Models Figure 1, REPLUG is extremely flexible and can be used with any existing black-box LM and retrieval model. We also introduce REPLUG LSR (REPLUG with LM- Supervised Retrieval), a training scheme that can further imp"}, {"rank": 4, "score": 0.65609646, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 9, "text_snippet": "22) that adapts language models to the retriever. We use a training objective which prefers retrieving documents that improve language model perplexity, while treating the LM as a frozen, black-box scoring function. Our experiments show tha"}, {"rank": 5, "score": 0.6525635, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 83, "text_snippet": "sser, S., and Leahy, C. The Pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027 , 2020.  REPLUG: Retrieval-Augmented Black-Box Language Models Goyal, A., Friesen, A., Banino, A., Weber, T., Ke, N. R"}]}
{"case_index": 149, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"This is particularly relevant for thematic questions that require integrating knowledge from multiple parts of a text, such as [MASK] an entire book, as in the NarrativeQA dataset (Ko ˇcisk`y et al., 2018).\"", "gold": "understanding", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 5.47, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.5990636, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 7, "text_snippet": "re. This is particularly relevant for thematic questions that require integrating knowledge from multiple parts of a text, such as understanding an entire book, as in the NarrativeQA dataset (Ko ˇcisk`y et al., 2018). Consider the fairy tal"}, {"rank": 2, "score": 0.5782665, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 3, "score": 0.5508973, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 4, "score": 0.5292426, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 37, "text_snippet": " To cast language modeling in the text-to-text framework, we consider a chunk of Nwords, and split this chunk in two sub-sequences of equal length N/2. Then, the ﬁrst sub-sequence is used as the query, and the second corresponds to the outp"}, {"rank": 5, "score": 0.52258563, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 22, "text_snippet": " RAPTOR addresses the issue of semantic depth and connection in reading by building a recursive tree structure that balances broader thematic comprehension with granular details and which allows nodes to be grouped based on semantic sim- il"}]}
{"case_index": 150, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"provide rigorous validation [MASK] diverse long-context tasks, including RAG, multi-turn conversations, and long document summarization, spanning a wide range of datasets.\"", "gold": "ofrefragacross", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.69, "llm_ms": 0.004, "top_contexts": [{"rank": 1, "score": 0.66124713, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 0, "text_snippet": "REFRAG: Rethinking RAG based Decoding Xiaoqiang Lin1,2,∗,Aritra Ghosh1,Bryan Kian Hsiang Low2,Anshumali Shrivastava1,3,Vijai Mohan1 1Meta Superintelligence Labs,2National University of Singapore,3Rice University ∗Work done at Meta Large Lan"}, {"rank": 2, "score": 0.6603263, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 4, "text_snippet": " provide rigorous validation ofREFRAGacross diverse long-context tasks, including RAG, multi-turn conversations, and long document summarization, spanning a wide range of datasets. Experimental results confirm thatREFRAGdelivers substantial"}, {"rank": 3, "score": 0.6544509, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 95, "text_snippet": "oducedREFRAG, a novel and efficient decoding framework tailored for RAG applications. By leveraging the inherent sparsity and block-diagonal attention patterns present in RAG contexts,REFRAG compresses, senses, and expands context represent"}, {"rank": 4, "score": 0.6378017, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 16, "text_snippet": "nsing most chunks for the query in RAG settings. We provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training and many real word long-context applications including RAG, multi-turn conversation with RA"}, {"rank": 5, "score": 0.61477613, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 96, "text_snippet": "ong document summarization, demonstrate thatREFRAG achieves up to30 .85×TTFT acceleration (3 .75×over previous state-of-the-art methods) without any loss in perplexity or downstream accuracy. Our results highlight the importance of speciali"}]}
{"case_index": 151, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"of retrieved passages prepended to input, or pre-train a retriever and LM jointly, followed by few- shot [MASK] on task datasets (Izacard et al., 2022b).\"", "gold": "fine-tuning", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.25, "hit_rank": 4, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.181, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6128423, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 2, "score": 0.6034124, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 22, "text_snippet": " the retriever and LM on instruction-tuning datasets in two steps. While we also train our model on diverse instruction-following datasets, SELF-RAGenables retrieval on demand and selection of the best possible model output via fine-grained"}, {"rank": 3, "score": 0.60324574, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 23, "text_snippet": "ess retrieved passages before using them to prompt the LM to generate the output. SELF-RAGprocesses passages in parallel and filters out irrelevant ones through self-reflection, without relying on external models at inference. Moreover, our"}, {"rank": 4, "score": 0.5973194, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 19, "text_snippet": "rieved text passages (Guu et al., 2020; Lewis et al., 2020), leading to large improvements in knowledge-intensive tasks after fine-tuning or used with off-the-shelf LMs (Ram et al., 2023). A more recent work (Luo et al., 2023) instruction-t"}, {"rank": 5, "score": 0.5961809, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 8, "text_snippet": "on demand (Step 1). Subsequently, SELF-RAGconcurrently processes multiple retrieved passages, evaluating their relevance and then generating corresponding task outputs (Step 2). It then generates critique tokens to criticize its own output "}]}
{"case_index": 152, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"This refinement operation involves knowledge decom- position, filter, and [MASK] (Section 4.4).\"", "gold": "recomposition", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 7.015, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.5275407, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 2, "score": 0.49733502, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 3, "score": 0.4805327, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 83, "text_snippet": " in Table 6. The findings indicate that the self-correction mecha- nism incurs only modest computational overheadwhile significantly enhancing performance, thereby validating its lightweight nature. 6 Conclusion & Limitation This paper stud"}, {"rank": 4, "score": 0.47305104, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 1, "text_snippet": "evitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Al- though retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heav"}, {"rank": 5, "score": 0.47016567, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 83, "text_snippet": ".49 28.22 5 20.18 17.3728.2419.6518.7127.08 820.5217.60 28.17 16.87 18.05 25.36 10 19.67 17.41 27.62 15.72 17.42 23.60 6 Related Works Retrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi- tec"}]}
{"case_index": 153, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"This process differs from [MASK] RAG (Figure 1 left), which 1Our code and trained models are available at https://selfrag.github.io/ .\"", "gold": "conventional", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 16.537, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6386746, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 8, "text_snippet": "on demand (Step 1). Subsequently, SELF-RAGconcurrently processes multiple retrieved passages, evaluating their relevance and then generating corresponding task outputs (Step 2). It then generates critique tokens to criticize its own output "}, {"rank": 2, "score": 0.59977597, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 16, "text_snippet": "Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform- ers Library [ 66] and can be found at https://github.com/huggingface/transformers/blob/master/ examples/rag/ . An interactive demo of RAG models ca"}, {"rank": 3, "score": 0.57972383, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 27, "text_snippet": "n (reasoning path consistency) and without retrieval. Recent work on LLM refinement (Dhuliawala et al., 2023; Madaan et al., 2023; Paul et al., 2023) prompts a model to generate task output, natural language feedback and refined task output"}, {"rank": 4, "score": 0.57532024, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 132, "text_snippet": "or: AsCRAG is a plug-and-play method, all generation models that can be uti- lized in RAG fit our approach as well. To be consistent with baselines for comparison, we adopted LLaMA2 (Touvron et al., 2023b) for the generation. We first intro"}, {"rank": 5, "score": 0.5747982, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 0, "text_snippet": "Preprint. SELF-RAG: LEARNING TO RETRIEVE , GENERATE ,AND CRITIQUE THROUGH SELF-REFLECTION Akari Asai†, Zeqiu Wu†, Yizhong Wang†§, Avirup Sil‡, Hannaneh Hajishirzi†§ †University of Washington§Allen Institute for AI‡IBM Research AI {akari,zeq"}]}
{"case_index": 154, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"To address this, we design an indexing and retrieval system that uses a tree [MASK] to cap\"", "gold": "structure", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 7.431, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.5491687, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 0, "text_snippet": "Published as a conference paper at ICLR 2024 RAPTOR: R ECURSIVE ABSTRACTIVE PROCESSING FOR TREE-ORGANIZED RETRIEVAL Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D. Manning Stanford University psarthi@cs."}, {"rank": 2, "score": 0.54353005, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 22, "text_snippet": " RAPTOR addresses the issue of semantic depth and connection in reading by building a recursive tree structure that balances broader thematic comprehension with granular details and which allows nodes to be grouped based on semantic sim- il"}, {"rank": 3, "score": 0.53992677, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 8, "text_snippet": " indexing and retrieval system that uses a tree structure to capture both high-level and low-level details about a text. As shown in Figure 1, our system, RAPTOR, clusters chunks of text, generates text summaries of those clusters, and then"}, {"rank": 4, "score": 0.51279986, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 5, "score": 0.51076543, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}]}
{"case_index": 155, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"Moreover, our [MASK] mechanism also evaluates other aspects of the model output quality including factuality.\"", "gold": "self-reflection", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 7.125, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.590831, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 27, "text_snippet": "n (reasoning path consistency) and without retrieval. Recent work on LLM refinement (Dhuliawala et al., 2023; Madaan et al., 2023; Paul et al., 2023) prompts a model to generate task output, natural language feedback and refined task output"}, {"rank": 2, "score": 0.5787014, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 0, "text_snippet": "Preprint. SELF-RAG: LEARNING TO RETRIEVE , GENERATE ,AND CRITIQUE THROUGH SELF-REFLECTION Akari Asai†, Zeqiu Wu†, Yizhong Wang†§, Avirup Sil‡, Hannaneh Hajishirzi†§ †University of Washington§Allen Institute for AI‡IBM Research AI {akari,zeq"}, {"rank": 3, "score": 0.5764481, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 15, "text_snippet": "st a critic model during training, reducing overhead. The critic model, in part, is supervised on a dataset of input, output, and corresponding reflection tokens collected by prompting a propriety LM (i.e., GPT-4; OpenAI 2023). While we dra"}, {"rank": 4, "score": 0.5743712, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 5, "score": 0.5710389, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 8, "text_snippet": "on demand (Step 1). Subsequently, SELF-RAGconcurrently processes multiple retrieved passages, evaluating their relevance and then generating corresponding task outputs (Step 2). It then generates critique tokens to criticize its own output "}]}
{"case_index": 156, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"•Thorough downstream experiments in few-shot settings, demonstrating [MASK] results on few-shot NaturalQuestions (+2.8%), TriviaQA (+3.3%), FEVER (+5.1%), and results on par or stronger than models with 15 ×more parameters on MMLU.\"", "gold": "state-of-the-art", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 9.093, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.67692995, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 2, "score": 0.6591857, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 13, "text_snippet": "ask at hand. •Thorough downstream experiments in few-shot settings, demonstrating state-of-the-art results on few-shot NaturalQuestions (+2.8%), TriviaQA (+3.3%), FEVER (+5.1%), and results on par or stronger than models with 15 ×more param"}, {"rank": 3, "score": 0.64625394, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 154, "text_snippet": "odel. By jointly pre-training the retriever module and the language model, we show that Atlashas strong few-shot learning capabilities on a wide range of knowledge intensive tasks, including NaturalQuestions, TriviaQA, FEVER, 8 KILT tasks a"}, {"rank": 4, "score": 0.6376653, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 235, "text_snippet": "ull results for the cyclic-permutation-de-biased Atlas-XXL can be found in Table 18. A.2 Question answering A.2.1 Training Details For question answering, similarly to the MMLU experiments, we format the input using the following template: "}, {"rank": 5, "score": 0.63125134, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 219, "text_snippet": "4.6 38.8 52.8 A Training details and additional results A.1 MMLU A.1.1 Training Details Featurization MMLU consists of multiple choice questions with four possible lexicalized answer options. We represent the input using the following templ"}]}
{"case_index": 157, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"The relevance score is quantified into a total of three confidence degrees and then triggered the [MASK] actions: { Correct ,Incorrect , Ambiguous } (Section 4.3).\"", "gold": "corresponding", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.082, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.55359393, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 2, "score": 0.5469291, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 27, "text_snippet": "or is constructed to estimate the relevance score of retrieved documents to the input query (Sec- tion 4.2). The relevance score is quantified into a total of three confidence degrees and then triggered the corresponding actions: { Correct "}, {"rank": 3, "score": 0.5372968, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 28, "text_snippet": "sition, filter, and recomposition (Section 4.4). If the action Incorrect is triggered, the retrieved documents will be discarded. Instead, web searches are resorted to and regarded as complementary knowledge sources for corrections (Section"}, {"rank": 4, "score": 0.52164525, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 39, "text_snippet": "ngly where the upper and lower thresholds are set. If the confidence score is higher than the upper threshold, the retrieved document is identified as Correct , while identified as Incorrect if below the lower threshold. Otherwise, a more s"}, {"rank": 5, "score": 0.51704425, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 31, "text_snippet": "neratorkex+xkex+ Figure 2: An overview of the proposed CRAG at inference. A retrieval evaluator is constructed to evaluate the relevance of the retrieved documents to the input, and estimate a confidence degree based on which different know"}]}
{"case_index": 158, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"The challenge, however, arises when the volume of data requires a RAG approach, since vector RAG approaches are unable to support [MASK] over an entire corpus.\"", "gold": "sensemaking", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 8.745, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.59918994, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 0, "text_snippet": "From Local to Global: A GraphRAG Approach to Query-Focused Summarization Darren Edge1†Ha Trinh1†Newman Cheng2Joshua Bradley2Alex Chao3 Apurva Mody3Steven Truitt2Dasha Metropolitansky1Robert Osazuwa Ness1 Jonathan Larson1 1Microsoft Research"}, {"rank": 2, "score": 0.58169913, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 1, "text_snippet": "ibuted equally to this work Abstract The use of retrieval-augmented generation (RAG) to retrieve relevant informa- tion from an external knowledge source enables large language models (LLMs) to answer questions over private and/or previousl"}, {"rank": 3, "score": 0.56858474, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 97, "text_snippet": "omprehensive- ness and diversity of answers, as well as favorable comparisons to a global but graph-free approach using map-reduce source text summarization. For situations requiring many global queries over the same dataset, summaries of r"}, {"rank": 4, "score": 0.5604001, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 16, "text_snippet": " may use alternative retrieval mechanisms, we collectively refer to the family of conventional approaches as vector RAG . GraphRAG contrasts with vector RAG in its ability to answer queries that require global sensemaking over the entire da"}, {"rank": 5, "score": 0.5600538, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 96, "text_snippet": "tigate these downstream risks for questions of a global nature, which might otherwise be answered by samples of retrieved facts falsely presented as global summaries. 7 Conclusion We have presented GraphRAG, a RAG approach that combines kno"}]}
{"case_index": 159, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"Atlasachieves strong downstream performance in both few-shot and [MASK] settings.\"", "gold": "resource-rich", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 7.271, "llm_ms": 0.004, "top_contexts": [{"rank": 1, "score": 0.63723516, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 2, "score": 0.62720037, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 111, "text_snippet": " arXiv:2208.03299, 2022. https://arxiv.org/abs/ 2208.03299. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Edouard Grave, and Sebastian Riedel. Atlas: Few-shot learn"}, {"rank": 3, "score": 0.62514347, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 8, "text_snippet": "ingFew-shotQuestion answering:Where is the Bermuda Triangle?Western part of the North Atlantic Ocean……Figure 1: We introduce Atlas, a retrieval-augmented language model that exhibits strong few-shot perfor- mance on knowledge tasks, and use"}, {"rank": 4, "score": 0.5990336, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 12, "text_snippet": "ntributions: •A thorough study on how to design and train retrieval-augmented language models, with a focus on downstream few-shot learning and sample eﬃciency. •The ﬁndings of this study lead to a retrieval-augmented language model, called"}, {"rank": 5, "score": 0.59645724, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 154, "text_snippet": "odel. By jointly pre-training the retriever module and the language model, we show that Atlashas strong few-shot learning capabilities on a wide range of knowledge intensive tasks, including NaturalQuestions, TriviaQA, FEVER, 8 KILT tasks a"}]}
{"case_index": 160, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"hat retrieval with recursive summaries offers significant improvements over tra- ditional [MASK] LMs on several tasks.\"", "gold": "retrieval-augmented", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 9.172, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.5798509, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 2, "text_snippet": "hat retrieval with recursive summaries offers significant improvements over tra- ditional retrieval-augmented LMs on several tasks. On question-answering tasks that involve complex, multi-step reasoning, we show state-of-the-art results; fo"}, {"rank": 2, "score": 0.57712233, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 0, "text_snippet": "Published as a conference paper at ICLR 2024 RAPTOR: R ECURSIVE ABSTRACTIVE PROCESSING FOR TREE-ORGANIZED RETRIEVAL Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D. Manning Stanford University psarthi@cs."}, {"rank": 3, "score": 0.5660876, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 4, "score": 0.5643815, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 1, "text_snippet": "rom a retrieval corpus, limiting holistic under- standing of the overall document context. We introduce the novel approach of recursively embedding, clustering, and summarizing chunks of text, constructing a tree with differing levels of su"}, {"rank": 5, "score": 0.54966235, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 73, "text_snippet": "an effectively handle a wider range of questions, from higher-order thematic queries to detail-oriented questions. Detailed results for additional stories and an ablation study on layer contributions can be found in Appendix I. 5 C ONCLUSIO"}]}
{"case_index": 161, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"(2020) in- troduced retrieval augmented [MASK] models for open domain question answering.\"", "gold": "generative", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.25, "hit_rank": 4, "matched_by": "doc_id+chunk_id", "retrieval_ms": 5.741, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6850953, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 86, "text_snippet": "models for open domain question an- swering. In Proceedings of the 16th Conference of the European Chapter of the Association for Compu- tational Linguistics: Main Volume , pp. 874–880, On- line, April 2021a. Association for Computational L"}, {"rank": 2, "score": 0.6755296, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 0, "text_snippet": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering Gautier Izacard1,2,3Edouard Grave1 1Facebook AI Research, Paris 2ENS, PSL University, Paris 3Inria, Paris gizacard|egrave@fb.com Abstract Generative mode"}, {"rank": 3, "score": 0.66786504, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}, {"rank": 4, "score": 0.64617556, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 16, "text_snippet": "0) and Lewis et al. (2020) in- troduced retrieval augmented generative models for open domain question answering. Our approach differs from these works by how the generative model processes the retrieved passages. This al- lows to scale to "}, {"rank": 5, "score": 0.63504016, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 101, "text_snippet": "ov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906 , 2020. URL https://arxiv.org/abs/2004.04906 . [27] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, "}]}
{"case_index": 162, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"Given a question, each community summary is used to generate a partial response, before all partial responses are again [MASK] in a final response to the user.\"", "gold": "summarized", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 9.361, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6324785, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 47, "text_snippet": "t within the context window. 3.1.6 Community Summaries →Community Answers →Global Answer Given a user query, the community summaries generated in the previous step can be used to generate a final answer in a multi-stage process. The hierarc"}, {"rank": 2, "score": 0.5955072, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 97, "text_snippet": "omprehensive- ness and diversity of answers, as well as favorable comparisons to a global but graph-free approach using map-reduce source text summarization. For situations requiring many global queries over the same dataset, summaries of r"}, {"rank": 3, "score": 0.59244823, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 0, "text_snippet": "From Local to Global: A GraphRAG Approach to Query-Focused Summarization Darren Edge1†Ha Trinh1†Newman Cheng2Joshua Bradley2Alex Chao3 Apurva Mody3Steven Truitt2Dasha Metropolitansky1Robert Osazuwa Ness1 Jonathan Larson1 1Microsoft Research"}, {"rank": 4, "score": 0.5877179, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 48, "text_snippet": "mmunity structure offers the best balance of summary detail and scope for general sensemaking questions (evaluated in section 4). For a given community level, the global answer to any user query is generated as follows: •Prepare community s"}, {"rank": 5, "score": 0.5821998, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 10, "text_snippet": "cted communities, with summaries at higher levels of the hierarchy recursively incorporating lower-level summaries. Together, these community summaries provide global descriptions and insights over the corpus. Finally, GraphRAG answers quer"}]}
{"case_index": 163, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"We propose compressors: (1) Extractive compressor which selects relevant sentences from retrieved document set; (2) Abstractive compressor which generates a summary [MASK] information from multiple retrieved documents.\"", "gold": "synthesizing", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.2, "hit_rank": 5, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.59, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.67045355, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 2, "text_snippet": "d an abstractive compressor which generates summaries by synthesizing information from multiple documents. Both compressors are trained to improve LMs’ performance on end tasks when the generated summaries are prepended to the LMs’ input, w"}, {"rank": 2, "score": 0.66930294, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 19, "text_snippet": "ining compressors for conciseness and effectiveness. We summarize the key ideas for our two compressors, extractive compressors and abstractive compressor here, and discuss their training schemes formally in Section 3. 2Improving retriever "}, {"rank": 3, "score": 0.66779315, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 0, "text_snippet": "RECOMP: I MPROVING RETRIEVAL -AUGMENTED LM S WITH COMPRESSION AND SELECTIVE AUGMENTATION Fangyuan Xu1, Weijia Shi2, Eunsol Choi1 Department of Computer Science 1The University of Texas at Austin 2University of Washington {fangyuan,eunsol }@"}, {"rank": 4, "score": 0.65811336, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 12, "text_snippet": "ompressors implement multi-document query-focused summarization (Xu & Lapata, 2020), where we summarize retrieved evidence document set with respect to the input query. As we aim to enable RALM to generate correct output when summary is pre"}, {"rank": 5, "score": 0.6526958, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 11, "text_snippet": " prepending it as input to a language model at inference time. The compressed summary guides the LM to generate the correct answer, while significantly reducing the computation costs required to encode the documents. We propose compressors:"}]}
{"case_index": 164, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"For language generation tasks, we ﬁnd that RAG models generate more speciﬁc, diverse and factual language than a [MASK] parametric-only seq2seq baseline.\"", "gold": "state-of-the-art", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.3333333333333333, "hit_rank": 3, "matched_by": "doc_id+chunk_id", "retrieval_ms": 39.854, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.6266967, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 3, "text_snippet": "cessed with a pre-trained neural retriever. We com- pare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. We ﬁne-tune and "}, {"rank": 2, "score": 0.6211056, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 3, "score": 0.61519814, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 4, "text_snippet": "architectures. For language generation tasks, we ﬁnd that RAG models generate more speciﬁc, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline. 1 Introduction Pre-trained neural language models have been s"}, {"rank": 4, "score": 0.6014526, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 5, "score": 0.59786963, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 83, "text_snippet": ".49 28.22 5 20.18 17.3728.2419.6518.7127.08 820.5217.60 28.17 16.87 18.05 25.36 10 19.67 17.41 27.62 15.72 17.42 23.60 6 Related Works Retrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi- tec"}]}
{"case_index": 165, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"of input sequence xi and candidate sentences sj, we measure 3Recent work (Zhang et al., 2022) shows that extractive approach does not always preserve [MASK], but such cases are still rare compared to abstractive approaches which can easily hallucinate.\"", "gold": "faithfulness", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 5.699, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.68735087, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 28, "text_snippet": " of input sequence xi and candidate sentences sj, we measure 3Recent work (Zhang et al., 2022) shows that extractive approach does not always preserve faithfulness, but such cases are still rare compared to abstractive approaches which can "}, {"rank": 2, "score": 0.6135876, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 0, "text_snippet": "RECOMP: I MPROVING RETRIEVAL -AUGMENTED LM S WITH COMPRESSION AND SELECTIVE AUGMENTATION Fangyuan Xu1, Weijia Shi2, Eunsol Choi1 Department of Computer Science 1The University of Texas at Austin 2University of Washington {fangyuan,eunsol }@"}, {"rank": 3, "score": 0.61023676, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 135, "text_snippet": "arXiv preprint arXiv:2305.00633 , 2023. URL https://arxiv.org/abs/2305.00633 . Fangyuan Xu, Weijia Shi, and Eunsol Choi. Recomp: Improving retrieval-augmented lms with compression and selective augmentation, 2023. URL https://arxiv.org/abs/"}, {"rank": 4, "score": 0.6049254, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 5, "score": 0.60001945, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 123, "text_snippet": "//huggingface.co/facebook/contriever 10https://huggingface.co/facebook/contriever-msmarco 15  Input: Base LM M, Compressor encoder encθ, Training data {xi,Si,yi}T 1where xiis input, Si={sj}n 1 is a set of candidate sentences from the retrie"}]}
{"case_index": 166, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"Next, it partitions the graph into a hierarchy of communities of closely related entities, before using an LLM to generate [MASK] summaries.\"", "gold": "community-level", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 6.046, "llm_ms": 0.004, "top_contexts": [{"rank": 1, "score": 0.6295783, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 0, "text_snippet": "From Local to Global: A GraphRAG Approach to Query-Focused Summarization Darren Edge1†Ha Trinh1†Newman Cheng2Joshua Bradley2Alex Chao3 Apurva Mody3Steven Truitt2Dasha Metropolitansky1Robert Osazuwa Ness1 Jonathan Larson1 1Microsoft Research"}, {"rank": 2, "score": 0.6245084, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 97, "text_snippet": "omprehensive- ness and diversity of answers, as well as favorable comparisons to a global but graph-free approach using map-reduce source text summarization. For situations requiring many global queries over the same dataset, summaries of r"}, {"rank": 3, "score": 0.6124275, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 10, "text_snippet": "cted communities, with summaries at higher levels of the hierarchy recursively incorporating lower-level summaries. Together, these community summaries provide global descriptions and insights over the corpus. Finally, GraphRAG answers quer"}, {"rank": 4, "score": 0.6010368, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 27, "text_snippet": "ns (Wang et al., 2023a; Zheng et al., 2024). Some prior work proposes criteria for having LLMs quantify the quality of 3  Source Documents Text Chunkstext extraction and chunking Entities & Relationshipsdomain-tailored summarization Knowled"}, {"rank": 5, "score": 0.57422084, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 43, "text_snippet": " B. 3.1.5 Graph Communities →Community Summaries The next step creates report-like summaries of each community in the community hierarchy, using a method designed to scale to very large datasets. These summaries are independently useful as "}]}
{"case_index": 167, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"We consider the sentence with the highest log [MASK] as a positive example pi(line 3).\"", "gold": "likelihood", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.3333333333333333, "hit_rank": 3, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.352, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.5794765, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 2, "score": 0.5563016, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 0, "text_snippet": "RECOMP: I MPROVING RETRIEVAL -AUGMENTED LM S WITH COMPRESSION AND SELECTIVE AUGMENTATION Fangyuan Xu1, Weijia Shi2, Eunsol Choi1 Department of Computer Science 1The University of Texas at Austin 2University of Washington {fangyuan,eunsol }@"}, {"rank": 3, "score": 0.5555998, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 29, "text_snippet": "g pM(y|[sj;xi]), log likelihood assigned to target output according to LM Mwhen candidate sentence is prepended to the input. We consider the sentence with the highest log likelihood as a positive example pi(line 3). To construct negative e"}, {"rank": 4, "score": 0.5376874, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 82, "text_snippet": " and allowing the compression models to perform selective augmentation. Our experiments show that our compressors can improve the efficiency of retrieval augmented LMs significantly with minimal drop in performances. ACKNOWLEDGEMENT We than"}, {"rank": 5, "score": 0.53542143, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 123, "text_snippet": "//huggingface.co/facebook/contriever 10https://huggingface.co/facebook/contriever-msmarco 15  Input: Base LM M, Compressor encoder encθ, Training data {xi,Si,yi}T 1where xiis input, Si={sj}n 1 is a set of candidate sentences from the retrie"}]}
{"case_index": 168, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"vector embeddings presents a challenge for traditional GMMs, as dis- tance metrics may behave poorly when used to measure similarity in [MASK] spaces (Ag- garwal et al., 2001).\"", "gold": "high-dimensional", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 5.551, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.5587299, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 29, "text_snippet": "vector embeddings presents a challenge for traditional GMMs, as dis- tance metrics may behave poorly when used to measure similarity in high-dimensional spaces (Ag- garwal et al., 2001). To mitigate this, we employ Uniform Manifold Approxim"}, {"rank": 2, "score": 0.537449, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 3, "score": 0.5325607, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 4, "score": 0.49297968, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 235, "text_snippet": "ull results for the cyclic-permutation-de-biased Atlas-XXL can be found in Table 18. A.2 Question answering A.2.1 Training Details For question answering, similarly to the MMLU experiments, we format the input using the following template: "}, {"rank": 5, "score": 0.48957682, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 60, "text_snippet": " more diverse than RAG-Token’s, and both are signiﬁcantly more diverse than BART without needing any diversity-promoting decoding. Retrieval Ablations A key feature of RAG is learning to retrieve relevant information for the task. To assess"}]}
{"case_index": 169, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"This flexibility is essen- tial because individual text segments often contain [MASK] relevant to various topics, thereby warranting their inclusion in multiple summaries.\"", "gold": "information", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 1.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 6.406, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.53923553, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 2, "score": 0.5384385, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 1, "text_snippet": "rom a retrieval corpus, limiting holistic under- standing of the overall document context. We introduce the novel approach of recursively embedding, clustering, and summarizing chunks of text, constructing a tree with differing levels of su"}, {"rank": 3, "score": 0.5301995, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 22, "text_snippet": " RAPTOR addresses the issue of semantic depth and connection in reading by building a recursive tree structure that balances broader thematic comprehension with granular details and which allows nodes to be grouped based on semantic sim- il"}, {"rank": 4, "score": 0.52489275, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 35, "text_snippet": "for summarization in Appendix D. While the summarization model generally produces reliable summaries, a focused annotation study revealed that about 4% of the summaries contained minor hallucinations. These did not propagate to parent nodes"}, {"rank": 5, "score": 0.5206476, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 20, "text_snippet": "ets but can sometimes be a lossy means of compression. The recursive-abstractive summarization model by Wu et al. (2021) employs task decomposition to summarize smaller text chunks, which are later integrated to form summaries of larger sec"}]}
{"case_index": 170, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"CRAG is plug-and-play and [MASK] implemented into RAG (Lewis et al., 2020) and Self-RAG (Asai et al., 2024) for demonstrating its adaptability to RAG-based approa\"", "gold": "experimentally", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.25, "hit_rank": 4, "matched_by": "doc_id+chunk_id", "retrieval_ms": 5.293, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.60218227, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 132, "text_snippet": "or: AsCRAG is a plug-and-play method, all generation models that can be uti- lized in RAG fit our approach as well. To be consistent with baselines for comparison, we adopted LLaMA2 (Touvron et al., 2023b) for the generation. We first intro"}, {"rank": 2, "score": 0.60134494, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 133, "text_snippet": "nched model, SelfRAG-LLaMA2-7b , as a new generator to be consistent with their work and study the specific improvement of our method. Self-CRAG: To demonstrate that our plug-and- play approach can be utilized in other concurrent studies, w"}, {"rank": 3, "score": 0.58410466, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 1, "text_snippet": "evitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Al- though retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heav"}, {"rank": 4, "score": 0.5837014, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 13, "text_snippet": "elpful for RAG, a decompose-then-recompose algorithm is meticulously crafted throughout the retrieval and utilization process. This algorithm ensures the refinement of retrieved information, optimizing the extraction of key insights and min"}, {"rank": 5, "score": 0.58314455, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 16, "text_snippet": "Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform- ers Library [ 66] and can be found at https://github.com/huggingface/transformers/blob/master/ examples/rag/ . An interactive demo of RAG models ca"}]}
{"case_index": 171, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"Experiments on four datasets covering short- and long-form generation tasks show that CRAG can [MASK] improve the performance of RAG-based approaches.1 1 Introduction Large language models (LLMs) have attracted increasing attention and exhibited impressive\"", "gold": "significantly", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 5.508, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.65683, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 1, "text_snippet": "evitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Al- though retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heav"}, {"rank": 2, "score": 0.6439647, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 83, "text_snippet": " in Table 6. The findings indicate that the self-correction mecha- nism incurs only modest computational overheadwhile significantly enhancing performance, thereby validating its lightweight nature. 6 Conclusion & Limitation This paper stud"}, {"rank": 3, "score": 0.6397511, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 83, "text_snippet": ".49 28.22 5 20.18 17.3728.2419.6518.7127.08 820.5217.60 28.17 16.87 18.05 25.36 10 19.67 17.41 27.62 15.72 17.42 23.60 6 Related Works Retrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi- tec"}, {"rank": 4, "score": 0.63748527, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 16, "text_snippet": "cient utilization of retrieved documents. 3) Experimental results extensively demonstrate CRAG ’s adaptability to RAG-based approaches and its generalizability across short- and long-form generation tasks. 2 Related Work Hallucinations of L"}, {"rank": 5, "score": 0.63539755, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 4, "text_snippet": "o substantial improvements over a conventional RAG baseline for both the comprehensiveness and diversity of generated answers. 1 Introduction Retrieval augmented generation (RAG) (Lewis et al., 2020) is an established approach to using LLMs"}]}
{"case_index": 172, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"We com- pare two RAG [MASK], one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token.\"", "gold": "formulations", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 5.779, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.642768, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 2, "score": 0.6258644, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 3, "text_snippet": "cessed with a pre-trained neural retriever. We com- pare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. We ﬁne-tune and "}, {"rank": 3, "score": 0.6214952, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 4, "score": 0.5622914, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 83, "text_snippet": ".49 28.22 5 20.18 17.3728.2419.6518.7127.08 820.5217.60 28.17 16.87 18.05 25.36 10 19.67 17.41 27.62 15.72 17.42 23.60 6 Related Works Retrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi- tec"}, {"rank": 5, "score": 0.557179, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 37, "text_snippet": " To cast language modeling in the text-to-text framework, we consider a chunk of Nwords, and split this chunk in two sub-sequences of equal length N/2. Then, the ﬁrst sub-sequence is used as the query, and the second corresponds to the outp"}]}
{"case_index": 173, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"By exploiting this attention sparsity structure, we demonstrate a30 .85×the [MASK] acceleration (3 .75×improvement to previous work) without loss in perplexity.\"", "gold": "time-to-first-token", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 5.235, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.63111544, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 3, "text_snippet": " on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30 .85×the time-t"}, {"rank": 2, "score": 0.5779767, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 12, "text_snippet": "is information is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-a"}, {"rank": 3, "score": 0.56711197, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 95, "text_snippet": "oducedREFRAG, a novel and efficient decoding framework tailored for RAG applications. By leveraging the inherent sparsity and block-diagonal attention patterns present in RAG contexts,REFRAG compresses, senses, and expands context represent"}, {"rank": 4, "score": 0.56379735, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 5, "score": 0.5627662, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 96, "text_snippet": "ong document summarization, demonstrate thatREFRAG achieves up to30 .85×TTFT acceleration (3 .75×over previous state-of-the-art methods) without any loss in perplexity or downstream accuracy. Our results highlight the importance of speciali"}]}
{"case_index": 174, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"As a result, LLM inference throughput degrades with larger contexts, limiting their [MASK] in scenarios demanding high throughput and low latency, such as web-scale discovery.\"", "gold": "applicability", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 7.788, "llm_ms": 0.008, "top_contexts": [{"rank": 1, "score": 0.6084625, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 3, "text_snippet": " on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30 .85×the time-t"}, {"rank": 2, "score": 0.59173495, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 0, "text_snippet": "REFRAG: Rethinking RAG based Decoding Xiaoqiang Lin1,2,∗,Aritra Ghosh1,Bryan Kian Hsiang Low2,Anshumali Shrivastava1,3,Vijai Mohan1 1Meta Superintelligence Labs,2National University of Singapore,3Rice University ∗Work done at Meta Large Lan"}, {"rank": 3, "score": 0.5781634, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 2, "text_snippet": "ated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns th"}, {"rank": 4, "score": 0.57622683, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 12, "text_snippet": "is information is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-a"}, {"rank": 5, "score": 0.57008964, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 94, "text_snippet": "Complementary approaches rank or prune context by estimated informativeness, e.g.,Selective Contextuses self-information to drop low-value tokens, and sentence-level methods learn context-aware encoders for question-specific compression and"}]}
{"case_index": 175, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"[MASK] passages in parallel and filters out irrelevant ones through self-reflection, without relying on external models at inference.\"", "gold": "self-ragprocesses", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 13.286, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.63341975, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 23, "text_snippet": "ess retrieved passages before using them to prompt the LM to generate the output. SELF-RAGprocesses passages in parallel and filters out irrelevant ones through self-reflection, without relying on external models at inference. Moreover, our"}, {"rank": 2, "score": 0.5959765, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 35, "text_snippet": "parallel and uses its own generated reflection tokens to enforce soft constraints (Section 3.3) or hard control (Algorithm 1) over the generated task output. For instance, in Figure 1 (right), the retrieved passages d1is selected at the fir"}, {"rank": 3, "score": 0.5851491, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 2, "text_snippet": "ew framework called Self-Reflective Retrieval-Augmented Gen- eration ( SELF-RAG)that enhances an LM’s quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passage"}, {"rank": 4, "score": 0.5838337, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 0, "text_snippet": "Preprint. SELF-RAG: LEARNING TO RETRIEVE , GENERATE ,AND CRITIQUE THROUGH SELF-REFLECTION Akari Asai†, Zeqiu Wu†, Yizhong Wang†§, Avirup Sil‡, Hannaneh Hajishirzi†§ †University of Washington§Allen Institute for AI‡IBM Research AI {akari,zeq"}, {"rank": 5, "score": 0.5765557, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 8, "text_snippet": "on demand (Step 1). Subsequently, SELF-RAGconcurrently processes multiple retrieved passages, evaluating their relevance and then generating corresponding task outputs (Step 2). It then generates critique tokens to criticize its own output "}]}
{"case_index": 176, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"[MASK] Generation (RAG)Ours: Self-reﬂective Retrieval-Augmented Genera\"", "gold": "retrieval-augmented", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 6.061, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.61771536, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 27, "text_snippet": "n (reasoning path consistency) and without retrieval. Recent work on LLM refinement (Dhuliawala et al., 2023; Madaan et al., 2023; Paul et al., 2023) prompts a model to generate task output, natural language feedback and refined task output"}, {"rank": 2, "score": 0.5912948, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 2, "text_snippet": "ew framework called Self-Reflective Retrieval-Augmented Gen- eration ( SELF-RAG)that enhances an LM’s quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passage"}, {"rank": 3, "score": 0.57785505, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 7, "text_snippet": "rmittent special tokens (i.e., reflection tokens ). Reflection tokens are categorized into retrieval andcritique tokens to indicate the need for retrieval and its generation quality respectively (Figure 1 right). In particular, given an inp"}, {"rank": 4, "score": 0.57476354, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 6, "text_snippet": "y trained to leverage and follow facts from provided passages. This work introduces Self-Reflective Retrieval-augmented Generation ( SELF-RAG)to improve an LLM’s generation quality, including its factual accuracy without hurting its versati"}, {"rank": 5, "score": 0.569278, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 0, "text_snippet": "Preprint. SELF-RAG: LEARNING TO RETRIEVE , GENERATE ,AND CRITIQUE THROUGH SELF-REFLECTION Akari Asai†, Zeqiu Wu†, Yizhong Wang†§, Avirup Sil‡, Hannaneh Hajishirzi†§ †University of Washington§Allen Institute for AI‡IBM Research AI {akari,zeq"}]}
{"case_index": 177, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"d as follows: the system gets a text query as input, and [MASK] a text output .\"", "gold": "generates", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 5.478, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.62177026, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 2, "score": 0.56040114, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 235, "text_snippet": "ull results for the cyclic-permutation-de-biased Atlas-XXL can be found in Table 18. A.2 Question answering A.2.1 Training Details For question answering, similarly to the MMLU experiments, we format the input using the following template: "}, {"rank": 3, "score": 0.55755556, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 4, "score": 0.54175866, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 37, "text_snippet": " To cast language modeling in the text-to-text framework, we consider a chunk of Nwords, and split this chunk in two sub-sequences of equal length N/2. Then, the ﬁrst sub-sequence is used as the query, and the second corresponds to the outp"}, {"rank": 5, "score": 0.52654564, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 83, "text_snippet": ".49 28.22 5 20.18 17.3728.2419.6518.7127.08 820.5217.60 28.17 16.87 18.05 25.36 10 19.67 17.41 27.62 15.72 17.42 23.60 6 Related Works Retrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi- tec"}]}
{"case_index": 178, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"The tree traversal method traverses the tree [MASK], pruning and selecting the most relevant nodes at each level.\"", "gold": "layer-by-layer", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 8.4, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.5458578, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 36, "text_snippet": "by RAPTOR: tree traversal and collapsed tree. These methods offer unique ways of traversing the multi-layered RAPTOR tree to retrieve relevant information, each with its own advantages and trade-offs. We provide the pseudocode of both metho"}, {"rank": 2, "score": 0.53580743, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 0, "text_snippet": "Published as a conference paper at ICLR 2024 RAPTOR: R ECURSIVE ABSTRACTIVE PROCESSING FOR TREE-ORGANIZED RETRIEVAL Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D. Manning Stanford University psarthi@cs."}, {"rank": 3, "score": 0.52912396, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 22, "text_snippet": " RAPTOR addresses the issue of semantic depth and connection in reading by building a recursive tree structure that balances broader thematic comprehension with granular details and which allows nodes to be grouped based on semantic sim- il"}, {"rank": 4, "score": 0.528913, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 26, "text_snippet": "d tree method evaluates nodes collectively across all layers to find the most relevant ones. Clustering Algorithm Clustering plays a key role in building the RAPTOR tree, organizing text segments into cohesive groups. This step groups relat"}, {"rank": 5, "score": 0.5258284, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 138, "text_snippet": "s that hallucination is not a major concerns for the summarization component in our RAPTOR architecture. F P SEUDOCODE FOR RETRIEVAL METHODS Algorithm 1 Tree Traversal Algorithm function TRAVERSE TREE(tree,query , k) Scurrent←tree.layer [0]"}]}
{"case_index": 179, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \", n }do 5: L ← ∅ 6: if Score (M,yi,[sj;xi]) + ϵ < Score (M,yi,[pi;xi])then 7: L ← L ∪ si 8: if|L|>0then 9: Ni←[MASK]5 sj∈L(⟨encθ(sj),encθ(\"", "gold": "argtop", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.796, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.64843, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 123, "text_snippet": "//huggingface.co/facebook/contriever 10https://huggingface.co/facebook/contriever-msmarco 15  Input: Base LM M, Compressor encoder encθ, Training data {xi,Si,yi}T 1where xiis input, Si={sj}n 1 is a set of candidate sentences from the retrie"}, {"rank": 2, "score": 0.64380896, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 25, "text_snippet": "b; Ram et al., 2023). Input: Base LM M, Compressor encθ, Training data {xi,Si,yi}T 1where xiis input, Si={sj}n 1is a set of candidate sentences from the retrieved documents for xi, yiis the target answer, and score threshold ϵ. Output: An u"}, {"rank": 3, "score": 0.6320499, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 124, "text_snippet": ": if Score (M,yi,[sj;xi])<Score (M,yi,[pi;xi])then 7: L ← L ∪ si 8: if|L|>0then 9: Ni←argTop5 sj∈L(⟨encθ(sj),encθ(xi)⟩) 10: T ← T ∪ { (xi,pi,Ni)} 11:encθ=Finetune (encθ,T) Figure 6: Learning an extractive compressor for QA task. The Score h"}, {"rank": 4, "score": 0.5924461, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 26, "text_snippet": " 7: L ← L ∪ si 8: if|L|>0then 9: Ni←argTop5 sj∈L(⟨encθ(sj),encθ(xi)⟩) 10: T ← T ∪ { (xi,pi,Ni)} 11:encθ=Finetune (encθ,T) Figure 2: Learning an extractive compressor for lan- guage modeling task.Model We train a dual-encoder model encθwhich"}, {"rank": 5, "score": 0.5838011, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 34, "text_snippet": "s the target answer. Output: An updated encdec θ 1:T ← ∅ 2:fori∈ {1, . . . , T }do 3: vr← −∞ 4: forj∈ {1, . . . , n }do 5: sj=Decode (Mt,[pj;xi;Di]) 6: vj=Score (M,yi,[sj;xi]) 7: ifvj> vrthen 8: st←sj,vr←vj 9: vd=Score (M,yi,[xi]) 10: ifvr<"}]}
{"case_index": 180, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"GMMs assume that data points are generated from a mixture of several Gaussian [MASK].\"", "gold": "distributions", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 22.321, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.5510582, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 33, "text_snippet": "e Gaussian assumption in GMMs may not perfectly align with the nature of text data, which often exhibits a sparse and skewed distribution, our empirical observations suggest that it offers an effective model for our purpose. We run an ablat"}, {"rank": 2, "score": 0.5239013, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 3, "score": 0.5144613, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 4, "score": 0.5107134, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 220, "text_snippet": "f the correct answer: [MASK_0] {correct answer option letter} This format closely matches the format of MLM pre-training objective, aiding few-shot learning. When training, we permute the order of the answer options, i.e. shuﬄing which answ"}, {"rank": 5, "score": 0.5001776, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 235, "text_snippet": "ull results for the cyclic-permutation-de-biased Atlas-XXL can be found in Table 18. A.2 Question answering A.2.1 Training Details For question answering, similarly to the MMLU experiments, we format the input using the following template: "}]}
{"case_index": 181, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"In this work, we de- sign criteria for evaluating [MASK] answers to global sensemaking questions and evaluate our results using the comparative approach.\"", "gold": "rag-generated", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 6.075, "llm_ms": 0.055, "top_contexts": [{"rank": 1, "score": 0.5671535, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 11, "text_snippet": "swers are combined and used to generate a final global answer. The GraphRAG method and its ability to perform global sensemaking over an entire corpus form the main contribution of this work. To demonstrate this ability, we developed a nove"}, {"rank": 2, "score": 0.56359094, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 0, "text_snippet": "From Local to Global: A GraphRAG Approach to Query-Focused Summarization Darren Edge1†Ha Trinh1†Newman Cheng2Joshua Bradley2Alex Chao3 Apurva Mody3Steven Truitt2Dasha Metropolitansky1Robert Osazuwa Ness1 Jonathan Larson1 1Microsoft Research"}, {"rank": 3, "score": 0.56283826, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 29, "text_snippet": "lements (nodes, edges, covariates) that the LLM can summarize in parallel at both indexing time and query time. The “global answer” to a given query is produced using a final round of query-focused summarization over all community summaries"}, {"rank": 4, "score": 0.55443126, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 97, "text_snippet": "omprehensive- ness and diversity of answers, as well as favorable comparisons to a global but graph-free approach using map-reduce source text summarization. For situations requiring many global queries over the same dataset, summaries of r"}, {"rank": 5, "score": 0.5444808, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 54, "text_snippet": "aluating Global Sensemaking Given the lack of gold standard answers to our activity-based sensemaking questions, we adopt the head-to-head comparison approach using an LLM evaluator that judges relative performance according to specific cri"}]}
{"case_index": 182, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"For [MASK] generation, we experiment with MS-MARCO [ 1] and Jeopardy question generation, and we ﬁnd that our models generate responses that are more factual, speciﬁc, and diverse than a BART baseline.\"", "gold": "knowledge-intensive", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 5.577, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6496532, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 14, "text_snippet": "active approaches. For knowledge-intensive generation, we experiment with MS-MARCO [ 1] and Jeopardy question generation, and we ﬁnd that our models generate responses that are more factual, speciﬁc, and diverse than a BART baseline. For FE"}, {"rank": 2, "score": 0.6400976, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 3, "score": 0.639039, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 4, "score": 0.60129416, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 83, "text_snippet": ".49 28.22 5 20.18 17.3728.2419.6518.7127.08 820.5217.60 28.17 16.87 18.05 25.36 10 19.67 17.41 27.62 15.72 17.42 23.60 6 Related Works Retrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi- tec"}, {"rank": 5, "score": 0.5850489, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 37, "text_snippet": "(line 9). This allows for selective augmentation and mitigates the risk of prepending irrelevant documents. 5https://huggingface.co/facebook/contriever-msmarco 6The exact prompts can be found in Table 6 in A.2. 7We use gpt-3.5-turbo in all "}]}
{"case_index": 183, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"Specifically, SELF-RAG outperforms ChatGPT and [MASK] Llama2-chat on Open-domain QA, reasoning and fact verification tasks, and it shows significant gains in improving factuality and citation accuracy for long-form generations relative to these models.1 1 I NTRODU\"", "gold": "retrieval-augmented", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 18.904, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.69564, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 3, "text_snippet": "e phase, enabling it to tailor its behavior to diverse task requirements. Experiments show that SELF- RAG(7B and 13B parameters) significantly outperforms state-of-the-art LLMs and retrieval-augmented models on a diverse set of tasks. Speci"}, {"rank": 2, "score": 0.6721777, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 77, "text_snippet": "s except ChatGPT. Gao et al. (2023) found that ChatGPT consistently exhibits superior efficacy in this particular task, surpassing smaller LMs. Our SELF-RAGbridges this performance gap, even outperforming ChatGPT in citation precision, whic"}, {"rank": 3, "score": 0.63334954, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 27, "text_snippet": "n (reasoning path consistency) and without retrieval. Recent work on LLM refinement (Dhuliawala et al., 2023; Madaan et al., 2023; Paul et al., 2023) prompts a model to generate task output, natural language feedback and refined task output"}, {"rank": 4, "score": 0.63236433, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 171, "text_snippet": " the correct predictions are not included in the provided passages, followed by Llama2-chat 13B (18%) and Alpaca (15%), while it is only 2% in SELF-RAG. When retrieved passages are not relevant, SELF-RAGgenerates ISREL=Irrelevant , indicati"}, {"rank": 5, "score": 0.6277512, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 0, "text_snippet": "Preprint. SELF-RAG: LEARNING TO RETRIEVE , GENERATE ,AND CRITIQUE THROUGH SELF-REFLECTION Akari Asai†, Zeqiu Wu†, Yizhong Wang†§, Avirup Sil‡, Hannaneh Hajishirzi†§ †University of Washington§Allen Institute for AI‡IBM Research AI {akari,zeq"}]}
{"case_index": 184, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"xact Match NaturalQuestions 5 10 25 50 100 Number of passages54565860626466 TriviaQA 5 10 25 50 100 Number of passages343638404244464850 SQuADFigure 3: Performance of [MASK] (base) on valid sets as a function of the number of retrieved passages.\"", "gold": "fusion-in-decoder", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.086, "llm_ms": 0.004, "top_contexts": [{"rank": 1, "score": 0.6691066, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 2, "score": 0.65789485, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 25, "text_snippet": "xact Match NaturalQuestions 5 10 25 50 100 Number of passages54565860626466 TriviaQA 5 10 25 50 100 Number of passages343638404244464850 SQuADFigure 3: Performance of Fusion-in-Decoder (base) on valid sets as a function of the number of ret"}, {"rank": 3, "score": 0.6334141, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 235, "text_snippet": "ull results for the cyclic-permutation-de-biased Atlas-XXL can be found in Table 18. A.2 Question answering A.2.1 Training Details For question answering, similarly to the MMLU experiments, we format the input using the following template: "}, {"rank": 4, "score": 0.6325056, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 33, "text_snippet": "es are competitive for knowl- edge retrieval tasks. Scaling with number of passages. In Figure 3, we report the performance with respect to the  NaturalQuestions TriviaQA Training Passages w/o ﬁnetuning w/ ﬁnetuning w/o ﬁnetuning w/ ﬁnetuni"}, {"rank": 5, "score": 0.6197941, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}]}
{"case_index": 185, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"Our main contribution is the idea of using text [MASK] to allow retrieval augmentation of context at different scales, and to show its effectiveness in experiments on collections of long doc- uments.\"", "gold": "summarization", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 7.683, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.577289, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 0, "text_snippet": "Published as a conference paper at ICLR 2024 RAPTOR: R ECURSIVE ABSTRACTIVE PROCESSING FOR TREE-ORGANIZED RETRIEVAL Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D. Manning Stanford University psarthi@cs."}, {"rank": 2, "score": 0.5752645, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 22, "text_snippet": " RAPTOR addresses the issue of semantic depth and connection in reading by building a recursive tree structure that balances broader thematic comprehension with granular details and which allows nodes to be grouped based on semantic sim- il"}, {"rank": 3, "score": 0.566927, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 4, "score": 0.5624964, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 5, "score": 0.5542837, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 8, "text_snippet": " indexing and retrieval system that uses a tree structure to capture both high-level and low-level details about a text. As shown in Figure 1, our system, RAPTOR, clusters chunks of text, generates text summaries of those clusters, and then"}]}
{"case_index": 186, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"In addition, in some [MASK] generation tasks, external knowl- edge is needed more than once, and when to retrieve should be concerned.\"", "gold": "long-text", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 14.443, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.5547782, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 1, "text_snippet": "evitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Al- though retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heav"}, {"rank": 2, "score": 0.54365796, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 83, "text_snippet": " in Table 6. The findings indicate that the self-correction mecha- nism incurs only modest computational overheadwhile significantly enhancing performance, thereby validating its lightweight nature. 6 Conclusion & Limitation This paper stud"}, {"rank": 3, "score": 0.5225446, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 18, "text_snippet": "pact the LLMs and exacerbate the problems. Thus, it is obvious that the lack of accurate and specific knowledge can lead to misleading or even inaccurate generation, which will severely hurt the experience of users in most practical applica"}, {"rank": 4, "score": 0.51278764, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 5, "score": 0.5115264, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 83, "text_snippet": ".49 28.22 5 20.18 17.3728.2419.6518.7127.08 820.5217.60 28.17 16.87 18.05 25.36 10 19.67 17.41 27.62 15.72 17.42 23.60 6 Related Works Retrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi- tec"}]}
{"case_index": 187, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"(2018) introduced a supervised [MASK] to rerank paragraphs based on BiLSTM, while Wang et al.\"", "gold": "learningmethod", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 5.372, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.63604504, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 2, "score": 0.62367123, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 12, "text_snippet": "018b) described a technique to aggregate answers from different paragraphs, using conﬁdence and coverage scores. Passage retrieval is an important step in open domain question answering, and is an active area of research to improve QA syste"}, {"rank": 3, "score": 0.6199429, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}, {"rank": 4, "score": 0.6185967, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 86, "text_snippet": "models for open domain question an- swering. In Proceedings of the 16th Conference of the European Chapter of the Association for Compu- tational Linguistics: Main Volume , pp. 874–880, On- line, April 2021a. Association for Computational L"}, {"rank": 5, "score": 0.6119948, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 109, "text_snippet": "ng. InInternational conference on machine learning, pages 3929–3938. PMLR, 2020. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. InPr"}]}
{"case_index": 188, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"The objective is to align any encoder–decoder combination so that the generations produced [MASK] contextclosely resemble those generated by the original decoder with access to the full context.\"", "gold": "withcompressed", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 5.763, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.56087935, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 12, "text_snippet": "is information is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-a"}, {"rank": 2, "score": 0.5545889, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 94, "text_snippet": "Complementary approaches rank or prune context by estimated informativeness, e.g.,Selective Contextuses self-information to drop low-value tokens, and sentence-level methods learn context-aware encoders for question-specific compression and"}, {"rank": 3, "score": 0.53955597, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 95, "text_snippet": "oducedREFRAG, a novel and efficient decoding framework tailored for RAG applications. By leveraging the inherent sparsity and block-diagonal attention patterns present in RAG contexts,REFRAG compresses, senses, and expands context represent"}, {"rank": 4, "score": 0.53480285, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 26, "text_snippet": "e detailed discussion 1REFRAGwithout cache means that we recompute the chunk embedding for the context and take this latency into account. 3  on empirical evaluation is in section A. 3 Methodology To align the encoder and decoder, we follow"}, {"rank": 5, "score": 0.53474826, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 3, "text_snippet": " on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30 .85×the time-t"}]}
{"case_index": 189, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"Our analysis demonstrates the effectiveness of training and inference with reflection tokens for overall performance improvements as well as test-time model [MASK] (e.g., balancing the trade-off between citation previsions and completeness).\"", "gold": "customizations", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.474, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.62926745, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 18, "text_snippet": "at (Touvron et al., 2023) and Alpaca (Dubois et al., 2023) on all tasks. Our analysis demonstrates the effectiveness of training and inference with reflection tokens for overall performance improvements as well as test-time model customizat"}, {"rank": 2, "score": 0.623855, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 36, "text_snippet": "text with reflection tokens by unifying them as next token predictions from the expanded model vocabulary (i.e., the original vocabulary plus reflection tokens). Specifically, we train the generator model Mon a curated corpus with interleav"}, {"rank": 3, "score": 0.6163937, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 48, "text_snippet": "gmented with reflection tokens Dgenusing the standard next token objective: max ME(x,y,r )∼DgenlogpM(y, r|x). (2) Unlike Ctraining (Eq. 1), Mlearns to predict the target output as well as the reflection tokens. During training, we mask out "}, {"rank": 4, "score": 0.6162487, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 35, "text_snippet": "parallel and uses its own generated reflection tokens to enforce soft constraints (Section 3.3) or hard control (Algorithm 1) over the generated task output. For instance, in Figure 1 (right), the retrieved passages d1is selected at the fir"}, {"rank": 5, "score": 0.6082156, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 0, "text_snippet": "Preprint. SELF-RAG: LEARNING TO RETRIEVE , GENERATE ,AND CRITIQUE THROUGH SELF-REFLECTION Akari Asai†, Zeqiu Wu†, Yizhong Wang†§, Avirup Sil‡, Hannaneh Hajishirzi†§ †University of Washington§Allen Institute for AI‡IBM Research AI {akari,zeq"}]}
{"case_index": 190, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"However, most existing methods target generic LLM tasks with long context and are largely [MASK] to our work.\"", "gold": "orthogonal", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 5.59, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.61185753, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 0, "text_snippet": "REFRAG: Rethinking RAG based Decoding Xiaoqiang Lin1,2,∗,Aritra Ghosh1,Bryan Kian Hsiang Low2,Anshumali Shrivastava1,3,Vijai Mohan1 1Meta Superintelligence Labs,2National University of Singapore,3Rice University ∗Work done at Meta Large Lan"}, {"rank": 2, "score": 0.60019433, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 3, "text_snippet": " on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30 .85×the time-t"}, {"rank": 3, "score": 0.58838767, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 2, "text_snippet": "ated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns th"}, {"rank": 4, "score": 0.58403516, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 12, "text_snippet": "is information is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-a"}, {"rank": 5, "score": 0.576196, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 94, "text_snippet": "Complementary approaches rank or prune context by estimated informativeness, e.g.,Selective Contextuses self-information to drop low-value tokens, and sentence-level methods learn context-aware encoders for question-specific compression and"}]}
{"case_index": 191, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"This is exactly the focus of this paper to improve the [MASK] of generation.\"", "gold": "robustness", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 4.726, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.5246989, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 83, "text_snippet": " in Table 6. The findings indicate that the self-correction mecha- nism incurs only modest computational overheadwhile significantly enhancing performance, thereby validating its lightweight nature. 6 Conclusion & Limitation This paper stud"}, {"rank": 2, "score": 0.5214999, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 1, "text_snippet": "evitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Al- though retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heav"}, {"rank": 3, "score": 0.5135981, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 83, "text_snippet": ".49 28.22 5 20.18 17.3728.2419.6518.7127.08 820.5217.60 28.17 16.87 18.05 25.36 10 19.67 17.41 27.62 15.72 17.42 23.60 6 Related Works Retrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi- tec"}, {"rank": 4, "score": 0.5109257, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 5, "score": 0.4989445, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 18, "text_snippet": "pact the LLMs and exacerbate the problems. Thus, it is obvious that the lack of accurate and specific knowledge can lead to misleading or even inaccurate generation, which will severely hurt the experience of users in most practical applica"}]}
{"case_index": 192, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"2.3 Adaptive benchmarking for RAG Evaluation Many benchmark datasets for open-domain question answering exist, including HotPotQA (Yang et al., 2018), [MASK] (Tang and Yang, 2024), and MT-Bench (Zheng et al., 2024).\"", "gold": "multihop-rag", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.237, "llm_ms": 0.004, "top_contexts": [{"rank": 1, "score": 0.68524575, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 22, "text_snippet": "gly global summaries by using the LLM to create summaries spanning this community hierarchy. 2.3 Adaptive benchmarking for RAG Evaluation Many benchmark datasets for open-domain question answering exist, including HotPotQA (Yang et al., 201"}, {"rank": 2, "score": 0.6131412, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 3, "score": 0.58936, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 0, "text_snippet": "From Local to Global: A GraphRAG Approach to Query-Focused Summarization Darren Edge1†Ha Trinh1†Newman Cheng2Joshua Bradley2Alex Chao3 Apurva Mody3Steven Truitt2Dasha Metropolitansky1Robert Osazuwa Ness1 Jonathan Larson1 1Microsoft Research"}, {"rank": 4, "score": 0.5730291, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 46, "text_snippet": "2 40.1 41.572.5 89.5RAG-Seq. 14.7 21.4 40.8 44.2 to more effective marginalization over documents. Furthermore, RAG can generate correct answers even when the correct answer is not in any retrieved document, achieving 11.8% accuracy in such"}, {"rank": 5, "score": 0.57246614, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 27, "text_snippet": "ns (Wang et al., 2023a; Zheng et al., 2024). Some prior work proposes criteria for having LLMs quantify the quality of 3  Source Documents Text Chunkstext extraction and chunking Entities & Relationshipsdomain-tailored summarization Knowled"}]}
{"case_index": 193, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"This model consists of a [MASK] foundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu et al., 2019)).\"", "gold": "decoder-only", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.685, "llm_ms": 0.004, "top_contexts": [{"rank": 1, "score": 0.6223365, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 18, "text_snippet": "f question and retrieval in this section. Model overview.Figure 1 shows the main architecture ofREFRAG. This model consists of a decoder-only foundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Rober"}, {"rank": 2, "score": 0.5935016, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 17, "text_snippet": "FRAG achieves better performance than LLaMA without incurring higher latency in the downstream applications. 2 Model Architecture We denote the decoder model as Mdecand the encoder model as Menc. Given an input with Ttokens x1, x2, . . . , "}, {"rank": 3, "score": 0.5931693, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 59, "text_snippet": "decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B as decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance varies with different encoder and decoder sizes. Figure 11 presents results for"}, {"rank": 4, "score": 0.57038665, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 0, "text_snippet": "REFRAG: Rethinking RAG based Decoding Xiaoqiang Lin1,2,∗,Aritra Ghosh1,Bryan Kian Hsiang Low2,Anshumali Shrivastava1,3,Vijai Mohan1 1Meta Superintelligence Labs,2National University of Singapore,3Rice University ∗Work done at Meta Large Lan"}, {"rank": 5, "score": 0.56770426, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}]}
{"case_index": 194, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"for long-form generations relative to these models.1 1 I NTRODUCTION [MASK] LLMs continue to struggle with factual errors (Mallen et al., 2023; Min et al., 2023) despite their increased model and data scale (Ouyang et al., 2022).\"", "gold": "state-of-the-art", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.339, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.67462647, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 4, "text_snippet": " for long-form generations relative to these models.1 1 I NTRODUCTION State-of-the-art LLMs continue to struggle with factual errors (Mallen et al., 2023; Min et al., 2023) despite their increased model and data scale (Ouyang et al., 2022)."}, {"rank": 2, "score": 0.6080631, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 0, "text_snippet": "Preprint. SELF-RAG: LEARNING TO RETRIEVE , GENERATE ,AND CRITIQUE THROUGH SELF-REFLECTION Akari Asai†, Zeqiu Wu†, Yizhong Wang†§, Avirup Sil‡, Hannaneh Hajishirzi†§ †University of Washington§Allen Institute for AI‡IBM Research AI {akari,zeq"}, {"rank": 3, "score": 0.588389, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 27, "text_snippet": "n (reasoning path consistency) and without retrieval. Recent work on LLM refinement (Dhuliawala et al., 2023; Madaan et al., 2023; Paul et al., 2023) prompts a model to generate task output, natural language feedback and refined task output"}, {"rank": 4, "score": 0.5709108, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 6, "text_snippet": "y trained to leverage and follow facts from provided passages. This work introduces Self-Reflective Retrieval-augmented Generation ( SELF-RAG)to improve an LLM’s generation quality, including its factual accuracy without hurting its versati"}, {"rank": 5, "score": 0.5709046, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 2, "text_snippet": "ew framework called Self-Reflective Retrieval-Augmented Gen- eration ( SELF-RAG)that enhances an LM’s quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passage"}]}
{"case_index": 195, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"cted communities, with summaries at higher levels of the hierarchy recursively [MASK] lower-level summaries.\"", "gold": "incorporating", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 5.444, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6302806, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 10, "text_snippet": "cted communities, with summaries at higher levels of the hierarchy recursively incorporating lower-level summaries. Together, these community summaries provide global descriptions and insights over the corpus. Finally, GraphRAG answers quer"}, {"rank": 2, "score": 0.61867726, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 0, "text_snippet": "From Local to Global: A GraphRAG Approach to Query-Focused Summarization Darren Edge1†Ha Trinh1†Newman Cheng2Joshua Bradley2Alex Chao3 Apurva Mody3Steven Truitt2Dasha Metropolitansky1Robert Osazuwa Ness1 Jonathan Larson1 1Microsoft Research"}, {"rank": 3, "score": 0.5962726, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 97, "text_snippet": "omprehensive- ness and diversity of answers, as well as favorable comparisons to a global but graph-free approach using map-reduce source text summarization. For situations requiring many global queries over the same dataset, summaries of r"}, {"rank": 4, "score": 0.5816221, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 47, "text_snippet": "t within the context window. 3.1.6 Community Summaries →Community Answers →Global Answer Given a user query, the community summaries generated in the previous step can be used to generate a final answer in a multi-stage process. The hierarc"}, {"rank": 5, "score": 0.5757029, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 35, "text_snippet": "for summarization in Appendix D. While the summarization model generally produces reliable summaries, a focused annotation study revealed that about 4% of the summaries contained minor hallucinations. These did not propagate to parent nodes"}]}
{"case_index": 196, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"odels in the output should be faithful to the original input, yet the main goal is [MASK].\"", "gold": "different", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.25, "hit_rank": 4, "matched_by": "doc_id+chunk_id", "retrieval_ms": 5.212, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.59258866, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 2, "score": 0.5731095, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 0, "text_snippet": "RECOMP: I MPROVING RETRIEVAL -AUGMENTED LM S WITH COMPRESSION AND SELECTIVE AUGMENTATION Fangyuan Xu1, Weijia Shi2, Eunsol Choi1 Department of Computer Science 1The University of Texas at Austin 2University of Washington {fangyuan,eunsol }@"}, {"rank": 3, "score": 0.546823, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 82, "text_snippet": " and allowing the compression models to perform selective augmentation. Our experiments show that our compressors can improve the efficiency of retrieval augmented LMs significantly with minimal drop in performances. ACKNOWLEDGEMENT We than"}, {"rank": 4, "score": 0.545116, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 23, "text_snippet": "odels in the output should be faithful to the original input, yet the main goal is different. Instead of capturing salient information for humans readers, compressors aim to produce a concise text that are useful for a LM on an end task. In"}, {"rank": 5, "score": 0.5442289, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 135, "text_snippet": "arXiv preprint arXiv:2305.00633 , 2023. URL https://arxiv.org/abs/2305.00633 . Fangyuan Xu, Weijia Shi, and Eunsol Choi. Recomp: Improving retrieval-augmented lms with compression and selective augmentation, 2023. URL https://arxiv.org/abs/"}]}
{"case_index": 197, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"We argue that specialized techniques exploiting the unique structure and sparsity inherent in RAG contexts can [MASK] reduce memory and computational overhead.\"", "gold": "substantially", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 12.686, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.67648333, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 95, "text_snippet": "oducedREFRAG, a novel and efficient decoding framework tailored for RAG applications. By leveraging the inherent sparsity and block-diagonal attention patterns present in RAG contexts,REFRAG compresses, senses, and expands context represent"}, {"rank": 2, "score": 0.66012454, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 12, "text_snippet": "is information is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-a"}, {"rank": 3, "score": 0.65685976, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 3, "text_snippet": " on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30 .85×the time-t"}, {"rank": 4, "score": 0.6440985, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 2, "text_snippet": "ated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns th"}, {"rank": 5, "score": 0.64067274, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 0, "text_snippet": "REFRAG: Rethinking RAG based Decoding Xiaoqiang Lin1,2,∗,Aritra Ghosh1,Bryan Kian Hsiang Low2,Anshumali Shrivastava1,3,Vijai Mohan1 1Meta Superintelligence Labs,2National University of Singapore,3Rice University ∗Work done at Meta Large Lan"}]}
{"case_index": 198, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"[MASK] For RAG-Sequence, the likelihood p(y|x)does not break into a conventional per- token likelihood, hence we cannot solve it with a single beam search.\"", "gold": "rag-sequence", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.3333333333333333, "hit_rank": 3, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.03, "llm_ms": 0.004, "top_contexts": [{"rank": 1, "score": 0.59474754, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 2, "score": 0.5676261, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 3, "score": 0.55373704, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 26, "text_snippet": " arg maxyp(y|x). RAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera- tor with transition probability: p′ θ(yi|x,y 1:i−1) =∑ z∈top-k(p(·|x))pη(zi|x)pθ(yi|x,zi,y1:i−1)To decode, we can plug p′ θ(yi|x,y 1:i−"}, {"rank": 4, "score": 0.53996485, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 3, "text_snippet": "cessed with a pre-trained neural retriever. We com- pare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. We ﬁne-tune and "}, {"rank": 5, "score": 0.53653693, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 37, "text_snippet": " To cast language modeling in the text-to-text framework, we consider a chunk of Nwords, and split this chunk in two sub-sequences of equal length N/2. Then, the ﬁrst sub-sequence is used as the query, and the second corresponds to the outp"}]}
{"case_index": 199, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"For language modelling, both trained compressors achieve a compression ratio of 25% with minimal [MASK] drop.\"", "gold": "performance", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.25, "hit_rank": 4, "matched_by": "doc_id+chunk_id", "retrieval_ms": 5.861, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6518116, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 82, "text_snippet": " and allowing the compression models to perform selective augmentation. Our experiments show that our compressors can improve the efficiency of retrieval augmented LMs significantly with minimal drop in performances. ACKNOWLEDGEMENT We than"}, {"rank": 2, "score": 0.6164163, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 0, "text_snippet": "RECOMP: I MPROVING RETRIEVAL -AUGMENTED LM S WITH COMPRESSION AND SELECTIVE AUGMENTATION Fangyuan Xu1, Weijia Shi2, Eunsol Choi1 Department of Computer Science 1The University of Texas at Austin 2University of Washington {fangyuan,eunsol }@"}, {"rank": 3, "score": 0.6106553, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 23, "text_snippet": "odels in the output should be faithful to the original input, yet the main goal is different. Instead of capturing salient information for humans readers, compressors aim to produce a concise text that are useful for a LM on an end task. In"}, {"rank": 4, "score": 0.60701704, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 15, "text_snippet": "6% and significantly outperforms prepending full documents. Our trained compressors also show promising results. For language modelling, both trained compressors achieve a compression ratio of 25% with minimal performance drop. When applied"}, {"rank": 5, "score": 0.6025435, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 3, "text_snippet": "n. We evaluate our approach on language modeling task and open domain question answering task. We achieve a compression rate of as low as 6% with minimal loss in performance for both tasks, significantly outperforming the off-the-shelf summ"}]}
{"case_index": 200, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"Despite this, the methods above usually ignore a [MASK], what if the retrieval goes wrong?\"", "gold": "question", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 6.024, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.5315308, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 1, "text_snippet": "evitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Al- though retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heav"}, {"rank": 2, "score": 0.526651, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 83, "text_snippet": " in Table 6. The findings indicate that the self-correction mecha- nism incurs only modest computational overheadwhile significantly enhancing performance, thereby validating its lightweight nature. 6 Conclusion & Limitation This paper stud"}, {"rank": 3, "score": 0.5230768, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 4, "score": 0.50941753, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 5, "score": 0.50068384, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 56, "text_snippet": " the reader and the retriever by leveraging the perplexity of the output generated by the reader. Sachan et al. (2021) and Lee et al. (2021a) both employ salient span masking to pre-train retrievers, leveraging the perplexity and attention "}]}
{"case_index": 201, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"[MASK] nodes thus storing varying levels of detail, keeping granular details.\"", "gold": "ntermediate", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.887, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.5309184, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 22, "text_snippet": " RAPTOR addresses the issue of semantic depth and connection in reading by building a recursive tree structure that balances broader thematic comprehension with granular details and which allows nodes to be grouped based on semantic sim- il"}, {"rank": 2, "score": 0.5231109, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 21, "text_snippet": "ntermediate nodes thus storing varying levels of detail, keeping granular details. However, both methods, due to their reliance on adjacency for grouping or summarizing adjacent nodes, may still overlook distant interdependencies within the"}, {"rank": 3, "score": 0.5200803, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 4, "score": 0.5197423, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 8, "text_snippet": " indexing and retrieval system that uses a tree structure to capture both high-level and low-level details about a text. As shown in Figure 1, our system, RAPTOR, clusters chunks of text, generates text summaries of those clusters, and then"}, {"rank": 5, "score": 0.51813877, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}]}
{"case_index": 202, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"Like T5 [ 51] or BART, RAG can be ﬁne-tuned on any seq2seq task, whereby both the generator and [MASK] are jointly learned.\"", "gold": "retriever", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 8.895, "llm_ms": 0.008, "top_contexts": [{"rank": 1, "score": 0.6134917, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 2, "score": 0.60138094, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 3, "score": 0.5708058, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 83, "text_snippet": ".49 28.22 5 20.18 17.3728.2419.6518.7127.08 820.5217.60 28.17 16.87 18.05 25.36 10 19.67 17.41 27.62 15.72 17.42 23.60 6 Related Works Retrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi- tec"}, {"rank": 4, "score": 0.5548763, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 24, "text_snippet": "tasks and outperforms comparably-sized T5 models [32]. We refer to the BART generator parameters θas the parametric memory henceforth. 2.4 Training We jointly train the retriever and generator components without any direct supervision on wh"}, {"rank": 5, "score": 0.5481458, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 3, "text_snippet": "cessed with a pre-trained neural retriever. We com- pare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. We ﬁne-tune and "}]}
{"case_index": 203, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"Atlasretrieves relevant documents based on the current context by using a [MASK] dense retriever using a dual-encoder architecture, based on the Contriever (Izacard et al., 2022).\"", "gold": "general-purpose", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.217, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.63765943, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 9, "text_snippet": " strong few-shot learning, despite having lower parameter counts than other powerful recent few-shot learners. Atlasretrieves relevant documents based on the current context by using a general-purpose dense retriever using a dual-encoder ar"}, {"rank": 2, "score": 0.63254005, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 111, "text_snippet": " arXiv:2208.03299, 2022. https://arxiv.org/abs/ 2208.03299. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Edouard Grave, and Sebastian Riedel. Atlas: Few-shot learn"}, {"rank": 3, "score": 0.6137971, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 2, "text_snippet": "ings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including"}, {"rank": 4, "score": 0.61318886, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 5, "score": 0.5841283, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 56, "text_snippet": " the reader and the retriever by leveraging the perplexity of the output generated by the reader. Sachan et al. (2021) and Lee et al. (2021a) both employ salient span masking to pre-train retrievers, leveraging the perplexity and attention "}]}
{"case_index": 204, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"ntributions: •A thorough study on how to design and train [MASK] language models, with a focus on downstream few-shot learning and sample eﬃciency.\"", "gold": "retrieval-augmented", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.347, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.6624579, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 12, "text_snippet": "ntributions: •A thorough study on how to design and train retrieval-augmented language models, with a focus on downstream few-shot learning and sample eﬃciency. •The ﬁndings of this study lead to a retrieval-augmented language model, called"}, {"rank": 2, "score": 0.62647897, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 3, "score": 0.62564427, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 111, "text_snippet": " arXiv:2208.03299, 2022. https://arxiv.org/abs/ 2208.03299. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Edouard Grave, and Sebastian Riedel. Atlas: Few-shot learn"}, {"rank": 4, "score": 0.60064423, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 0, "text_snippet": "Atlas: Few-shot Learning with Retrieval Augmented Language Models Gautier Izacard∗ ∗,♦,♣,♥gizacard@fb.com Patrick Lewis∗,♦plewis@fb.com Maria Lomeli♦marialomeli@fb.com Lucas Hosseini♦hoss@fb.com Fabio Petroni♦fabiopetroni@fb.com Timo Schick"}, {"rank": 5, "score": 0.5906112, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 110, "text_snippet": "s of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 874–880, Online, April 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.eacl-main.74. https://aclant"}]}
{"case_index": 205, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"Clark and Gardner (2018) proposed to use a global [MASK] over all the span corresponding to the answer, which was later applied to BERT based models (Wang et al., 2019).\"", "gold": "normalization", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 8.485, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.65066946, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 86, "text_snippet": "models for open domain question an- swering. In Proceedings of the 16th Conference of the European Chapter of the Association for Compu- tational Linguistics: Main Volume , pp. 874–880, On- line, April 2021a. Association for Computational L"}, {"rank": 2, "score": 0.63221145, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 0, "text_snippet": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering Gautier Izacard1,2,3Edouard Grave1 1Facebook AI Research, Paris 2ENS, PSL University, Paris 3Inria, Paris gizacard|egrave@fb.com Abstract Generative mode"}, {"rank": 3, "score": 0.63045794, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}, {"rank": 4, "score": 0.62763864, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 5, "score": 0.6169785, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 109, "text_snippet": "ng. InInternational conference on machine learning, pages 3929–3938. PMLR, 2020. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. InPr"}]}
{"case_index": 206, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"Furthermore, current methods mostly treat complete documents as reference knowledge both during retrieval and [MASK].\"", "gold": "utilization", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 11.288, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.5626835, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 1, "text_snippet": "evitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Al- though retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heav"}, {"rank": 2, "score": 0.56065756, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 3, "score": 0.5388453, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 4, "score": 0.53644884, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 18, "text_snippet": "pact the LLMs and exacerbate the problems. Thus, it is obvious that the lack of accurate and specific knowledge can lead to misleading or even inaccurate generation, which will severely hurt the experience of users in most practical applica"}, {"rank": 5, "score": 0.5306797, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 83, "text_snippet": " in Table 6. The findings indicate that the self-correction mecha- nism incurs only modest computational overheadwhile significantly enhancing performance, thereby validating its lightweight nature. 6 Conclusion & Limitation This paper stud"}]}
{"case_index": 207, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"Unlike prior methods (Yen et al., 2024),[MASK] compression of token chunks at arbitrary positions (see figure 1) while preserving the autoregressive nature of the decod\"", "gold": "refragsupports", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 11.095, "llm_ms": 0.008, "top_contexts": [{"rank": 1, "score": 0.5910366, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 14, "text_snippet": "ortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now "}, {"rank": 2, "score": 0.56312007, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 12, "text_snippet": "is information is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-a"}, {"rank": 3, "score": 0.56245494, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 15, "text_snippet": "igure 1) while preserving the autoregressive nature of the decoder, thereby supporting multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight reinforcement learning (RL) policy that sel"}, {"rank": 4, "score": 0.5542595, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 13, "text_snippet": "nd memory usage during decoding, allwithout requiring modificationsto the LLM architecture or introducing new decoder parameters. REFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved passag"}, {"rank": 5, "score": 0.5489421, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 26, "text_snippet": "e detailed discussion 1REFRAGwithout cache means that we recompute the chunk embedding for the context and take this latency into account. 3  on empirical evaluation is in section A. 3 Methodology To align the encoder and decoder, we follow"}]}
{"case_index": 208, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"sations (Roller et al., 2021; Zhang et al., 2020), [MASK] historical dialogue into the context enables LLMs to respond more effectively to user queries.\"", "gold": "incorporating", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 37.393, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.56912136, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 0, "text_snippet": "REFRAG: Rethinking RAG based Decoding Xiaoqiang Lin1,2,∗,Aritra Ghosh1,Bryan Kian Hsiang Low2,Anshumali Shrivastava1,3,Vijai Mohan1 1Meta Superintelligence Labs,2National University of Singapore,3Rice University ∗Work done at Meta Large Lan"}, {"rank": 2, "score": 0.5473851, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 6, "text_snippet": "sations (Roller et al., 2021; Zhang et al., 2020), incorporating historical dialogue into the context enables LLMs to respond more effectively to user queries. In retrieval-augmented generation (RAG) (Guu et al., 2020; Izacard et al., 2022)"}, {"rank": 3, "score": 0.52295417, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 2, "text_snippet": "ated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns th"}, {"rank": 4, "score": 0.5212207, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 5, "text_snippet": "l., 2024; Liu et al., 2023). In the canonical RAG setup, the system has access to a large external corpus of text records and retrieves a subset of records that are individually relevant to the query and collectively small enough to fit int"}, {"rank": 5, "score": 0.5198177, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 17, "text_snippet": "FRAG achieves better performance than LLaMA without incurring higher latency in the downstream applications. 2 Model Architecture We denote the decoder model as Mdecand the encoder model as Menc. Given an input with Ttokens x1, x2, . . . , "}]}
{"case_index": 209, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"(3) Faithful :sshould be a faithful and [MASK] summary of the input document set (i.e., smust be entailed by the input document set ( [d1, d2, ...dN])).\"", "gold": "interpretable", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 5.626, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.59807444, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 18, "text_snippet": "cy. If the retrieved documents do not contain relevant information or retrieval augmentation is not necessary, scan be an empty sequence. (2) Effecive : when sis prepended to input sequence xand provided to LM Mas a prompt, LM should genera"}, {"rank": 2, "score": 0.55714786, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 0, "text_snippet": "RECOMP: I MPROVING RETRIEVAL -AUGMENTED LM S WITH COMPRESSION AND SELECTIVE AUGMENTATION Fangyuan Xu1, Weijia Shi2, Eunsol Choi1 Department of Computer Science 1The University of Texas at Austin 2University of Washington {fangyuan,eunsol }@"}, {"rank": 3, "score": 0.5486604, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 4, "score": 0.5315213, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 5, "score": 0.52551615, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 16, "text_snippet": "oth its strength and weaknesses, thereby building foundation for future work. 2 P ROBLEM FORMULATION :RECOMP Given an input sequence x, a target output sequence yand a set of Nretrieved documents D ([d1, d2, ...dN]),2RECOMP compresses retri"}]}
{"case_index": 210, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"Our algorithm varies [MASK] to create a hierar- chical clustering structure: it first identifies global clusters and then performs local clustering within these global clusters.\"", "gold": "nneighbors", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.735, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.58534265, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 30, "text_snippet": "he preservation of local and global structures. Our algorithm varies nneighbors to create a hierar- chical clustering structure: it first identifies global clusters and then performs local clustering within these global clusters. This two-s"}, {"rank": 2, "score": 0.56779027, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 26, "text_snippet": "d tree method evaluates nodes collectively across all layers to find the most relevant ones. Clustering Algorithm Clustering plays a key role in building the RAPTOR tree, organizing text segments into cohesive groups. This step groups relat"}, {"rank": 3, "score": 0.5430567, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 8, "text_snippet": " indexing and retrieval system that uses a tree structure to capture both high-level and low-level details about a text. As shown in Figure 1, our system, RAPTOR, clusters chunks of text, generates text summaries of those clusters, and then"}, {"rank": 4, "score": 0.53331244, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 24, "text_snippet": "r text chunks, we employ a clustering algorithm. Once clustered, a Language Model is used to summarize the grouped texts. These summarized texts are then re-embedded, and the cycle of embedding, clustering, and summarization continues until"}, {"rank": 5, "score": 0.520037, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 128, "text_snippet": "s. In contrast, the alternative setup involved creating a balanced tree by recursively encoding and summarizing contiguous text chunks. We determined the window size for this setup based on the average cluster size observed in RAPTOR, which"}]}
{"case_index": 211, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"In the canonical RAG setup, the system has access to a large external corpus of text records and retrieves a subset of records that are [MASK] relevant to the query and collectively small enough to fit into the context window of the LLM.\"", "gold": "individually", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 4.987, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.61865294, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 5, "text_snippet": "l., 2024; Liu et al., 2023). In the canonical RAG setup, the system has access to a large external corpus of text records and retrieves a subset of records that are individually relevant to the query and collectively small enough to fit int"}, {"rank": 2, "score": 0.6146066, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 0, "text_snippet": "From Local to Global: A GraphRAG Approach to Query-Focused Summarization Darren Edge1†Ha Trinh1†Newman Cheng2Joshua Bradley2Alex Chao3 Apurva Mody3Steven Truitt2Dasha Metropolitansky1Robert Osazuwa Ness1 Jonathan Larson1 1Microsoft Research"}, {"rank": 3, "score": 0.60294825, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 15, "text_snippet": "e LLM’s context window. In canonical RAG approaches, the retrieval process returns a set number of records that are seman- tically similar to the query and the generated answer uses only the information in those retrieved records. A common "}, {"rank": 4, "score": 0.60166144, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 1, "text_snippet": "ibuted equally to this work Abstract The use of retrieval-augmented generation (RAG) to retrieve relevant informa- tion from an external knowledge source enables large language models (LLMs) to answer questions over private and/or previousl"}, {"rank": 5, "score": 0.5852813, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 29, "text_snippet": "lements (nodes, edges, covariates) that the LLM can summarize in parallel at both indexing time and query time. The “global answer” to a given query is produced using a final round of query-focused summarization over all community summaries"}]}
{"case_index": 212, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"R EPLUG We introduce REPLUG (Retrieve and Plug ), a new retrieval- augmented LM paradigm where the language model is treated as black box and the retrieval component is added as a [MASK] tuneable module.\"", "gold": "potentially", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 7.985, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.7355257, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 0, "text_snippet": "REPLUG: Retrieval-Augmented Black-Box Language Models Weijia Shi,1 *Sewon Min,1Michihiro Yasunaga,2Minjoon Seo,3Rich James,4Mike Lewis,4 Luke Zettlemoyer1 4Wen-tau Yih4 Abstract We introduce REPLUG, a retrieval-augmented lan- guage modeling"}, {"rank": 2, "score": 0.7306794, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 71, "text_snippet": "a black box and augments it with a tuneable retrieval model. Our evaluation shows that REPLUG can be integrated with any existing language model to improve their performance  REPLUG: Retrieval-Augmented Black-Box Language Models on language"}, {"rank": 3, "score": 0.69542783, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 8, "text_snippet": "Black-Box Language Models Figure 1, REPLUG is extremely flexible and can be used with any existing black-box LM and retrieval model. We also introduce REPLUG LSR (REPLUG with LM- Supervised Retrieval), a training scheme that can further imp"}, {"rank": 4, "score": 0.6726694, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 6, "text_snippet": "PIs. Internal representations of such models are not exposed and fine-tuning is not supported. In this work, we introduce REPLUG (Retrieve and Plug ), a new retrieval-augmented LM framework where the lan- guage model is viewed as a black bo"}, {"rank": 5, "score": 0.67263234, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 7, "text_snippet": " retrieved documents are prepended to the input context and fed into the black-box LM to make the final prediction. Because the LM context length limits the number of documents that can be prepended, we also introduce a new ensemble scheme "}]}
{"case_index": 213, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"Our RAG models achieve [MASK] results on open Natural Questions [ 29], WebQuestions [ 3] and CuratedTrec [ 2] and strongly outperform recent approaches that use specialised pre-training objectives on TriviaQA [ 24].\"", "gold": "state-of-the-art", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.369, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.6653547, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 2, "score": 0.6179209, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 13, "text_snippet": "sive tasks —tasks that humans could not reasonably be expected to perform without access to an external knowledge source. Our RAG models achieve state-of-the-art results on open Natural Questions [ 29], WebQuestions [ 3] and CuratedTrec [ 2"}, {"rank": 3, "score": 0.6108749, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 32, "text_snippet": "ametric knowledge. We consider four popular open-domain QA datasets: Natural Questions (NQ) [ 29], TriviaQA (TQA) [ 24]. WebQuestions (WQ) [ 3] and CuratedTrec (CT) [ 2]. As CT and WQ are small, we follow DPR [ 26] by initializing CT and WQ"}, {"rank": 4, "score": 0.60770285, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 3, "text_snippet": "cessed with a pre-trained neural retriever. We com- pare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. We ﬁne-tune and "}, {"rank": 5, "score": 0.59342206, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 134, "text_snippet": "utational Linguistics. doi: 10.18653/v1/D19-1253. URL https://www.aclweb.org/anthology/D19-1253 . [68] Wanjun Zhong, Jingjing Xu, Duyu Tang, Zenan Xu, Nan Duan, Ming Zhou, Jiahai Wang, and Jian Yin. Reasoning over semantic-level graph for f"}]}
{"case_index": 214, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"2 Published as a conference paper at ICLR 2024 Despite a diversity in methods, the [MASK] compon\"", "gold": "retrieving", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 6.609, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.56624174, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 0, "text_snippet": "Published as a conference paper at ICLR 2024 RAPTOR: R ECURSIVE ABSTRACTIVE PROCESSING FOR TREE-ORGANIZED RETRIEVAL Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D. Manning Stanford University psarthi@cs."}, {"rank": 2, "score": 0.5151445, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 3, "score": 0.51391006, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 129, "text_snippet": "esented in table 9. The results from this ablation study clearly indicate an improvement in accuracy when employing RAPTOR’s clustering mechanism over the recency-based tree approach. This finding substantiates our hypothesis that the clust"}, {"rank": 4, "score": 0.51033556, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 5, "score": 0.5027684, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 104, "text_snippet": "uage Processing , pp. 6997–7008, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021. emnlp-main.560. URL https://aclanthology.org/2021.emnlp-main.560 . Sewon Min, Weiji"}]}
{"case_index": 215, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"CRAG is [MASK] and can be seamlessly coupled with various RAG-based approaches.\"", "gold": "plug-and-play", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 9.055, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.5911367, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 1, "text_snippet": "evitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Al- though retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heav"}, {"rank": 2, "score": 0.5645478, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 10, "text_snippet": " should not have been equally referred to and involved in RAG. On account of the above issues, this paper particularly studies the scenarios where the retriever returns inaccurate results. A method named Corrective Retrieval- Augmented Gene"}, {"rank": 3, "score": 0.5596835, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 133, "text_snippet": "nched model, SelfRAG-LLaMA2-7b , as a new generator to be consistent with their work and study the specific improvement of our method. Self-CRAG: To demonstrate that our plug-and- play approach can be utilized in other concurrent studies, w"}, {"rank": 4, "score": 0.5595829, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 13, "text_snippet": "elpful for RAG, a decompose-then-recompose algorithm is meticulously crafted throughout the retrieval and utilization process. This algorithm ensures the refinement of retrieved information, optimizing the extraction of key insights and min"}, {"rank": 5, "score": 0.55562395, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 16, "text_snippet": "Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform- ers Library [ 66] and can be found at https://github.com/huggingface/transformers/blob/master/ examples/rag/ . An interactive demo of RAG models ca"}]}
{"case_index": 216, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the [MASK] memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever.\"", "gold": "non-parametric", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 13.211, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.66141623, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 9, "text_snippet": "y to the “workhorse of NLP,” i.e. sequence-to-sequence (seq2seq) models. We endow pre-trained, parametric-memory generation models with a non-parametric memory through a general-purpose ﬁne-tuning approach which we refer to as retrieval-aug"}, {"rank": 2, "score": 0.630005, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 2, "text_snippet": "anism to explicit non-parametric memory have so far been only investigated for extractive downstream tasks. We explore a general-purpose ﬁne-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trained parametri"}, {"rank": 3, "score": 0.6248265, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 4, "score": 0.60100996, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 3, "text_snippet": "cessed with a pre-trained neural retriever. We com- pare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. We ﬁne-tune and "}, {"rank": 5, "score": 0.57812214, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}]}
{"case_index": 217, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"In ad- dition, versions of the GraphRAG approach are also available as extensions to multiple open- source libraries, including LangChain (LangChain, 2024), LlamaIndex (LlamaIndex, 2024), Nebu- laGraph ([MASK], 2024), and Neo4J (Neo4J, 2024).\"", "gold": "nebulagraph", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 9.062, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6402819, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 0, "text_snippet": "From Local to Global: A GraphRAG Approach to Query-Focused Summarization Darren Edge1†Ha Trinh1†Newman Cheng2Joshua Bradley2Alex Chao3 Apurva Mody3Steven Truitt2Dasha Metropolitansky1Robert Osazuwa Ness1 Jonathan Larson1 1Microsoft Research"}, {"rank": 2, "score": 0.6145719, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 13, "text_snippet": "ps://github .com/microsoft/graphrag. In ad- dition, versions of the GraphRAG approach are also available as extensions to multiple open- source libraries, including LangChain (LangChain, 2024), LlamaIndex (LlamaIndex, 2024), Nebu- laGraph ("}, {"rank": 3, "score": 0.58764863, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 17, "text_snippet": " are later used to answer queries as in Mao et al. 2020). These summaries are generated in parallel and iteratively aggregated into global summaries, similar to prior techniques (Feng et al., 2023; Gao et al., 2023; Khattab et al., 2022; Sh"}, {"rank": 4, "score": 0.5861138, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 21, "text_snippet": "edges encoding lexical and semantical similarity or structural relation- ships. GraphRAG contrasts with these approaches by focusing on a previously unexplored quality of graphs in this context: their inherent modularity (Newman, 2006) and "}, {"rank": 5, "score": 0.58256155, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 97, "text_snippet": "omprehensive- ness and diversity of answers, as well as favorable comparisons to a global but graph-free approach using map-reduce source text summarization. For situations requiring many global queries over the same dataset, summaries of r"}]}
{"case_index": 218, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"3 M ETHODS Overview of RAPTOR Building on the idea that long texts often present subtopics and hierarchi- cal [MASK] (Cao & Wang, 2022; Dong et al., 2023b), RAPTOR addresses the issue of semantic depth and connection in\"", "gold": "structures", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 5.852, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.6727754, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 22, "text_snippet": " RAPTOR addresses the issue of semantic depth and connection in reading by building a recursive tree structure that balances broader thematic comprehension with granular details and which allows nodes to be grouped based on semantic sim- il"}, {"rank": 2, "score": 0.6693182, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 21, "text_snippet": "ntermediate nodes thus storing varying levels of detail, keeping granular details. However, both methods, due to their reliance on adjacency for grouping or summarizing adjacent nodes, may still overlook distant interdependencies within the"}, {"rank": 3, "score": 0.60508525, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 8, "text_snippet": " indexing and retrieval system that uses a tree structure to capture both high-level and low-level details about a text. As shown in Figure 1, our system, RAPTOR, clusters chunks of text, generates text summaries of those clusters, and then"}, {"rank": 4, "score": 0.60141355, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 0, "text_snippet": "Published as a conference paper at ICLR 2024 RAPTOR: R ECURSIVE ABSTRACTIVE PROCESSING FOR TREE-ORGANIZED RETRIEVAL Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D. Manning Stanford University psarthi@cs."}, {"rank": 5, "score": 0.59600115, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 67, "text_snippet": "ring approaches, which allows it to capture a range of information, from general themes to specific details, contributing to its overall strong performance. 4.1 C ONTRIBUTION OF THE TREE STRUCTURE We examine the contribution of each layer o"}]}
{"case_index": 219, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"Here, this loss is only used to optimize the [MASK] of the retriever, and not the language model.\"", "gold": "parameters", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 11.552, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.6118429, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 2, "score": 0.5756801, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 111, "text_snippet": " arXiv:2208.03299, 2022. https://arxiv.org/abs/ 2208.03299. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Edouard Grave, and Sebastian Riedel. Atlas: Few-shot learn"}, {"rank": 3, "score": 0.5731162, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 83, "text_snippet": ".49 28.22 5 20.18 17.3728.2419.6518.7127.08 820.5217.60 28.17 16.87 18.05 25.36 10 19.67 17.41 27.62 15.72 17.42 23.60 6 Related Works Retrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi- tec"}, {"rank": 4, "score": 0.5627278, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 2, "text_snippet": "ings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including"}, {"rank": 5, "score": 0.5588409, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 23, "text_snippet": "ent loss functions to train the retriever jointly with the language model. We consider loss functions that leverage the language model to provide supervisory signal to train the retriever. In other words, if the language model ﬁnds a docume"}]}
{"case_index": 220, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"To [MASK] others to reproduce our results, we will publish all source code l\"", "gold": "facilitate", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 6.952, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.4913883, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 2, "score": 0.4694017, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 3, "score": 0.4462746, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 15, "text_snippet": "hers to reproduce our results, we will publish all source code later. In summary, our contributions in this paper are three-fold: 1) This paper studies the scenarios where the retriever returns inaccurate results and, to the best of our kno"}, {"rank": 4, "score": 0.44087595, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 1, "text_snippet": "evitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Al- though retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heav"}, {"rank": 5, "score": 0.43908653, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 83, "text_snippet": ".49 28.22 5 20.18 17.3728.2419.6518.7127.08 820.5217.60 28.17 16.87 18.05 25.36 10 19.67 17.41 27.62 15.72 17.42 23.60 6 Related Works Retrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi- tec"}]}
{"case_index": 221, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"In addition, our optimization framework for large context [MASK] extend the context size of LLMs by16 ×.\"", "gold": "enablesrefragto", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.578, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6326933, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 3, "text_snippet": " on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30 .85×the time-t"}, {"rank": 2, "score": 0.58850837, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 40, "text_snippet": " (see Figure 1);REFRAG kdenotes compression rate k, REFRAG RLuses RL-based selective compression. LLaMA K: LLaMA-2-7B evaluated on xs+1:s+owith the truncated sequencex s−K:Tas input to match the token count ofREFRAG. Table 1 reports perform"}, {"rank": 3, "score": 0.58792627, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 0, "text_snippet": "REFRAG: Rethinking RAG based Decoding Xiaoqiang Lin1,2,∗,Aritra Ghosh1,Bryan Kian Hsiang Low2,Anshumali Shrivastava1,3,Vijai Mohan1 1Meta Superintelligence Labs,2National University of Singapore,3Rice University ∗Work done at Meta Large Lan"}, {"rank": 4, "score": 0.58424884, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 42, "text_snippet": "xt lengths s∈ { 4096,8192,16384}. Although our model is trained on s+o= 6144, both REFRAG 8andREFRAG 16maintain superior performance at longer contexts. The original Llama-2-7B supports only a4k context window, whereas our approach enables "}, {"rank": 5, "score": 0.58333683, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 69, "text_snippet": "r, due to the compression, we are able to have more context information and hence achieve better performance. Surprisingly, REFRAG 16andREFRAG 32both outperform the LLaMA FTmodel despite having2 ×and4×fewer tokens in the decoder (i.e., lowe"}]}
{"case_index": 222, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"As this approach is extractive, we assume the faithfulness criteria is mostly satisfied.3 Abstractive Compressor We train an [MASK] model encdec θto serve as\"", "gold": "encoder-decoder", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.2, "hit_rank": 5, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.352, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.6314018, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 23, "text_snippet": "odels in the output should be faithful to the original input, yet the main goal is different. Instead of capturing salient information for humans readers, compressors aim to produce a concise text that are useful for a LM on an end task. In"}, {"rank": 2, "score": 0.62405837, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 82, "text_snippet": " and allowing the compression models to perform selective augmentation. Our experiments show that our compressors can improve the efficiency of retrieval augmented LMs significantly with minimal drop in performances. ACKNOWLEDGEMENT We than"}, {"rank": 3, "score": 0.6143826, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 4, "score": 0.61331904, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 123, "text_snippet": "//huggingface.co/facebook/contriever 10https://huggingface.co/facebook/contriever-msmarco 15  Input: Base LM M, Compressor encoder encθ, Training data {xi,Si,yi}T 1where xiis input, Si={sj}n 1 is a set of candidate sentences from the retrie"}, {"rank": 5, "score": 0.6053163, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 20, "text_snippet": "er model encθwhich embeds sentence siand the input sequence xinto fixed- dimensional embeddings respectively. Their inner product represents how helpful it would be for the LMMto prepend sito the input xto generate y. The final summary sfro"}]}
{"case_index": 223, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"We use a [MASK] bi-encoder from DPR to initialize our retriever and to build the document index.\"", "gold": "pre-trained", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 6.205, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6081354, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 2, "score": 0.6071018, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 3, "score": 0.54045385, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 10, "text_snippet": "eural retriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The retriever (Dense Passage Retriever [ 26], henceforth DPR) provides latent documents conditioned on the input, and the seq2seq model (BART"}, {"rank": 4, "score": 0.53572035, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 56, "text_snippet": " the reader and the retriever by leveraging the perplexity of the output generated by the reader. Sachan et al. (2021) and Lee et al. (2021a) both employ salient span masking to pre-train retrievers, leveraging the perplexity and attention "}, {"rank": 5, "score": 0.5267064, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 83, "text_snippet": ".49 28.22 5 20.18 17.3728.2419.6518.7127.08 820.5217.60 28.17 16.87 18.05 25.36 10 19.67 17.41 27.62 15.72 17.42 23.60 6 Related Works Retrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi- tec"}]}
{"case_index": 224, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"They are able to learn new tasks with very few examples or even from [MASK] alone.\"", "gold": "instructions", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 9.721, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.59072423, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 111, "text_snippet": " arXiv:2208.03299, 2022. https://arxiv.org/abs/ 2208.03299. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Edouard Grave, and Sebastian Riedel. Atlas: Few-shot learn"}, {"rank": 2, "score": 0.587248, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 3, "score": 0.5746969, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 0, "text_snippet": "Atlas: Few-shot Learning with Retrieval Augmented Language Models Gautier Izacard∗ ∗,♦,♣,♥gizacard@fb.com Patrick Lewis∗,♦plewis@fb.com Maria Lomeli♦marialomeli@fb.com Lucas Hosseini♦hoss@fb.com Fabio Petroni♦fabiopetroni@fb.com Timo Schick"}, {"rank": 4, "score": 0.5678925, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 2, "text_snippet": "ings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including"}, {"rank": 5, "score": 0.56487733, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 110, "text_snippet": "s of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 874–880, Online, April 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.eacl-main.74. https://aclant"}]}
{"case_index": 225, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"This al- lows to scale to large numbers of [MASK], and to beneﬁt from this large amount of evidence.\"", "gold": "documents", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.25, "hit_rank": 4, "matched_by": "doc_id+chunk_id", "retrieval_ms": 15.948, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.63415474, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 2, "score": 0.6207644, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 0, "text_snippet": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering Gautier Izacard1,2,3Edouard Grave1 1Facebook AI Research, Paris 2ENS, PSL University, Paris 3Inria, Paris gizacard|egrave@fb.com Abstract Generative mode"}, {"rank": 3, "score": 0.61845565, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}, {"rank": 4, "score": 0.61384463, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 16, "text_snippet": "0) and Lewis et al. (2020) in- troduced retrieval augmented generative models for open domain question answering. Our approach differs from these works by how the generative model processes the retrieved passages. This al- lows to scale to "}, {"rank": 5, "score": 0.60796833, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 9, "text_snippet": "rticular, we show that the performance of our method signiﬁcantly improves when the number of retrieved passages increases. We believe that this is evidence that generative mod- els are good at combining evidence from multiple passages, com"}]}
{"case_index": 226, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"For this [MASK] ability to emerge, the key ingredients are scaling both the parameter count of the model, and the size of the training data.\"", "gold": "generalisation", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 10.624, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.62113917, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 2, "score": 0.6045053, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 98, "text_snippet": "s, and standard ﬁne-tuning for larger datasets. 4.5 Training and evaluating Atlas In this section, we apply the ﬁndings from the ablations of the previous sections to train a family of Atlasmodels, ranging from 770M to 11B parameters. More "}, {"rank": 3, "score": 0.5921922, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 83, "text_snippet": ".49 28.22 5 20.18 17.3728.2419.6518.7127.08 820.5217.60 28.17 16.87 18.05 25.36 10 19.67 17.41 27.62 15.72 17.42 23.60 6 Related Works Retrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi- tec"}, {"rank": 4, "score": 0.58751476, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 111, "text_snippet": " arXiv:2208.03299, 2022. https://arxiv.org/abs/ 2208.03299. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Edouard Grave, and Sebastian Riedel. Atlas: Few-shot learn"}, {"rank": 5, "score": 0.57795095, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 2, "text_snippet": "ings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including"}]}
{"case_index": 227, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"f the input (i.e., s≫q) and hence the overall input to the decoder will be [MASK] by a factor of ≃k.\"", "gold": "reduced", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.158, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.5817147, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 22, "text_snippet": "f the input (i.e., s≫q) and hence the overall input to the decoder will be reduced by a factor of ≃k. This architectural design leads to significant reductions in both latency and memory usage, primarily due to the shortened input sequence."}, {"rank": 2, "score": 0.5284926, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 40, "text_snippet": " (see Figure 1);REFRAG kdenotes compression rate k, REFRAG RLuses RL-based selective compression. LLaMA K: LLaMA-2-7B evaluated on xs+1:s+owith the truncated sequencex s−K:Tas input to match the token count ofREFRAG. Table 1 reports perform"}, {"rank": 3, "score": 0.522907, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 17, "text_snippet": "FRAG achieves better performance than LLaMA without incurring higher latency in the downstream applications. 2 Model Architecture We denote the decoder model as Mdecand the encoder model as Menc. Given an input with Ttokens x1, x2, . . . , "}, {"rank": 4, "score": 0.5120555, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 28, "text_snippet": "decoding procedure as “Thorough Decoding.” For longer output sequences,|Y|can become large, requiring many forward passes. For more efﬁcient decoding, we can make a further approximation that pθ(y|x,zi)≈0whereywas not generated during beam "}, {"rank": 5, "score": 0.5116109, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 18, "text_snippet": "f question and retrieval in this section. Model overview.Figure 1 shows the main architecture ofREFRAG. This model consists of a decoder-only foundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Rober"}]}
{"case_index": 228, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"Here, we propose an alternative which gives slightly stronger results, which relies on the following [MASK].\"", "gold": "observation", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 10.801, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.63087213, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 2, "score": 0.5915073, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 83, "text_snippet": ".49 28.22 5 20.18 17.3728.2419.6518.7127.08 820.5217.60 28.17 16.87 18.05 25.36 10 19.67 17.41 27.62 15.72 17.42 23.60 6 Related Works Retrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi- tec"}, {"rank": 3, "score": 0.58965015, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 111, "text_snippet": " arXiv:2208.03299, 2022. https://arxiv.org/abs/ 2208.03299. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Edouard Grave, and Sebastian Riedel. Atlas: Few-shot learn"}, {"rank": 4, "score": 0.5737669, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 5, "score": 0.5731596, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 235, "text_snippet": "ull results for the cyclic-permutation-de-biased Atlas-XXL can be found in Table 18. A.2 Question answering A.2.1 Training Details For question answering, similarly to the MMLU experiments, we format the input using the following template: "}]}
{"case_index": 229, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"Recent work on LLM refinement (Dhuliawala et al., 2023; Madaan et al., 2023; Paul et al., 2023) prompts a model to generate task output, natural language feedback and refined task output [MASK], but at the cost of inference efficiency.\"", "gold": "iteratively", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 5.744, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6864747, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 27, "text_snippet": "n (reasoning path consistency) and without retrieval. Recent work on LLM refinement (Dhuliawala et al., 2023; Madaan et al., 2023; Paul et al., 2023) prompts a model to generate task output, natural language feedback and refined task output"}, {"rank": 2, "score": 0.6284099, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 0, "text_snippet": "Preprint. SELF-RAG: LEARNING TO RETRIEVE , GENERATE ,AND CRITIQUE THROUGH SELF-REFLECTION Akari Asai†, Zeqiu Wu†, Yizhong Wang†§, Avirup Sil‡, Hannaneh Hajishirzi†§ †University of Washington§Allen Institute for AI‡IBM Research AI {akari,zeq"}, {"rank": 3, "score": 0.6101109, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 23, "text_snippet": "ess retrieved passages before using them to prompt the LM to generate the output. SELF-RAGprocesses passages in parallel and filters out irrelevant ones through self-reflection, without relying on external models at inference. Moreover, our"}, {"rank": 4, "score": 0.6079242, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 18, "text_snippet": "at (Touvron et al., 2023) and Alpaca (Dubois et al., 2023) on all tasks. Our analysis demonstrates the effectiveness of training and inference with reflection tokens for overall performance improvements as well as test-time model customizat"}, {"rank": 5, "score": 0.60720533, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 135, "text_snippet": "ns con- sumed and generated by the model. Self-reflection is a prompt engineering technique where the LLM generates an answer, and is then prompted to evaluate its output for correctness, clarity, or completeness, then finally generate an i"}]}
{"case_index": 230, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"This means that all the tasks are framed as follows: the system gets a text query as input, and [MASK]\"", "gold": "generat", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 17.094, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.61347604, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 2, "score": 0.56587243, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 235, "text_snippet": "ull results for the cyclic-permutation-de-biased Atlas-XXL can be found in Table 18. A.2 Question answering A.2.1 Training Details For question answering, similarly to the MMLU experiments, we format the input using the following template: "}, {"rank": 3, "score": 0.5537372, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 4, "score": 0.5433756, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 37, "text_snippet": " To cast language modeling in the text-to-text framework, we consider a chunk of Nwords, and split this chunk in two sub-sequences of equal length N/2. Then, the ﬁrst sub-sequence is used as the query, and the second corresponds to the outp"}, {"rank": 5, "score": 0.53866357, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 2, "text_snippet": "ings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including"}]}
{"case_index": 231, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"that answers do not [MASK] to spans in support documents, thus requiring ab- stractive models.\"", "gold": "correspond", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.524, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.67375004, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 15, "text_snippet": " that answers do not correspond to spans in support documents, thus requiring ab- stractive models. Raffel et al. (2019) showed that generative models are competitive for reading com- prehension tasks such as SQuAD (Rajpurkar et al., 2016),"}, {"rank": 2, "score": 0.66390586, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 86, "text_snippet": "models for open domain question an- swering. In Proceedings of the 16th Conference of the European Chapter of the Association for Compu- tational Linguistics: Main Volume , pp. 874–880, On- line, April 2021a. Association for Computational L"}, {"rank": 3, "score": 0.6549975, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}, {"rank": 4, "score": 0.64280903, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 0, "text_snippet": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering Gautier Izacard1,2,3Edouard Grave1 1Facebook AI Research, Paris 2ENS, PSL University, Paris 3Inria, Paris gizacard|egrave@fb.com Abstract Generative mode"}, {"rank": 5, "score": 0.62410235, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}]}
{"case_index": 232, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"Retrieval Methods [MASK] language models (RALMs) have seen improvements in various components: the retriever, the reader, and end-to-end system trai\"", "gold": "retrieval-augmented", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 5.669, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.593789, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 0, "text_snippet": "Published as a conference paper at ICLR 2024 RAPTOR: R ECURSIVE ABSTRACTIVE PROCESSING FOR TREE-ORGANIZED RETRIEVAL Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D. Manning Stanford University psarthi@cs."}, {"rank": 2, "score": 0.5732921, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 14, "text_snippet": "omponents: the retriever, the reader, and end-to-end system training. Retrieval methods have transitioned from traditional term-based techniques like TF-IDF (Sp¨arck Jones, 1972) and BM25 (Robertson et al., 1995; Roberts et al., 2020) to de"}, {"rank": 3, "score": 0.56726277, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 4, "score": 0.55045307, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 56, "text_snippet": " the reader and the retriever by leveraging the perplexity of the output generated by the reader. Sachan et al. (2021) and Lee et al. (2021a) both employ salient span masking to pre-train retrievers, leveraging the perplexity and attention "}, {"rank": 5, "score": 0.5460738, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 96, "text_snippet": "for Computational Linguistics , 6:437–450, 2018. doi: 10.1162/tacl_a_00030. URL https://www.aclweb.org/anthology/Q18-1031 . 11  [20] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. REALM: Retrieval-augmented languag"}]}
{"case_index": 233, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"We achieve a compression rate of as low as 6% with minimal loss in performance for both tasks, significantly outperforming the off-the-shelf [MASK] models.\"", "gold": "summarization", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.2, "hit_rank": 5, "matched_by": "doc_id+chunk_id", "retrieval_ms": 5.907, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6539011, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 82, "text_snippet": " and allowing the compression models to perform selective augmentation. Our experiments show that our compressors can improve the efficiency of retrieval augmented LMs significantly with minimal drop in performances. ACKNOWLEDGEMENT We than"}, {"rank": 2, "score": 0.62749505, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 0, "text_snippet": "RECOMP: I MPROVING RETRIEVAL -AUGMENTED LM S WITH COMPRESSION AND SELECTIVE AUGMENTATION Fangyuan Xu1, Weijia Shi2, Eunsol Choi1 Department of Computer Science 1The University of Texas at Austin 2University of Washington {fangyuan,eunsol }@"}, {"rank": 3, "score": 0.60920006, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 135, "text_snippet": "arXiv preprint arXiv:2305.00633 , 2023. URL https://arxiv.org/abs/2305.00633 . Fangyuan Xu, Weijia Shi, and Eunsol Choi. Recomp: Improving retrieval-augmented lms with compression and selective augmentation, 2023. URL https://arxiv.org/abs/"}, {"rank": 4, "score": 0.602628, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 5, "score": 0.5998297, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 3, "text_snippet": "n. We evaluate our approach on language modeling task and open domain question answering task. We achieve a compression rate of as low as 6% with minimal loss in performance for both tasks, significantly outperforming the off-the-shelf summ"}]}
{"case_index": 234, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"If the retrieved documents do not contain relevant information or retrieval [MASK] is not necessary, scan be an empty sequence.\"", "gold": "augmentation", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 5.788, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.61370623, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 18, "text_snippet": "cy. If the retrieved documents do not contain relevant information or retrieval augmentation is not necessary, scan be an empty sequence. (2) Effecive : when sis prepended to input sequence xand provided to LM Mas a prompt, LM should genera"}, {"rank": 2, "score": 0.6030486, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 0, "text_snippet": "RECOMP: I MPROVING RETRIEVAL -AUGMENTED LM S WITH COMPRESSION AND SELECTIVE AUGMENTATION Fangyuan Xu1, Weijia Shi2, Eunsol Choi1 Department of Computer Science 1The University of Texas at Austin 2University of Washington {fangyuan,eunsol }@"}, {"rank": 3, "score": 0.56452036, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 4, "score": 0.5444338, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 135, "text_snippet": "arXiv preprint arXiv:2305.00633 , 2023. URL https://arxiv.org/abs/2305.00633 . Fangyuan Xu, Weijia Shi, and Eunsol Choi. Recomp: Improving retrieval-augmented lms with compression and selective augmentation, 2023. URL https://arxiv.org/abs/"}, {"rank": 5, "score": 0.53554726, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}]}
{"case_index": 235, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"GraphRAG contrasts with these approaches by generating a graph index from the source data, then applying graph-based community detection to create a thematic [MASK] of the data.\"", "gold": "partitioning", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 5.96, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6517692, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 0, "text_snippet": "From Local to Global: A GraphRAG Approach to Query-Focused Summarization Darren Edge1†Ha Trinh1†Newman Cheng2Joshua Bradley2Alex Chao3 Apurva Mody3Steven Truitt2Dasha Metropolitansky1Robert Osazuwa Ness1 Jonathan Larson1 1Microsoft Research"}, {"rank": 2, "score": 0.6261193, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 97, "text_snippet": "omprehensive- ness and diversity of answers, as well as favorable comparisons to a global but graph-free approach using map-reduce source text summarization. For situations requiring many global queries over the same dataset, summaries of r"}, {"rank": 3, "score": 0.6190758, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 17, "text_snippet": " are later used to answer queries as in Mao et al. 2020). These summaries are generated in parallel and iteratively aggregated into global summaries, similar to prior techniques (Feng et al., 2023; Gao et al., 2023; Khattab et al., 2022; Sh"}, {"rank": 4, "score": 0.61890715, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 81, "text_snippet": "ng. Tuning element extraction prompts may help to retain more of these details in the GraphRAG index. Community summaries vs. source texts . When comparing community summaries to source texts using GraphRAG, community summaries generally pr"}, {"rank": 5, "score": 0.6129183, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 21, "text_snippet": "edges encoding lexical and semantical similarity or structural relation- ships. GraphRAG contrasts with these approaches by focusing on a previously unexplored quality of graphs in this context: their inherent modularity (Newman, 2006) and "}]}
{"case_index": 236, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"igure 1) while preserving the [MASK] nature of the decoder, thereby supporting multi-turn and agentic applications.\"", "gold": "autoregressive", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.289, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.5836485, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 15, "text_snippet": "igure 1) while preserving the autoregressive nature of the decoder, thereby supporting multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight reinforcement learning (RL) policy that sel"}, {"rank": 2, "score": 0.57783294, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 0, "text_snippet": "REFRAG: Rethinking RAG based Decoding Xiaoqiang Lin1,2,∗,Aritra Ghosh1,Bryan Kian Hsiang Low2,Anshumali Shrivastava1,3,Vijai Mohan1 1Meta Superintelligence Labs,2National University of Singapore,3Rice University ∗Work done at Meta Large Lan"}, {"rank": 3, "score": 0.57706296, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 12, "text_snippet": "is information is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-a"}, {"rank": 4, "score": 0.5654154, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 95, "text_snippet": "oducedREFRAG, a novel and efficient decoding framework tailored for RAG applications. By leveraging the inherent sparsity and block-diagonal attention patterns present in RAG contexts,REFRAG compresses, senses, and expands context represent"}, {"rank": 5, "score": 0.5590765, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 17, "text_snippet": "FRAG achieves better performance than LLaMA without incurring higher latency in the downstream applications. 2 Model Architecture We denote the decoder model as Mdecand the encoder model as Menc. Given an input with Ttokens x1, x2, . . . , "}]}
{"case_index": 237, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"Reflection tokens, inspired by reward models used in [MASK] learning (Ziegler et al., 2019; Ouyang et al., 2022), are inserted offline into the original corpus by a trained critic model.\"", "gold": "reinforcement", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.629, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6632607, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 14, "text_snippet": "t with reflection tokens by unifying them as the next token prediction from the expanded model vocabulary. We train our generator LM on a diverse collection of text interleaved with reflection tokens and retrieved passages. Reflection token"}, {"rank": 2, "score": 0.6347679, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 15, "text_snippet": "st a critic model during training, reducing overhead. The critic model, in part, is supervised on a dataset of input, output, and corresponding reflection tokens collected by prompting a propriety LM (i.e., GPT-4; OpenAI 2023). While we dra"}, {"rank": 3, "score": 0.6298074, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 36, "text_snippet": "text with reflection tokens by unifying them as next token predictions from the expanded model vocabulary (i.e., the original vocabulary plus reflection tokens). Specifically, we train the generator model Mon a curated corpus with interleav"}, {"rank": 4, "score": 0.61216986, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 48, "text_snippet": "gmented with reflection tokens Dgenusing the standard next token objective: max ME(x,y,r )∼DgenlogpM(y, r|x). (2) Unlike Ctraining (Eq. 1), Mlearns to predict the target output as well as the reflection tokens. During training, we mask out "}, {"rank": 5, "score": 0.6089572, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 37, "text_snippet": "e quality of a given task output (Section 3.2.1). Using the critic model, we update the training corpus by inserting reflection tokens into task outputs offline. Subsequently, we train the final generator model ( M) using the conventional L"}]}
{"case_index": 238, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"3)Unusually Structured and Sparse Attention.Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero [MASK] between chunks (see figure 7).\"", "gold": "cross-attention", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.159, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.7033225, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 12, "text_snippet": "is information is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-a"}, {"rank": 2, "score": 0.5747347, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 2, "text_snippet": "ated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns th"}, {"rank": 3, "score": 0.5680995, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 95, "text_snippet": "oducedREFRAG, a novel and efficient decoding framework tailored for RAG applications. By leveraging the inherent sparsity and block-diagonal attention patterns present in RAG contexts,REFRAG compresses, senses, and expands context represent"}, {"rank": 4, "score": 0.5617449, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 13, "text_snippet": "nd memory usage during decoding, allwithout requiring modificationsto the LLM architecture or introducing new decoder parameters. REFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved passag"}, {"rank": 5, "score": 0.56167376, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 3, "text_snippet": " on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30 .85×the time-t"}]}
{"case_index": 239, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"rticular, we show that the [MASK] of our method signiﬁcantly improves when the number of retrieved passages increases.\"", "gold": "performance", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 5.495, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6783631, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}, {"rank": 2, "score": 0.6752748, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 9, "text_snippet": "rticular, we show that the performance of our method signiﬁcantly improves when the number of retrieved passages increases. We believe that this is evidence that generative mod- els are good at combining evidence from multiple passages, com"}, {"rank": 3, "score": 0.65934163, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 0, "text_snippet": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering Gautier Izacard1,2,3Edouard Grave1 1Facebook AI Research, Paris 2ENS, PSL University, Paris 3Inria, Paris gizacard|egrave@fb.com Abstract Generative mode"}, {"rank": 4, "score": 0.64815444, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 16, "text_snippet": "0) and Lewis et al. (2020) in- troduced retrieval augmented generative models for open domain question answering. Our approach differs from these works by how the generative model processes the retrieved passages. This al- lows to scale to "}, {"rank": 5, "score": 0.64643836, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}]}
{"case_index": 240, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG [MASK].\"", "gold": "applications", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 16.208, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.7151817, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 3, "text_snippet": " on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30 .85×the time-t"}, {"rank": 2, "score": 0.70766044, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 95, "text_snippet": "oducedREFRAG, a novel and efficient decoding framework tailored for RAG applications. By leveraging the inherent sparsity and block-diagonal attention patterns present in RAG contexts,REFRAG compresses, senses, and expands context represent"}, {"rank": 3, "score": 0.6717525, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 12, "text_snippet": "is information is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-a"}, {"rank": 4, "score": 0.6661481, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 94, "text_snippet": "Complementary approaches rank or prune context by estimated informativeness, e.g.,Selective Contextuses self-information to drop low-value tokens, and sentence-level methods learn context-aware encoders for question-specific compression and"}, {"rank": 5, "score": 0.636888, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 0, "text_snippet": "REFRAG: Rethinking RAG based Decoding Xiaoqiang Lin1,2,∗,Aritra Ghosh1,Bryan Kian Hsiang Low2,Anshumali Shrivastava1,3,Vijai Mohan1 1Meta Superintelligence Labs,2National University of Singapore,3Rice University ∗Work done at Meta Large Lan"}]}
{"case_index": 241, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"Nevertheless, LLMs inevitably manifest [MASK] (Ji et al., 2023) due to their struggle with factual errors (Mallen et al., 2023; Min et al., 2023) and inability to secure the accuracy of generated texts solely by *Equal contribution.\"", "gold": "hallucinations", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 12.602, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6498237, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 4, "text_snippet": " for long-form generations relative to these models.1 1 I NTRODUCTION State-of-the-art LLMs continue to struggle with factual errors (Mallen et al., 2023; Min et al., 2023) despite their increased model and data scale (Ouyang et al., 2022)."}, {"rank": 2, "score": 0.61372125, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 1, "text_snippet": "evitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Al- though retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heav"}, {"rank": 3, "score": 0.61277956, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 83, "text_snippet": ".49 28.22 5 20.18 17.3728.2419.6518.7127.08 820.5217.60 28.17 16.87 18.05 25.36 10 19.67 17.41 27.62 15.72 17.42 23.60 6 Related Works Retrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi- tec"}, {"rank": 4, "score": 0.6104915, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 18, "text_snippet": "pact the LLMs and exacerbate the problems. Thus, it is obvious that the lack of accurate and specific knowledge can lead to misleading or even inaccurate generation, which will severely hurt the experience of users in most practical applica"}, {"rank": 5, "score": 0.5974802, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 0, "text_snippet": "Corrective Retrieval Augmented Generation Shi-Qi Yan1*, Jia-Chen Gu2*, Yun Zhu3, Zhen-Hua Ling1 1National Engineering Research Center of Speech and Language Information Processing, University of Science and Technology of China, Hefei, China"}]}
{"case_index": 242, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform- ers Library [ 66] and can be found at https://github.com/huggingface/[MASK]/blob/master/ examples/rag/ .\"", "gold": "transformers", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.684, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.64802444, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 16, "text_snippet": "Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform- ers Library [ 66] and can be found at https://github.com/huggingface/transformers/blob/master/ examples/rag/ . An interactive demo of RAG models ca"}, {"rank": 2, "score": 0.59731835, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 3, "score": 0.5727575, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 139, "text_snippet": "Wikipedia. After submission, We have ported our code to HuggingFace Transformers [ 66]3, which achieves equivalent performance to the previous version but is a cleaner and easier to use implementation. This version is also open-sourced. We "}, {"rank": 4, "score": 0.5609434, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 3, "text_snippet": "cessed with a pre-trained neural retriever. We com- pare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. We ﬁne-tune and "}, {"rank": 5, "score": 0.5397721, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 113, "text_snippet": ", Rockt ¨aschel, T., et al. (2020). Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems , 33:9459–9474. Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni,"}]}
{"case_index": 243, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"(2021), which is inspired by the [MASK] algorithm, treating re\"", "gold": "expectation-maximization", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 5.267, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.602772, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 2, "score": 0.5659042, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 111, "text_snippet": " arXiv:2208.03299, 2022. https://arxiv.org/abs/ 2208.03299. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Edouard Grave, and Sebastian Riedel. Atlas: Few-shot learn"}, {"rank": 3, "score": 0.5312027, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 83, "text_snippet": ".49 28.22 5 20.18 17.3728.2419.6518.7127.08 820.5217.60 28.17 16.87 18.05 25.36 10 19.67 17.41 27.62 15.72 17.42 23.60 6 Related Works Retrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi- tec"}, {"rank": 4, "score": 0.5276021, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 5, "score": 0.5219915, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 235, "text_snippet": "ull results for the cyclic-permutation-de-biased Atlas-XXL can be found in Table 18. A.2 Question answering A.2.1 Training Details For question answering, similarly to the MMLU experiments, we format the input using the following template: "}]}
{"case_index": 244, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"is exciting, such models do have down- sides: They cannot easily expand or revise their memory, can’t [MASK] provide insight into their predictions, and may produce “hallucinations” [ 38].\"", "gold": "straightforwardly", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 7.676, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6044584, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 2, "score": 0.5911173, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 206, "text_snippet": "ational Linguistics. doi: 10.18653/v1/2020.emnlp-main.346. URL https://aclanthology.org/2020.emnlp-main.346 . 8 Kurt Shuster, Spencer Poﬀ, Moya Chen, Douwe Kiela, and Jason Weston. Retrieval augmentation reduces hallucination in conversatio"}, {"rank": 3, "score": 0.5737407, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 4, "score": 0.5619147, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 109, "text_snippet": " 2022. Tegtok: Augmenting text generation via task-specific and open-world knowledge. In Findings of the Association for Computational Linguistics: ACL 2022, Dublin, Ireland, May 22-27, 2022 , pages 1597–1609. Association for Computational "}, {"rank": 5, "score": 0.55129105, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 108, "text_snippet": " Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021. Retrieval augmentation reduces hallucination in conversation. InFindings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana, "}]}
{"case_index": 245, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"In other words, we would like the [MASK] to find documents that result in lower perplex- ity scores.\"", "gold": "retriever", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 6.367, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.5695738, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 2, "score": 0.5664475, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 70, "text_snippet": "performance. Figure 6. Rare entities benefit from retrieval . After incorporat- ing the retrieved document during inference, the entity \" Li Bai \" and the token \" greatest \" in the continuation show the most im- provement in perplexity (15%"}, {"rank": 3, "score": 0.5636851, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 20, "text_snippet": "n improve GPT- 3 performance on open-domain question answering, we approach the problem in a more general setting, including language modeling and understanding tasks. We also pro- pose an ensemble method to incorporate more documents  REPL"}, {"rank": 4, "score": 0.5618653, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 7, "text_snippet": " retrieved documents are prepended to the input context and fed into the black-box LM to make the final prediction. Because the LM context length limits the number of documents that can be prepended, we also introduce a new ensemble scheme "}, {"rank": 5, "score": 0.558466, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 52, "text_snippet": "modeling Table 1 reports the results on language modeling task. Allretrieval augmen- tation methods improve perplexity over no retrieval setting across three LMs. Heuristic token / phrase-level compression methods (BoW and NE) are worse tha"}]}
{"case_index": 246, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"3.1.1 Source [MASK] →Text Chunks To start, the documents in the corpus are split into text chunks.\"", "gold": "documents", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 7.302, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.60767424, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 2, "score": 0.5540626, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 37, "text_snippet": " To cast language modeling in the text-to-text framework, we consider a chunk of Nwords, and split this chunk in two sub-sequences of equal length N/2. Then, the ﬁrst sub-sequence is used as the query, and the second corresponds to the outp"}, {"rank": 3, "score": 0.53890043, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 0, "text_snippet": "From Local to Global: A GraphRAG Approach to Query-Focused Summarization Darren Edge1†Ha Trinh1†Newman Cheng2Joshua Bradley2Alex Chao3 Apurva Mody3Steven Truitt2Dasha Metropolitansky1Robert Osazuwa Ness1 Jonathan Larson1 1Microsoft Research"}, {"rank": 4, "score": 0.52591574, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 27, "text_snippet": "ns (Wang et al., 2023a; Zheng et al., 2024). Some prior work proposes criteria for having LLMs quantify the quality of 3  Source Documents Text Chunkstext extraction and chunking Entities & Relationshipsdomain-tailored summarization Knowled"}, {"rank": 5, "score": 0.52568763, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}]}
{"case_index": 247, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"First, it increases computational costs as LMs now encode [MASK] more tokens.\"", "gold": "substantially", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 5.868, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6122097, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 0, "text_snippet": "RECOMP: I MPROVING RETRIEVAL -AUGMENTED LM S WITH COMPRESSION AND SELECTIVE AUGMENTATION Fangyuan Xu1, Weijia Shi2, Eunsol Choi1 Department of Computer Science 1The University of Texas at Austin 2University of Washington {fangyuan,eunsol }@"}, {"rank": 2, "score": 0.56617475, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 3, "score": 0.55800843, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 82, "text_snippet": " and allowing the compression models to perform selective augmentation. Our experiments show that our compressors can improve the efficiency of retrieval augmented LMs significantly with minimal drop in performances. ACKNOWLEDGEMENT We than"}, {"rank": 4, "score": 0.54986984, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 135, "text_snippet": "arXiv preprint arXiv:2305.00633 , 2023. URL https://arxiv.org/abs/2305.00633 . Fangyuan Xu, Weijia Shi, and Eunsol Choi. Recomp: Improving retrieval-augmented lms with compression and selective augmentation, 2023. URL https://arxiv.org/abs/"}, {"rank": 5, "score": 0.546932, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 219, "text_snippet": "4.6 38.8 52.8 A Training details and additional results A.1 MMLU A.1.1 Training Details Featurization MMLU consists of multiple choice questions with four possible lexicalized answer options. We represent the input using the following templ"}]}
{"case_index": 248, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token [MASK] ef\"", "gold": "allocation", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 5.917, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.56574774, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 13, "text_snippet": "nd memory usage during decoding, allwithout requiring modificationsto the LLM architecture or introducing new decoder parameters. REFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved passag"}, {"rank": 2, "score": 0.5506933, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 12, "text_snippet": "is information is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-a"}, {"rank": 3, "score": 0.54639316, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 3, "text_snippet": " on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30 .85×the time-t"}, {"rank": 4, "score": 0.5434299, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 95, "text_snippet": "oducedREFRAG, a novel and efficient decoding framework tailored for RAG applications. By leveraging the inherent sparsity and block-diagonal attention patterns present in RAG contexts,REFRAG compresses, senses, and expands context represent"}, {"rank": 5, "score": 0.5363814, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 17, "text_snippet": "FRAG achieves better performance than LLaMA without incurring higher latency in the downstream applications. 2 Model Architecture We denote the decoder model as Mdecand the encoder model as Menc. Given an input with Ttokens x1, x2, . . . , "}]}
{"case_index": 249, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"lements (nodes, edges, [MASK]) that the LLM can summarize in parallel at both indexing time and query time.\"", "gold": "covariates", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 9.185, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.63538325, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 29, "text_snippet": "lements (nodes, edges, covariates) that the LLM can summarize in parallel at both indexing time and query time. The “global answer” to a given query is produced using a final round of query-focused summarization over all community summaries"}, {"rank": 2, "score": 0.6273544, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 0, "text_snippet": "From Local to Global: A GraphRAG Approach to Query-Focused Summarization Darren Edge1†Ha Trinh1†Newman Cheng2Joshua Bradley2Alex Chao3 Apurva Mody3Steven Truitt2Dasha Metropolitansky1Robert Osazuwa Ness1 Jonathan Larson1 1Microsoft Research"}, {"rank": 3, "score": 0.5987668, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 27, "text_snippet": "ns (Wang et al., 2023a; Zheng et al., 2024). Some prior work proposes criteria for having LLMs quantify the quality of 3  Source Documents Text Chunkstext extraction and chunking Entities & Relationshipsdomain-tailored summarization Knowled"}, {"rank": 4, "score": 0.5820803, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 97, "text_snippet": "omprehensive- ness and diversity of answers, as well as favorable comparisons to a global but graph-free approach using map-reduce source text summarization. For situations requiring many global queries over the same dataset, summaries of r"}, {"rank": 5, "score": 0.55266434, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 35, "text_snippet": "for summarization in Appendix D. While the summarization model generally produces reliable summaries, a focused annotation study revealed that about 4% of the summaries contained minor hallucinations. These did not propagate to parent nodes"}]}
{"case_index": 250, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"In BM25, passages are [MASK] as bag of words, and the ranking function is based on term and inverse doc- ument frequencies.\"", "gold": "represented", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 10.022, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.6359852, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 2, "score": 0.6321744, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 86, "text_snippet": "models for open domain question an- swering. In Proceedings of the 16th Conference of the European Chapter of the Association for Compu- tational Linguistics: Main Volume , pp. 874–880, On- line, April 2021a. Association for Computational L"}, {"rank": 3, "score": 0.6241591, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 4, "score": 0.61658716, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}, {"rank": 5, "score": 0.61153877, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 0, "text_snippet": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering Gautier Izacard1,2,3Edouard Grave1 1Facebook AI Research, Paris 2ENS, PSL University, Paris 3Inria, Paris gizacard|egrave@fb.com Abstract Generative mode"}]}
{"case_index": 251, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"at increasing prompt length for contextual learning leads to higher latency and greater memory [MASK] during inference (Yen et al., 2024).\"", "gold": "consumption", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 9.317, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.5757128, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 0, "text_snippet": "REFRAG: Rethinking RAG based Decoding Xiaoqiang Lin1,2,∗,Aritra Ghosh1,Bryan Kian Hsiang Low2,Anshumali Shrivastava1,3,Vijai Mohan1 1Meta Superintelligence Labs,2National University of Singapore,3Rice University ∗Work done at Meta Large Lan"}, {"rank": 2, "score": 0.57521844, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 3, "text_snippet": " on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30 .85×the time-t"}, {"rank": 3, "score": 0.56848043, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 94, "text_snippet": "Complementary approaches rank or prune context by estimated informativeness, e.g.,Selective Contextuses self-information to drop low-value tokens, and sentence-level methods learn context-aware encoders for question-specific compression and"}, {"rank": 4, "score": 0.56828105, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 26, "text_snippet": "e detailed discussion 1REFRAGwithout cache means that we recompute the chunk embedding for the context and take this latency into account. 3  on empirical evaluation is in section A. 3 Methodology To align the encoder and decoder, we follow"}, {"rank": 5, "score": 0.55637157, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 95, "text_snippet": "oducedREFRAG, a novel and efficient decoding framework tailored for RAG applications. By leveraging the inherent sparsity and block-diagonal attention patterns present in RAG contexts,REFRAG compresses, senses, and expands context represent"}]}
{"case_index": 252, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"t prior probability pη(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be [MASK] solved in sub-linear time [ 23].\"", "gold": "approximately", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 49.411, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.61427206, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 22, "text_snippet": "t prior probability pη(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [ 23]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index."}, {"rank": 2, "score": 0.5726552, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 3, "score": 0.560279, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 4, "score": 0.5292351, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 98, "text_snippet": "hology.org/Q19-1026 . Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen tau Yih, Tim Rockt ¨aschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented gener"}, {"rank": 5, "score": 0.516234, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 235, "text_snippet": "ull results for the cyclic-permutation-de-biased Atlas-XXL can be found in Table 18. A.2 Question answering A.2.1 Training Details For question answering, similarly to the MMLU experiments, we format the input using the following template: "}]}
{"case_index": 253, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"Then, these documents are fed to the language model, along with the query, which in turns [MASK] the output.\"", "gold": "generates", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 8.668, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.6354396, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 2, "score": 0.60503256, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 37, "text_snippet": " To cast language modeling in the text-to-text framework, we consider a chunk of Nwords, and split this chunk in two sub-sequences of equal length N/2. Then, the ﬁrst sub-sequence is used as the query, and the second corresponds to the outp"}, {"rank": 3, "score": 0.59511566, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 4, "score": 0.5845547, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 2, "text_snippet": "ings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including"}, {"rank": 5, "score": 0.58354694, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 83, "text_snippet": ".49 28.22 5 20.18 17.3728.2419.6518.7127.08 820.5217.60 28.17 16.87 18.05 25.36 10 19.67 17.41 27.62 15.72 17.42 23.60 6 Related Works Retrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi- tec"}]}
{"case_index": 254, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"n improve GPT- 3 performance on open-domain question answering, we approach the problem in a more general setting, including language modeling and [MASK] tasks.\"", "gold": "understanding", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.385, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.6947516, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 20, "text_snippet": "n improve GPT- 3 performance on open-domain question answering, we approach the problem in a more general setting, including language modeling and understanding tasks. We also pro- pose an ensemble method to incorporate more documents  REPL"}, {"rank": 2, "score": 0.6130291, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 3, "score": 0.61281335, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 94, "text_snippet": "iedel, S. Question and an- swer test-train overlap in open-domain question answer- ing datasets. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume , pp. 1000–1008, 20"}, {"rank": 4, "score": 0.6074087, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 83, "text_snippet": "sser, S., and Leahy, C. The Pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027 , 2020.  REPLUG: Retrieval-Augmented Black-Box Language Models Goyal, A., Friesen, A., Banino, A., Weber, T., Ke, N. R"}, {"rank": 5, "score": 0.60286224, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 45, "text_snippet": "t that covers exam questions from 57 tasks includ- ing mathematics, computer science, law, US history and etc. The 57 tasks are grouped into 4 categories: humani- ties, STEM, social sciences and other. Following Chung  REPLUG: Retrieval-Aug"}]}
{"case_index": 255, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"Controlled experiments show that retrieval with recursive summaries offers [MASK] improv\"", "gold": "significant", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.372, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.58146125, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 1, "text_snippet": "rom a retrieval corpus, limiting holistic under- standing of the overall document context. We introduce the novel approach of recursively embedding, clustering, and summarizing chunks of text, constructing a tree with differing levels of su"}, {"rank": 2, "score": 0.548749, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 74, "text_snippet": "employing recursive clustering and summarization techniques, RAPTOR creates a hierarchical tree structure that is capable of synthesizing information across various sections of the retrieval corpora. During the query phase, RAPTOR leverages"}, {"rank": 3, "score": 0.5385518, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 4, "score": 0.5377269, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 35, "text_snippet": "for summarization in Appendix D. While the summarization model generally produces reliable summaries, a focused annotation study revealed that about 4% of the summaries contained minor hallucinations. These did not propagate to parent nodes"}, {"rank": 5, "score": 0.5358958, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 0, "text_snippet": "Published as a conference paper at ICLR 2024 RAPTOR: R ECURSIVE ABSTRACTIVE PROCESSING FOR TREE-ORGANIZED RETRIEVAL Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D. Manning Stanford University psarthi@cs."}]}
{"case_index": 256, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"This yields a set of [MASK] Y, some of which may not have appeared in the beams of all documents.\"", "gold": "hypotheses", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 6.755, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.60712934, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 2, "score": 0.57767195, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 3, "score": 0.52125347, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 37, "text_snippet": " To cast language modeling in the text-to-text framework, we consider a chunk of Nwords, and split this chunk in two sub-sequences of equal length N/2. Then, the ﬁrst sub-sequence is used as the query, and the second corresponds to the outp"}, {"rank": 4, "score": 0.50326884, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 83, "text_snippet": ".49 28.22 5 20.18 17.3728.2419.6518.7127.08 820.5217.60 28.17 16.87 18.05 25.36 10 19.67 17.41 27.62 15.72 17.42 23.60 6 Related Works Retrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi- tec"}, {"rank": 5, "score": 0.49257565, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 98, "text_snippet": "hology.org/Q19-1026 . Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen tau Yih, Tim Rockt ¨aschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented gener"}]}
{"case_index": 257, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"Earlier studies adopt either sparse or dense retrievers at the front end of a pre- trained language model that [MASK] in response generation.\"", "gold": "specializes", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 5.576, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6291851, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 56, "text_snippet": " the reader and the retriever by leveraging the perplexity of the output generated by the reader. Sachan et al. (2021) and Lee et al. (2021a) both employ salient span masking to pre-train retrievers, leveraging the perplexity and attention "}, {"rank": 2, "score": 0.6098399, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 83, "text_snippet": ".49 28.22 5 20.18 17.3728.2419.6518.7127.08 820.5217.60 28.17 16.87 18.05 25.36 10 19.67 17.41 27.62 15.72 17.42 23.60 6 Related Works Retrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi- tec"}, {"rank": 3, "score": 0.59554553, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 4, "score": 0.5866635, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 5, "score": 0.58365047, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 57, "text_snippet": "nted generators using a second “informed” retriever with access to the output, which the test-time retriever can be distilled from, and Hofstätter et al. (2022) recently proposed a training set ﬁltering/weighting approach to train stronger "}]}
{"case_index": 258, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"RAG is ideal when the total number of [MASK] in a data source is too large to include in a single prompt to the LLM, i.e.\"", "gold": "records", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.304, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.6195343, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 0, "text_snippet": "From Local to Global: A GraphRAG Approach to Query-Focused Summarization Darren Edge1†Ha Trinh1†Newman Cheng2Joshua Bradley2Alex Chao3 Apurva Mody3Steven Truitt2Dasha Metropolitansky1Robert Osazuwa Ness1 Jonathan Larson1 1Microsoft Research"}, {"rank": 2, "score": 0.5767015, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 14, "text_snippet": "es, whereupon this information is incorporated into the generation of a response to the query by an LLM (or other generative AI model, such as a multi-media model). The query and retrieved records populate a prompt template, which is then p"}, {"rank": 3, "score": 0.5730778, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 29, "text_snippet": "lements (nodes, edges, covariates) that the LLM can summarize in parallel at both indexing time and query time. The “global answer” to a given query is produced using a final round of query-focused summarization over all community summaries"}, {"rank": 4, "score": 0.57106817, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 22, "text_snippet": "gly global summaries by using the LLM to create summaries spanning this community hierarchy. 2.3 Adaptive benchmarking for RAG Evaluation Many benchmark datasets for open-domain question answering exist, including HotPotQA (Yang et al., 201"}, {"rank": 5, "score": 0.5705606, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 1, "text_snippet": "ibuted equally to this work Abstract The use of retrieval-augmented generation (RAG) to retrieve relevant informa- tion from an external knowledge source enables large language models (LLMs) to answer questions over private and/or previousl"}]}
{"case_index": 259, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"We ﬁne-tune and evaluate our models on a wide range of knowledge- intensive NLP tasks and set the state of the art on three open domain QA tasks, outperforming parametric seq2seq models and task-speciﬁc [MASK] architectures.\"", "gold": "retrieve-and-extract", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.3333333333333333, "hit_rank": 3, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.207, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.62048566, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 2, "score": 0.6030524, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 3, "score": 0.57250345, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 3, "text_snippet": "cessed with a pre-trained neural retriever. We com- pare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. We ﬁne-tune and "}, {"rank": 4, "score": 0.56976694, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 37, "text_snippet": " To cast language modeling in the text-to-text framework, we consider a chunk of Nwords, and split this chunk in two sub-sequences of equal length N/2. Then, the ﬁrst sub-sequence is used as the query, and the second corresponds to the outp"}, {"rank": 5, "score": 0.5648006, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 0, "text_snippet": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks Patrick Lewis†‡, Ethan Perez⋆, Aleksandra Piktus†, Fabio Petroni†, Vladimir Karpukhin†, Naman Goyal†, Heinrich Küttler†, Mike Lewis†, Wen-tau Yih†, Tim Rocktäschel†‡, Sebastia"}]}
{"case_index": 260, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"[MASK] Generation RAG (Lewis et al., 2020; Guu et al., 2020) is regarded as a useful method to address the issues above, which enhances the input questions of generative LMs with retrieved documents.\"", "gold": "retrieval-augmented", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 5.26, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.64602196, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 18, "text_snippet": "pact the LLMs and exacerbate the problems. Thus, it is obvious that the lack of accurate and specific knowledge can lead to misleading or even inaccurate generation, which will severely hurt the experience of users in most practical applica"}, {"rank": 2, "score": 0.63082916, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 83, "text_snippet": ".49 28.22 5 20.18 17.3728.2419.6518.7127.08 820.5217.60 28.17 16.87 18.05 25.36 10 19.67 17.41 27.62 15.72 17.42 23.60 6 Related Works Retrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi- tec"}, {"rank": 3, "score": 0.6265844, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 83, "text_snippet": " in Table 6. The findings indicate that the self-correction mecha- nism incurs only modest computational overheadwhile significantly enhancing performance, thereby validating its lightweight nature. 6 Conclusion & Limitation This paper stud"}, {"rank": 4, "score": 0.62401485, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 1, "text_snippet": "evitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Al- though retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heav"}, {"rank": 5, "score": 0.61849606, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 7, "text_snippet": "ieval-augmented generation (RAG) (Lewis et al., 2020). In this framework, the input to models is augmented by prepending relevant documents that are retrieved from an external knowledge corpus (Guu et al., 2020). While RAG serves as a pract"}]}
{"case_index": 261, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"We endow pre-trained, parametric-memory generation models with a non-parametric memory through a general-purpose ﬁne-tuning approach which we refer to as [MASK] generation (RAG).\"", "gold": "retrieval-augmented", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 9.24, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.63432467, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 9, "text_snippet": "y to the “workhorse of NLP,” i.e. sequence-to-sequence (seq2seq) models. We endow pre-trained, parametric-memory generation models with a non-parametric memory through a general-purpose ﬁne-tuning approach which we refer to as retrieval-aug"}, {"rank": 2, "score": 0.5975719, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 3, "score": 0.5974604, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 4, "score": 0.5935122, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 2, "text_snippet": "anism to explicit non-parametric memory have so far been only investigated for extractive downstream tasks. We explore a general-purpose ﬁne-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trained parametri"}, {"rank": 5, "score": 0.5827678, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 83, "text_snippet": ".49 28.22 5 20.18 17.3728.2419.6518.7127.08 820.5217.60 28.17 16.87 18.05 25.36 10 19.67 17.41 27.62 15.72 17.42 23.60 6 Related Works Retrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi- tec"}]}
{"case_index": 262, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"It usually provides an extra knowledge source from a specific corpus, i.e., Wikipedia, which greatly improves the per- formance of LMs in a variety of tasks, especially in the [MASK] ones.\"", "gold": "knowledge-intensive", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.2, "hit_rank": 5, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.677, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.58045244, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 2, "score": 0.5561378, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 3, "score": 0.5543195, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 83, "text_snippet": ".49 28.22 5 20.18 17.3728.2419.6518.7127.08 820.5217.60 28.17 16.87 18.05 25.36 10 19.67 17.41 27.62 15.72 17.42 23.60 6 Related Works Retrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi- tec"}, {"rank": 4, "score": 0.53154224, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 83, "text_snippet": " in Table 6. The findings indicate that the self-correction mecha- nism incurs only modest computational overheadwhile significantly enhancing performance, thereby validating its lightweight nature. 6 Conclusion & Limitation This paper stud"}, {"rank": 5, "score": 0.527433, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 19, "text_snippet": " documents. It usually provides an extra knowledge source from a specific corpus, i.e., Wikipedia, which greatly improves the per- formance of LMs in a variety of tasks, especially in the knowledge-intensive ones. The proposed  methods gene"}]}
{"case_index": 263, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"(2023) uses [MASK] and snippets of passages, which improves correctness on most datasets but can sometimes be a lossy means of compression.\"", "gold": "summarizations", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 16.709, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.5871922, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 2, "score": 0.54243064, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 3, "score": 0.5200305, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 0, "text_snippet": "Published as a conference paper at ICLR 2024 RAPTOR: R ECURSIVE ABSTRACTIVE PROCESSING FOR TREE-ORGANIZED RETRIEVAL Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D. Manning Stanford University psarthi@cs."}, {"rank": 4, "score": 0.5141823, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 22, "text_snippet": " RAPTOR addresses the issue of semantic depth and connection in reading by building a recursive tree structure that balances broader thematic comprehension with granular details and which allows nodes to be grouped based on semantic sim- il"}, {"rank": 5, "score": 0.4917394, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 8, "text_snippet": " indexing and retrieval system that uses a tree structure to capture both high-level and low-level details about a text. As shown in Figure 1, our system, RAPTOR, clusters chunks of text, generates text summaries of those clusters, and then"}]}
{"case_index": 264, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"We refer to this decoding [MASK] as “Thorough Decoding.” For longer output seq\"", "gold": "procedure", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 6.283, "llm_ms": 0.004, "top_contexts": [{"rank": 1, "score": 0.57007325, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 2, "score": 0.56622815, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 3, "score": 0.50821733, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 37, "text_snippet": " To cast language modeling in the text-to-text framework, we consider a chunk of Nwords, and split this chunk in two sub-sequences of equal length N/2. Then, the ﬁrst sub-sequence is used as the query, and the second corresponds to the outp"}, {"rank": 4, "score": 0.47981614, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 56, "text_snippet": " the reader and the retriever by leveraging the perplexity of the output generated by the reader. Sachan et al. (2021) and Lee et al. (2021a) both employ salient span masking to pre-train retrievers, leveraging the perplexity and attention "}, {"rank": 5, "score": 0.47850227, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 28, "text_snippet": "decoding procedure as “Thorough Decoding.” For longer output sequences,|Y|can become large, requiring many forward passes. For more efﬁcient decoding, we can make a further approximation that pθ(y|x,zi)≈0whereywas not generated during beam "}]}
{"case_index": 265, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"If so, it outputs a retrieval token that calls a [MASK] model on demand (Step 1).\"", "gold": "retriever", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.961, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6059133, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 7, "text_snippet": "rmittent special tokens (i.e., reflection tokens ). Reflection tokens are categorized into retrieval andcritique tokens to indicate the need for retrieval and its generation quality respectively (Figure 1 right). In particular, given an inp"}, {"rank": 2, "score": 0.5968894, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 3, "score": 0.58933926, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 27, "text_snippet": "n (reasoning path consistency) and without retrieval. Recent work on LLM refinement (Dhuliawala et al., 2023; Madaan et al., 2023; Paul et al., 2023) prompts a model to generate task output, natural language feedback and refined task output"}, {"rank": 4, "score": 0.5888784, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 8, "text_snippet": "on demand (Step 1). Subsequently, SELF-RAGconcurrently processes multiple retrieved passages, evaluating their relevance and then generating corresponding task outputs (Step 2). It then generates critique tokens to criticize its own output "}, {"rank": 5, "score": 0.57628083, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 50, "text_snippet": "hat incorporates special tokens to control generation (Keskar et al., 2019; Lu et al., 2022; Korbak et al., 2023). Our SELF-RAG learns to generate special tokens to evaluate its own prediction after each generated segment, enabling the use "}]}
{"case_index": 266, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"However, in order to produce a fair evaluation, our method avoids generating the questions directly from the corpus itself (as an alternative [MASK], one can use a subset of the corpus held out from subsequen\"", "gold": "implementation", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.2, "hit_rank": 5, "matched_by": "doc_id+chunk_id", "retrieval_ms": 4.938, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.5848942, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 2, "score": 0.5714865, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 0, "text_snippet": "From Local to Global: A GraphRAG Approach to Query-Focused Summarization Darren Edge1†Ha Trinh1†Newman Cheng2Joshua Bradley2Alex Chao3 Apurva Mody3Steven Truitt2Dasha Metropolitansky1Robert Osazuwa Ness1 Jonathan Larson1 1Microsoft Research"}, {"rank": 3, "score": 0.5624062, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 97, "text_snippet": "omprehensive- ness and diversity of answers, as well as favorable comparisons to a global but graph-free approach using map-reduce source text summarization. For situations requiring many global queries over the same dataset, summaries of r"}, {"rank": 4, "score": 0.55989504, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 5, "score": 0.5579279, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 23, "text_snippet": " this work, we propose an approach for generating a set of questions for evaluating global sensemaking over the entirety of the corpus. Our approach is related to LLM methods that use a corpus to generate questions whose answers would be su"}]}
{"case_index": 267, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"FRAG achieves better performance than LLaMA without incurring higher latency in the downstream [MASK].\"", "gold": "applications", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 5.476, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6357439, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 17, "text_snippet": "FRAG achieves better performance than LLaMA without incurring higher latency in the downstream applications. 2 Model Architecture We denote the decoder model as Mdecand the encoder model as Menc. Given an input with Ttokens x1, x2, . . . , "}, {"rank": 2, "score": 0.6279855, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 3, "text_snippet": " on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30 .85×the time-t"}, {"rank": 3, "score": 0.61552453, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 40, "text_snippet": " (see Figure 1);REFRAG kdenotes compression rate k, REFRAG RLuses RL-based selective compression. LLaMA K: LLaMA-2-7B evaluated on xs+1:s+owith the truncated sequencex s−K:Tas input to match the token count ofREFRAG. Table 1 reports perform"}, {"rank": 4, "score": 0.5857897, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 13, "text_snippet": "nd memory usage during decoding, allwithout requiring modificationsto the LLM architecture or introducing new decoder parameters. REFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved passag"}, {"rank": 5, "score": 0.57734627, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 12, "text_snippet": "is information is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-a"}]}
{"case_index": 268, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"xamples, [MASK] a 540B parameters model by 3% despite having 50x fewer parameters.\"", "gold": "outperforming", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 5.293, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.63796026, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 2, "score": 0.60798645, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 83, "text_snippet": ".49 28.22 5 20.18 17.3728.2419.6518.7127.08 820.5217.60 28.17 16.87 18.05 25.36 10 19.67 17.41 27.62 15.72 17.42 23.60 6 Related Works Retrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi- tec"}, {"rank": 3, "score": 0.6033705, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 98, "text_snippet": "s, and standard ﬁne-tuning for larger datasets. 4.5 Training and evaluating Atlas In this section, we apply the ﬁndings from the ablations of the previous sections to train a family of Atlasmodels, ranging from 770M to 11B parameters. More "}, {"rank": 4, "score": 0.5852885, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 111, "text_snippet": " arXiv:2208.03299, 2022. https://arxiv.org/abs/ 2208.03299. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Edouard Grave, and Sebastian Riedel. Atlas: Few-shot learn"}, {"rank": 5, "score": 0.57660997, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 2, "text_snippet": "ings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including"}]}
{"case_index": 269, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"generated texts such as “fluency” (Wang et al., 2023a) Some of these criteria are generic to vector RAG systems and not relevant to global sensemaking, such as “context relevance”, “[MASK]”, and “answer relevance” (RAGAS, Es et al\"", "gold": "faithfulness", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 5.877, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.65411794, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 29, "text_snippet": "lements (nodes, edges, covariates) that the LLM can summarize in parallel at both indexing time and query time. The “global answer” to a given query is produced using a final round of query-focused summarization over all community summaries"}, {"rank": 2, "score": 0.6217189, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 96, "text_snippet": "tigate these downstream risks for questions of a global nature, which might otherwise be answered by samples of retrieved facts falsely presented as global summaries. 7 Conclusion We have presented GraphRAG, a RAG approach that combines kno"}, {"rank": 3, "score": 0.6188542, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 1, "text_snippet": "ibuted equally to this work Abstract The use of retrieval-augmented generation (RAG) to retrieve relevant informa- tion from an external knowledge source enables large language models (LLMs) to answer questions over private and/or previousl"}, {"rank": 4, "score": 0.6142434, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 0, "text_snippet": "From Local to Global: A GraphRAG Approach to Query-Focused Summarization Darren Edge1†Ha Trinh1†Newman Cheng2Joshua Bradley2Alex Chao3 Apurva Mody3Steven Truitt2Dasha Metropolitansky1Robert Osazuwa Ness1 Jonathan Larson1 1Microsoft Research"}, {"rank": 5, "score": 0.61200297, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 22, "text_snippet": "gly global summaries by using the LLM to create summaries spanning this community hierarchy. 2.3 Adaptive benchmarking for RAG Evaluation Many benchmark datasets for open-domain question answering exist, including HotPotQA (Yang et al., 201"}]}
{"case_index": 270, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"We use this approach to compare GraphRAG to vector RAG on two [MASK] real-world text datasets.\"", "gold": "representative", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 5.708, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.61821043, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 0, "text_snippet": "From Local to Global: A GraphRAG Approach to Query-Focused Summarization Darren Edge1†Ha Trinh1†Newman Cheng2Joshua Bradley2Alex Chao3 Apurva Mody3Steven Truitt2Dasha Metropolitansky1Robert Osazuwa Ness1 Jonathan Larson1 1Microsoft Research"}, {"rank": 2, "score": 0.60476536, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 16, "text_snippet": " may use alternative retrieval mechanisms, we collectively refer to the family of conventional approaches as vector RAG . GraphRAG contrasts with vector RAG in its ability to answer queries that require global sensemaking over the entire da"}, {"rank": 3, "score": 0.5859083, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 96, "text_snippet": "tigate these downstream risks for questions of a global nature, which might otherwise be answered by samples of retrieved facts falsely presented as global summaries. 7 Conclusion We have presented GraphRAG, a RAG approach that combines kno"}, {"rank": 4, "score": 0.58137435, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 97, "text_snippet": "omprehensive- ness and diversity of answers, as well as favorable comparisons to a global but graph-free approach using map-reduce source text summarization. For situations requiring many global queries over the same dataset, summaries of r"}, {"rank": 5, "score": 0.5722619, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 63, "text_snippet": "siness, sports, technology, health, and science (Tang and Yang, 2024). The corpus is divided into 3197 ×600-token text chunks, with 100-token overlaps between chunks ( ∼1.7 million tokens). 4.1.2 Conditions We compared six conditions includ"}]}
{"case_index": 271, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"These datasets were gen- erated in a way that answers do not [MASK] to spans in support documents, t\"", "gold": "correspond", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 6.679, "llm_ms": 0.004, "top_contexts": [{"rank": 1, "score": 0.6414823, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 86, "text_snippet": "models for open domain question an- swering. In Proceedings of the 16th Conference of the European Chapter of the Association for Compu- tational Linguistics: Main Volume , pp. 874–880, On- line, April 2021a. Association for Computational L"}, {"rank": 2, "score": 0.6379597, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 3, "score": 0.63654315, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 4, "score": 0.63296205, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 0, "text_snippet": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering Gautier Izacard1,2,3Edouard Grave1 1Facebook AI Research, Paris 2ENS, PSL University, Paris 3Inria, Paris gizacard|egrave@fb.com Abstract Generative mode"}, {"rank": 5, "score": 0.6326442, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 15, "text_snippet": " that answers do not correspond to spans in support documents, thus requiring ab- stractive models. Raffel et al. (2019) showed that generative models are competitive for reading com- prehension tasks such as SQuAD (Rajpurkar et al., 2016),"}]}
{"case_index": 272, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"While Toolformer (Schick et al., 2023) is [MASK] for calling APIs such as Wikipedia.\"", "gold": "pre-trained", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.3333333333333333, "hit_rank": 3, "matched_by": "doc_id+chunk_id", "retrieval_ms": 5.251, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.5594702, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 2, "score": 0.501814, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 3, "score": 0.4943406, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 22, "text_snippet": "rrelevant context and improve robustness. SAIL (Luo et al., 2023) is tuned on instructions to insert retrieved documents before instructions. While Toolformer (Schick et al., 2023) is pre-trained for calling APIs such as Wikipedia. In addit"}, {"rank": 4, "score": 0.47025165, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 165, "text_snippet": "9. URL https://aclanthology. org/2022.tacl-1.15 . 9, 17 Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wizard of wikipedia: Knowledge-powered conversational agents. In International Conference on Lear"}, {"rank": 5, "score": 0.4690244, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 83, "text_snippet": ".49 28.22 5 20.18 17.3728.2419.6518.7127.08 820.5217.60 28.17 16.87 18.05 25.36 10 19.67 17.41 27.62 15.72 17.42 23.60 6 Related Works Retrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi- tec"}]}
{"case_index": 273, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"Type Input Output Definitions Retrieve x/x, y {yes, no, continue } Decides when to retrieve with R ISREL x, d {relevant , irrelevant } dprovides useful [MASK] to solve x.\"", "gold": "information", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 1.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.25, "hit_rank": 4, "matched_by": "doc_id+chunk_id", "retrieval_ms": 5.401, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.5639758, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 31, "text_snippet": "tially supported, no support }All of the verification-worthy statement in y is supported by d. ISUSE x, y {5, 4, 3, 2, 1 } yis a useful response to x. Table 1: Four types of reflection tokens used in SELF-RAG. Each type uses several tokens "}, {"rank": 2, "score": 0.5603967, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 7, "text_snippet": "rmittent special tokens (i.e., reflection tokens ). Reflection tokens are categorized into retrieval andcritique tokens to indicate the need for retrieval and its generation quality respectively (Figure 1 right). In particular, given an inp"}, {"rank": 3, "score": 0.55334395, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 33, "text_snippet": "n x, yt, dfor each d∈D ▷Critique 7: Rank ytbased on ISREL,ISSUP,ISUSE ▷Detailed in Section 3.3 8:else if Retrieve ==Nothen 9: Mgenpredicts ytgiven x ▷ Generate 10: Mgenpredicts ISUSEgiven x, yt ▷Critique Inference overview. Figure 1 and Alg"}, {"rank": 4, "score": 0.54650295, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 30, "text_snippet": "ection tokens (Table 1). 2All work is arXived within a week of this preprint. 3In this paper, we treat one sentence as a segment in our experiments, but our framework is applicable to any segment unit (i.e., sub-sentence). 3  Preprint. Type"}, {"rank": 5, "score": 0.5458863, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 158, "text_snippet": "ility ( t=Tonly) 10: Sample d 11: else if Retrieve is not predicted then 12: Cpredicts ISUSEgiven x, y Add augmented (x, y, d, r )toDgen Training examples. Table 4 show several training examples used for Mtraining. A.3 S ELF-RAGINFERENCE De"}]}
{"case_index": 274, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"der allows to scale to large number of contexts, as it only [MASK] self attention over one context at a time.\"", "gold": "performs", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 17.185, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6174784, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}, {"rank": 2, "score": 0.6086433, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 0, "text_snippet": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering Gautier Izacard1,2,3Edouard Grave1 1Facebook AI Research, Paris 2ENS, PSL University, Paris 3Inria, Paris gizacard|egrave@fb.com Abstract Generative mode"}, {"rank": 3, "score": 0.608158, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 4, "score": 0.6070542, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 86, "text_snippet": "models for open domain question an- swering. In Proceedings of the 16th Conference of the European Chapter of the Association for Compu- tational Linguistics: Main Volume , pp. 874–880, On- line, April 2021a. Association for Computational L"}, {"rank": 5, "score": 0.59666526, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 109, "text_snippet": "ng. InInternational conference on machine learning, pages 3929–3938. PMLR, 2020. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. InPr"}]}
{"case_index": 275, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"Average pooling is applied over the outputs of the last layer to obtain one vector [MASK] per query or document.\"", "gold": "representation", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 8.94, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.599187, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 2, "score": 0.5723673, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 3, "score": 0.56855446, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 235, "text_snippet": "ull results for the cyclic-permutation-de-biased Atlas-XXL can be found in Table 18. A.2 Question answering A.2.1 Training Details For question answering, similarly to the MMLU experiments, we format the input using the following template: "}, {"rank": 4, "score": 0.55552554, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 83, "text_snippet": ".49 28.22 5 20.18 17.3728.2419.6518.7127.08 820.5217.60 28.17 16.87 18.05 25.36 10 19.67 17.41 27.62 15.72 17.42 23.60 6 Related Works Retrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi- tec"}, {"rank": 5, "score": 0.54351807, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 37, "text_snippet": " To cast language modeling in the text-to-text framework, we consider a chunk of Nwords, and split this chunk in two sub-sequences of equal length N/2. Then, the ﬁrst sub-sequence is used as the query, and the second corresponds to the outp"}]}
{"case_index": 276, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"Step 3: Critique outputs and select best [MASK] in a 16th-century novel Las Sergas de Esplandián.\"", "gold": "segmentorigins", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 8.795, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.498073, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 8, "text_snippet": "on demand (Step 1). Subsequently, SELF-RAGconcurrently processes multiple retrieved passages, evaluating their relevance and then generating corresponding task outputs (Step 2). It then generates critique tokens to criticize its own output "}, {"rank": 2, "score": 0.46161354, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 34, "text_snippet": "l predicts the next output segment, as it does in a standard LM. If retrieval is needed, the model generates: a critique token to evaluate the retrieved passage’s relevance, the next response segment, and a critique token to evaluate if the"}, {"rank": 3, "score": 0.4541008, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 48, "text_snippet": "gmented with reflection tokens Dgenusing the standard next token objective: max ME(x,y,r )∼DgenlogpM(y, r|x). (2) Unlike Ctraining (Eq. 1), Mlearns to predict the target output as well as the reflection tokens. During training, we mask out "}, {"rank": 4, "score": 0.45407724, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 15, "text_snippet": "st a critic model during training, reducing overhead. The critic model, in part, is supervised on a dataset of input, output, and corresponding reflection tokens collected by prompting a propriety LM (i.e., GPT-4; OpenAI 2023). While we dra"}, {"rank": 5, "score": 0.44028366, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}]}
{"case_index": 277, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"One simple way to [MASK] the retrieved documents as part of the input to the LM is to prepend xwith all kdocuments.\"", "gold": "incorporate", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.3333333333333333, "hit_rank": 3, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.16, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.62226456, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 7, "text_snippet": " retrieved documents are prepended to the input context and fed into the black-box LM to make the final prediction. Because the LM context length limits the number of documents that can be prepended, we also introduce a new ensemble scheme "}, {"rank": 2, "score": 0.62141377, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 3, "score": 0.58888113, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 25, "text_snippet": "beddings. 3.2. Input Reformulation The retrieved top- kdocuments provide rich information about the original input context xand can potentially help the LM to make a better prediction. One simple way to incorporate the retrieved documents a"}, {"rank": 4, "score": 0.5790074, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 37, "text_snippet": " To cast language modeling in the text-to-text framework, we consider a chunk of Nwords, and split this chunk in two sub-sequences of equal length N/2. Then, the ﬁrst sub-sequence is used as the query, and the second corresponds to the outp"}, {"rank": 5, "score": 0.568547, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 28, "text_snippet": "ethod of prepending all the retrieved docu-  REPLUG: Retrieval-Augmented Black-Box Language Models ments, our ensemble methods do not incur additional com- putational cost overhead. 4. R EPLUG LSR: Training the Dense Retriever Instead of re"}]}
{"case_index": 278, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"During CPT, we input the first stokens x1:sinto the encoder and use its output to assist the decoder in [MASK] the next otokens xs+1:s+o.\"", "gold": "predicting", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.022, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.5445838, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 17, "text_snippet": "FRAG achieves better performance than LLaMA without incurring higher latency in the downstream applications. 2 Model Architecture We denote the decoder model as Mdecand the encoder model as Menc. Given an input with Ttokens x1, x2, . . . , "}, {"rank": 2, "score": 0.5296746, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 27, "text_snippet": "repare the model for downstream tasks utilizing chunk embeddings. To further enhance performance, we introduce selective compression via RL. After aligning the encoder and decoder through CPT, we apply supervised fine-tuning (SFT) to adapt "}, {"rank": 3, "score": 0.5245559, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 34, "text_snippet": "s the target answer. Output: An updated encdec θ 1:T ← ∅ 2:fori∈ {1, . . . , T }do 3: vr← −∞ 4: forj∈ {1, . . . , n }do 5: sj=Decode (Mt,[pj;xi;Di]) 6: vj=Score (M,yi,[sj;xi]) 7: ifvj> vrthen 8: st←sj,vr←vj 9: vd=Score (M,yi,[xi]) 10: ifvr<"}, {"rank": 4, "score": 0.5142608, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 26, "text_snippet": "e detailed discussion 1REFRAGwithout cache means that we recompute the chunk embedding for the context and take this latency into account. 3  on empirical evaluation is in section A. 3 Methodology To align the encoder and decoder, we follow"}, {"rank": 5, "score": 0.5051883, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 28, "text_snippet": " assist the decoder in predicting the next otokens xs+1:s+o. This task encourages the model to leverage contextual information for next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any en"}]}
{"case_index": 279, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"The ﬁrst loss that we consider is based on the attention scores of the language model, and is heavily [MASK] by Izacard & Grave (2021).\"", "gold": "inspired", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 9.789, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.6139324, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 111, "text_snippet": " arXiv:2208.03299, 2022. https://arxiv.org/abs/ 2208.03299. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Edouard Grave, and Sebastian Riedel. Atlas: Few-shot learn"}, {"rank": 2, "score": 0.6131694, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 3, "score": 0.5702045, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 110, "text_snippet": "s of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 874–880, Online, April 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.eacl-main.74. https://aclant"}, {"rank": 4, "score": 0.5693468, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 0, "text_snippet": "Atlas: Few-shot Learning with Retrieval Augmented Language Models Gautier Izacard∗ ∗,♦,♣,♥gizacard@fb.com Patrick Lewis∗,♦plewis@fb.com Maria Lomeli♦marialomeli@fb.com Lucas Hosseini♦hoss@fb.com Fabio Petroni♦fabiopetroni@fb.com Timo Schick"}, {"rank": 5, "score": 0.56583524, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 83, "text_snippet": ".49 28.22 5 20.18 17.3728.2419.6518.7127.08 820.5217.60 28.17 16.87 18.05 25.36 10 19.67 17.41 27.62 15.72 17.42 23.60 6 Related Works Retrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi- tec"}]}
{"case_index": 280, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"ous NLP tasks, including language mod- eling (Min et al., 2022; Borgeaud et al., 2022; Khandelwal et al., 2020) and [MASK] question answering (Lewis et al., 2020; Izacard et al., 2022b; Hu et al., 2022).\"", "gold": "open-domain", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 5.774, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6221816, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 83, "text_snippet": "sser, S., and Leahy, C. The Pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027 , 2020.  REPLUG: Retrieval-Augmented Black-Box Language Models Goyal, A., Friesen, A., Banino, A., Weber, T., Ke, N. R"}, {"rank": 2, "score": 0.61783934, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 94, "text_snippet": "iedel, S. Question and an- swer test-train overlap in open-domain question answer- ing datasets. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume , pp. 1000–1008, 20"}, {"rank": 3, "score": 0.60551655, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 37, "text_snippet": " To cast language modeling in the text-to-text framework, we consider a chunk of Nwords, and split this chunk in two sub-sequences of equal length N/2. Then, the ﬁrst sub-sequence is used as the query, and the second corresponds to the outp"}, {"rank": 4, "score": 0.60228974, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 0, "text_snippet": "REPLUG: Retrieval-Augmented Black-Box Language Models Weijia Shi,1 *Sewon Min,1Michihiro Yasunaga,2Minjoon Seo,3Rich James,4Mike Lewis,4 Luke Zettlemoyer1 4Wen-tau Yih4 Abstract We introduce REPLUG, a retrieval-augmented lan- guage modeling"}, {"rank": 5, "score": 0.5994417, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 104, "text_snippet": "2a. Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V ., et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 , 2022b. Zhong, Z., Lei, T., and Chen"}]}
{"case_index": 281, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"The Contriever uses a dual-encoder architecture, where the query and documents are embedded [MASK] by a transformer encoder (Huang et al., 2013; Karpukhin et al., 2020).\"", "gold": "independently", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 8.99, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6201328, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 2, "score": 0.5949644, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 9, "text_snippet": " strong few-shot learning, despite having lower parameter counts than other powerful recent few-shot learners. Atlasretrieves relevant documents based on the current context by using a general-purpose dense retriever using a dual-encoder ar"}, {"rank": 3, "score": 0.5910046, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 111, "text_snippet": " arXiv:2208.03299, 2022. https://arxiv.org/abs/ 2208.03299. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Edouard Grave, and Sebastian Riedel. Atlas: Few-shot learn"}, {"rank": 4, "score": 0.58302104, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 20, "text_snippet": "eir corresponding embeddings. The Contriever model is pre-trained using the MoCo contrastive loss (He et al., 2020), and uses unsupervised data only. As shown in the following section, an advantage of dense retrievers is that both query and"}, {"rank": 5, "score": 0.5756145, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 18, "text_snippet": "y retrieving the top-k relevant documents from a large corpus of text with the retriever. Then, these documents are fed to the language model, along with the query, which in turns generates the output. Both the retriever and the language mo"}]}
{"case_index": 282, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"Recently, multiple works show that retrieval systems entirely based on dense [MASK] and approximate nearest neighbors were competi- tive with traditional approaches.\"", "gold": "representation", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 5.558, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.67482823, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}, {"rank": 2, "score": 0.66421974, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 101, "text_snippet": "ov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906 , 2020. URL https://arxiv.org/abs/2004.04906 . [27] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, "}, {"rank": 3, "score": 0.6571807, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 0, "text_snippet": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering Gautier Izacard1,2,3Edouard Grave1 1Facebook AI Research, Paris 2ENS, PSL University, Paris 3Inria, Paris gizacard|egrave@fb.com Abstract Generative mode"}, {"rank": 4, "score": 0.6312203, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 86, "text_snippet": "models for open domain question an- swering. In Proceedings of the 16th Conference of the European Chapter of the Association for Compu- tational Linguistics: Main Volume , pp. 874–880, On- line, April 2021a. Association for Computational L"}, {"rank": 5, "score": 0.62850523, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}]}
{"case_index": 283, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"Both the retriever and the language model are based on [MASK] transformer networks, which we describe in more detail below.\"", "gold": "pre-trained", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 6.009, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.619191, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 2, "score": 0.6109183, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 111, "text_snippet": " arXiv:2208.03299, 2022. https://arxiv.org/abs/ 2208.03299. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Edouard Grave, and Sebastian Riedel. Atlas: Few-shot learn"}, {"rank": 3, "score": 0.608487, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 83, "text_snippet": ".49 28.22 5 20.18 17.3728.2419.6518.7127.08 820.5217.60 28.17 16.87 18.05 25.36 10 19.67 17.41 27.62 15.72 17.42 23.60 6 Related Works Retrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi- tec"}, {"rank": 4, "score": 0.58528614, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 2, "text_snippet": "ings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including"}, {"rank": 5, "score": 0.58269143, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 104, "text_snippet": "2a. Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V ., et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 , 2022b. Zhong, Z., Lei, T., and Chen"}]}
{"case_index": 284, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"Ms) have attracted increasing attention and exhibited impressive abili- ties to understand [MASK] and generate fluent language texts (Brown et al., 2020; Ouyang et al., 2022; Touvron et al., 2023a).\"", "gold": "instructions", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 5.546, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.5924872, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 83, "text_snippet": ".49 28.22 5 20.18 17.3728.2419.6518.7127.08 820.5217.60 28.17 16.87 18.05 25.36 10 19.67 17.41 27.62 15.72 17.42 23.60 6 Related Works Retrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi- tec"}, {"rank": 2, "score": 0.56154025, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 3, "score": 0.5586165, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 4, "score": 0.5580071, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 0, "text_snippet": "Corrective Retrieval Augmented Generation Shi-Qi Yan1*, Jia-Chen Gu2*, Yun Zhu3, Zhen-Hua Ling1 1National Engineering Research Center of Speech and Language Information Processing, University of Science and Technology of China, Hefei, China"}, {"rank": 5, "score": 0.54903626, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 56, "text_snippet": " the reader and the retriever by leveraging the perplexity of the output generated by the reader. Sachan et al. (2021) and Lee et al. (2021a) both employ salient span masking to pre-train retrievers, leveraging the perplexity and attention "}]}
{"case_index": 285, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"Experiments show that SELF- RAG(7B and 13B parameters) significantly outperforms state-of-the-art LLMs and [MASK] models on a diverse set of tasks.\"", "gold": "retrieval-augmented", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 14.325, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6620238, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 3, "text_snippet": "e phase, enabling it to tailor its behavior to diverse task requirements. Experiments show that SELF- RAG(7B and 13B parameters) significantly outperforms state-of-the-art LLMs and retrieval-augmented models on a diverse set of tasks. Speci"}, {"rank": 2, "score": 0.6440586, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 0, "text_snippet": "Preprint. SELF-RAG: LEARNING TO RETRIEVE , GENERATE ,AND CRITIQUE THROUGH SELF-REFLECTION Akari Asai†, Zeqiu Wu†, Yizhong Wang†§, Avirup Sil‡, Hannaneh Hajishirzi†§ †University of Washington§Allen Institute for AI‡IBM Research AI {akari,zeq"}, {"rank": 3, "score": 0.6383876, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 96, "text_snippet": "ains an LM to learn to retrieve, generate, and critique text passages and its own generation by predicting the next tokens from its original vocabulary as well as newly added special tokens, called reflection tokens. SELF-RAGfurther enables"}, {"rank": 4, "score": 0.62047833, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 27, "text_snippet": "n (reasoning path consistency) and without retrieval. Recent work on LLM refinement (Dhuliawala et al., 2023; Madaan et al., 2023; Paul et al., 2023) prompts a model to generate task output, natural language feedback and refined task output"}, {"rank": 5, "score": 0.6195218, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 57, "text_snippet": "en studied in RLHF (Touvron et al., 2023; Wu et al., 2023), which often requires training to change models’ behaviors. S ELF-RAGtailors an LM with no additional training. 4 E XPERIMENTS 4.1 T ASKS AND DATASETS We conduct evaluations of our "}]}
{"case_index": 286, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"GraphRAG leverages summaries over large sections of the data source as a form of ”[MASK]” (described in Cheng et al.\"", "gold": "self-memory", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 5.791, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.7064409, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 0, "text_snippet": "From Local to Global: A GraphRAG Approach to Query-Focused Summarization Darren Edge1†Ha Trinh1†Newman Cheng2Joshua Bradley2Alex Chao3 Apurva Mody3Steven Truitt2Dasha Metropolitansky1Robert Osazuwa Ness1 Jonathan Larson1 1Microsoft Research"}, {"rank": 2, "score": 0.6579758, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 17, "text_snippet": " are later used to answer queries as in Mao et al. 2020). These summaries are generated in parallel and iteratively aggregated into global summaries, similar to prior techniques (Feng et al., 2023; Gao et al., 2023; Khattab et al., 2022; Sh"}, {"rank": 3, "score": 0.63853955, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 97, "text_snippet": "omprehensive- ness and diversity of answers, as well as favorable comparisons to a global but graph-free approach using map-reduce source text summarization. For situations requiring many global queries over the same dataset, summaries of r"}, {"rank": 4, "score": 0.6297163, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 96, "text_snippet": "tigate these downstream risks for questions of a global nature, which might otherwise be answered by samples of retrieved facts falsely presented as global summaries. 7 Conclusion We have presented GraphRAG, a RAG approach that combines kno"}, {"rank": 5, "score": 0.6089819, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 81, "text_snippet": "ng. Tuning element extraction prompts may help to retain more of these details in the GraphRAG index. Community summaries vs. source texts . When comparing community summaries to source texts using GraphRAG, community summaries generally pr"}]}
{"case_index": 287, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"Training a [MASK] for QA task works similarly, but scoring will evaluate whether the LM will generate t\"", "gold": "compressor", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 9.221, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.61272234, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 30, "text_snippet": "ilarly, but scoring will evaluate whether the LM will generate the correct answer with summary prepended (change in line 6). Pseudo code for the QA tasks is in Figure 6 the Appendix. We train our encoder with a contrastive loss (Karpukhin e"}, {"rank": 2, "score": 0.61229676, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 219, "text_snippet": "4.6 38.8 52.8 A Training details and additional results A.1 MMLU A.1.1 Training Details Featurization MMLU consists of multiple choice questions with four possible lexicalized answer options. We represent the input using the following templ"}, {"rank": 3, "score": 0.6050768, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 4, "score": 0.58824396, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 0, "text_snippet": "RECOMP: I MPROVING RETRIEVAL -AUGMENTED LM S WITH COMPRESSION AND SELECTIVE AUGMENTATION Fangyuan Xu1, Weijia Shi2, Eunsol Choi1 Department of Computer Science 1The University of Texas at Austin 2University of Washington {fangyuan,eunsol }@"}, {"rank": 5, "score": 0.5597747, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 83, "text_snippet": ".49 28.22 5 20.18 17.3728.2419.6518.7127.08 820.5217.60 28.17 16.87 18.05 25.36 10 19.67 17.41 27.62 15.72 17.42 23.60 6 Related Works Retrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi- tec"}]}
{"case_index": 288, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"This suggests that selecting the most relevant information for [MASK] tasks is still crucial.\"", "gold": "knowledge-intensive", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.2, "hit_rank": 5, "matched_by": "doc_id+chunk_id", "retrieval_ms": 5.847, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.54495525, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 2, "score": 0.52642006, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 0, "text_snippet": "Published as a conference paper at ICLR 2024 RAPTOR: R ECURSIVE ABSTRACTIVE PROCESSING FOR TREE-ORGANIZED RETRIEVAL Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D. Manning Stanford University psarthi@cs."}, {"rank": 3, "score": 0.51780975, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 67, "text_snippet": "ring approaches, which allows it to capture a range of information, from general themes to specific details, contributing to its overall strong performance. 4.1 C ONTRIBUTION OF THE TREE STRUCTURE We examine the contribution of each layer o"}, {"rank": 4, "score": 0.5043218, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 22, "text_snippet": " RAPTOR addresses the issue of semantic depth and connection in reading by building a recursive tree structure that balances broader thematic comprehension with granular details and which allows nodes to be grouped based on semantic sim- il"}, {"rank": 5, "score": 0.500209, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 13, "text_snippet": "nd to underutilize long-range context and see diminishing performance as con- text length increases, especially when pertinent information is embedded within a lengthy context. Moreover, practically, use of long contexts is expensive and sl"}]}
{"case_index": 289, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"Large-scale unregulated training data collection, low proportion of high-quality sampling data, [MASK] of data allocation in the input space, and many other realistic factors could impact the LLMs and exacerbate the problems.\"", "gold": "imperfection", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 5.078, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6090854, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 4, "text_snippet": " for long-form generations relative to these models.1 1 I NTRODUCTION State-of-the-art LLMs continue to struggle with factual errors (Mallen et al., 2023; Min et al., 2023) despite their increased model and data scale (Ouyang et al., 2022)."}, {"rank": 2, "score": 0.6016692, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 83, "text_snippet": ".49 28.22 5 20.18 17.3728.2419.6518.7127.08 820.5217.60 28.17 16.87 18.05 25.36 10 19.67 17.41 27.62 15.72 17.42 23.60 6 Related Works Retrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi- tec"}, {"rank": 3, "score": 0.58754635, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 18, "text_snippet": "pact the LLMs and exacerbate the problems. Thus, it is obvious that the lack of accurate and specific knowledge can lead to misleading or even inaccurate generation, which will severely hurt the experience of users in most practical applica"}, {"rank": 4, "score": 0.5872122, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 83, "text_snippet": " in Table 6. The findings indicate that the self-correction mecha- nism incurs only modest computational overheadwhile significantly enhancing performance, thereby validating its lightweight nature. 6 Conclusion & Limitation This paper stud"}, {"rank": 5, "score": 0.5730638, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 219, "text_snippet": "4.6 38.8 52.8 A Training details and additional results A.1 MMLU A.1.1 Training Details Featurization MMLU consists of multiple choice questions with four possible lexicalized answer options. We represent the input using the following templ"}]}
{"case_index": 290, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"Early US models include X, S and PRO-4X, with a choice of 6-speed manual or 5-speed automatic [MASK], a choice of [...] moved from Smyrna, Tennessee, to Nissan's facility in Canton, Mississippi.\"", "gold": "transmissions", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.533, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.68121755, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 8, "text_snippet": " moved from Smyrna, Tennessee, to Nissan's facility in Canton, Mississippi. Early US models include X, S and PRO-4X, with a choice of 6-speed manual or 5-speed automatic transmissions, a choice of [...] moved from Smyrna, Tennessee, to Niss"}, {"rank": 2, "score": 0.67016196, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 9, "text_snippet": "acility in Canton, Mississippi. Early US models include X, S and PRO-4X, with a choice of 6-speed manual or 5-speed automatic transmissions, a choice of [...] moved from Smyrna, Tennessee, to Nissan's facility in Canton, Mississippi. Early "}, {"rank": 3, "score": 0.5522467, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 10, "text_snippet": "els include X, S and PRO-4X, with a choice of 6-speed manual…Retrieved documents DRECOMP (58 tokens)RetrieveCompressPrependNo retrieval (0 tokens)RALM (749 tokens)2010 ❌ ✅2015SummaryInput query xwhen did they stop making the nissan xterra?B"}, {"rank": 4, "score": 0.4585326, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 5, "score": 0.45177418, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 7, "text_snippet": "rieved evidence documents, and guide RALM to generate desired outputs when prepended to the input. To satisfy both efficiency and effectiveness constraints, our compressor strategically performs selective augmentation by generating an empty"}]}
{"case_index": 291, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"This allows the generator to choose content from several documents when [MASK] an answer.\"", "gold": "producing", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 20.999, "llm_ms": 0.008, "top_contexts": [{"rank": 1, "score": 0.6095701, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 2, "score": 0.6054773, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 3, "score": 0.53462464, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 37, "text_snippet": "(line 9). This allows for selective augmentation and mitigates the risk of prepending irrelevant documents. 5https://huggingface.co/facebook/contriever-msmarco 6The exact prompts can be found in Table 6 in A.2. 7We use gpt-3.5-turbo in all "}, {"rank": 4, "score": 0.53153944, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 83, "text_snippet": ".49 28.22 5 20.18 17.3728.2419.6518.7127.08 820.5217.60 28.17 16.87 18.05 25.36 10 19.67 17.41 27.62 15.72 17.42 23.60 6 Related Works Retrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi- tec"}, {"rank": 5, "score": 0.53041935, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 18, "text_snippet": "pact the LLMs and exacerbate the problems. Thus, it is obvious that the lack of accurate and specific knowledge can lead to misleading or even inaccurate generation, which will severely hurt the experience of users in most practical applica"}]}
{"case_index": 292, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting [MASK] in generative modeling and retrieval for open domain question answering.\"", "gold": "developments", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.534, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6981604, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}, {"rank": 2, "score": 0.6953155, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 0, "text_snippet": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering Gautier Izacard1,2,3Edouard Grave1 1Facebook AI Research, Paris 2ENS, PSL University, Paris 3Inria, Paris gizacard|egrave@fb.com Abstract Generative mode"}, {"rank": 3, "score": 0.6760254, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 86, "text_snippet": "models for open domain question an- swering. In Proceedings of the 16th Conference of the European Chapter of the Association for Compu- tational Linguistics: Main Volume , pp. 874–880, On- line, April 2021a. Association for Computational L"}, {"rank": 4, "score": 0.6447518, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 16, "text_snippet": "0) and Lewis et al. (2020) in- troduced retrieval augmented generative models for open domain question answering. Our approach differs from these works by how the generative model processes the retrieved passages. This al- lows to scale to "}, {"rank": 5, "score": 0.627645, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 175, "text_snippet": "ext generation with relevance sampling. ArXiv, abs/2207.03030, 2022. 7, 15 Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry Heck. Learning deep structured semantic models for web search using clickthrough data. In Pro"}]}
{"case_index": 293, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"further confuse LMs with irrelevant information, degrading model [MASK] (Mallen et al., 2022; Shi et al., 2023a).\"", "gold": "performances", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.398, "llm_ms": 0.004, "top_contexts": [{"rank": 1, "score": 0.6181774, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 135, "text_snippet": "arXiv preprint arXiv:2305.00633 , 2023. URL https://arxiv.org/abs/2305.00633 . Fangyuan Xu, Weijia Shi, and Eunsol Choi. Recomp: Improving retrieval-augmented lms with compression and selective augmentation, 2023. URL https://arxiv.org/abs/"}, {"rank": 2, "score": 0.5957153, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 6, "text_snippet": "further confuse LMs with irrelevant information, degrading model performances (Mallen et al., 2022; Shi et al., 2023a). To overcome such limitations, we propose RECOMP (Retrieve, Com press, Prepend), an inter- mediate step for RALMs which c"}, {"rank": 3, "score": 0.5945981, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 0, "text_snippet": "RECOMP: I MPROVING RETRIEVAL -AUGMENTED LM S WITH COMPRESSION AND SELECTIVE AUGMENTATION Fangyuan Xu1, Weijia Shi2, Eunsol Choi1 Department of Computer Science 1The University of Texas at Austin 2University of Washington {fangyuan,eunsol }@"}, {"rank": 4, "score": 0.5596653, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 5, "score": 0.5586947, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 82, "text_snippet": " and allowing the compression models to perform selective augmentation. Our experiments show that our compressors can improve the efficiency of retrieval augmented LMs significantly with minimal drop in performances. ACKNOWLEDGEMENT We than"}]}
{"case_index": 294, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"[MASK] inference latency for LLMs with extensive context is an active area of research, with approaches 1arXiv:2509.01092v2 [cs.CL] 12 Oct 2\"", "gold": "optimizing", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 6.384, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.58981895, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 3, "text_snippet": " on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30 .85×the time-t"}, {"rank": 2, "score": 0.5798217, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 0, "text_snippet": "REFRAG: Rethinking RAG based Decoding Xiaoqiang Lin1,2,∗,Aritra Ghosh1,Bryan Kian Hsiang Low2,Anshumali Shrivastava1,3,Vijai Mohan1 1Meta Superintelligence Labs,2National University of Singapore,3Rice University ∗Work done at Meta Large Lan"}, {"rank": 3, "score": 0.5587587, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 96, "text_snippet": "ong document summarization, demonstrate thatREFRAG achieves up to30 .85×TTFT acceleration (3 .75×over previous state-of-the-art methods) without any loss in perplexity or downstream accuracy. Our results highlight the importance of speciali"}, {"rank": 4, "score": 0.55580103, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 2, "text_snippet": "ated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns th"}, {"rank": 5, "score": 0.55432665, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 12, "text_snippet": "is information is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-a"}]}
{"case_index": 295, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"Unfortunately, this approach does not scale with the number of documents, since the [MASK] in the encoder results in a quadratic complexity with respect to the number of documents.\"", "gold": "self-attention", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 7.249, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.60432804, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 2, "score": 0.5807221, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 235, "text_snippet": "ull results for the cyclic-permutation-de-biased Atlas-XXL can be found in Table 18. A.2 Question answering A.2.1 Training Details For question answering, similarly to the MMLU experiments, we format the input using the following template: "}, {"rank": 3, "score": 0.5781486, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 4, "score": 0.5627892, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 83, "text_snippet": ".49 28.22 5 20.18 17.3728.2419.6518.7127.08 820.5217.60 28.17 16.87 18.05 25.36 10 19.67 17.41 27.62 15.72 17.42 23.60 6 Related Works Retrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi- tec"}, {"rank": 5, "score": 0.56048083, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 111, "text_snippet": " arXiv:2208.03299, 2022. https://arxiv.org/abs/ 2208.03299. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Edouard Grave, and Sebastian Riedel. Atlas: Few-shot learn"}]}
{"case_index": 296, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"We conclude with careful analyses of our approach that reveal both its strength and [MASK], thereby building foundation for\"", "gold": "weaknesses", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 30.848, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.61454666, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 0, "text_snippet": "RECOMP: I MPROVING RETRIEVAL -AUGMENTED LM S WITH COMPRESSION AND SELECTIVE AUGMENTATION Fangyuan Xu1, Weijia Shi2, Eunsol Choi1 Department of Computer Science 1The University of Texas at Austin 2University of Washington {fangyuan,eunsol }@"}, {"rank": 2, "score": 0.5928918, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 82, "text_snippet": " and allowing the compression models to perform selective augmentation. Our experiments show that our compressors can improve the efficiency of retrieval augmented LMs significantly with minimal drop in performances. ACKNOWLEDGEMENT We than"}, {"rank": 3, "score": 0.58899075, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 135, "text_snippet": "arXiv preprint arXiv:2305.00633 , 2023. URL https://arxiv.org/abs/2305.00633 . Fangyuan Xu, Weijia Shi, and Eunsol Choi. Recomp: Improving retrieval-augmented lms with compression and selective augmentation, 2023. URL https://arxiv.org/abs/"}, {"rank": 4, "score": 0.56505287, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 6, "text_snippet": "further confuse LMs with irrelevant information, degrading model performances (Mallen et al., 2022; Shi et al., 2023a). To overcome such limitations, we propose RECOMP (Retrieve, Com press, Prepend), an inter- mediate step for RALMs which c"}, {"rank": 5, "score": 0.5618687, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}]}
{"case_index": 297, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"A [MASK] retrieval evaluator is designed to assess the overall quality of retrieved documents for a query.\"", "gold": "lightweight", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.806, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.5977147, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 10, "text_snippet": " should not have been equally referred to and involved in RAG. On account of the above issues, this paper particularly studies the scenarios where the retriever returns inaccurate results. A method named Corrective Retrieval- Augmented Gene"}, {"rank": 2, "score": 0.57479584, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 29, "text_snippet": "bitrary generative model can be adopted. 4.2 Retrieval Evaluator It is natural to wonder whether the retrieved docu- ments are accurate or not before using them, which is significant since irrelevant or misleading mes- sages can be identifi"}, {"rank": 3, "score": 0.5715806, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 28, "text_snippet": "sition, filter, and recomposition (Section 4.4). If the action Incorrect is triggered, the retrieved documents will be discarded. Instead, web searches are resorted to and regarded as complementary knowledge sources for corrections (Section"}, {"rank": 4, "score": 0.570997, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 1, "text_snippet": "evitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Al- though retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heav"}, {"rank": 5, "score": 0.55805385, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 31, "text_snippet": "neratorkex+xkex+ Figure 2: An overview of the proposed CRAG at inference. A retrieval evaluator is constructed to evaluate the relevance of the retrieved documents to the input, and estimate a confidence degree based on which different know"}]}
{"case_index": 298, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"F-RAGis a framework that enhances the quality and factuality of an LLM through retrieval and [MASK], without sacrificing LLM’s original creativity and versatility.\"", "gold": "self-reflection", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 9.475, "llm_ms": 0.008, "top_contexts": [{"rank": 1, "score": 0.7293253, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 28, "text_snippet": "F-RAGis a framework that enhances the quality and factuality of an LLM through retrieval and self-reflection, without sacrificing LLM’s original creativity and versatility. Our end-to-end training lets an LM Mgenerate text informed by retri"}, {"rank": 2, "score": 0.63444304, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 6, "text_snippet": "y trained to leverage and follow facts from provided passages. This work introduces Self-Reflective Retrieval-augmented Generation ( SELF-RAG)to improve an LLM’s generation quality, including its factual accuracy without hurting its versati"}, {"rank": 3, "score": 0.62548804, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 2, "text_snippet": "ew framework called Self-Reflective Retrieval-Augmented Gen- eration ( SELF-RAG)that enhances an LM’s quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passage"}, {"rank": 4, "score": 0.62082744, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 27, "text_snippet": "n (reasoning path consistency) and without retrieval. Recent work on LLM refinement (Dhuliawala et al., 2023; Madaan et al., 2023; Paul et al., 2023) prompts a model to generate task output, natural language feedback and refined task output"}, {"rank": 5, "score": 0.6186753, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 96, "text_snippet": "ains an LM to learn to retrieve, generate, and critique text passages and its own generation by predicting the next tokens from its original vocabulary as well as newly added special tokens, called reflection tokens. SELF-RAGfurther enables"}]}
{"case_index": 299, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"The overall probability [MASK] is a weighted combination P(x) =PK k=1πkN(x;µk,Σk), where πksignifies the mixture weight for the kthGaussian distribution.\"", "gold": "distribution", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.25, "hit_rank": 4, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.011, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.5552677, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 2, "score": 0.52879804, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 3, "score": 0.5244428, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 220, "text_snippet": "f the correct answer: [MASK_0] {correct answer option letter} This format closely matches the format of MLM pre-training objective, aiding few-shot learning. When training, we permute the order of the answer options, i.e. shuﬄing which answ"}, {"rank": 4, "score": 0.5106281, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 28, "text_snippet": " Published as a conference paper at ICLR 2024 Given a set of Ntext segments, each represented as a d-dimensional dense vector embedding, the likelihood of a text vector, x, given its membership in the kthGaussian distribution, is denoted by"}, {"rank": 5, "score": 0.4998168, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 235, "text_snippet": "ull results for the cyclic-permutation-de-biased Atlas-XXL can be found in Table 18. A.2 Question answering A.2.1 Training Details For question answering, similarly to the MMLU experiments, we format the input using the following template: "}]}
{"case_index": 300, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"[MASK], GraphRAG recursively creates increasingly global summaries by using the LLM to create summaries spanni\"", "gold": "specifically", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 6.19, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6467674, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 0, "text_snippet": "From Local to Global: A GraphRAG Approach to Query-Focused Summarization Darren Edge1†Ha Trinh1†Newman Cheng2Joshua Bradley2Alex Chao3 Apurva Mody3Steven Truitt2Dasha Metropolitansky1Robert Osazuwa Ness1 Jonathan Larson1 1Microsoft Research"}, {"rank": 2, "score": 0.5906429, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 17, "text_snippet": " are later used to answer queries as in Mao et al. 2020). These summaries are generated in parallel and iteratively aggregated into global summaries, similar to prior techniques (Feng et al., 2023; Gao et al., 2023; Khattab et al., 2022; Sh"}, {"rank": 3, "score": 0.5884789, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 10, "text_snippet": "cted communities, with summaries at higher levels of the hierarchy recursively incorporating lower-level summaries. Together, these community summaries provide global descriptions and insights over the corpus. Finally, GraphRAG answers quer"}, {"rank": 4, "score": 0.5864656, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 97, "text_snippet": "omprehensive- ness and diversity of answers, as well as favorable comparisons to a global but graph-free approach using map-reduce source text summarization. For situations requiring many global queries over the same dataset, summaries of r"}, {"rank": 5, "score": 0.5765391, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 22, "text_snippet": "gly global summaries by using the LLM to create summaries spanning this community hierarchy. 2.3 Adaptive benchmarking for RAG Evaluation Many benchmark datasets for open-domain question answering exist, including HotPotQA (Yang et al., 201"}]}
