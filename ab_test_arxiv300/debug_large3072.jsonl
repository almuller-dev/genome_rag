{"case_index": 1, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"Empirical results on six tasks, including reasoning and long-form generation, demonstrate that SELF- RAGsignificantly outperforms pre-trained and [MASK] LLMs that have more parameters and widely adopted RAG approaches with higher citation accuracy.\"", "gold": "instruction-tuned", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.2, "hit_rank": 5, "matched_by": "doc_id+chunk_id", "retrieval_ms": 12.803, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.74234056, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 96, "text_snippet": "ains an LM to learn to retrieve, generate, and critique text passages and its own generation by predicting the next tokens from its original vocabulary as well as newly added special tokens, called reflection tokens. SELF-RAGfurther enables"}, {"rank": 2, "score": 0.7248726, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 3, "text_snippet": "e phase, enabling it to tailor its behavior to diverse task requirements. Experiments show that SELF- RAG(7B and 13B parameters) significantly outperforms state-of-the-art LLMs and retrieval-augmented models on a diverse set of tasks. Speci"}, {"rank": 3, "score": 0.6820439, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 75, "text_snippet": "s Llama2 65Bto refine output. Comparison against baselines with retrieval. As shown in Tables 2 (bottom), our SELF-RAGalso outperforms existing RAG in many tasks, obtaining the best performance among non-proprietary LM-based models on all t"}, {"rank": 4, "score": 0.66343606, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 6, "text_snippet": "y trained to leverage and follow facts from provided passages. This work introduces Self-Reflective Retrieval-augmented Generation ( SELF-RAG)to improve an LLM’s generation quality, including its factual accuracy without hurting its versati"}, {"rank": 5, "score": 0.66015214, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 17, "text_snippet": "el beam search using the weighted linear sum of the reflection token probabilities as segment score. Empirical results on six tasks, including reasoning and long-form generation, demonstrate that SELF- RAGsignificantly outperforms pre-train"}]}
{"case_index": 2, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"In real [MASK] (e.g., RAG), the context is the dominating part of the input (i.e., s≫q) and hence the overall input to the decod\"", "gold": "applications", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.3333333333333333, "hit_rank": 3, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.583, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.60023385, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 12, "text_snippet": "is information is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-a"}, {"rank": 2, "score": 0.579626, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 3, "text_snippet": " on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30 .85×the time-t"}, {"rank": 3, "score": 0.5742265, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 21, "text_snippet": "uery  Encoder Figure 1The main design ofREFRAG. The input context is chunked and processed by the light-weight encoder to produce chunk embeddings, which are precomputable for efficient reuse. A light-weight RL policy decide few chunks to e"}, {"rank": 4, "score": 0.55964506, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 95, "text_snippet": "oducedREFRAG, a novel and efficient decoding framework tailored for RAG applications. By leveraging the inherent sparsity and block-diagonal attention patterns present in RAG contexts,REFRAG compresses, senses, and expands context represent"}, {"rank": 5, "score": 0.55824006, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 1, "text_snippet": "AG). However, processing long-context inputs introduces significant system latency and demands substantial memory for the key-value cache, resulting in reduced throughput and a fundamental trade-off between knowledge enrichment and system e"}]}
{"case_index": 3, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"[MASK] language models (Khandelwal et al., 2020; Borgeaud et al., 2022; Izacard et al., 2022b; Yasunaga et al., 2022), in con- trast, can retrieve knowledge from an external datastore when needed, potentially reducing hallucination and increas- ing coverage.\"", "gold": "retrieval-augmented", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 13.461, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.77433604, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 3, "text_snippet": "itly in their parameters. However, they are also prone to hallucination and cannot represent the full long tail of knowledge from the training corpus. Retrieval-augmented language models (Khandelwal et al., 2020; Borgeaud et al., 2022; Izac"}, {"rank": 2, "score": 0.6815374, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 15, "text_snippet": "anguage models makes this approach infeasible. To ad- dress the challenges posed by large language models, we investigate retrieval-augmentation in the black-box setting , where users only have access to the model predictions and cannot acc"}, {"rank": 3, "score": 0.6731906, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 4, "text_snippet": "trieval-augmented language models (RALMs) (Khandelwal et al., 2019; Izacard et al., 2022; Lewis et al., 2020; Borgeaud et al., 2022) have shown impressive performance on knowledge-intensive tasks (Kwiatkowski et al., 2019; Petroni et al., 2"}, {"rank": 4, "score": 0.6695597, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 5, "text_snippet": "ting the LM’s parameters, REPLUG treats the language model as a black box and augments it with a frozen or tunable retriever. This black-box assumption makes REPLUG applicable to large LMs (i.e., >100B parameters), which are often served vi"}, {"rank": 5, "score": 0.66894734, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 12, "text_snippet": "gnals, resulting in improved retrieval quality. •Evaluations on language modeling (§6), open-domain QA and MMLU demonstrate that REPLUG can im- prove the performance of various language models such as GPT, OPT and BLOOM, including very larg"}]}
{"case_index": 4, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"(2019): •[MASK] (Kwiatkowski et al., 2019) contains questions corresponding to Google search queries.\"", "gold": "naturalquestions", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.2, "hit_rank": 5, "matched_by": "doc_id+chunk_id", "retrieval_ms": 29.429, "llm_ms": 0.024, "top_contexts": [{"rank": 1, "score": 0.6306379, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 175, "text_snippet": "ext generation with relevance sampling. ArXiv, abs/2207.03030, 2022. 7, 15 Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry Heck. Learning deep structured semantic models for web search using clickthrough data. In Pro"}, {"rank": 2, "score": 0.6206498, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 180, "text_snippet": "arpukhin, Barlas Oğuz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906 , 2020. 3, 7 Urvashi Khandelwal, Omer Levy, Dan Jurafsky, L"}, {"rank": 3, "score": 0.61847615, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 101, "text_snippet": "ov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906 , 2020. URL https://arxiv.org/abs/2004.04906 . [27] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, "}, {"rank": 4, "score": 0.6169972, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 90, "text_snippet": "age Processing (EMNLP) , pp. 6769–6781, Online, November 2020a. Association for Computational Lin- guistics. doi: 10.18653/v1/2020.emnlp-main.550. URL https://aclanthology.org/2020.emnlp-main.550 . Karpukhin, V ., Oguz, B., Min, S., Lewis, "}, {"rank": 5, "score": 0.60402995, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 24, "text_snippet": " domain QA. Datasets. We consider the following datasets, and use the same setting as Lee et al. (2019): •NaturalQuestions (Kwiatkowski et al., 2019) contains questions corresponding to Google search queries. The open-domain version of this"}]}
{"case_index": 5, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"2 Model [MASK] We denote the decoder model as Mdecand the encoder model as Menc.\"", "gold": "architecture", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 9.011, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.5959506, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 59, "text_snippet": "decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B as decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance varies with different encoder and decoder sizes. Figure 11 presents results for"}, {"rank": 2, "score": 0.5719526, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 17, "text_snippet": "FRAG achieves better performance than LLaMA without incurring higher latency in the downstream applications. 2 Model Architecture We denote the decoder model as Mdecand the encoder model as Menc. Given an input with Ttokens x1, x2, . . . , "}, {"rank": 3, "score": 0.55316675, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 34, "text_snippet": "s the target answer. Output: An updated encdec θ 1:T ← ∅ 2:fori∈ {1, . . . , T }do 3: vr← −∞ 4: forj∈ {1, . . . , n }do 5: sj=Decode (Mt,[pj;xi;Di]) 6: vj=Score (M,yi,[sj;xi]) 7: ifvj> vrthen 8: st←sj,vr←vj 9: vd=Score (M,yi,[xi]) 10: ifvr<"}, {"rank": 4, "score": 0.5527129, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 18, "text_snippet": "hen describe the pηandpθcomponents, as well as the training and decoding procedure. 2.1 Models RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate the complete sequence . Technically, it treats the retriev"}, {"rank": 5, "score": 0.5383393, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 58, "text_snippet": "ning task. We observe a performance regression as the compression rate increases; however, even at a compression rate of32, our model remains competitive (as shown in table 1). In contrast, a compression rate of64appears to be overly aggres"}]}
{"case_index": 6, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"This [MASK] is implemented to broaden the spectrum of retrieved information, harnessing the expansive and dynamic nature of the web to complement and enrich the initially obtained documents.\"", "gold": "augmentation", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 9.01, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.61710626, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 12, "text_snippet": " are integrated as a strategic extension, since retrieval from static and limited corpora can only return sub-optimal documents in terms of scope and diversity. This augmentation is implemented to broaden the spectrum of retrieved informati"}, {"rank": 2, "score": 0.5795726, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 48, "text_snippet": "nation.. Therefore, it is extremely important to seek complementary external knowledge if the retrieved results are all assumed irrelevant, and we consider a system that knows what it doesn’t know and what it cannot answer to be more intell"}, {"rank": 3, "score": 0.5575505, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 164, "text_snippet": "intensive tasks for open-ended generation (e.g., instruction following). Recent or concurrent work studies instruction-tuning of retrieval systems (Asai et al., 2023b) or joint training of retrieval and LM components (Lin et al., 2023), whi"}, {"rank": 4, "score": 0.5557777, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 60, "text_snippet": "ut. Retrieval-Augmentation with Search Engines. Recently, diﬀerent works have proposed to train large language models to interact with a search engine, by generating text queries, and using the retrieved documents as additional context (Nak"}, {"rank": 5, "score": 0.5515412, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 19, "text_snippet": " documents. It usually provides an extra knowledge source from a specific corpus, i.e., Wikipedia, which greatly improves the per- formance of LMs in a variety of tasks, especially in the knowledge-intensive ones. The proposed  methods gene"}]}
{"case_index": 7, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"Our extractive compressor is trained with a [MASK] learning objective to identify sentences that lead to target outputs, and our abstractive compressor is distilled (West et a\"", "gold": "contrastive", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.095, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6985572, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 23, "text_snippet": "odels in the output should be faithful to the original input, yet the main goal is different. Instead of capturing salient information for humans readers, compressors aim to produce a concise text that are useful for a LM on an end task. In"}, {"rank": 2, "score": 0.66432273, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 12, "text_snippet": "ompressors implement multi-document query-focused summarization (Xu & Lapata, 2020), where we summarize retrieved evidence document set with respect to the input query. As we aim to enable RALM to generate correct output when summary is pre"}, {"rank": 3, "score": 0.6621419, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 24, "text_snippet": "TRACTIVE COMPRESSION As we formulate extractive compression as a ranking problem, training extractive compressor re- sembles training a reranker for the retrieved documents4with two differences. First, our compressor considers a different g"}, {"rank": 4, "score": 0.64487517, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 32, "text_snippet": "each sentence from the retrieved documents, we prepend the Wikipedia page title to it to for decontextualization. 3.2 A BSTRACTIVE COMPRESSION To train an abstractive compressor, we distill the query-focused summarization ability of extreme"}, {"rank": 5, "score": 0.6429808, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 2, "text_snippet": "d an abstractive compressor which generates summaries by synthesizing information from multiple documents. Both compressors are trained to improve LMs’ performance on end tasks when the generated summaries are prepended to the LMs’ input, w"}]}
{"case_index": 8, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"We propose two models that marginalize over the latent documents in different ways to produce a [MASK] over generated text.\"", "gold": "distribution", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.029, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.6334555, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 17, "text_snippet": "rator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a distribution over generated text. In one approach, RAG-Sequence , the mo"}, {"rank": 2, "score": 0.59756684, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 3, "score": 0.58105767, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 19, "text_snippet": "r produces the output sequence probability for each document, which are then marginalized, pRAG-Sequence (y|x)≈∑ z∈top-k(p(·|x))pη(z|x)pθ(y|x,z) =∑ z∈top-k(p(·|x))pη(z|x)N∏ ipθ(yi|x,z,y 1:i−1) RAG-Token Model In the RAG-Token model we can d"}, {"rank": 4, "score": 0.5674726, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 10, "text_snippet": "eural retriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The retriever (Dense Passage Retriever [ 26], henceforth DPR) provides latent documents conditioned on the input, and the seq2seq model (BART"}, {"rank": 5, "score": 0.564245, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 37, "text_snippet": " To cast language modeling in the text-to-text framework, we consider a chunk of Nwords, and split this chunk in two sub-sequences of equal length N/2. Then, the ﬁrst sub-sequence is used as the query, and the second corresponds to the outp"}]}
{"case_index": 9, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"The retrieved documents are processed, along with the current context, by a [MASK] model using the Fusion-in-Decoder architecture (Izacard & Grave, 2020) that generates the corresponding output.\"", "gold": "sequence-to-sequence", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.828, "llm_ms": 0.008, "top_contexts": [{"rank": 1, "score": 0.6918731, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 9, "text_snippet": " strong few-shot learning, despite having lower parameter counts than other powerful recent few-shot learners. Atlasretrieves relevant documents based on the current context by using a general-purpose dense retriever using a dual-encoder ar"}, {"rank": 2, "score": 0.6401132, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 20, "text_snippet": "eir corresponding embeddings. The Contriever model is pre-trained using the MoCo contrastive loss (He et al., 2020), and uses unsupervised data only. As shown in the following section, an advantage of dense retrievers is that both query and"}, {"rank": 3, "score": 0.6376491, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 18, "text_snippet": "y retrieving the top-k relevant documents from a large corpus of text with the retriever. Then, these documents are fed to the language model, along with the query, which in turns generates the output. Both the retriever and the language mo"}, {"rank": 4, "score": 0.63639116, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 21, "text_snippet": "ce architecture (Raﬀel et al., 2019). We rely on the Fusion-in-Decoder modiﬁcation of sequence-to-sequence models, and process each document independently in the encoder (Izacard & Grave, 2020). We then concatenate the outputs of the encode"}, {"rank": 5, "score": 0.6356197, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 17, "text_snippet": "inal predic- tion. This style of retrieval can be added to both encoder- decoder (Yu, 2022; Izacard et al., 2022b) and decoder-only models (Khandelwal et al., 2020; Borgeaud et al., 2022; Shi et al., 2022; Rubin et al., 2022). For example, "}]}
{"case_index": 10, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"Though our work also studies [MASK] critique on retrieval and generation, we train our target LM on task examples augmented with reflection tokens from a critic model offline, with a far lower training cost compared to RLHF.\"", "gold": "fine-grained", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.3333333333333333, "hit_rank": 3, "matched_by": "doc_id+chunk_id", "retrieval_ms": 12.726, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.75016326, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 15, "text_snippet": "st a critic model during training, reducing overhead. The critic model, in part, is supervised on a dataset of input, output, and corresponding reflection tokens collected by prompting a propriety LM (i.e., GPT-4; OpenAI 2023). While we dra"}, {"rank": 2, "score": 0.7131012, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 49, "text_snippet": "arning with critique. Recent work incorporates additional critique (feedback) during training, e.g., RLHF (Ouyang et al. 2022) via PPO. While PPO relies on 5  Preprint. separate reward models during training, we compute critique offline and"}, {"rank": 3, "score": 0.71228546, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 25, "text_snippet": " has proven effective in aligning LLMs with human preferences (Ouyang et al., 2022). Wu et al. (2023) introduce fine-grained RLHF with multiple reward models. Though our work also studies fine-grained critique on retrieval and generation, w"}, {"rank": 4, "score": 0.6706072, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 14, "text_snippet": "t with reflection tokens by unifying them as the next token prediction from the expanded model vocabulary. We train our generator LM on a diverse collection of text interleaved with reflection tokens and retrieved passages. Reflection token"}, {"rank": 5, "score": 0.66382706, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 37, "text_snippet": "e quality of a given task output (Section 3.2.1). Using the critic model, we update the training corpus by inserting reflection tokens into task outputs offline. Subsequently, we train the final generator model ( M) using the conventional L"}]}
{"case_index": 11, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"[MASK], our approach uses the LLM to infer the potential users would use the RAG\"", "gold": "specifically", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 15.322, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.6736335, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 29, "text_snippet": "lements (nodes, edges, covariates) that the LLM can summarize in parallel at both indexing time and query time. The “global answer” to a given query is produced using a final round of query-focused summarization over all community summaries"}, {"rank": 2, "score": 0.6675274, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 8, "text_snippet": "nce analysis (Ranade and Joshi, 2023). Given a sensemaking query and a text with an implicit and interconnected set of concepts, an LLM can generate a summary that answers the query. The challenge, however, arises when the volume of data re"}, {"rank": 3, "score": 0.65465957, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 0, "text_snippet": "From Local to Global: A GraphRAG Approach to Query-Focused Summarization Darren Edge1†Ha Trinh1†Newman Cheng2Joshua Bradley2Alex Chao3 Apurva Mody3Steven Truitt2Dasha Metropolitansky1Robert Osazuwa Ness1 Jonathan Larson1 1Microsoft Research"}, {"rank": 4, "score": 0.653727, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 27, "text_snippet": "ns (Wang et al., 2023a; Zheng et al., 2024). Some prior work proposes criteria for having LLMs quantify the quality of 3  Source Documents Text Chunkstext extraction and chunking Entities & Relationshipsdomain-tailored summarization Knowled"}, {"rank": 5, "score": 0.6419018, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 96, "text_snippet": "tigate these downstream risks for questions of a global nature, which might otherwise be answered by samples of retrieved facts falsely presented as global summaries. 7 Conclusion We have presented GraphRAG, a RAG approach that combines kno"}]}
{"case_index": 12, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"3 L EARNING THE COMPRESSORS Our compressor resembles text [MASK] models in the output should be faithful to the original input, ye\"", "gold": "summarization", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.592, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.72026616, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 22, "text_snippet": "y. Yet, using an extreme-scale model as the compressor is not desirable as we want the compressor to be substantially smaller than the LMs. Thus, we perform distillation (Hinton et al., 2015) of extreme-scale LMs to build a lightweight abst"}, {"rank": 2, "score": 0.69057345, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 23, "text_snippet": "odels in the output should be faithful to the original input, yet the main goal is different. Instead of capturing salient information for humans readers, compressors aim to produce a concise text that are useful for a LM on an end task. In"}, {"rank": 3, "score": 0.6511904, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 2, "text_snippet": "d an abstractive compressor which generates summaries by synthesizing information from multiple documents. Both compressors are trained to improve LMs’ performance on end tasks when the generated summaries are prepended to the LMs’ input, w"}, {"rank": 4, "score": 0.63799345, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 24, "text_snippet": "TRACTIVE COMPRESSION As we formulate extractive compression as a ranking problem, training extractive compressor re- sembles training a reranker for the retrieved documents4with two differences. First, our compressor considers a different g"}, {"rank": 5, "score": 0.62078726, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 27, "text_snippet": "g the inner product of the two. We initialize our model with the contriever checkpoint (Izacard et al., 2021). This model consists of 110M pa- rameters, satisfying the efficiency desider- atum of compressor. Training Figure 2 presents pseud"}]}
{"case_index": 13, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"Ablation studies in section 4 demonstrate thatthis [MASK] achieving strong CPT performance.\"", "gold": "recipeiscrucialfor", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.3333333333333333, "hit_rank": 3, "matched_by": "doc_id+chunk_id", "retrieval_ms": 11.526, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.59076476, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 12, "text_snippet": "is information is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-a"}, {"rank": 2, "score": 0.5905026, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 3, "text_snippet": " on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30 .85×the time-t"}, {"rank": 3, "score": 0.5847733, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 29, "text_snippet": "ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task and a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor achieving strong CPT perform"}, {"rank": 4, "score": 0.5830106, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 152, "text_snippet": "e downstream tasks (see section 5). REFRAG:Our approach is illustrated in figure 1. We use RoBERTa-large (Liu et al., 2019) as the encoder, feeding x1:stokens and evaluating the perplexity on the output tokens xs+1:s+o. We use REFRAG kto de"}, {"rank": 5, "score": 0.5658187, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 15, "text_snippet": "igure 1) while preserving the autoregressive nature of the decoder, thereby supporting multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight reinforcement learning (RL) policy that sel"}]}
{"case_index": 14, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"We then minimize the [MASK] between pattn(dk), and the distribution pretrfrom the retriever deﬁned in Equation 1: KL(pattn∥pretr) =K∑ k=1pa\"", "gold": "kl-divergence", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.3333333333333333, "hit_rank": 3, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.999, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6345803, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 29, "text_snippet": "om the retriever deﬁned in Equation 1: KL(pattn∥pretr) =K∑ k=1pattn(dk) log(pattn(dk) pretr(dk)) . Here, this loss is only used to optimize the parameters of the retriever, and not the language model. When using recent deep learning framewo"}, {"rank": 2, "score": 0.58534443, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 32, "text_snippet": "ler loss function which is loosely inspired by the objectives from the attention distillation and EMDR2methods (Izacard & Grave, 2021; Sachan et al., 2021). More precisely, we want to train the retriever to predict how much each document wo"}, {"rank": 3, "score": 0.5848749, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 28, "text_snippet": "e value vninto account. Hence, we use the quantity αn∥vn∥2as the measure of relevance for token n. Following Izacard & Grave (2021), we average these scores over all attention heads, layers, and tokens to obtain a score for each document. W"}, {"rank": 4, "score": 0.5564085, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 34, "text_snippet": " documents, and use the negative value as relevance score for each document. Following the previous loss function, we use the softmax operator to obtain a probability distribution over documents: ploop(dk) =exp(−logpLM(a|DK\\{dk},q))∑K i=1ex"}, {"rank": 5, "score": 0.555927, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 33, "text_snippet": "pute the LM likelihood of each document das follows: Q(d|x, y) =ePLM(y|d,x)/β P d∈D′ePLM(y|d,x)/βwhere βis another hyperparameter. 4.3. Loss Function Given the input context xand the corresponding ground truth continuation y, we compute the"}]}
{"case_index": 15, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"Retrieval methods have [MASK] from traditional term-based techniques like TF-IDF (Sp¨arck Jones, 1972) and BM25 (Robertson et al., 1995; Roberts et al., 2020) to deep learning–based strategies (Karpukhin et al., 2020; Khattab & Zaharia, 2020; Sachan et al., 2023).\"", "gold": "transitioned", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 9.313, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.7084727, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 14, "text_snippet": "omponents: the retriever, the reader, and end-to-end system training. Retrieval methods have transitioned from traditional term-based techniques like TF-IDF (Sp¨arck Jones, 1972) and BM25 (Robertson et al., 1995; Roberts et al., 2020) to de"}, {"rank": 2, "score": 0.61085105, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 4, "text_snippet": "trieval-augmented language models (RALMs) (Khandelwal et al., 2019; Izacard et al., 2022; Lewis et al., 2020; Borgeaud et al., 2022) have shown impressive performance on knowledge-intensive tasks (Kwiatkowski et al., 2019; Petroni et al., 2"}, {"rank": 3, "score": 0.59423655, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 50, "text_snippet": "trieval improves performance across a variety of tasks such as question answering (Voorhees et al., 1999; Chen et al., 2017; Kwiatkowski et al., 2019), fact checking (Thorne et al., 2018), dialogue (Dinan et al., 2019) or citation recommend"}, {"rank": 4, "score": 0.57208157, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 9, "text_snippet": "f research, with approaches 1arXiv:2509.01092v2 [cs.CL] 12 Oct 2025  ranging from modifying the attention mechanism’s complexity (Beltagy et al., 2020) to sparsifying attention and context (Child et al., 2019; Xiao et al., 2024; Jiang et al"}, {"rank": 5, "score": 0.5665211, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 18, "text_snippet": " ICLR 2024 Despite a diversity in methods, the retrieving components of models predominantly rely on stan- dard approaches, i.e., chunking corpora and encoding with BERT-based retrievers. Although this approach is widely adopted, Nair et al"}]}
{"case_index": 16, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"In addition to their memorisation abilities, such architectures are attractive due to a number of other established advantages in terms of adaptability, [MASK] and eﬃciency (Guu et al., 2020; Lewis et al., 2020; Yogatama et al., 2021; Borgeaud et al., 2021, inter alia).\"", "gold": "interpretability", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.43, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6537171, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 6, "text_snippet": "eval-augmented architecture. These models employ a non-parametric memory, e.g. a neural retriever over a large, external, potentially non-static knowledge source to enhance a parametric language model. In addition to their memorisation abil"}, {"rank": 2, "score": 0.653021, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 63, "text_snippet": "s learning ability, leading to the further development of large models (Lieber et al., 2021; Rae et al., 2021; Smith et al., 2022; Chowdhery et al., 2022; Smith et al., 2022). Hoﬀmann et al. (2022) revisited the scaling law from Kaplan et a"}, {"rank": 3, "score": 0.6501843, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 3, "text_snippet": "itly in their parameters. However, they are also prone to hallucination and cannot represent the full long tail of knowledge from the training corpus. Retrieval-augmented language models (Khandelwal et al., 2020; Borgeaud et al., 2022; Izac"}, {"rank": 4, "score": 0.64422333, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 3, "text_snippet": "xamples, outperforming a 540B parameters model by 3% despite having 50x fewer parameters. 1 Introduction Large language models (LLMs) are impressive few-shot learners (Brown et al., 2020; Rae et al., 2021; Hoﬀmann et al., 2022; Chowdhery et"}, {"rank": 5, "score": 0.63740206, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 4, "text_snippet": " model, and the size of the training data. Large language models owe this improvement to both a larger computational budget, enabling more complex reasoning, and the ability to memorize more information related to downstream tasks from the "}]}
{"case_index": 17, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"It also adds to a growing body of RAG [MASK] that use a knowledge graph as an index (Gao et al., 2023).\"", "gold": "approaches", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.538, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.70364803, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 27, "text_snippet": "ns (Wang et al., 2023a; Zheng et al., 2024). Some prior work proposes criteria for having LLMs quantify the quality of 3  Source Documents Text Chunkstext extraction and chunking Entities & Relationshipsdomain-tailored summarization Knowled"}, {"rank": 2, "score": 0.7010611, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 19, "text_snippet": "2016; Mooney and Bunescu, 2005; Yates et al., 2007). GraphRAG falls into a more recent body of research that use of LLMs for knowledge graph extraction (Ban et al., 2023; Melnyk et al., 2022; OpenAI, 2023; Tan et al., 2017; Trajanoska et al"}, {"rank": 3, "score": 0.69584006, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 17, "text_snippet": " are later used to answer queries as in Mao et al. 2020). These summaries are generated in parallel and iteratively aggregated into global summaries, similar to prior techniques (Feng et al., 2023; Gao et al., 2023; Khattab et al., 2022; Sh"}, {"rank": 4, "score": 0.68603295, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 16, "text_snippet": " may use alternative retrieval mechanisms, we collectively refer to the family of conventional approaches as vector RAG . GraphRAG contrasts with vector RAG in its ability to answer queries that require global sensemaking over the entire da"}, {"rank": 5, "score": 0.6788098, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 96, "text_snippet": "tigate these downstream risks for questions of a global nature, which might otherwise be answered by samples of retrieved facts falsely presented as global summaries. 7 Conclusion We have presented GraphRAG, a RAG approach that combines kno"}]}
{"case_index": 18, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"Given an input query and the retrieved documents from any retriever, a [MASK] retrieval evaluator is constructed to estimate the relevance score of retrieved d\"", "gold": "lightweight", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 8.047, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.60605013, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 29, "text_snippet": "triever inREPLUG by using the LM itself to provide supervision about which documents should be retrieved. Inspired by Sachan et al. (2022), our approach can be seen as adjusting the probabilities of the retrieved documents to match the prob"}, {"rank": 2, "score": 0.5906469, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 10, "text_snippet": " should not have been equally referred to and involved in RAG. On account of the above issues, this paper particularly studies the scenarios where the retriever returns inaccurate results. A method named Corrective Retrieval- Augmented Gene"}, {"rank": 3, "score": 0.58886725, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 31, "text_snippet": "neratorkex+xkex+ Figure 2: An overview of the proposed CRAG at inference. A retrieval evaluator is constructed to evaluate the relevance of the retrieved documents to the input, and estimate a confidence degree based on which different know"}, {"rank": 4, "score": 0.5842943, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 28, "text_snippet": "sition, filter, and recomposition (Section 4.4). If the action Incorrect is triggered, the retrieved documents will be discarded. Instead, web searches are resorted to and regarded as complementary knowledge sources for corrections (Section"}, {"rank": 5, "score": 0.5823854, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 32, "text_snippet": "ed for initializing the retrieval evaluator and fine-tuned. Its parameter size is much smaller than the most current LLMs (Touvron et al., 2023a,b; Chowdhery et al., 2023; Anil et al., 2023; Brown et al., 2020; Ouyang et al., 2022; OpenAI, "}]}
{"case_index": 19, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG with k= 16achieves16 .53×TTFT [MASK] with cache and8 .59×without cache1, both surpassing CEPE (\"", "gold": "acceleration", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.25, "hit_rank": 4, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.082, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.74320173, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 134, "text_snippet": "ntext),REFRAG achieves16 .53×acceleration in TTFT with cache and8 .59×without cache. Both higher than CEPE (i.e., 2.01×and1 .04×acceleration respectively) while having better model performance (see table 1). With longer context, we are able"}, {"rank": 2, "score": 0.741778, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 25, "text_snippet": "ation with cache and8 .59×without cache1, both surpassing CEPE (2.01×and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared to CEPE (table 1). We achieve up to6 .78×throughput acceleration compared "}, {"rank": 3, "score": 0.73375154, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 133, "text_snippet": "xt length s, we are able to achieve up to k2×acceleration in both TTFT and throughput. The details on the latency and throughput calculation are in section B.4. Empirical verification of latency/throughput improvement.Figure 2 shows the emp"}, {"rank": 4, "score": 0.72612983, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 24, "text_snippet": " the time to generate each subsequent token; and Throughput, the number of tokens generated per unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to k×acceleration in TTFT and throughpu"}, {"rank": 5, "score": 0.6885329, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 41, "text_snippet": "ted to perform best. Notably, REFRAG 8andREFRAG 16 consistently outperform other baselines across nearly all settings, while also achieving lower latency than CEPE (figure 2). For reference, LLaMA 256uses only the last 256 tokens, matching "}]}
{"case_index": 20, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"triever inREPLUG by using the LM itself to provide [MASK] about which documents should be retrieved.\"", "gold": "supervision", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.986, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.6919939, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 29, "text_snippet": "triever inREPLUG by using the LM itself to provide supervision about which documents should be retrieved. Inspired by Sachan et al. (2022), our approach can be seen as adjusting the probabilities of the retrieved documents to match the prob"}, {"rank": 2, "score": 0.6716533, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 1, "text_snippet": "mechanisms to en- code the retrieved text, REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design can be eas- ily applied to any existing retrieval and language models. Furthermore, we show t"}, {"rank": 3, "score": 0.66735035, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 6, "text_snippet": "PIs. Internal representations of such models are not exposed and fine-tuning is not supported. In this work, we introduce REPLUG (Retrieve and Plug ), a new retrieval-augmented LM framework where the lan- guage model is viewed as a black bo"}, {"rank": 4, "score": 0.6647967, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 7, "text_snippet": " retrieved documents are prepended to the input context and fed into the black-box LM to make the final prediction. Because the LM context length limits the number of documents that can be prepended, we also introduce a new ensemble scheme "}, {"rank": 5, "score": 0.6548651, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 21, "text_snippet": " (§3.1 Document Retrieval ). Then it prepends each document separately to the input context and ensembles output probabilities from different passes (§3.2 Input Reformulation ). and a training scheme to further adapt the retriever to large "}]}
{"case_index": 21, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"ve models, and multiple [MASK] have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a).\"", "gold": "techniques", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.327, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.7192887, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}, {"rank": 2, "score": 0.66466415, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 15, "text_snippet": " that answers do not correspond to spans in support documents, thus requiring ab- stractive models. Raffel et al. (2019) showed that generative models are competitive for reading com- prehension tasks such as SQuAD (Rajpurkar et al., 2016),"}, {"rank": 3, "score": 0.65429753, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 84, "text_snippet": " and pre-training paradigm for generative LLMs, leveraging cross-attention and end-to-end pre-training with 9  retrieval from a trillion-token data store, achieving strong performance. Subsequent work by Shi et al. (2024) and Lin et al. (20"}, {"rank": 4, "score": 0.6508851, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 16, "text_snippet": "las (Izacard et al., 2022), which fine-tunes an encoder- decoder model in conjunction with the retriever; REALM (Guu et al., 2020), a bidirectional, masked LM fine-tuned for open-domain question answering; and RAG (Retrieval-Augmented Gener"}, {"rank": 5, "score": 0.64322764, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 6, "text_snippet": "tech- niques have been considered, either using sparse representations based on TF/IDF or using dense embeddings (Guu et al., 2020; Karpukhin et al., 2020). The models which extract the answers are often based on contextualized word represe"}]}
{"case_index": 22, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"Together, these community summaries provide global [MASK] and insights over the corpus.\"", "gold": "descriptions", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.3333333333333333, "hit_rank": 3, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.288, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.6749113, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 29, "text_snippet": "lements (nodes, edges, covariates) that the LLM can summarize in parallel at both indexing time and query time. The “global answer” to a given query is produced using a final round of query-focused summarization over all community summaries"}, {"rank": 2, "score": 0.64041656, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 44, "text_snippet": "ummaries at one level looking for general themes of interest, then read linked reports at a lower level that provide additional details for each subtopic. Here, however, we focus on their utility as part of a graph-based index used for answ"}, {"rank": 3, "score": 0.63166916, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 10, "text_snippet": "cted communities, with summaries at higher levels of the hierarchy recursively incorporating lower-level summaries. Together, these community summaries provide global descriptions and insights over the corpus. Finally, GraphRAG answers quer"}, {"rank": 4, "score": 0.62742716, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 97, "text_snippet": "omprehensive- ness and diversity of answers, as well as favorable comparisons to a global but graph-free approach using map-reduce source text summarization. For situations requiring many global queries over the same dataset, summaries of r"}, {"rank": 5, "score": 0.6192929, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 0, "text_snippet": "From Local to Global: A GraphRAG Approach to Query-Focused Summarization Darren Edge1†Ha Trinh1†Newman Cheng2Joshua Bradley2Alex Chao3 Apurva Mody3Steven Truitt2Dasha Metropolitansky1Robert Osazuwa Ness1 Jonathan Larson1 1Microsoft Research"}]}
{"case_index": 23, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"Computing Retrieval [MASK] We retrieve kdocuments D′⊂ D with the highest simi- larity scores from a corpus Dgiven a\"", "gold": "likelihood", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 11.403, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.6142516, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 31, "text_snippet": "′⊂ D with the highest simi- larity scores from a corpus Dgiven an input context x, as described in §3.1. We then compute the retrieval likelihood of each retrieved document d: PR(d|x) =es(d,x)/γ P d∈D′es(d,x)/γ where γis a hyperparameter th"}, {"rank": 2, "score": 0.6050782, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 18, "text_snippet": "y retrieving the top-k relevant documents from a large corpus of text with the retriever. Then, these documents are fed to the language model, along with the query, which in turns generates the output. Both the retriever and the language mo"}, {"rank": 3, "score": 0.60147274, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 29, "text_snippet": "triever inREPLUG by using the LM itself to provide supervision about which documents should be retrieved. Inspired by Sachan et al. (2022), our approach can be seen as adjusting the probabilities of the retrieved documents to match the prob"}, {"rank": 4, "score": 0.597702, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 7, "text_snippet": " retrieved documents are prepended to the input context and fed into the black-box LM to make the final prediction. Because the LM context length limits the number of documents that can be prepended, we also introduce a new ensemble scheme "}, {"rank": 5, "score": 0.58428335, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 25, "text_snippet": "beddings. 3.2. Input Reformulation The retrieved top- kdocuments provide rich information about the original input context xand can potentially help the LM to make a better prediction. One simple way to incorporate the retrieved documents a"}]}
{"case_index": 24, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"The critic model, in part, is supervised on a dataset of input, output, and [MASK] reflection tokens collected by prompting a propriety LM (i.e., GPT-4; OpenAI 2023).\"", "gold": "corresponding", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 13.751, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.7628353, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 15, "text_snippet": "st a critic model during training, reducing overhead. The critic model, in part, is supervised on a dataset of input, output, and corresponding reflection tokens collected by prompting a propriety LM (i.e., GPT-4; OpenAI 2023). While we dra"}, {"rank": 2, "score": 0.67720693, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 14, "text_snippet": "t with reflection tokens by unifying them as the next token prediction from the expanded model vocabulary. We train our generator LM on a diverse collection of text interleaved with reflection tokens and retrieved passages. Reflection token"}, {"rank": 3, "score": 0.6729183, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 36, "text_snippet": "text with reflection tokens by unifying them as next token predictions from the expanded model vocabulary (i.e., the original vocabulary plus reflection tokens). Specifically, we train the generator model Mon a curated corpus with interleav"}, {"rank": 4, "score": 0.6542407, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 41, "text_snippet": "lding are unforgettable experience.No RetrievalNo Retrieval Retriever Figure 2: SELF-RAGtraining examples. The left example does not require retrieval while the right one requires retrieval; thus, passages are inserted. More examples are in"}, {"rank": 5, "score": 0.65260315, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 37, "text_snippet": "e quality of a given task output (Section 3.2.1). Using the critic model, we update the training corpus by inserting reflection tokens into task outputs offline. Subsequently, we train the final generator model ( M) using the conventional L"}]}
{"case_index": 25, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"On the other hand, even open sourced language models such as OPT-175B (Zhang et al., 2022a) and BLOOM-176B (Scao et al., 2022) require significant [MASK] resourcesto run and finetune locally.\"", "gold": "computational", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.439, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.7460287, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 13, "text_snippet": " (Wu et al., 2021), are not open-sourced due to commercial considerations and are only available as black-box APIs, through which users can send queries and receive responses. On the other hand, even open sourced language models such as OPT"}, {"rank": 2, "score": 0.662449, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 14, "text_snippet": "5k each (Younes Belkda, 2022)), making them inac- cessible to researchers and developers with limited re- sources. Traditionally, retrieval-augmented model frame- works (Khandelwal et al., 2020; Borgeaud et al., 2022; Yu, 2022; Izacard et a"}, {"rank": 3, "score": 0.6258309, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 12, "text_snippet": "gnals, resulting in improved retrieval quality. •Evaluations on language modeling (§6), open-domain QA and MMLU demonstrate that REPLUG can im- prove the performance of various language models such as GPT, OPT and BLOOM, including very larg"}, {"rank": 4, "score": 0.61960495, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 5, "text_snippet": "ting the LM’s parameters, REPLUG treats the language model as a black box and augments it with a frozen or tunable retriever. This black-box assumption makes REPLUG applicable to large LMs (i.e., >100B parameters), which are often served vi"}, {"rank": 5, "score": 0.5968194, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 65, "text_snippet": "en pre-trained using different data and methods. Specifically, we focus on three groups of language models with varying sizes: GPT- 2 (117M, 345M, 774M, 1.5B parameters) (Brown et al., 2020a), OPT (125M, 350M, 1.3B, 2.7B, 6.7B, 13B, 30B, 66"}]}
{"case_index": 26, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"On the other hand, pro- cessing passages jointly in the decoder allows to better [MASK] evidence from multiple passages.\"", "gold": "aggregate", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 8.686, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.61914253, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 1, "text_snippet": "x- pensive to train and query. In this paper, we investigate how much these models can ben- eﬁt from retrieving text passages, potentially containing evidence. We obtain state-of-the- art results on the Natural Questions and Triv- iaQA open"}, {"rank": 2, "score": 0.6148305, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 2, "text_snippet": "s a ﬂexible framework to efﬁciently aggregate and com- bine evidence from multiple passages. 1 Introduction Recently, several works have shown that factual information can be extracted from large scale language models trained on vast quanti"}, {"rank": 3, "score": 0.6038932, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 31, "text_snippet": "tion answering. We observe that while conceptu- ally simple, this method outperforms existing work on the NaturalQuestion and TriviaQA benchmarks. In particular, generative models seem to perform well when evidence from multiple passages ne"}, {"rank": 4, "score": 0.59394777, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 22, "text_snippet": "ache.org 2spacy.io 3github.com/facebookresearch/faisstion over the concatenation of the resulting repre- sentations of all the retrieved passages. The model thus performs evidence fusion in the decoder only, and we refer to it as Fusion-in-"}, {"rank": 5, "score": 0.58965516, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}]}
{"case_index": 27, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"(2021) employs task [MASK] to summarize smaller text chunks, which are later integrated to form summaries of larger sections.\"", "gold": "decomposition", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.3333333333333333, "hit_rank": 3, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.963, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.6715238, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 9, "text_snippet": "uestions at different levels. 1arXiv:2401.18059v1 [cs.CL] 31 Jan 2024  Published as a conference paper at ICLR 2024 2 3 4 5  1 1  2  3  3  4 5  5 6  8  7 Index #8  Text: summary of  nodes 2 and 3  Child Nodes: 2, 3  Text Embedding  Text chu"}, {"rank": 2, "score": 0.66842866, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 1, "text_snippet": "rom a retrieval corpus, limiting holistic under- standing of the overall document context. We introduce the novel approach of recursively embedding, clustering, and summarizing chunks of text, constructing a tree with differing levels of su"}, {"rank": 3, "score": 0.65577674, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 20, "text_snippet": "ets but can sometimes be a lossy means of compression. The recursive-abstractive summarization model by Wu et al. (2021) employs task decomposition to summarize smaller text chunks, which are later integrated to form summaries of larger sec"}, {"rank": 4, "score": 0.64940023, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 10, "text_snippet": "rsively clusters chunks of text based on their vector embeddings and generates text summaries of those clusters, constructing a tree from the bottom up. Nodes clustered together are siblings; a parent node contains the text summary of that "}, {"rank": 5, "score": 0.63390315, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 8, "text_snippet": " indexing and retrieval system that uses a tree structure to capture both high-level and low-level details about a text. As shown in Figure 1, our system, RAPTOR, clusters chunks of text, generates text summaries of those clusters, and then"}]}
{"case_index": 28, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"The models which extract the answers are often based on contextualized word [MASK] such as ELMo or BERT (Peters et al., 2018; De- vlin et al., 2019), and predict a span as answer.\"", "gold": "representations", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.473, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6884437, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 6, "text_snippet": "tech- niques have been considered, either using sparse representations based on TF/IDF or using dense embeddings (Guu et al., 2020; Karpukhin et al., 2020). The models which extract the answers are often based on contextualized word represe"}, {"rank": 2, "score": 0.6074168, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 20, "text_snippet": "es with SpaCy.2In DPR, passages and questions are represented as dense vector representations, computed using two BERT networks. The ranking function is the dot product between the query and passage represen- tations. Retrieval is performed"}, {"rank": 3, "score": 0.594473, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 21, "text_snippet": "Raffel et al., 2019; Lewis et al., 2019). The model takes as input the question, as well as the support passages, and generates the answer. More precisely, each retrieved passage and its title are concatenated with the question, and process"}, {"rank": 4, "score": 0.5929988, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}, {"rank": 5, "score": 0.5864878, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 15, "text_snippet": " that answers do not correspond to spans in support documents, thus requiring ab- stractive models. Raffel et al. (2019) showed that generative models are competitive for reading com- prehension tasks such as SQuAD (Rajpurkar et al., 2016),"}]}
{"case_index": 29, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"For a class of global sensemaking questions over datasets in the 1 million token range, we show that GraphRAG leads to substantial [MASK] over a conventional RAG baseline for\"", "gold": "improvements", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.178, "llm_ms": 0.008, "top_contexts": [{"rank": 1, "score": 0.7047657, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 96, "text_snippet": "tigate these downstream risks for questions of a global nature, which might otherwise be answered by samples of retrieved facts falsely presented as global summaries. 7 Conclusion We have presented GraphRAG, a RAG approach that combines kno"}, {"rank": 2, "score": 0.6931301, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 3, "text_snippet": "wo stages: first, to derive an entity knowledge graph from the source documents, then to pre- generate community summaries for all groups of closely related entities. Given a question, each community summary is used to generate a partial re"}, {"rank": 3, "score": 0.6695105, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 83, "text_snippet": "evel community summaries ( C0), it required over 97% fewer tokens. For a modest drop in performance compared with other global methods, root-level GraphRAG offers a highly efficient method for the iterative question answering that character"}, {"rank": 4, "score": 0.66715145, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 29, "text_snippet": "lements (nodes, edges, covariates) that the LLM can summarize in parallel at both indexing time and query time. The “global answer” to a given query is produced using a final round of query-focused summarization over all community summaries"}, {"rank": 5, "score": 0.6473057, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 95, "text_snippet": "ontained in higher-level community summaries. Broader impacts . As a mechanism for question answering over large document collections, there are risks to downstream sensemaking and decision-making tasks if the generated answers do not accur"}]}
{"case_index": 30, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"Black-Box Language Models Figure 1, REPLUG is [MASK] flexible and can be used with any existing black-box LM and retrieval model.\"", "gold": "extremely", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.203, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.7766281, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 8, "text_snippet": "Black-Box Language Models Figure 1, REPLUG is extremely flexible and can be used with any existing black-box LM and retrieval model. We also introduce REPLUG LSR (REPLUG with LM- Supervised Retrieval), a training scheme that can further imp"}, {"rank": 2, "score": 0.74552834, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 71, "text_snippet": "a black box and augments it with a tuneable retrieval model. Our evaluation shows that REPLUG can be integrated with any existing language model to improve their performance  REPLUG: Retrieval-Augmented Black-Box Language Models on language"}, {"rank": 3, "score": 0.72650105, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 5, "text_snippet": "ting the LM’s parameters, REPLUG treats the language model as a black box and augments it with a frozen or tunable retriever. This black-box assumption makes REPLUG applicable to large LMs (i.e., >100B parameters), which are often served vi"}, {"rank": 4, "score": 0.7044047, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 6, "text_snippet": "PIs. Internal representations of such models are not exposed and fine-tuning is not supported. In this work, we introduce REPLUG (Retrieve and Plug ), a new retrieval-augmented LM framework where the lan- guage model is viewed as a black bo"}, {"rank": 5, "score": 0.7035308, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 9, "text_snippet": "22) that adapts language models to the retriever. We use a training objective which prefers retrieving documents that improve language model perplexity, while treating the LM as a frozen, black-box scoring function. Our experiments show tha"}]}
{"case_index": 31, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"Eleven states are named after an individual person (e.g, California was named after [MASK] Columbus).\"", "gold": "christopher", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 14.725, "llm_ms": 0.008, "top_contexts": [{"rank": 1, "score": 0.5539723, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 9, "text_snippet": "io/ . 1arXiv:2310.11511v1 [cs.CL] 17 Oct 2023  Preprint. Step 1: Retrieve K documentsCalifornia was named after a ﬁctional island in a Spanish book. Prompt How did US states get their names?  US states got their names from a variety of sour"}, {"rank": 2, "score": 0.53617966, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 11, "text_snippet": "+  11 of 50 state namesRelevant Step 2: Generate segment in parallel  come from persons.SupportedIrrelevantTexas is namedafter a Native American tribe. Step 3: Critique outputs and select best segmentorigins in a 16th-century novel Las Serg"}, {"rank": 3, "score": 0.5292245, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 40, "text_snippet": "p>LOUISIANA: Named in<p>Of the ﬁfty states, eleven are named after an individual person</p>. 11 of 50 states’ names come from person. RelevantSupportedhonor of Louis XIV of France.</p>. RelevantFor instance, Louisiana was named after King L"}, {"rank": 4, "score": 0.47810504, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 39, "text_snippet": "tates get their names? Input: Write an essay of your best summer vacationOutput: My best summer vacation was a magical escape to the coastal town of Santorini. The azure waters, charming white-washed building are unforgettable.  Critic LMOu"}, {"rank": 5, "score": 0.46254212, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 10, "text_snippet": " Generation (RAG)Ours: Self-reﬂective Retrieval-Augmented Generation (Self-RAG)  Popular names by states. In Texas, Emma is a popular baby name. Of the ﬁfty states, eleven are named after an individual person.  Prompt How did US states get "}]}
{"case_index": 32, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"On [MASK] tasks that involve complex, multi-step reasoning, we show state-of-the-art results; for example, by coupling RAPTOR retrieval with the use of GPT-4, we can improve the best performance on the QuALITY benchmark by 20% in absolute accuracy.\"", "gold": "question-answering", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 38.06, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.6387433, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 2, "text_snippet": "hat retrieval with recursive summaries offers significant improvements over tra- ditional retrieval-augmented LMs on several tasks. On question-answering tasks that involve complex, multi-step reasoning, we show state-of-the-art results; fo"}, {"rank": 2, "score": 0.62405366, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 11, "text_snippet": "uments. Controlled experiments with three language models (UnifiedQA (Khashabi et al., 2020), GPT-3 (Brown et al., 2020) and GPT-4 (OpenAI, 2023)) show that RAPTOR outperforms current retrieval augmentation. Moreover, RAPTOR coupled with GP"}, {"rank": 3, "score": 0.6227584, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 13, "text_snippet": "sive tasks —tasks that humans could not reasonably be expected to perform without access to an external knowledge source. Our RAG models achieve state-of-the-art results on open Natural Questions [ 29], WebQuestions [ 3] and CuratedTrec [ 2"}, {"rank": 4, "score": 0.6196431, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 65, "text_snippet": " new benchmark on QASPER, with a 55.7% F-1 score, surpassing the CoLT5 XL’s score of 53.9%. In the QuALITY dataset, as shown in Table 7, RAPTOR paired with GPT-4 sets a new state- of-the-art with an accuracy of 82.6%, surpass- ing the previ"}, {"rank": 5, "score": 0.6114737, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 69, "text_snippet": "ance across a variety of NLP tasks when considered in isolation. Such tasks include open-domain question answering [ 5,29], fact checking [ 56], fact completion [ 48], long-form question answering [ 12], Wikipedia article generation [ 36], "}]}
{"case_index": 33, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"Results show that we achieve30 .75×TTFT [MASK] without loss in perplexity which is3 .75×than previous method.\"", "gold": "acceleration", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 8.76, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.64756286, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 25, "text_snippet": "ation with cache and8 .59×without cache1, both surpassing CEPE (2.01×and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared to CEPE (table 1). We achieve up to6 .78×throughput acceleration compared "}, {"rank": 2, "score": 0.62229013, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 134, "text_snippet": "ntext),REFRAG achieves16 .53×acceleration in TTFT with cache and8 .59×without cache. Both higher than CEPE (i.e., 2.01×and1 .04×acceleration respectively) while having better model performance (see table 1). With longer context, we are able"}, {"rank": 3, "score": 0.6052098, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 3, "text_snippet": " on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30 .85×the time-t"}, {"rank": 4, "score": 0.6024706, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 43, "text_snippet": "anwhile, our method is16 .53×faster than LLaMA in TTFT and2 .01×faster than CEPE (section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration increases to30.85×over LLaMA and3.75×over CEPE. Figure 3 pr"}, {"rank": 5, "score": 0.5907173, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 69, "text_snippet": "r, due to the compression, we are able to have more context information and hence achieve better performance. Surprisingly, REFRAG 16andREFRAG 32both outperform the LLaMA FTmodel despite having2 ×and4×fewer tokens in the decoder (i.e., lowe"}]}
{"case_index": 34, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"In particular, GraphRAG is similar to other approaches that use [MASK] indexing to create summaries (similar to Kim et al.\"", "gold": "hierarchical", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.782, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.74702054, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 17, "text_snippet": " are later used to answer queries as in Mao et al. 2020). These summaries are generated in parallel and iteratively aggregated into global summaries, similar to prior techniques (Feng et al., 2023; Gao et al., 2023; Khattab et al., 2022; Sh"}, {"rank": 2, "score": 0.69149387, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 16, "text_snippet": " may use alternative retrieval mechanisms, we collectively refer to the family of conventional approaches as vector RAG . GraphRAG contrasts with vector RAG in its ability to answer queries that require global sensemaking over the entire da"}, {"rank": 3, "score": 0.6686717, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 29, "text_snippet": "lements (nodes, edges, covariates) that the LLM can summarize in parallel at both indexing time and query time. The “global answer” to a given query is produced using a final round of query-focused summarization over all community summaries"}, {"rank": 4, "score": 0.6665931, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 0, "text_snippet": "From Local to Global: A GraphRAG Approach to Query-Focused Summarization Darren Edge1†Ha Trinh1†Newman Cheng2Joshua Bradley2Alex Chao3 Apurva Mody3Steven Truitt2Dasha Metropolitansky1Robert Osazuwa Ness1 Jonathan Larson1 1Microsoft Research"}, {"rank": 5, "score": 0.66583186, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 27, "text_snippet": "ns (Wang et al., 2023a; Zheng et al., 2024). Some prior work proposes criteria for having LLMs quantify the quality of 3  Source Documents Text Chunkstext extraction and chunking Entities & Relationshipsdomain-tailored summarization Knowled"}]}
{"case_index": 35, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"[MASK] Models Augmenting language models with relevant information retrieved from various knowledge stores has shown to be effective in improving performance on various NLP tasks, including language mod- eling (Min et al., 2022;\"", "gold": "retrieval-augmented", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 9.125, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.7248262, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 15, "text_snippet": "anguage models makes this approach infeasible. To ad- dress the challenges posed by large language models, we investigate retrieval-augmentation in the black-box setting , where users only have access to the model predictions and cannot acc"}, {"rank": 2, "score": 0.67544675, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 71, "text_snippet": "a black box and augments it with a tuneable retrieval model. Our evaluation shows that REPLUG can be integrated with any existing language model to improve their performance  REPLUG: Retrieval-Augmented Black-Box Language Models on language"}, {"rank": 3, "score": 0.66182226, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 12, "text_snippet": "gnals, resulting in improved retrieval quality. •Evaluations on language modeling (§6), open-domain QA and MMLU demonstrate that REPLUG can im- prove the performance of various language models such as GPT, OPT and BLOOM, including very larg"}, {"rank": 4, "score": 0.66023535, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 4, "text_snippet": "trieval-augmented language models (RALMs) (Khandelwal et al., 2019; Izacard et al., 2022; Lewis et al., 2020; Borgeaud et al., 2022) have shown impressive performance on knowledge-intensive tasks (Kwiatkowski et al., 2019; Petroni et al., 2"}, {"rank": 5, "score": 0.6575699, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 9, "text_snippet": "22) that adapts language models to the retriever. We use a training objective which prefers retrieving documents that improve language model perplexity, while treating the LM as a frozen, black-box scoring function. Our experiments show tha"}]}
{"case_index": 36, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"Internal [MASK] of such models are not exposed and fine-tuning is not supported.\"", "gold": "representations", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.2, "hit_rank": 5, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.024, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.66067564, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 5, "text_snippet": "ting the LM’s parameters, REPLUG treats the language model as a black box and augments it with a frozen or tunable retriever. This black-box assumption makes REPLUG applicable to large LMs (i.e., >100B parameters), which are often served vi"}, {"rank": 2, "score": 0.60491073, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 71, "text_snippet": "a black box and augments it with a tuneable retrieval model. Our evaluation shows that REPLUG can be integrated with any existing language model to improve their performance  REPLUG: Retrieval-Augmented Black-Box Language Models on language"}, {"rank": 3, "score": 0.6038957, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 8, "text_snippet": "Black-Box Language Models Figure 1, REPLUG is extremely flexible and can be used with any existing black-box LM and retrieval model. We also introduce REPLUG LSR (REPLUG with LM- Supervised Retrieval), a training scheme that can further imp"}, {"rank": 4, "score": 0.6014569, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 12, "text_snippet": "gnals, resulting in improved retrieval quality. •Evaluations on language modeling (§6), open-domain QA and MMLU demonstrate that REPLUG can im- prove the performance of various language models such as GPT, OPT and BLOOM, including very larg"}, {"rank": 5, "score": 0.5913053, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 6, "text_snippet": "PIs. Internal representations of such models are not exposed and fine-tuning is not supported. In this work, we introduce REPLUG (Retrieve and Plug ), a new retrieval-augmented LM framework where the lan- guage model is viewed as a black bo"}]}
{"case_index": 37, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"This approach first uses one LLM to generate a diverse set of global [MASK] questions based on corpus-sp\"", "gold": "sensemaking", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 7.043, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.64340234, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 23, "text_snippet": " this work, we propose an approach for generating a set of questions for evaluating global sensemaking over the entirety of the corpus. Our approach is related to LLM methods that use a corpus to generate questions whose answers would be su"}, {"rank": 2, "score": 0.6399586, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 1, "text_snippet": "ibuted equally to this work Abstract The use of retrieval-augmented generation (RAG) to retrieve relevant informa- tion from an external knowledge source enables large language models (LLMs) to answer questions over private and/or previousl"}, {"rank": 3, "score": 0.63553965, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 29, "text_snippet": "lements (nodes, edges, covariates) that the LLM can summarize in parallel at both indexing time and query time. The “global answer” to a given query is produced using a final round of query-focused summarization over all community summaries"}, {"rank": 4, "score": 0.61009383, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 50, "text_snippet": "il the token limit is reached. This final context is used to generate the global answer returned to the user. 3.2 Global Sensemaking Question Generation To evaluate the effectiveness of RAG systems for global sensemaking tasks, we use an LL"}, {"rank": 5, "score": 0.6072184, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 0, "text_snippet": "From Local to Global: A GraphRAG Approach to Query-Focused Summarization Darren Edge1†Ha Trinh1†Newman Cheng2Joshua Bradley2Alex Chao3 Apurva Mody3Steven Truitt2Dasha Metropolitansky1Robert Osazuwa Ness1 Jonathan Larson1 1Microsoft Research"}]}
{"case_index": 38, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"For clarity, we focus on a single turn of [MASK] and retrieval in this section.\"", "gold": "question", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.562, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.5961698, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 12, "text_snippet": "is information is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-a"}, {"rank": 2, "score": 0.5886358, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 17, "text_snippet": "FRAG achieves better performance than LLaMA without incurring higher latency in the downstream applications. 2 Model Architecture We denote the decoder model as Mdecand the encoder model as Menc. Given an input with Ttokens x1, x2, . . . , "}, {"rank": 3, "score": 0.58740807, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 15, "text_snippet": "igure 1) while preserving the autoregressive nature of the decoder, thereby supporting multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight reinforcement learning (RL) policy that sel"}, {"rank": 4, "score": 0.5748173, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 3, "text_snippet": " on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30 .85×the time-t"}, {"rank": 5, "score": 0.56283665, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 11, "text_snippet": "with many retrieved passages being uninformative and reused across multiple inferences. Allocating memory/computation for all the tokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The ret"}]}
{"case_index": 39, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"This style of retrieval can be added to both encoder- decoder (Yu, 2022; Izacard et al., 2022b) and [MASK] models (Khandelwal et al., 2020; Borgeaud et al., 2022; Shi et al., 2022; Rubin et al., 2022).\"", "gold": "decoder-only", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.99, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.77256906, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 17, "text_snippet": "inal predic- tion. This style of retrieval can be added to both encoder- decoder (Yu, 2022; Izacard et al., 2022b) and decoder-only models (Khandelwal et al., 2020; Borgeaud et al., 2022; Shi et al., 2022; Rubin et al., 2022). For example, "}, {"rank": 2, "score": 0.6736661, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 4, "text_snippet": "trieval-augmented language models (RALMs) (Khandelwal et al., 2019; Izacard et al., 2022; Lewis et al., 2020; Borgeaud et al., 2022) have shown impressive performance on knowledge-intensive tasks (Kwiatkowski et al., 2019; Petroni et al., 2"}, {"rank": 3, "score": 0.6692134, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 83, "text_snippet": "sser, S., and Leahy, C. The Pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027 , 2020.  REPLUG: Retrieval-Augmented Black-Box Language Models Goyal, A., Friesen, A., Banino, A., Weber, T., Ke, N. R"}, {"rank": 4, "score": 0.66890997, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 8, "text_snippet": "Black-Box Language Models Figure 1, REPLUG is extremely flexible and can be used with any existing black-box LM and retrieval model. We also introduce REPLUG LSR (REPLUG with LM- Supervised Retrieval), a training scheme that can further imp"}, {"rank": 5, "score": 0.663504, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 57, "text_snippet": "nted generators using a second “informed” retriever with access to the output, which the test-time retriever can be distilled from, and Hofstätter et al. (2022) recently proposed a training set ﬁltering/weighting approach to train stronger "}]}
{"case_index": 40, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"A more recent work (Luo et al., 2023) [MASK] an LM with a fixed number 2 Preprint.\"", "gold": "instruction-tunes", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 7.044, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6182238, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 21, "text_snippet": " al., 2023a; Gao et al., 2023). We introduce a method to train an arbitrary LM to learn to use retrieval on-demand for diverse instruction-following queries and introduce controlled generation guided by reflections tokens to further improve"}, {"rank": 2, "score": 0.60362244, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 86, "text_snippet": "M. Dai, Orhan Firat, Melvin John- son, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica"}, {"rank": 3, "score": 0.5926416, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 22, "text_snippet": " the retriever and LM on instruction-tuning datasets in two steps. While we also train our model on diverse instruction-following datasets, SELF-RAGenables retrieval on demand and selection of the best possible model output via fine-grained"}, {"rank": 4, "score": 0.5904398, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 96, "text_snippet": "ains an LM to learn to retrieve, generate, and critique text passages and its own generation by predicting the next tokens from its original vocabulary as well as newly added special tokens, called reflection tokens. SELF-RAGfurther enables"}, {"rank": 5, "score": 0.5865347, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 104, "text_snippet": "uage Processing , pp. 6997–7008, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021. emnlp-main.560. URL https://aclanthology.org/2021.emnlp-main.560 . Sewon Min, Weiji"}]}
{"case_index": 41, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"This avoids the need to run [MASK] forward passes once the candidate set Yhas been generated.\"", "gold": "additional", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 10.319, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.53228414, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 2, "score": 0.5254705, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 46, "text_snippet": "pervised data that precisely mimics the SELF- RAGinference-time process (Section 3.1). For each segment yt∈y, we run Cto assess whether additional passages could help to enhance generation. If retrieval is required, the retrieval special to"}, {"rank": 3, "score": 0.511242, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 27, "text_snippet": "n beam search for each document z, scoring each hypothesis using pθ(yi|x,z,y 1:i−1). This yields a set of hypotheses Y, some of which may not have appeared in the beams of all documents. To estimate the probability of an hypothesis ywe run "}, {"rank": 4, "score": 0.50811183, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 25, "text_snippet": "rator G. The retriever Raims to retrieve the top- Kdocuments D={dr1, ..., d rk}that are relevant to the input Xfrom the corpus C. Based on the input Xand the retrieved results D, the generator Gis responsible for generating the output Y. Th"}, {"rank": 5, "score": 0.50427616, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}]}
{"case_index": 42, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"(2023) highlights a potential [MASK]: contiguous seg- mentation might not capture the complete semantic depth of the text.\"", "gold": "shortcoming", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.269, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.60951126, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 22, "text_snippet": " RAPTOR addresses the issue of semantic depth and connection in reading by building a recursive tree structure that balances broader thematic comprehension with granular details and which allows nodes to be grouped based on semantic sim- il"}, {"rank": 2, "score": 0.5916532, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 18, "text_snippet": " ICLR 2024 Despite a diversity in methods, the retrieving components of models predominantly rely on stan- dard approaches, i.e., chunking corpora and encoding with BERT-based retrievers. Although this approach is widely adopted, Nair et al"}, {"rank": 3, "score": 0.5883045, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 21, "text_snippet": "ntermediate nodes thus storing varying levels of detail, keeping granular details. However, both methods, due to their reliance on adjacency for grouping or summarizing adjacent nodes, may still overlook distant interdependencies within the"}, {"rank": 4, "score": 0.5612059, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 5, "score": 0.56039333, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 1, "text_snippet": "rom a retrieval corpus, limiting holistic under- standing of the overall document context. We introduce the novel approach of recursively embedding, clustering, and summarizing chunks of text, constructing a tree with differing levels of su"}]}
{"case_index": 43, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"3 [MASK] To align the encoder and decoder, we follow the work of Yen et al.\"", "gold": "methodology", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.489, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6031163, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 12, "text_snippet": "is information is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-a"}, {"rank": 2, "score": 0.5975019, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 26, "text_snippet": "e detailed discussion 1REFRAGwithout cache means that we recompute the chunk embedding for the context and take this latency into account. 3  on empirical evaluation is in section A. 3 Methodology To align the encoder and decoder, we follow"}, {"rank": 3, "score": 0.59373933, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 3, "text_snippet": " on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30 .85×the time-t"}, {"rank": 4, "score": 0.5753507, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 0, "text_snippet": "REFRAG: Rethinking RAG based Decoding Xiaoqiang Lin1,2,∗,Aritra Ghosh1,Bryan Kian Hsiang Low2,Anshumali Shrivastava1,3,Vijai Mohan1 1Meta Superintelligence Labs,2National University of Singapore,3Rice University ∗Work done at Meta Large Lan"}, {"rank": 5, "score": 0.5698378, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 15, "text_snippet": "igure 1) while preserving the autoregressive nature of the decoder, thereby supporting multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight reinforcement learning (RL) policy that sel"}]}
{"case_index": 44, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"with many retrieved passages being [MASK] and reused across multiple inferences.\"", "gold": "uninformative", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.447, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.6494138, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 11, "text_snippet": "with many retrieved passages being uninformative and reused across multiple inferences. Allocating memory/computation for all the tokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The ret"}, {"rank": 2, "score": 0.62405163, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 2, "text_snippet": "ated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns th"}, {"rank": 3, "score": 0.5981656, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 29, "text_snippet": "teness. In contrast, common RAG approaches retrieve passages indiscriminately, without ensuring complete support from cited sources. 3.1 P ROBLEM FORMALIZATION AND OVERVIEW Formally, given input x, we train Mto sequentially generate textual"}, {"rank": 4, "score": 0.578534, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 13, "text_snippet": "nd memory usage during decoding, allwithout requiring modificationsto the LLM architecture or introducing new decoder parameters. REFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved passag"}, {"rank": 5, "score": 0.57159346, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 46, "text_snippet": "pervised data that precisely mimics the SELF- RAGinference-time process (Section 3.1). For each segment yt∈y, we run Cto assess whether additional passages could help to enhance generation. If retrieval is required, the retrieval special to"}]}
{"case_index": 45, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"Despite these being extractive tasks, we ﬁnd that [MASK] generation outperforms previous extractive approaches.\"", "gold": "unconstrained", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 9.611, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.6159448, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 13, "text_snippet": "sive tasks —tasks that humans could not reasonably be expected to perform without access to an external knowledge source. Our RAG models achieve state-of-the-art results on open Natural Questions [ 29], WebQuestions [ 3] and CuratedTrec [ 2"}, {"rank": 2, "score": 0.5839653, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 74, "text_snippet": "nstrates a substantial performance advantage over supervised fine-tuned LLMs in all tasks and even outperforms ChatGPT in PubHealth, PopQA, biography generations, and ASQA (Rouge and MAUVE). Our approach also significantly outperforms a con"}, {"rank": 3, "score": 0.58278537, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 14, "text_snippet": "active approaches. For knowledge-intensive generation, we experiment with MS-MARCO [ 1] and Jeopardy question generation, and we ﬁnd that our models generate responses that are more factual, speciﬁc, and diverse than a BART baseline. For FE"}, {"rank": 4, "score": 0.5781596, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 69, "text_snippet": "ance across a variety of NLP tasks when considered in isolation. Such tasks include open-domain question answering [ 5,29], fact checking [ 56], fact completion [ 48], long-form question answering [ 12], Wikipedia article generation [ 36], "}, {"rank": 5, "score": 0.5729071, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 15, "text_snippet": " that answers do not correspond to spans in support documents, thus requiring ab- stractive models. Raffel et al. (2019) showed that generative models are competitive for reading com- prehension tasks such as SQuAD (Rajpurkar et al., 2016),"}]}
{"case_index": 46, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"2.5 Decoding At test time, [MASK] and RAG-Token require different ways to approximate arg maxyp(y|x).\"", "gold": "rag-sequence", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 11.873, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.63322574, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 26, "text_snippet": " arg maxyp(y|x). RAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera- tor with transition probability: p′ θ(yi|x,y 1:i−1) =∑ z∈top-k(p(·|x))pη(zi|x)pθ(yi|x,zi,y1:i−1)To decode, we can plug p′ θ(yi|x,y 1:i−"}, {"rank": 2, "score": 0.62867785, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 25, "text_snippet": "escent with Adam [ 28]. Updating the document encoder BERTdduring training is costly as it requires the document index to be periodically updated as REALM does during pre-training [ 20]. We do not ﬁnd this step necessary for strong performa"}, {"rank": 3, "score": 0.58066934, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 19, "text_snippet": "r produces the output sequence probability for each document, which are then marginalized, pRAG-Sequence (y|x)≈∑ z∈top-k(p(·|x))pη(z|x)pθ(y|x,z) =∑ z∈top-k(p(·|x))pη(z|x)N∏ ipθ(yi|x,z,y 1:i−1) RAG-Token Model In the RAG-Token model we can d"}, {"rank": 4, "score": 0.54209995, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 221, "text_snippet": "x logits for the tokens A, B, C and D, and performing a softmax over them to obtain a distribution over the 4 answer options. For standard inference, we then simply return the answer corresponding to the argmax of this distribution. De-bias"}, {"rank": 5, "score": 0.52897877, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 20, "text_snippet": "are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we deﬁne: pRAG-Token ("}]}
{"case_index": 47, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"Encoder Encoder Encoder Context Text Decoder-only Foundation Model Sequence [MASK] Light-weight Encoder Who is the President of USA?\"", "gold": "precomputable", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.419, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.71045303, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 20, "text_snippet": " the 2  Donald Trump is  the President of  the United States . He assumed office  on January 20,  2025, making him the 47th  President of the  United States. Encoder Encoder Encoder  Context Text Decoder-only Foundation Model  Sequence  Pre"}, {"rank": 2, "score": 0.6337684, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 146, "text_snippet": "works (Bello et al., 2017) to constrain the action space. 5Unless specified, we use the pre-trained checkpoint. The reason of choosing this model is that existing baselines (Yen et al., 2024; Shi et al., 2024) adapts LLaMA-2-7B. If we use o"}, {"rank": 3, "score": 0.60925144, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 19, "text_snippet": "1 }. The encoder model then processes all the chunks to obtain a chunk embedding for each chunkc i=Menc(Ci). This chunk embedding is then projected with a projection layer ϕto match the size of the token embedding of the decoder model, ecnk"}, {"rank": 4, "score": 0.5412735, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 147, "text_snippet": "ecoder Tokenizer &  Embedding  Decoder Input Text Token Embedding  Chunk  Embedding  RL-trained chunk expansion policy  Reward = - Log(Perplexity)  Donald Trump  Answer Figure 5A demonstration of selective token compression. For all chunks,"}, {"rank": 5, "score": 0.47518927, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 21, "text_snippet": "uery  Encoder Figure 1The main design ofREFRAG. The input context is chunked and processed by the light-weight encoder to produce chunk embeddings, which are precomputable for efficient reuse. A light-weight RL policy decide few chunks to e"}]}
{"case_index": 48, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"be trained using weak supervision in the form of [MASK] pairs (Karpukhin et al., 2020), or pretrained using a cloze task and ﬁnetuned end-to- end (Guu et al., 2020; Lee et al., 2019).\"", "gold": "question-answer", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 14.551, "llm_ms": 0.008, "top_contexts": [{"rank": 1, "score": 0.72659814, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 14, "text_snippet": " be trained using weak supervision in the form of question-answer pairs (Karpukhin et al., 2020), or pretrained using a cloze task and ﬁnetuned end-to- end (Guu et al., 2020; Lee et al., 2019). Generative question answering was mostly consi"}, {"rank": 2, "score": 0.6366348, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}, {"rank": 3, "score": 0.62080276, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 56, "text_snippet": " the reader and the retriever by leveraging the perplexity of the output generated by the reader. Sachan et al. (2021) and Lee et al. (2021a) both employ salient span masking to pre-train retrievers, leveraging the perplexity and attention "}, {"rank": 4, "score": 0.6197689, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 84, "text_snippet": " and pre-training paradigm for generative LLMs, leveraging cross-attention and end-to-end pre-training with 9  retrieval from a trillion-token data store, achieving strong performance. Subsequent work by Shi et al. (2024) and Lin et al. (20"}, {"rank": 5, "score": 0.613482, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 187, "text_snippet": "10.18653/v1/2021.acl-long.518. URL https://aclanthology.org/2021.acl-long.518 . 7 Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised open domain question answering. In Proc. ACL , 2019. 7 Brian Lester"}]}
{"case_index": 49, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"As shown in Figure 1, our system, RAPTOR, clusters chunks of text, generates text summaries of those clusters, and then repeats, [MASK] a tree from the bottom up.\"", "gold": "generating", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.3333333333333333, "hit_rank": 3, "matched_by": "doc_id+chunk_id", "retrieval_ms": 9.364, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.690781, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 9, "text_snippet": "uestions at different levels. 1arXiv:2401.18059v1 [cs.CL] 31 Jan 2024  Published as a conference paper at ICLR 2024 2 3 4 5  1 1  2  3  3  4 5  5 6  8  7 Index #8  Text: summary of  nodes 2 and 3  Child Nodes: 2, 3  Text Embedding  Text chu"}, {"rank": 2, "score": 0.6850537, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 1, "text_snippet": "rom a retrieval corpus, limiting holistic under- standing of the overall document context. We introduce the novel approach of recursively embedding, clustering, and summarizing chunks of text, constructing a tree with differing levels of su"}, {"rank": 3, "score": 0.67453295, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 8, "text_snippet": " indexing and retrieval system that uses a tree structure to capture both high-level and low-level details about a text. As shown in Figure 1, our system, RAPTOR, clusters chunks of text, generates text summaries of those clusters, and then"}, {"rank": 4, "score": 0.6723014, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 10, "text_snippet": "rsively clusters chunks of text based on their vector embeddings and generates text summaries of those clusters, constructing a tree from the bottom up. Nodes clustered together are siblings; a parent node contains the text summary of that "}, {"rank": 5, "score": 0.6578089, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 24, "text_snippet": "r text chunks, we employ a clustering algorithm. Once clustered, a Language Model is used to summarize the grouped texts. These summarized texts are then re-embedded, and the cycle of embedding, clustering, and summarization continues until"}]}
{"case_index": 50, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"More precisely, each retrieved passage and its title are [MASK] with the question, and processed in- dependently from other passages by the encoder.\"", "gold": "concatenated", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.061, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.5905572, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 21, "text_snippet": "Raffel et al., 2019; Lewis et al., 2019). The model takes as input the question, as well as the support passages, and generates the answer. More precisely, each retrieved passage and its title are concatenated with the question, and process"}, {"rank": 2, "score": 0.5695104, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}, {"rank": 3, "score": 0.55489653, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 8, "text_snippet": "3 Feb 2021  Question + Passage 1encoderQuestion + Passage 2encoderQuestion + Passage NencoderdecoderAnswerconcat… …… …Figure 2: Architecture of the Fusion-in-Decoder method. representations. Then, a sequence-to-sequence model generates the "}, {"rank": 4, "score": 0.5481352, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 20, "text_snippet": "es with SpaCy.2In DPR, passages and questions are represented as dense vector representations, computed using two BERT networks. The ranking function is the dot product between the query and passage represen- tations. Retrieval is performed"}, {"rank": 5, "score": 0.5477685, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 5, "text_snippet": "t, it retrieves support text passages from an external source of knowledge such as Wikipedia. Then, a generative encoder-decoder model produces the answer, conditioned on the question and the re- trieved passages. This approach scales well "}]}
{"case_index": 51, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"Building on that [MASK] and the advances in pretrain- ing of natural language processing models, Roberts et al.\"", "gold": "observation", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.249, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.67257774, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 84, "text_snippet": " and pre-training paradigm for generative LLMs, leveraging cross-attention and end-to-end pre-training with 9  retrieval from a trillion-token data store, achieving strong performance. Subsequent work by Shi et al. (2024) and Lin et al. (20"}, {"rank": 2, "score": 0.6534401, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 2, "text_snippet": "s a ﬂexible framework to efﬁciently aggregate and com- bine evidence from multiple passages. 1 Introduction Recently, several works have shown that factual information can be extracted from large scale language models trained on vast quanti"}, {"rank": 3, "score": 0.64468396, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}, {"rank": 4, "score": 0.6421443, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 0, "text_snippet": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering Gautier Izacard1,2,3Edouard Grave1 1Facebook AI Research, Paris 2ENS, PSL University, Paris 3Inria, Paris gizacard|egrave@fb.com Abstract Generative mode"}, {"rank": 5, "score": 0.6252615, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 15, "text_snippet": " that answers do not correspond to spans in support documents, thus requiring ab- stractive models. Raffel et al. (2019) showed that generative models are competitive for reading com- prehension tasks such as SQuAD (Rajpurkar et al., 2016),"}]}
{"case_index": 52, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"The retriever Raims to retrieve the top- [MASK] D={dr1, ..., d rk}that are relevant to the input Xfrom the corpus C.\"", "gold": "kdocuments", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.544, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.62069863, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 25, "text_snippet": "rator G. The retriever Raims to retrieve the top- Kdocuments D={dr1, ..., d rk}that are relevant to the input Xfrom the corpus C. Based on the input Xand the retrieved results D, the generator Gis responsible for generating the output Y. Th"}, {"rank": 2, "score": 0.5459491, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 18, "text_snippet": "y retrieving the top-k relevant documents from a large corpus of text with the retriever. Then, these documents are fed to the language model, along with the query, which in turns generates the output. Both the retriever and the language mo"}, {"rank": 3, "score": 0.5304789, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 31, "text_snippet": "′⊂ D with the highest simi- larity scores from a corpus Dgiven an input context x, as described in §3.1. We then compute the retrieval likelihood of each retrieved document d: PR(d|x) =es(d,x)/γ P d∈D′es(d,x)/γ where γis a hyperparameter th"}, {"rank": 4, "score": 0.51794255, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 29, "text_snippet": "triever inREPLUG by using the LM itself to provide supervision about which documents should be retrieved. Inspired by Sachan et al. (2022), our approach can be seen as adjusting the probabilities of the retrieved documents to match the prob"}, {"rank": 5, "score": 0.51704293, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 10, "text_snippet": "eural retriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The retriever (Dense Passage Retriever [ 26], henceforth DPR) provides latent documents conditioned on the input, and the seq2seq model (BART"}]}
{"case_index": 53, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"(2023) fine-tune both the retriever and LM on [MASK] datasets in two step\"", "gold": "instruction-tuning", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.25, "hit_rank": 4, "matched_by": "doc_id+chunk_id", "retrieval_ms": 9.094, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.61690545, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 22, "text_snippet": " the retriever and LM on instruction-tuning datasets in two steps. While we also train our model on diverse instruction-following datasets, SELF-RAGenables retrieval on demand and selection of the best possible model output via fine-grained"}, {"rank": 2, "score": 0.5994331, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 41, "text_snippet": "lding are unforgettable experience.No RetrievalNo Retrieval Retriever Figure 2: SELF-RAGtraining examples. The left example does not require retrieval while the right one requires retrieval; thus, passages are inserted. More examples are in"}, {"rank": 3, "score": 0.5834221, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 66, "text_snippet": "onsists of diverse instruction-following input-output pairs. In particular, we sample instances from Open-Instruct processed data (Wang et al., 2023) and knowledge-intensive datasets (Petroni et al., 2021; Stelmakh et al., 2022; Mihaylov et"}, {"rank": 4, "score": 0.5832133, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 21, "text_snippet": " al., 2023a; Gao et al., 2023). We introduce a method to train an arbitrary LM to learn to use retrieval on-demand for diverse instruction-following queries and introduce controlled generation guided by reflections tokens to further improve"}, {"rank": 5, "score": 0.580251, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 63, "text_snippet": "tuning dataset to facilitate model fine-tuning. 5.1 Retrieval Augmented Generation Training dataset.We follow the work of Lin et al. (2024) and use a combination of question answering datasets from 5 domains to fine-tune our model, which co"}]}
{"case_index": 54, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"We train an arbitrary LM in an end-to-end manner to learn to reflect on its own generation process given a task input by generating both task output and [MASK] special tokens (i.e., reflection tokens ).\"", "gold": "intermittent", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.51, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.70155096, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 6, "text_snippet": "y trained to leverage and follow facts from provided passages. This work introduces Self-Reflective Retrieval-augmented Generation ( SELF-RAG)to improve an LLM’s generation quality, including its factual accuracy without hurting its versati"}, {"rank": 2, "score": 0.6947686, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 14, "text_snippet": "t with reflection tokens by unifying them as the next token prediction from the expanded model vocabulary. We train our generator LM on a diverse collection of text interleaved with reflection tokens and retrieved passages. Reflection token"}, {"rank": 3, "score": 0.68334746, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 2, "text_snippet": "ew framework called Self-Reflective Retrieval-Augmented Gen- eration ( SELF-RAG)that enhances an LM’s quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passage"}, {"rank": 4, "score": 0.682747, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 7, "text_snippet": "rmittent special tokens (i.e., reflection tokens ). Reflection tokens are categorized into retrieval andcritique tokens to indicate the need for retrieval and its generation quality respectively (Figure 1 right). In particular, given an inp"}, {"rank": 5, "score": 0.68118596, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 36, "text_snippet": "text with reflection tokens by unifying them as next token predictions from the expanded model vocabulary (i.e., the original vocabulary plus reflection tokens). Specifically, we train the generator model Mon a curated corpus with interleav"}]}
{"case_index": 55, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"[MASK] Izacard & Grave (2021), we average these scores over all attention heads, layers, and tokens to obtain a score for each document.\"", "gold": "following", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.2, "hit_rank": 5, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.759, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6220039, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 25, "text_snippet": "n Distillation (ADist). The ﬁrst loss that we consider is based on the attention scores of the language model, and is heavily inspired by Izacard & Grave (2021). The main idea is that the cross-attention scores between the input documents a"}, {"rank": 2, "score": 0.6199486, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 27, "text_snippet": " paper, it was proposed to use the pre-softmax scores from the decoder cross-attentions, and average across heads, layers and tokens. Here, we propose an alternative which gives slightly stronger results, which relies on the following obser"}, {"rank": 3, "score": 0.5805423, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 4, "score": 0.5798994, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 21, "text_snippet": "ce architecture (Raﬀel et al., 2019). We rely on the Fusion-in-Decoder modiﬁcation of sequence-to-sequence models, and process each document independently in the encoder (Izacard & Grave, 2020). We then concatenate the outputs of the encode"}, {"rank": 5, "score": 0.5796991, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 28, "text_snippet": "e value vninto account. Hence, we use the quantity αn∥vn∥2as the measure of relevance for token n. Following Izacard & Grave (2021), we average these scores over all attention heads, layers, and tokens to obtain a score for each document. W"}]}
{"case_index": 56, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"For query x, we use Maximum Inner Product Search (MIPS) to ﬁnd the top-K [MASK] zi.\"", "gold": "documents", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 12.064, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.5984673, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 8, "text_snippet": " retriever ( Query Encoder +Document Index ) with a pre-trained seq2seq model ( Generator ) and ﬁne-tune end-to-end. For query x, we use Maximum Inner Product Search (MIPS) to ﬁnd the top-K documents zi. For ﬁnal prediction y, we treatzas a"}, {"rank": 2, "score": 0.51612586, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 3, "score": 0.4940422, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 18, "text_snippet": "y retrieving the top-k relevant documents from a large corpus of text with the retriever. Then, these documents are fed to the language model, along with the query, which in turns generates the output. Both the retriever and the language mo"}, {"rank": 4, "score": 0.4926173, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 31, "text_snippet": "′⊂ D with the highest simi- larity scores from a corpus Dgiven an input context x, as described in §3.1. We then compute the retrieval likelihood of each retrieved document d: PR(d|x) =es(d,x)/γ P d∈D′es(d,x)/γ where γis a hyperparameter th"}, {"rank": 5, "score": 0.48718914, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 24, "text_snippet": "to the input context xto obtain a query embedding E(x). The similarity between the query embedding and the document embedding is computed by their cosine similarity: s(d, x) = cos( E(d),E(x)) (1) The top- kdocuments that have the highest si"}]}
{"case_index": 57, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"This graph index spans nodes (e.g., entities), edges (e.g., [MASK]), and covariates (e.g., claims) that have been detected, extracted, and summarized by LLM prompts tailored to the domain of the dataset.\"", "gold": "relationships", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 12.485, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6200338, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 29, "text_snippet": "lements (nodes, edges, covariates) that the LLM can summarize in parallel at both indexing time and query time. The “global answer” to a given query is produced using a final round of query-focused summarization over all community summaries"}, {"rank": 2, "score": 0.6138028, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 28, "text_snippet": "n Indexing Time Query Time Pipeline Stage Figure 1: Graph RAG pipeline using an LLM-derived graph index of source document text. This graph index spans nodes (e.g., entities), edges (e.g., relationships), and covariates (e.g., claims) that "}, {"rank": 3, "score": 0.58639586, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 27, "text_snippet": "ns (Wang et al., 2023a; Zheng et al., 2024). Some prior work proposes criteria for having LLMs quantify the quality of 3  Source Documents Text Chunkstext extraction and chunking Entities & Relationshipsdomain-tailored summarization Knowled"}, {"rank": 4, "score": 0.5441766, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 38, "text_snippet": "rompts and details on our implementation of entity and claim extraction. 3.1.3 Entities & Relationships →Knowledge Graph The use of an LLM to extract entities, relationships, and claims is a form of abstractive summariza- tion – these are m"}, {"rank": 5, "score": 0.54034907, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 20, "text_snippet": "elements of the graph, or properties of the graph structure directly in the prompt (Baek et al., 2023; He et al., 2024; Zhang, 2023) or as factual grounding for generated outputs (Kang et al., 2023; Ranade and Joshi, 2023). Other techniques"}]}
{"case_index": 58, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"This [MASK] the need to host a critic model during training, reducing overhead.\"", "gold": "eliminates", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.25, "hit_rank": 4, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.535, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.64779896, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 15, "text_snippet": "st a critic model during training, reducing overhead. The critic model, in part, is supervised on a dataset of input, output, and corresponding reflection tokens collected by prompting a propriety LM (i.e., GPT-4; OpenAI 2023). While we dra"}, {"rank": 2, "score": 0.5760707, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 37, "text_snippet": "e quality of a given task output (Section 3.2.1). Using the critic model, we update the training corpus by inserting reflection tokens into task outputs offline. Subsequently, we train the final generator model ( M) using the conventional L"}, {"rank": 3, "score": 0.5544271, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 36, "text_snippet": "text with reflection tokens by unifying them as next token predictions from the expanded model vocabulary (i.e., the original vocabulary plus reflection tokens). Specifically, we train the generator model Mon a curated corpus with interleav"}, {"rank": 4, "score": 0.54596174, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 14, "text_snippet": "t with reflection tokens by unifying them as the next token prediction from the expanded model vocabulary. We train our generator LM on a diverse collection of text interleaved with reflection tokens and retrieved passages. Reflection token"}, {"rank": 5, "score": 0.53362495, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 24, "text_snippet": "ering tasks and to generate with tree search, guided by LM-generated value scores. While their value function simply indicates an overall score of each generation, SELF-RAGtrains to an arbitrary LM to learn to generate fine-grained self-ref"}]}
{"case_index": 59, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"This approach scales well with the number of retrieved passages, as the [MASK] keeps improving when retrieving up to one hundred passages.\"", "gold": "performance", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 8.311, "llm_ms": 0.013, "top_contexts": [{"rank": 1, "score": 0.65480566, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 37, "text_snippet": "nswering, which relies on retriev- ing support passages before processing them with a generative model. We show that while conceptually simple, this approach is competitive with existing methods, and that it scales well with the number of r"}, {"rank": 2, "score": 0.6418313, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}, {"rank": 3, "score": 0.63131225, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 31, "text_snippet": "tion answering. We observe that while conceptu- ally simple, this method outperforms existing work on the NaturalQuestion and TriviaQA benchmarks. In particular, generative models seem to perform well when evidence from multiple passages ne"}, {"rank": 4, "score": 0.6289451, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 34, "text_snippet": "s are reported on dev sets. number of retrieved passages. In particular, we observe that increasing the number of passages from 10 to 100 leads to 6% improvement on Trivi- aQA and 3.5% improvement on NaturalQuestions. On the other hand, the"}, {"rank": 5, "score": 0.62611616, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 36, "text_snippet": "the number of training passages leads to a decrease of accuracy. Further, we propose to ﬁnetune the previous models using 100 passages for 1000 steps. This allows to reduce the accuracy gap, while using signiﬁcantly less computational resou"}]}
{"case_index": 60, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"For instance, REPLUG can improve Codex (175B) performance on MMLU by 4.5%, achieving compa- rable results to the 540B, [MASK] Flan-PaLM.\"", "gold": "instruction-finetuned", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.189, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.72784555, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 10, "text_snippet": "l., 2017). For instance, REPLUG can improve Codex (175B) performance on MMLU by 4.5%, achieving compa- rable results to the 540B, instruction-finetuned Flan-PaLM. Furthermore, tuning the retriever with our training scheme (i.e., REPLUG LSR "}, {"rank": 2, "score": 0.69247967, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 51, "text_snippet": ", and the output probabilities are ensemble together. 1Code-Davinci-002Results Table 2 presents the results from the baselines, REPLUG, and REPLUG LSR on the MMLU dataset. We observe that both the REPLUG andREPLUG LSR improve the original C"}, {"rank": 3, "score": 0.6788429, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 12, "text_snippet": "gnals, resulting in improved retrieval quality. •Evaluations on language modeling (§6), open-domain QA and MMLU demonstrate that REPLUG can im- prove the performance of various language models such as GPT, OPT and BLOOM, including very larg"}, {"rank": 4, "score": 0.6649523, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 9, "text_snippet": "22) that adapts language models to the retriever. We use a training objective which prefers retrieving documents that improve language model perplexity, while treating the LM as a frozen, black-box scoring function. Our experiments show tha"}, {"rank": 5, "score": 0.66300535, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 63, "text_snippet": " the performance of REPLUG andREPLUG LSR improved monotonically. How- ever, a small number of documents (e.g., 10) was sufficient to achieve large performance gains.  REPLUG: Retrieval-Augmented Black-Box Language Models Perplexity 14.0016."}]}
{"case_index": 61, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"RetrieveStep 1: Retrieve on demand Prompt + 11 of 50 state [MASK] Step 2: Generate segment in para\"", "gold": "namesrelevant", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.25, "hit_rank": 4, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.747, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.5618364, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 12, "text_snippet": "ding Utah.  Prompt: Write an essay of your best summer vacation Prompt: Write an essay of your best summer vacation No RetrievalMy best summer vacation is when my family and I embarked on a road trip along …My best…  >Repeat.… No informatio"}, {"rank": 2, "score": 0.5512291, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 7, "text_snippet": "rmittent special tokens (i.e., reflection tokens ). Reflection tokens are categorized into retrieval andcritique tokens to indicate the need for retrieval and its generation quality respectively (Figure 1 right). In particular, given an inp"}, {"rank": 3, "score": 0.5438913, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 32, "text_snippet": ", respectively. Algorithm 1 SELF-RAGInference Require: Generator LM M, Retriever R, Large-scale passage collections {d1, . . . , d N} 1:Input: input prompt xand preceding generation y<t,Output: next output segment yt 2:Mpredicts Retrieve gi"}, {"rank": 4, "score": 0.53660727, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 10, "text_snippet": " Generation (RAG)Ours: Self-reﬂective Retrieval-Augmented Generation (Self-RAG)  Popular names by states. In Texas, Emma is a popular baby name. Of the ﬁfty states, eleven are named after an individual person.  Prompt How did US states get "}, {"rank": 5, "score": 0.53648275, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 34, "text_snippet": "l predicts the next output segment, as it does in a standard LM. If retrieval is needed, the model generates: a critique token to evaluate the retrieved passage’s relevance, the next response segment, and a critique token to evaluate if the"}]}
{"case_index": 62, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"Advanced RAG Many advanced [MASK] have been developed from the original RAG in recent years (Zhang et al., 2024; Kim et al., 2024; Wa\"", "gold": "approaches", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 12.937, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.69069076, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 20, "text_snippet": "model that specializes in response generation. Despite this, the methods above usually ignore a question, what if the retrieval goes wrong? Since the purpose of introducing a retrieval is to secure that generative LMs can obtain relevant an"}, {"rank": 2, "score": 0.6357527, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 21, "text_snippet": " al., 2023a; Gao et al., 2023). We introduce a method to train an arbitrary LM to learn to use retrieval on-demand for diverse instruction-following queries and introduce controlled generation guided by reflections tokens to further improve"}, {"rank": 3, "score": 0.62894166, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 21, "text_snippet": "al RAG in recent years (Zhang et al., 2024; Kim et al., 2024; Wang et al., 2024; Liu et al., 2024). Considering that retrieval is sometimes unnecessary for some queries, conversely, responses without retrieval are even more accurate in many"}, {"rank": 4, "score": 0.6139154, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 7, "text_snippet": "ieval-augmented generation (RAG) (Lewis et al., 2020). In this framework, the input to models is augmented by prepending relevant documents that are retrieved from an external knowledge corpus (Guu et al., 2020). While RAG serves as a pract"}, {"rank": 5, "score": 0.61360335, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 4, "text_snippet": " for long-form generations relative to these models.1 1 I NTRODUCTION State-of-the-art LLMs continue to struggle with factual errors (Mallen et al., 2023; Min et al., 2023) despite their increased model and data scale (Ouyang et al., 2022)."}]}
{"case_index": 63, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"The examples show that a low-quality retriever is prone to introducing a [MASK] amount of irrelevant information, impeding the generators from acquiring accurate knowledge and potentially misleading them.\"", "gold": "substantial", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 9.206, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6943537, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 6, "text_snippet": " The examples show that a low-quality retriever is prone to introducing a substantial amount of irrelevant information, impeding the generators from acquiring accurate knowledge and potentially misleading them. the parametric knowledge they"}, {"rank": 2, "score": 0.64368606, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 8, "text_snippet": "wledge raises significant concerns about the model’s behavior and performance in scenarios where retrieval may fail or return inaccu- rate results (Shi et al., 2023). As Figure 1 shows that a low-quality retriever is prone to introducingarX"}, {"rank": 3, "score": 0.56306964, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 18, "text_snippet": "pact the LLMs and exacerbate the problems. Thus, it is obvious that the lack of accurate and specific knowledge can lead to misleading or even inaccurate generation, which will severely hurt the experience of users in most practical applica"}, {"rank": 4, "score": 0.55375195, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 1, "text_snippet": "evitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Al- though retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heav"}, {"rank": 5, "score": 0.54989433, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 41, "text_snippet": "od is further designed which will be elaborated in Section 4.4. Incorrect Besides, a retrieval is assumed Incorrect when the confidence scores of all retrieved documents are below the lower threshold. This indicates that all retrieved docum"}]}
{"case_index": 64, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"In summary, our [MASK] in this paper are three-fold: 1) This paper studies the scenarios where the retriever returns inaccurate results and, to the best of our knowledge, makes the first attempt to design corrective strategies for RAG to improve its robustness.\"", "gold": "contributions", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.253, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.63227975, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 15, "text_snippet": "hers to reproduce our results, we will publish all source code later. In summary, our contributions in this paper are three-fold: 1) This paper studies the scenarios where the retriever returns inaccurate results and, to the best of our kno"}, {"rank": 2, "score": 0.62978363, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 23, "text_snippet": "t to retrieve in long-form generation. Compared with recent studies (Schick et al., 2023; Luo et al., 2023; Asai et al., 2024) that are the most relevant to our work, a main difference should be highlighted. These approaches target on explo"}, {"rank": 3, "score": 0.6210809, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 10, "text_snippet": " should not have been equally referred to and involved in RAG. On account of the above issues, this paper particularly studies the scenarios where the retriever returns inaccurate results. A method named Corrective Retrieval- Augmented Gene"}, {"rank": 4, "score": 0.59710044, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 24, "text_snippet": " knowledge, this paper makes the first attempt to explore and design corrective strategies for RAG to improve its robustness of generation. 3 Task Formulation Following previous work (Lewis et al., 2020; Asai et al., 2024), given input Xand"}, {"rank": 5, "score": 0.5881561, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 83, "text_snippet": " in Table 6. The findings indicate that the self-correction mecha- nism incurs only modest computational overheadwhile significantly enhancing performance, thereby validating its lightweight nature. 6 Conclusion & Limitation This paper stud"}]}
{"case_index": 65, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"For example, in the case of question answering, the query [MASK] to the question and the model needs to generate the answer.\"", "gold": "corresponds", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 7.725, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.6255157, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 2, "score": 0.60046697, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 235, "text_snippet": "ull results for the cyclic-permutation-de-biased Atlas-XXL can be found in Table 18. A.2 Question answering A.2.1 Training Details For question answering, similarly to the MMLU experiments, we format the input using the following template: "}, {"rank": 3, "score": 0.5901332, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 1, "text_snippet": " Abstract Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter count"}, {"rank": 4, "score": 0.58974576, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 220, "text_snippet": "f the correct answer: [MASK_0] {correct answer option letter} This format closely matches the format of MLM pre-training objective, aiding few-shot learning. When training, we permute the order of the answer options, i.e. shuﬄing which answ"}, {"rank": 5, "score": 0.58334965, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 60, "text_snippet": "ut. Retrieval-Augmentation with Search Engines. Recently, diﬀerent works have proposed to train large language models to interact with a search engine, by generating text queries, and using the retrieved documents as additional context (Nak"}]}
{"case_index": 66, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"3 Score (M,yi,[sj;xi]) = log pM(y|[sj;xi]), log [MASK] assigned to target output accord\"", "gold": "likelihood", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 9.993, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.5967785, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 29, "text_snippet": "g pM(y|[sj;xi]), log likelihood assigned to target output according to LM Mwhen candidate sentence is prepended to the input. We consider the sentence with the highest log likelihood as a positive example pi(line 3). To construct negative e"}, {"rank": 2, "score": 0.5757563, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 28, "text_snippet": " of input sequence xi and candidate sentences sj, we measure 3Recent work (Zhang et al., 2022) shows that extractive approach does not always preserve faithfulness, but such cases are still rare compared to abstractive approaches which can "}, {"rank": 3, "score": 0.5129252, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 220, "text_snippet": "f the correct answer: [MASK_0] {correct answer option letter} This format closely matches the format of MLM pre-training objective, aiding few-shot learning. When training, we permute the order of the answer options, i.e. shuﬄing which answ"}, {"rank": 4, "score": 0.5070007, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 31, "text_snippet": "er are updated by applying a StopGradient operator 4  aroundplm. One should note that the probability distribution over documents that maximizes this loss function is an indicator of the document corresponding to the highest probability of "}, {"rank": 5, "score": 0.50104946, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 33, "text_snippet": "istribution according to the language model, using a uniform prior: pk∝pLM(a|dk,q). Using the Softmax operator, we have that pk=exp(logpLM(a|dk,q))∑K i=1exp(logpLM(a|di,q)). Leave-one-out Perplexity Distillation (LOOP). Finally, we propose "}]}
{"case_index": 67, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"We evaluate our approach on language modeling task and open domain question [MASK] task.\"", "gold": "answering", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 13.448, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.6001284, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 69, "text_snippet": "ance across a variety of NLP tasks when considered in isolation. Such tasks include open-domain question answering [ 5,29], fact checking [ 56], fact completion [ 48], long-form question answering [ 12], Wikipedia article generation [ 36], "}, {"rank": 2, "score": 0.59877354, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 3, "text_snippet": "n. We evaluate our approach on language modeling task and open domain question answering task. We achieve a compression rate of as low as 6% with minimal loss in performance for both tasks, significantly outperforming the off-the-shelf summ"}, {"rank": 3, "score": 0.5985805, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 16, "text_snippet": "ous NLP tasks, including language mod- eling (Min et al., 2022; Borgeaud et al., 2022; Khandelwal et al., 2020) and open-domain question answering (Lewis et al., 2020; Izacard et al., 2022b; Hu et al., 2022). Specifi- cally, using the input"}, {"rank": 4, "score": 0.5911191, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 42, "text_snippet": "g documents of 100 words. During retrieval, articles containing the input sequence xis removed from the corpus to prevent data contamination. Following Ram et al. (2023), we perform retrieval every 32 tokens. 4.2 O PEN-DOMAIN QA Datasets We"}, {"rank": 5, "score": 0.5900653, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 67, "text_snippet": "s in few-shot and full dataset settings. 4.1 Benchmarks To evaluate our retrieval-augmented language models we consider the following benchmarks, which include diﬀerent tasks. 8  Knowledge-Intensive Language Tasks (KILT). First, we use the "}]}
{"case_index": 68, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"In this paper, we investigate whether few-shot learning requires models to store a large amount of information in their parameters, and if memorisation can be decoupled from [MASK].\"", "gold": "generalisation", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 12.557, "llm_ms": 0.008, "top_contexts": [{"rank": 1, "score": 0.7368901, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 5, "text_snippet": "ally, it is unclear to what extent eﬀective few-shot learning requires vast knowledge in the parameters of the model. In this paper, we investigate whether few-shot learning requires models to store a large amount of information in their pa"}, {"rank": 2, "score": 0.658774, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 4, "text_snippet": " model, and the size of the training data. Large language models owe this improvement to both a larger computational budget, enabling more complex reasoning, and the ability to memorize more information related to downstream tasks from the "}, {"rank": 3, "score": 0.6562754, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 1, "text_snippet": " Abstract Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter count"}, {"rank": 4, "score": 0.5890975, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 3, "text_snippet": "xamples, outperforming a 540B parameters model by 3% despite having 50x fewer parameters. 1 Introduction Large language models (LLMs) are impressive few-shot learners (Brown et al., 2020; Rae et al., 2021; Hoﬀmann et al., 2022; Chowdhery et"}, {"rank": 5, "score": 0.58226186, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 12, "text_snippet": "ck- augmented networks [ 25] and memory layers [ 30]. In contrast, we explore a setting where both parametric and non-parametric memory components are pre-trained and pre-loaded with extensive knowledge. Crucially, by using pre-trained acce"}]}
{"case_index": 69, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"3.1 P ROBLEM [MASK] AND OVERVIEW Formally, given input x, we train Mto sequentially generate textual outputs yconsisting of multiple segments y= [y1, .\"", "gold": "formalization", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.643, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6683855, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 29, "text_snippet": "teness. In contrast, common RAG approaches retrieve passages indiscriminately, without ensuring complete support from cited sources. 3.1 P ROBLEM FORMALIZATION AND OVERVIEW Formally, given input x, we train Mto sequentially generate textual"}, {"rank": 2, "score": 0.5774483, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 48, "text_snippet": "gmented with reflection tokens Dgenusing the standard next token objective: max ME(x,y,r )∼DgenlogpM(y, r|x). (2) Unlike Ctraining (Eq. 1), Mlearns to predict the target output as well as the reflection tokens. During training, we mask out "}, {"rank": 3, "score": 0.57524884, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 37, "text_snippet": "e quality of a given task output (Section 3.2.1). Using the critic model, we update the training corpus by inserting reflection tokens into task outputs offline. Subsequently, we train the final generator model ( M) using the conventional L"}, {"rank": 4, "score": 0.557116, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 32, "text_snippet": ", respectively. Algorithm 1 SELF-RAGInference Require: Generator LM M, Retriever R, Large-scale passage collections {d1, . . . , d N} 1:Input: input prompt xand preceding generation y<t,Output: next output segment yt 2:Mpredicts Retrieve gi"}, {"rank": 5, "score": 0.5523255, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 27, "text_snippet": "n (reasoning path consistency) and without retrieval. Recent work on LLM refinement (Dhuliawala et al., 2023; Madaan et al., 2023; Paul et al., 2023) prompts a model to generate task output, natural language feedback and refined task output"}]}
{"case_index": 70, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"wledge raises [MASK] concerns about the model’s behavior and performance in scenarios where retrieval may fail or return inaccu- rate results (Shi et al., 2023).\"", "gold": "significant", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.466, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.7253527, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 8, "text_snippet": "wledge raises significant concerns about the model’s behavior and performance in scenarios where retrieval may fail or return inaccu- rate results (Shi et al., 2023). As Figure 1 shows that a low-quality retriever is prone to introducingarX"}, {"rank": 2, "score": 0.61850935, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 6, "text_snippet": " The examples show that a low-quality retriever is prone to introducing a substantial amount of irrelevant information, impeding the generators from acquiring accurate knowledge and potentially misleading them. the parametric knowledge they"}, {"rank": 3, "score": 0.57766944, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 1, "text_snippet": "evitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Al- though retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heav"}, {"rank": 4, "score": 0.5733581, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 3, "text_snippet": "itly in their parameters. However, they are also prone to hallucination and cannot represent the full long tail of knowledge from the training corpus. Retrieval-augmented language models (Khandelwal et al., 2020; Borgeaud et al., 2022; Izac"}, {"rank": 5, "score": 0.56699795, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 18, "text_snippet": "pact the LLMs and exacerbate the problems. Thus, it is obvious that the lack of accurate and specific knowledge can lead to misleading or even inaccurate generation, which will severely hurt the experience of users in most practical applica"}]}
{"case_index": 71, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"In contrast, we explore a setting where both parametric and [MASK] memory components are pre-trained and pre-loaded with extensive knowledge.\"", "gold": "non-parametric", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.67, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.73555905, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 12, "text_snippet": "ck- augmented networks [ 25] and memory layers [ 30]. In contrast, we explore a setting where both parametric and non-parametric memory components are pre-trained and pre-loaded with extensive knowledge. Crucially, by using pre-trained acce"}, {"rank": 2, "score": 0.62483084, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 1, "text_snippet": "ameters, and achieve state-of-the-art results when ﬁne-tuned on down- stream NLP tasks. However, their ability to access and precisely manipulate knowl- edge is still limited, and hence on knowledge-intensive tasks, their performance lags b"}, {"rank": 3, "score": 0.5996485, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 6, "text_snippet": "eval-augmented architecture. These models employ a non-parametric memory, e.g. a neural retriever over a large, external, potentially non-static knowledge source to enhance a parametric language model. In addition to their memorisation abil"}, {"rank": 4, "score": 0.5853052, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 3, "text_snippet": "itly in their parameters. However, they are also prone to hallucination and cannot represent the full long tail of knowledge from the training corpus. Retrieval-augmented language models (Khandelwal et al., 2020; Borgeaud et al., 2022; Izac"}, {"rank": 5, "score": 0.58506286, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 14, "text_snippet": "active approaches. For knowledge-intensive generation, we experiment with MS-MARCO [ 1] and Jeopardy question generation, and we ﬁnd that our models generate responses that are more factual, speciﬁc, and diverse than a BART baseline. For FE"}]}
{"case_index": 72, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"However, it requires models containing billions of parameters, since all the [MASK] needs to be stored in the weights.\"", "gold": "information", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 1.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.3333333333333333, "hit_rank": 3, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.271, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.5849103, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 151, "text_snippet": "h 11 Billion trainable parameters. The T5 model with the closest number of parameters to our models is T5-large (770M parameters), which achieves a score of 28.9 EM on Natural Questions [ 52], substantially below the 44.5 that RAG-Sequence "}, {"rank": 2, "score": 0.56759584, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 4, "text_snippet": " tasks (Roberts et al., 2020). Nevertheless, even a large model does not contain sufficient domain- specific knowledge for particular tasks and the world continues to change, invalidating facts in the LLM. Updating the knowledge of these mo"}, {"rank": 3, "score": 0.5675369, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 3, "text_snippet": "s et al. (2020) introduced a generative model for open domain question answering. Without relying on external knowledge, this method obtained compet- itive results on several benchmarks. However, it requires models containing billions of pa"}, {"rank": 4, "score": 0.5635454, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 0, "text_snippet": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering Gautier Izacard1,2,3Edouard Grave1 1Facebook AI Research, Paris 2ENS, PSL University, Paris 3Inria, Paris gizacard|egrave@fb.com Abstract Generative mode"}, {"rank": 5, "score": 0.55743104, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 150, "text_snippet": "in the datasets used. *A hidden subset of this data is used for evaluation Task Train Development Test Natural Questions 79169 8758 3611 TriviaQA 78786 8838 11314 WebQuestions 3418 362 2033 CuratedTrec 635 134 635 Jeopardy Question Generati"}]}
{"case_index": 73, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"When performing a task, from question answering to generating Wikipedia articles, our model starts by [MASK] the top-k relevant documents from a large corpus of\"", "gold": "retrieving", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 9.3, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.65360343, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 17, "text_snippet": "chmark in Figure 2. As many natural language processing tasks require knowledge , our goal is to enhance standard text-to-text models with retrieval, which, as we hypothesise in the introduction, may be crucial to endow models with few-shot"}, {"rank": 2, "score": 0.6152345, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 19, "text_snippet": " documents. It usually provides an extra knowledge source from a specific corpus, i.e., Wikipedia, which greatly improves the per- formance of LMs in a variety of tasks, especially in the knowledge-intensive ones. The proposed  methods gene"}, {"rank": 3, "score": 0.61426306, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 164, "text_snippet": "intensive tasks for open-ended generation (e.g., instruction following). Recent or concurrent work studies instruction-tuning of retrieval systems (Asai et al., 2023b) or joint training of retrieval and LM components (Lin et al., 2023), whi"}, {"rank": 4, "score": 0.61379445, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 2, "text_snippet": "ings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including"}, {"rank": 5, "score": 0.6116626, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}]}
{"case_index": 74, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"o substantial improvements over a conventional RAG baseline for both the [MASK] and diversity of generated answers.\"", "gold": "comprehensiveness", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 33.779, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.6843293, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 96, "text_snippet": "tigate these downstream risks for questions of a global nature, which might otherwise be answered by samples of retrieved facts falsely presented as global summaries. 7 Conclusion We have presented GraphRAG, a RAG approach that combines kno"}, {"rank": 2, "score": 0.67862725, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 4, "text_snippet": "o substantial improvements over a conventional RAG baseline for both the comprehensiveness and diversity of generated answers. 1 Introduction Retrieval augmented generation (RAG) (Lewis et al., 2020) is an established approach to using LLMs"}, {"rank": 3, "score": 0.66960746, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 29, "text_snippet": "lements (nodes, edges, covariates) that the LLM can summarize in parallel at both indexing time and query time. The “global answer” to a given query is produced using a final round of query-focused summarization over all community summaries"}, {"rank": 4, "score": 0.63625586, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 22, "text_snippet": "gly global summaries by using the LLM to create summaries spanning this community hierarchy. 2.3 Adaptive benchmarking for RAG Evaluation Many benchmark datasets for open-domain question answering exist, including HotPotQA (Yang et al., 201"}, {"rank": 5, "score": 0.6314087, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 46, "text_snippet": "2 40.1 41.572.5 89.5RAG-Seq. 14.7 21.4 40.8 44.2 to more effective marginalization over documents. Furthermore, RAG can generate correct answers even when the correct answer is not in any retrieved document, achieving 11.8% accuracy in such"}]}
{"case_index": 75, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"[MASK] Large language models (LLMs) such as GPT-3 (Brown et al., 2020a) and Codex (Chen et al., 2021a), have demonstrated impressive performance on a wide range of language tasks.\"", "gold": "introduction", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.213, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.6864314, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 2, "text_snippet": "mproves the performance of GPT-3 (175B) on language modeling by 6.3%, as well as the perfor- mance of Codex on five-shot MMLU by 5.1%. 1. Introduction Large language models (LLMs) such as GPT-3 (Brown et al., 2020a) and Codex (Chen et al., "}, {"rank": 2, "score": 0.6735248, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 12, "text_snippet": "gnals, resulting in improved retrieval quality. •Evaluations on language modeling (§6), open-domain QA and MMLU demonstrate that REPLUG can im- prove the performance of various language models such as GPT, OPT and BLOOM, including very larg"}, {"rank": 3, "score": 0.6330355, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 62, "text_snippet": "arge Language models. Providing language models with natural language descriptions of tasks, as proposed by Radford et al. (2019) has led to signiﬁcant developments in few-shot learning. GPT-3 (Brown et al., 2020) demonstrated the ability o"}, {"rank": 4, "score": 0.6308154, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 45, "text_snippet": "t that covers exam questions from 57 tasks includ- ing mathematics, computer science, law, US history and etc. The 57 tasks are grouped into 4 categories: humani- ties, STEM, social sciences and other. Following Chung  REPLUG: Retrieval-Aug"}, {"rank": 5, "score": 0.6287464, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 10, "text_snippet": "l., 2017). For instance, REPLUG can improve Codex (175B) performance on MMLU by 4.5%, achieving compa- rable results to the 540B, instruction-finetuned Flan-PaLM. Furthermore, tuning the retriever with our training scheme (i.e., REPLUG LSR "}]}
{"case_index": 76, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"Reading extracted snippets from technical or [MASK] documents may lack important context making them difficult to read or even misleading.\"", "gold": "scientific", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.832, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.5537129, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 18, "text_snippet": " ICLR 2024 Despite a diversity in methods, the retrieving components of models predominantly rely on stan- dard approaches, i.e., chunking corpora and encoding with BERT-based retrievers. Although this approach is widely adopted, Nair et al"}, {"rank": 2, "score": 0.5305316, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 19, "text_snippet": "ortant context making them difficult to read or even misleading. (Cohan & Goharian, 2017; Newman et al., 2023; Zhang et al., 2023). Recursive summarization as Context Summarization techniques provide a condensed view of documents, enabling "}, {"rank": 3, "score": 0.5161623, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 9, "text_snippet": "nations (Zhang et al., 2023b). However, most conventional RAG ap- proaches indiscriminately incorporate the retrieved documents, regardless of whether these documents are relevant or not (Rony et al., 2022). Furthermore, current methods mos"}, {"rank": 4, "score": 0.51524097, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 5, "score": 0.5145518, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 8, "text_snippet": " indexing and retrieval system that uses a tree structure to capture both high-level and low-level details about a text. As shown in Figure 1, our system, RAPTOR, clusters chunks of text, generates text summaries of those clusters, and then"}]}
{"case_index": 77, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"1 [MASK] Large language models (LLMs) are impressive few-shot learners (Brown et al., 2020; Rae et al., 2021; Hoﬀmann et al., 2022; Chowdhery et al., 2022).\"", "gold": "introduction", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 12.788, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.74070793, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 3, "text_snippet": "xamples, outperforming a 540B parameters model by 3% despite having 50x fewer parameters. 1 Introduction Large language models (LLMs) are impressive few-shot learners (Brown et al., 2020; Rae et al., 2021; Hoﬀmann et al., 2022; Chowdhery et"}, {"rank": 2, "score": 0.69788265, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 61, "text_snippet": " added to the prompt of a large language model performing in-context learning. 3.2 Few-shot learning Few-shot learning, the task of learning from very few examples, has been studied for decades (Thrun & Pratt, 1998; Fink, 2005; Vinyals et a"}, {"rank": 3, "score": 0.68298936, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 62, "text_snippet": "arge Language models. Providing language models with natural language descriptions of tasks, as proposed by Radford et al. (2019) has led to signiﬁcant developments in few-shot learning. GPT-3 (Brown et al., 2020) demonstrated the ability o"}, {"rank": 4, "score": 0.67162704, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 63, "text_snippet": "s learning ability, leading to the further development of large models (Lieber et al., 2021; Rae et al., 2021; Smith et al., 2022; Chowdhery et al., 2022; Smith et al., 2022). Hoﬀmann et al. (2022) revisited the scaling law from Kaplan et a"}, {"rank": 5, "score": 0.6710799, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 0, "text_snippet": "Atlas: Few-shot Learning with Retrieval Augmented Language Models Gautier Izacard∗ ∗,♦,♣,♥gizacard@fb.com Patrick Lewis∗,♦plewis@fb.com Maria Lomeli♦marialomeli@fb.com Lucas Hosseini♦hoss@fb.com Fabio Petroni♦fabiopetroni@fb.com Timo Schick"}]}
{"case_index": 78, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"As shown in the following section, an advantage of dense retrievers is that both query and document encoders can be trained without document annotation, using standard techniques such as gradient descent and [MASK].\"", "gold": "distillation", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 12.386, "llm_ms": 0.008, "top_contexts": [{"rank": 1, "score": 0.65970707, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 20, "text_snippet": "eir corresponding embeddings. The Contriever model is pre-trained using the MoCo contrastive loss (He et al., 2020), and uses unsupervised data only. As shown in the following section, an advantage of dense retrievers is that both query and"}, {"rank": 2, "score": 0.61025494, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 54, "text_snippet": " hallucination in dialogue agents (Shuster et al., 2021). Retriever training. The need for expensive query-document annotations for training the retriever can be bypassed, by leveraging signals from the language model, or using unsupervised"}, {"rank": 3, "score": 0.60399634, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 23, "text_snippet": "ent loss functions to train the retriever jointly with the language model. We consider loss functions that leverage the language model to provide supervisory signal to train the retriever. In other words, if the language model ﬁnds a docume"}, {"rank": 4, "score": 0.599517, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 22, "text_snippet": "ther way to process the retrieved documents in the language model would be to concatenate the query and all the documents, and to use this long sequence as input of the model. Unfortunately, this approach does not scale with the number of d"}, {"rank": 5, "score": 0.59822315, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 40, "text_snippet": "ent retriever ﬁne-tuning Retrieval is facilitated by using a document index, which is a pre-computed collection of the document embeddings for all the documents in the retrieval corpus. When jointly training the retriever and language model"}]}
{"case_index": 79, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"In canonical RAG approaches, the retrieval process returns a set number of records that are seman- tically similar to the query and the generated answer uses only the [MASK] in those retrieved records.\"", "gold": "information", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 1.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.311, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.65047807, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 15, "text_snippet": "e LLM’s context window. In canonical RAG approaches, the retrieval process returns a set number of records that are seman- tically similar to the query and the generated answer uses only the information in those retrieved records. A common "}, {"rank": 2, "score": 0.6283444, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 29, "text_snippet": "lements (nodes, edges, covariates) that the LLM can summarize in parallel at both indexing time and query time. The “global answer” to a given query is produced using a final round of query-focused summarization over all community summaries"}, {"rank": 3, "score": 0.5711653, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 0, "text_snippet": "From Local to Global: A GraphRAG Approach to Query-Focused Summarization Darren Edge1†Ha Trinh1†Newman Cheng2Joshua Bradley2Alex Chao3 Apurva Mody3Steven Truitt2Dasha Metropolitansky1Robert Osazuwa Ness1 Jonathan Larson1 1Microsoft Research"}, {"rank": 4, "score": 0.5701768, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 17, "text_snippet": " are later used to answer queries as in Mao et al. 2020). These summaries are generated in parallel and iteratively aggregated into global summaries, similar to prior techniques (Feng et al., 2023; Gao et al., 2023; Khattab et al., 2022; Sh"}, {"rank": 5, "score": 0.5645333, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 14, "text_snippet": "es, whereupon this information is incorporated into the generation of a response to the query by an LLM (or other generative AI model, such as a multi-media model). The query and retrieved records populate a prompt template, which is then p"}]}
{"case_index": 80, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"We introduce a method to train an arbitrary LM to learn to use retrieval on-demand for diverse [MASK] queries and introduce controlled generation guided by reflections tokens to further improve generation quality and attributions.\"", "gold": "instruction-following", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.25, "hit_rank": 4, "matched_by": "doc_id+chunk_id", "retrieval_ms": 12.178, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.73016524, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 6, "text_snippet": "y trained to leverage and follow facts from provided passages. This work introduces Self-Reflective Retrieval-augmented Generation ( SELF-RAG)to improve an LLM’s generation quality, including its factual accuracy without hurting its versati"}, {"rank": 2, "score": 0.72326076, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 2, "text_snippet": "ew framework called Self-Reflective Retrieval-Augmented Gen- eration ( SELF-RAG)that enhances an LM’s quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passage"}, {"rank": 3, "score": 0.6994902, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 96, "text_snippet": "ains an LM to learn to retrieve, generate, and critique text passages and its own generation by predicting the next tokens from its original vocabulary as well as newly added special tokens, called reflection tokens. SELF-RAGfurther enables"}, {"rank": 4, "score": 0.69840634, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 21, "text_snippet": " al., 2023a; Gao et al., 2023). We introduce a method to train an arbitrary LM to learn to use retrieval on-demand for diverse instruction-following queries and introduce controlled generation guided by reflections tokens to further improve"}, {"rank": 5, "score": 0.696805, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 41, "text_snippet": "lding are unforgettable experience.No RetrievalNo Retrieval Retriever Figure 2: SELF-RAGtraining examples. The left example does not require retrieval while the right one requires retrieval; thus, passages are inserted. More examples are in"}]}
{"case_index": 81, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"When applied to QA datasets, our best model compresses the documents to 5 - 10% of the original tokens with at most less than 10% relative [MASK] drop.\"", "gold": "performance", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.067, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6939608, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 15, "text_snippet": "6% and significantly outperforms prepending full documents. Our trained compressors also show promising results. For language modelling, both trained compressors achieve a compression ratio of 25% with minimal performance drop. When applied"}, {"rank": 2, "score": 0.6434175, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 57, "text_snippet": "ewer tokens. Comparing to prepending one document, we achieve a compression ratio of 25% at minimum performance drop. Our trained abstractive compressor performs the best across the board, achieving the lowest perplexity and the highest com"}, {"rank": 3, "score": 0.637068, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 14, "text_snippet": " significantly fewer tokens compared to RALM without compression. We present two oracle compression methods – an extractive oracle which selects a sentence in evidence documents that leads to the best task performance and an abstractive ora"}, {"rank": 4, "score": 0.62379944, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 56, "text_snippet": "7 36.57 44.22 38 58.99 65.26 75 30.40 40.14 Abstractive compression of top 5 documents Oracle 51 45.68 53.66 37 71.01 76.38 102 35.80 46.25 GPT-3.5 56 37.12 46.35 41 62.03 69.66 107 31.60 42.65 T5 10 25.90 34.63 7 55.18 62.34 7 23.20 33.19 "}, {"rank": 5, "score": 0.62221974, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 3, "text_snippet": "n. We evaluate our approach on language modeling task and open domain question answering task. We achieve a compression rate of as low as 6% with minimal loss in performance for both tasks, significantly outperforming the off-the-shelf summ"}]}
{"case_index": 82, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"Eventually, when it cannot [MASK] make a correct or incorrect judgment, a soft and balanced action Ambiguous which combines both of them is triggered.\"", "gold": "confidently", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 14.081, "llm_ms": 0.008, "top_contexts": [{"rank": 1, "score": 0.53247184, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 28, "text_snippet": "sition, filter, and recomposition (Section 4.4). If the action Incorrect is triggered, the retrieved documents will be discarded. Instead, web searches are resorted to and regarded as complementary knowledge sources for corrections (Section"}, {"rank": 2, "score": 0.5300607, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 42, "text_snippet": " fabricated facts. Therefore, we need to seek new sources of knowledge for correction. Here, web search is introduced to search from the Internet as elaborated in Section 4.5. This corrective action helps overcome the embarrassing challenge"}, {"rank": 3, "score": 0.50973773, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 43, "text_snippet": "of the retrieval is hard to distinguish and the evaluator gives an intermediate score. Since the retrieval evaluator is not confident in its judgment, both types of processed knowledge in Correct andIncorrect are combined to comple- ment ea"}, {"rank": 4, "score": 0.49590886, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 68, "text_snippet": "hange in terms of accuracy. Specifically, when the action Correct orIncorrect was removed, it was merged with Ambiguous so that the proportion that originally triggered Correct orIncorrect would trigger Ambiguous . On the other hand, when t"}, {"rank": 5, "score": 0.48807514, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 1, "text_snippet": "evitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Al- though retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heav"}]}
{"case_index": 83, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"Some [MASK] use subgraphs, elements of the graph, or properties of the graph structure dire\"", "gold": "techniques", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 9.57, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.6044626, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 20, "text_snippet": "elements of the graph, or properties of the graph structure directly in the prompt (Baek et al., 2023; He et al., 2024; Zhang, 2023) or as factual grounding for generated outputs (Kang et al., 2023; Ranade and Joshi, 2023). Other techniques"}, {"rank": 2, "score": 0.596163, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 27, "text_snippet": "ns (Wang et al., 2023a; Zheng et al., 2024). Some prior work proposes criteria for having LLMs quantify the quality of 3  Source Documents Text Chunkstext extraction and chunking Entities & Relationshipsdomain-tailored summarization Knowled"}, {"rank": 3, "score": 0.58102673, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 17, "text_snippet": " are later used to answer queries as in Mao et al. 2020). These summaries are generated in parallel and iteratively aggregated into global summaries, similar to prior techniques (Feng et al., 2023; Gao et al., 2023; Khattab et al., 2022; Sh"}, {"rank": 4, "score": 0.57655114, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 29, "text_snippet": "lements (nodes, edges, covariates) that the LLM can summarize in parallel at both indexing time and query time. The “global answer” to a given query is produced using a final round of query-focused summarization over all community summaries"}, {"rank": 5, "score": 0.5626948, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 44, "text_snippet": "ummaries at one level looking for general themes of interest, then read linked reports at a lower level that provide additional details for each subtopic. Here, however, we focus on their utility as part of a graph-based index used for answ"}]}
{"case_index": 84, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"In the following, we formally introduce both models and then describe the pηandpθ[MASK], as well as the training and\"", "gold": "components", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 9.15, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.556449, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 2, "score": 0.54108894, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 17, "text_snippet": "rator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a distribution over generated text. In one approach, RAG-Sequence , the mo"}, {"rank": 3, "score": 0.52607197, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 37, "text_snippet": " To cast language modeling in the text-to-text framework, we consider a chunk of Nwords, and split this chunk in two sub-sequences of equal length N/2. Then, the ﬁrst sub-sequence is used as the query, and the second corresponds to the outp"}, {"rank": 4, "score": 0.5225102, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 24, "text_snippet": "tasks and outperforms comparably-sized T5 models [32]. We refer to the BART generator parameters θas the parametric memory henceforth. 2.4 Training We jointly train the retriever and generator components without any direct supervision on wh"}, {"rank": 5, "score": 0.51973826, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 18, "text_snippet": "hen describe the pηandpθcomponents, as well as the training and decoding procedure. 2.1 Models RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate the complete sequence . Technically, it treats the retriev"}]}
{"case_index": 85, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"The training [MASK] is to minimize −logesim (xi,pi) esim (xi,pi)+P nj∈Niesim (xi,nj).\"", "gold": "objective", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 7.83, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.5410321, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 29, "text_snippet": "g pM(y|[sj;xi]), log likelihood assigned to target output according to LM Mwhen candidate sentence is prepended to the input. We consider the sentence with the highest log likelihood as a positive example pi(line 3). To construct negative e"}, {"rank": 2, "score": 0.5349271, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 32, "text_snippet": "ler loss function which is loosely inspired by the objectives from the attention distillation and EMDR2methods (Izacard & Grave, 2021; Sachan et al., 2021). More precisely, we want to train the retriever to predict how much each document wo"}, {"rank": 3, "score": 0.50772053, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 31, "text_snippet": "er are updated by applying a StopGradient operator 4  aroundplm. One should note that the probability distribution over documents that maximizes this loss function is an indicator of the document corresponding to the highest probability of "}, {"rank": 4, "score": 0.50654995, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 29, "text_snippet": "om the retriever deﬁned in Equation 1: KL(pattn∥pretr) =K∑ k=1pattn(dk) log(pattn(dk) pretr(dk)) . Here, this loss is only used to optimize the parameters of the retriever, and not the language model. When using recent deep learning framewo"}, {"rank": 5, "score": 0.50345397, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 27, "text_snippet": "g the inner product of the two. We initialize our model with the contriever checkpoint (Izacard et al., 2021). This model consists of 110M pa- rameters, satisfying the efficiency desider- atum of compressor. Training Figure 2 presents pseud"}]}
{"case_index": 86, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"rsively clusters chunks of text based on their vector embeddings and generates text summaries of those clusters, [MASK] a tree from the bottom up.\"", "gold": "constructing", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.897, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.74597406, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 10, "text_snippet": "rsively clusters chunks of text based on their vector embeddings and generates text summaries of those clusters, constructing a tree from the bottom up. Nodes clustered together are siblings; a parent node contains the text summary of that "}, {"rank": 2, "score": 0.72396564, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 9, "text_snippet": "uestions at different levels. 1arXiv:2401.18059v1 [cs.CL] 31 Jan 2024  Published as a conference paper at ICLR 2024 2 3 4 5  1 1  2  3  3  4 5  5 6  8  7 Index #8  Text: summary of  nodes 2 and 3  Child Nodes: 2, 3  Text Embedding  Text chu"}, {"rank": 3, "score": 0.6965054, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 1, "text_snippet": "rom a retrieval corpus, limiting holistic under- standing of the overall document context. We introduce the novel approach of recursively embedding, clustering, and summarizing chunks of text, constructing a tree with differing levels of su"}, {"rank": 4, "score": 0.67891467, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 24, "text_snippet": "r text chunks, we employ a clustering algorithm. Once clustered, a Language Model is used to summarize the grouped texts. These summarized texts are then re-embedded, and the cycle of embedding, clustering, and summarization continues until"}, {"rank": 5, "score": 0.67250764, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 8, "text_snippet": " indexing and retrieval system that uses a tree structure to capture both high-level and low-level details about a text. As shown in Figure 1, our system, RAPTOR, clusters chunks of text, generates text summaries of those clusters, and then"}]}
{"case_index": 87, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"However, they are also prone to [MASK] and cannot represent the full long tail of knowledge from the training corpus.\"", "gold": "hallucination", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.349, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.61591375, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 3, "text_snippet": "itly in their parameters. However, they are also prone to hallucination and cannot represent the full long tail of knowledge from the training corpus. Retrieval-augmented language models (Khandelwal et al., 2020; Borgeaud et al., 2022; Izac"}, {"rank": 2, "score": 0.5539023, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 4, "text_snippet": " tasks (Roberts et al., 2020). Nevertheless, even a large model does not contain sufficient domain- specific knowledge for particular tasks and the world continues to change, invalidating facts in the LLM. Updating the knowledge of these mo"}, {"rank": 3, "score": 0.5515243, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 12, "text_snippet": "gnals, resulting in improved retrieval quality. •Evaluations on language modeling (§6), open-domain QA and MMLU demonstrate that REPLUG can im- prove the performance of various language models such as GPT, OPT and BLOOM, including very larg"}, {"rank": 4, "score": 0.54819363, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 15, "text_snippet": "anguage models makes this approach infeasible. To ad- dress the challenges posed by large language models, we investigate retrieval-augmentation in the black-box setting , where users only have access to the model predictions and cannot acc"}, {"rank": 5, "score": 0.5380659, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}]}
{"case_index": 88, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"This structure enables RAPTOR to load into an LLM’s context chunks [MASK] the text at different levels so that it can effectively and efficiently answer questions at different levels.\"", "gold": "representing", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 11.24, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.6942851, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 8, "text_snippet": " indexing and retrieval system that uses a tree structure to capture both high-level and low-level details about a text. As shown in Figure 1, our system, RAPTOR, clusters chunks of text, generates text summaries of those clusters, and then"}, {"rank": 2, "score": 0.6489812, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 9, "text_snippet": "uestions at different levels. 1arXiv:2401.18059v1 [cs.CL] 31 Jan 2024  Published as a conference paper at ICLR 2024 2 3 4 5  1 1  2  3  3  4 5  5 6  8  7 Index #8  Text: summary of  nodes 2 and 3  Child Nodes: 2, 3  Text Embedding  Text chu"}, {"rank": 3, "score": 0.62878084, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 22, "text_snippet": " RAPTOR addresses the issue of semantic depth and connection in reading by building a recursive tree structure that balances broader thematic comprehension with granular details and which allows nodes to be grouped based on semantic sim- il"}, {"rank": 4, "score": 0.60847193, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 1, "text_snippet": "rom a retrieval corpus, limiting holistic under- standing of the overall document context. We introduce the novel approach of recursively embedding, clustering, and summarizing chunks of text, constructing a tree with differing levels of su"}, {"rank": 5, "score": 0.60552585, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 72, "text_snippet": "e from the leaf nodes and each upper layer, as well as from different contiguous subsets of the layers. We show findings specific to one story in Table 8, revealing that a full-tree search, utilizing all layers, outperformed retrieval strat"}]}
{"case_index": 89, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"[MASK] methods were pro- posed to tackle the setting where no gold spans are given to the system, but only the correct answer.\"", "gold": "different", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 9.134, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.6480025, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 11, "text_snippet": " Different methods were pro- posed to tackle the setting where no gold spans are given to the system, but only the correct answer. Clark and Gardner (2018) proposed to use a global normalization over all the span corresponding to the answer"}, {"rank": 2, "score": 0.6360772, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 6, "text_snippet": "tech- niques have been considered, either using sparse representations based on TF/IDF or using dense embeddings (Guu et al., 2020; Karpukhin et al., 2020). The models which extract the answers are often based on contextualized word represe"}, {"rank": 3, "score": 0.63012505, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}, {"rank": 4, "score": 0.595838, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 14, "text_snippet": " be trained using weak supervision in the form of question-answer pairs (Karpukhin et al., 2020), or pretrained using a cloze task and ﬁnetuned end-to- end (Guu et al., 2020; Lee et al., 2019). Generative question answering was mostly consi"}, {"rank": 5, "score": 0.59309334, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 10, "text_snippet": " problem in natural lan- guage processing (V oorhees et al., 1999), this task has recently regained interest following the work by Chen et al. (2017). In that version of the prob- lem, strong supervision is available to the learning system,"}]}
{"case_index": 90, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"Here, we bring hybrid parametric and [MASK] memory to the “workhorse of NLP,” i.e.\"", "gold": "non-parametric", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 12.11, "llm_ms": 0.008, "top_contexts": [{"rank": 1, "score": 0.66417134, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 12, "text_snippet": "ck- augmented networks [ 25] and memory layers [ 30]. In contrast, we explore a setting where both parametric and non-parametric memory components are pre-trained and pre-loaded with extensive knowledge. Crucially, by using pre-trained acce"}, {"rank": 2, "score": 0.6141312, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 6, "text_snippet": "eval-augmented architecture. These models employ a non-parametric memory, e.g. a neural retriever over a large, external, potentially non-static knowledge source to enhance a parametric language model. In addition to their memorisation abil"}, {"rank": 3, "score": 0.61277807, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 1, "text_snippet": "ameters, and achieve state-of-the-art results when ﬁne-tuned on down- stream NLP tasks. However, their ability to access and precisely manipulate knowl- edge is still limited, and hence on knowledge-intensive tasks, their performance lags b"}, {"rank": 4, "score": 0.60459036, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 9, "text_snippet": "y to the “workhorse of NLP,” i.e. sequence-to-sequence (seq2seq) models. We endow pre-trained, parametric-memory generation models with a non-parametric memory through a general-purpose ﬁne-tuning approach which we refer to as retrieval-aug"}, {"rank": 5, "score": 0.59266317, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 14, "text_snippet": "active approaches. For knowledge-intensive generation, we experiment with MS-MARCO [ 1] and Jeopardy question generation, and we ﬁnd that our models generate responses that are more factual, speciﬁc, and diverse than a BART baseline. For FE"}]}
{"case_index": 91, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"1arXiv:2310.04408v1 [cs.CL] 6 Oct 2023 RECOMP during inference moved from Smyrna, [MASK], to Nissan's facility in Canton, M\"", "gold": "tennessee", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.503, "llm_ms": 0.008, "top_contexts": [{"rank": 1, "score": 0.662029, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 7, "text_snippet": "rieved evidence documents, and guide RALM to generate desired outputs when prepended to the input. To satisfy both efficiency and effectiveness constraints, our compressor strategically performs selective augmentation by generating an empty"}, {"rank": 2, "score": 0.51981276, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 9, "text_snippet": "acility in Canton, Mississippi. Early US models include X, S and PRO-4X, with a choice of 6-speed manual or 5-speed automatic transmissions, a choice of [...] moved from Smyrna, Tennessee, to Nissan's facility in Canton, Mississippi. Early "}, {"rank": 3, "score": 0.48491728, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 8, "text_snippet": " moved from Smyrna, Tennessee, to Nissan's facility in Canton, Mississippi. Early US models include X, S and PRO-4X, with a choice of 6-speed manual or 5-speed automatic transmissions, a choice of [...] moved from Smyrna, Tennessee, to Niss"}, {"rank": 4, "score": 0.46110028, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 0, "text_snippet": "RECOMP: I MPROVING RETRIEVAL -AUGMENTED LM S WITH COMPRESSION AND SELECTIVE AUGMENTATION Fangyuan Xu1, Weijia Shi2, Eunsol Choi1 Department of Computer Science 1The University of Texas at Austin 2University of Washington {fangyuan,eunsol }@"}, {"rank": 5, "score": 0.45495594, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 118, "text_snippet": "//openreview.net/ forum?id=22OTbutug9. Barys Liskavets, Maxim Ushakov, Shuvendu Roy, Mark Klibanov, Ali Etemad, and Shane Luke. Prompt compression with context-aware sentence encoding for fast and improved llm inference.arXiv preprint arXiv"}]}
{"case_index": 92, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"1 [MASK] Recently, several works have shown that factual information can be extracted from large scale language models trained on vast quantities of data (Radford et al., 2019; Petroni et al., 2019; Jiang et al., 2019; Talmor et al., 2019).\"", "gold": "introduction", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 9.491, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.71797824, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 2, "text_snippet": "s a ﬂexible framework to efﬁciently aggregate and com- bine evidence from multiple passages. 1 Introduction Recently, several works have shown that factual information can be extracted from large scale language models trained on vast quanti"}, {"rank": 2, "score": 0.6644287, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 4, "text_snippet": " tasks (Roberts et al., 2020). Nevertheless, even a large model does not contain sufficient domain- specific knowledge for particular tasks and the world continues to change, invalidating facts in the LLM. Updating the knowledge of these mo"}, {"rank": 3, "score": 0.6639998, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}, {"rank": 4, "score": 0.6619504, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 0, "text_snippet": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering Gautier Izacard1,2,3Edouard Grave1 1Facebook AI Research, Paris 2ENS, PSL University, Paris 3Inria, Paris gizacard|egrave@fb.com Abstract Generative mode"}, {"rank": 5, "score": 0.6540817, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 15, "text_snippet": " that answers do not correspond to spans in support documents, thus requiring ab- stractive models. Raffel et al. (2019) showed that generative models are competitive for reading com- prehension tasks such as SQuAD (Rajpurkar et al., 2016),"}]}
{"case_index": 93, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"2 Extractive Compressor Given [MASK] [s1,s2...sn]in the input document set ( [d1, d2, ...dN]), we train a dual encoder model encθwhich embeds sentence siand the input sequence xint\"", "gold": "nsentences", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.435, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.7625357, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 19, "text_snippet": "ining compressors for conciseness and effectiveness. We summarize the key ideas for our two compressors, extractive compressors and abstractive compressor here, and discuss their training schemes formally in Section 3. 2Improving retriever "}, {"rank": 2, "score": 0.6724819, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 21, "text_snippet": "Compressor We train an encoder-decoder model encdec θto serve as an abstractive compressor, which takes the input sequence xand a concatenation of retrieved document set D [d1;d2;...dN]) and output a summary s. Although we do not have human"}, {"rank": 3, "score": 0.6694113, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 24, "text_snippet": "TRACTIVE COMPRESSION As we formulate extractive compression as a ranking problem, training extractive compressor re- sembles training a reranker for the retrieved documents4with two differences. First, our compressor considers a different g"}, {"rank": 4, "score": 0.6639886, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 27, "text_snippet": "g the inner product of the two. We initialize our model with the contriever checkpoint (Izacard et al., 2021). This model consists of 110M pa- rameters, satisfying the efficiency desider- atum of compressor. Training Figure 2 presents pseud"}, {"rank": 5, "score": 0.6594894, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 26, "text_snippet": " 7: L ← L ∪ si 8: if|L|>0then 9: Ni←argTop5 sj∈L(⟨encθ(sj),encθ(xi)⟩) 10: T ← T ∪ { (xi,pi,Ni)} 11:encθ=Finetune (encθ,T) Figure 2: Learning an extractive compressor for lan- guage modeling task.Model We train a dual-encoder model encθwhich"}]}
{"case_index": 94, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"How- ever, the increasing scale and black-box nature of large language models makes this approach [MASK].\"", "gold": "infeasible", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.2, "hit_rank": 5, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.047, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.63335216, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 5, "text_snippet": "ting the LM’s parameters, REPLUG treats the language model as a black box and augments it with a frozen or tunable retriever. This black-box assumption makes REPLUG applicable to large LMs (i.e., >100B parameters), which are often served vi"}, {"rank": 2, "score": 0.6237445, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 12, "text_snippet": "gnals, resulting in improved retrieval quality. •Evaluations on language modeling (§6), open-domain QA and MMLU demonstrate that REPLUG can im- prove the performance of various language models such as GPT, OPT and BLOOM, including very larg"}, {"rank": 3, "score": 0.6130328, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 15, "text_snippet": "anguage models makes this approach infeasible. To ad- dress the challenges posed by large language models, we investigate retrieval-augmentation in the black-box setting , where users only have access to the model predictions and cannot acc"}, {"rank": 4, "score": 0.61122954, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 71, "text_snippet": "a black box and augments it with a tuneable retrieval model. Our evaluation shows that REPLUG can be integrated with any existing language model to improve their performance  REPLUG: Retrieval-Augmented Black-Box Language Models on language"}, {"rank": 5, "score": 0.6010083, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 14, "text_snippet": "5k each (Younes Belkda, 2022)), making them inac- cessible to researchers and developers with limited re- sources. Traditionally, retrieval-augmented model frame- works (Khandelwal et al., 2020; Borgeaud et al., 2022; Yu, 2022; Izacard et a"}]}
{"case_index": 95, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"Ideally, the retrieval likelihood is computed by [MASK] over all the documents in the corpus D, which is intractable in practice.\"", "gold": "marginalizing", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 13.605, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.6236123, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 31, "text_snippet": "′⊂ D with the highest simi- larity scores from a corpus Dgiven an input context x, as described in §3.1. We then compute the retrieval likelihood of each retrieved document d: PR(d|x) =es(d,x)/γ P d∈D′es(d,x)/γ where γis a hyperparameter th"}, {"rank": 2, "score": 0.57099915, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 32, "text_snippet": "e retrieval likelihood by only marginalizing over the retrieved documents D′. 4.2. Computing LM likelihood We use the LM as a scoring function to measure how much each document could improve the LM perplexity. Specifi- cally, we first compu"}, {"rank": 3, "score": 0.5601316, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 7, "text_snippet": " retrieved documents are prepended to the input context and fed into the black-box LM to make the final prediction. Because the LM context length limits the number of documents that can be prepended, we also introduce a new ensemble scheme "}, {"rank": 4, "score": 0.5599939, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 29, "text_snippet": "triever inREPLUG by using the LM itself to provide supervision about which documents should be retrieved. Inspired by Sachan et al. (2022), our approach can be seen as adjusting the probabilities of the retrieved documents to match the prob"}, {"rank": 5, "score": 0.5548554, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 37, "text_snippet": "vision LM to com- pute the LM likelihood. Training data We use 800K sequences of 256 tokens each, sampled from the Pile training data (Gao et al., 2020), as our training queries. Each query is split into two parts: the first 128 tokens are "}]}
{"case_index": 96, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"2020) augment the input of LLMs with relevant retrieved passages, reducing factual errors in [MASK] tasks (Ram et al., 2023; Asai et al., 2023a).\"", "gold": "knowledge-intensive", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.25, "hit_rank": 4, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.263, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.6998416, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 23, "text_snippet": "ess retrieved passages before using them to prompt the LM to generate the output. SELF-RAGprocesses passages in parallel and filters out irrelevant ones through self-reflection, without relying on external models at inference. Moreover, our"}, {"rank": 2, "score": 0.69845986, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 5, "text_snippet": "ks (Ram et al., 2023; Asai et al., 2023a). However, these methods may hinder the versatility of LLMs or introduce unnecessary or off-topic passages that lead to low-quality generations (Shi et al., 2023) since they retrieve passages indiscr"}, {"rank": 3, "score": 0.6936739, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 6, "text_snippet": "y trained to leverage and follow facts from provided passages. This work introduces Self-Reflective Retrieval-augmented Generation ( SELF-RAG)to improve an LLM’s generation quality, including its factual accuracy without hurting its versati"}, {"rank": 4, "score": 0.6786662, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 4, "text_snippet": " for long-form generations relative to these models.1 1 I NTRODUCTION State-of-the-art LLMs continue to struggle with factual errors (Mallen et al., 2023; Min et al., 2023) despite their increased model and data scale (Ouyang et al., 2022)."}, {"rank": 5, "score": 0.6673331, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 19, "text_snippet": "rieved text passages (Guu et al., 2020; Lewis et al., 2020), leading to large improvements in knowledge-intensive tasks after fine-tuning or used with off-the-shelf LMs (Ram et al., 2023). A more recent work (Luo et al., 2023) instruction-t"}]}
{"case_index": 97, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"1 I NTRODUCTION Large Language Models (LLMs) have emerged as [MASK] tools showing impressive perfor- mance on m\"", "gold": "transformative", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.695, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6373493, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 3, "text_snippet": "ed as transformative tools showing impressive perfor- mance on many tasks. With the growing size of LLMs, they can serve standalone as very effective knowledge stores, with facts encoded within their parameters (Petroni et al., 2019; Jiang "}, {"rank": 2, "score": 0.6322383, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 2, "text_snippet": "hat retrieval with recursive summaries offers significant improvements over tra- ditional retrieval-augmented LMs on several tasks. On question-answering tasks that involve complex, multi-step reasoning, we show state-of-the-art results; fo"}, {"rank": 3, "score": 0.59129405, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 70, "text_snippet": "re is capable of achieving strong performance across several tasks. 8  General-Purpose Architectures for NLP Prior work on general-purpose architectures for NLP tasks has shown great success without the use of retrieval. A single, pre-train"}, {"rank": 4, "score": 0.573068, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 3, "text_snippet": "xamples, outperforming a 540B parameters model by 3% despite having 50x fewer parameters. 1 Introduction Large language models (LLMs) are impressive few-shot learners (Brown et al., 2020; Rae et al., 2021; Hoﬀmann et al., 2022; Chowdhery et"}, {"rank": 5, "score": 0.5729967, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 2, "text_snippet": "mproves the performance of GPT-3 (175B) on language modeling by 6.3%, as well as the perfor- mance of Codex on five-shot MMLU by 5.1%. 1. Introduction Large language models (LLMs) such as GPT-3 (Brown et al., 2020a) and Codex (Chen et al., "}]}
{"case_index": 98, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"Then, a [MASK] model generates the answer, taking as input the re- trieved passages in addition to the question.\"", "gold": "sequence-to-sequence", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 7.622, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.6552318, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 21, "text_snippet": "Raffel et al., 2019; Lewis et al., 2019). The model takes as input the question, as well as the support passages, and generates the answer. More precisely, each retrieved passage and its title are concatenated with the question, and process"}, {"rank": 2, "score": 0.6450869, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}, {"rank": 3, "score": 0.633174, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 5, "text_snippet": "t, it retrieves support text passages from an external source of knowledge such as Wikipedia. Then, a generative encoder-decoder model produces the answer, conditioned on the question and the re- trieved passages. This approach scales well "}, {"rank": 4, "score": 0.6310846, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 0, "text_snippet": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering Gautier Izacard1,2,3Edouard Grave1 1Facebook AI Research, Paris 2ENS, PSL University, Paris 3Inria, Paris gizacard|egrave@fb.com Abstract Generative mode"}, {"rank": 5, "score": 0.6288774, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 31, "text_snippet": "tion answering. We observe that while conceptu- ally simple, this method outperforms existing work on the NaturalQuestion and TriviaQA benchmarks. In particular, generative models seem to perform well when evidence from multiple passages ne"}]}
{"case_index": 99, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"As shown in Figure 2, given an input context, REPLUG first [MASK] a small set of relevant documents from an external corpus using a retriever (§3.1).\"", "gold": "retrieves", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.197, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.7235878, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 22, "text_snippet": " module. As shown in Figure 2, given an input context, REPLUG first retrieves a small set of relevant documents from an external corpus using a retriever (§3.1). Then we pass the concate- nation of each retrieved document with the input con"}, {"rank": 2, "score": 0.7173115, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 7, "text_snippet": " retrieved documents are prepended to the input context and fed into the black-box LM to make the final prediction. Because the LM context length limits the number of documents that can be prepended, we also introduce a new ensemble scheme "}, {"rank": 3, "score": 0.71398944, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 21, "text_snippet": " (§3.1 Document Retrieval ). Then it prepends each document separately to the input context and ensembles output probabilities from different passes (§3.2 Input Reformulation ). and a training scheme to further adapt the retriever to large "}, {"rank": 4, "score": 0.6859479, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 6, "text_snippet": "PIs. Internal representations of such models are not exposed and fine-tuning is not supported. In this work, we introduce REPLUG (Retrieve and Plug ), a new retrieval-augmented LM framework where the lan- guage model is viewed as a black bo"}, {"rank": 5, "score": 0.6849319, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 20, "text_snippet": "n improve GPT- 3 performance on open-domain question answering, we approach the problem in a more general setting, including language modeling and understanding tasks. We also pro- pose an ensemble method to incorporate more documents  REPL"}]}
{"case_index": 100, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"model parameters), for both reducing LM perplexity and and im- proving in-context learning [MASK].\"", "gold": "performance", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.844, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6768776, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 11, "text_snippet": " model parameters), for both reducing LM perplexity and and im- proving in-context learning performance. We summarize our contributions as follows: •We introduce REPLUG (§3), the first retrieval- augmented language modeling framework for en"}, {"rank": 2, "score": 0.6478941, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 9, "text_snippet": "22) that adapts language models to the retriever. We use a training objective which prefers retrieving documents that improve language model perplexity, while treating the LM as a frozen, black-box scoring function. Our experiments show tha"}, {"rank": 3, "score": 0.6160216, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 63, "text_snippet": " the performance of REPLUG andREPLUG LSR improved monotonically. How- ever, a small number of documents (e.g., 10) was sufficient to achieve large performance gains.  REPLUG: Retrieval-Augmented Black-Box Language Models Perplexity 14.0016."}, {"rank": 4, "score": 0.6118417, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 10, "text_snippet": "l., 2017). For instance, REPLUG can improve Codex (175B) performance on MMLU by 4.5%, achieving compa- rable results to the 540B, instruction-finetuned Flan-PaLM. Furthermore, tuning the retriever with our training scheme (i.e., REPLUG LSR "}, {"rank": 5, "score": 0.6019296, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 12, "text_snippet": "gnals, resulting in improved retrieval quality. •Evaluations on language modeling (§6), open-domain QA and MMLU demonstrate that REPLUG can im- prove the performance of various language models such as GPT, OPT and BLOOM, including very larg"}]}
{"case_index": 101, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"In this work we address this gap, and present Atlas, a [MASK] language model capable of strong few-shot learning, despite having lower parameter counts\"", "gold": "retrieval-augmented", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 9.99, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.7270612, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 1, "text_snippet": " Abstract Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter count"}, {"rank": 2, "score": 0.7099651, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 12, "text_snippet": "ntributions: •A thorough study on how to design and train retrieval-augmented language models, with a focus on downstream few-shot learning and sample eﬃciency. •The ﬁndings of this study lead to a retrieval-augmented language model, called"}, {"rank": 3, "score": 0.7012097, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 0, "text_snippet": "Atlas: Few-shot Learning with Retrieval Augmented Language Models Gautier Izacard∗ ∗,♦,♣,♥gizacard@fb.com Patrick Lewis∗,♦plewis@fb.com Maria Lomeli♦marialomeli@fb.com Lucas Hosseini♦hoss@fb.com Fabio Petroni♦fabiopetroni@fb.com Timo Schick"}, {"rank": 4, "score": 0.67943656, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 2, "text_snippet": "ings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including"}, {"rank": 5, "score": 0.66785926, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 154, "text_snippet": "odel. By jointly pre-training the retriever module and the language model, we show that Atlashas strong few-shot learning capabilities on a wide range of knowledge intensive tasks, including NaturalQuestions, TriviaQA, FEVER, 8 KILT tasks a"}]}
{"case_index": 102, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"(Wu et al., 2021), are not open-sourced due to commercial [MASK] and are only available as black-box APIs, through which users can send queries and receive responses.\"", "gold": "considerations", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.366, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.72344965, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 13, "text_snippet": " (Wu et al., 2021), are not open-sourced due to commercial considerations and are only available as black-box APIs, through which users can send queries and receive responses. On the other hand, even open sourced language models such as OPT"}, {"rank": 2, "score": 0.60314083, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 14, "text_snippet": "5k each (Younes Belkda, 2022)), making them inac- cessible to researchers and developers with limited re- sources. Traditionally, retrieval-augmented model frame- works (Khandelwal et al., 2020; Borgeaud et al., 2022; Yu, 2022; Izacard et a"}, {"rank": 3, "score": 0.59704685, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 5, "text_snippet": "ting the LM’s parameters, REPLUG treats the language model as a black box and augments it with a frozen or tunable retriever. This black-box assumption makes REPLUG applicable to large LMs (i.e., >100B parameters), which are often served vi"}, {"rank": 4, "score": 0.5718334, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 12, "text_snippet": "gnals, resulting in improved retrieval quality. •Evaluations on language modeling (§6), open-domain QA and MMLU demonstrate that REPLUG can im- prove the performance of various language models such as GPT, OPT and BLOOM, including very larg"}, {"rank": 5, "score": 0.5372374, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 71, "text_snippet": "a black box and augments it with a tuneable retrieval model. Our evaluation shows that REPLUG can be integrated with any existing language model to improve their performance  REPLUG: Retrieval-Augmented Black-Box Language Models on language"}]}
{"case_index": 103, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"A common approach to [MASK] RAG is to use text embeddings, retrieving records closest to the query in vector space where closeness corresponds to semantic similarity (Gao et al., 2023).\"", "gold": "conventional", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 9.639, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.71320844, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 15, "text_snippet": "e LLM’s context window. In canonical RAG approaches, the retrieval process returns a set number of records that are seman- tically similar to the query and the generated answer uses only the information in those retrieved records. A common "}, {"rank": 2, "score": 0.6798168, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 16, "text_snippet": " may use alternative retrieval mechanisms, we collectively refer to the family of conventional approaches as vector RAG . GraphRAG contrasts with vector RAG in its ability to answer queries that require global sensemaking over the entire da"}, {"rank": 3, "score": 0.67346656, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 6, "text_snippet": "2006; Laskar et al., 2020; Yao et al., 2017). This conventional approach, which we collectively call vector RAG , works well for queries that can be answered with information localized within a small set of records. However, vector RAG appr"}, {"rank": 4, "score": 0.66683125, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 29, "text_snippet": "lements (nodes, edges, covariates) that the LLM can summarize in parallel at both indexing time and query time. The “global answer” to a given query is produced using a final round of query-focused summarization over all community summaries"}, {"rank": 5, "score": 0.647125, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 5, "text_snippet": "l., 2024; Liu et al., 2023). In the canonical RAG setup, the system has access to a large external corpus of text records and retrieves a subset of records that are individually relevant to the query and collectively small enough to fit int"}]}
{"case_index": 104, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"It has obtained [MASK] results on a diverse set of generation tasks and outperforms comparably-sized T5 models [32].\"", "gold": "state-of-the-art", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 9.567, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6060089, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 74, "text_snippet": "nstrates a substantial performance advantage over supervised fine-tuned LLMs in all tasks and even outperforms ChatGPT in PubHealth, PopQA, biography generations, and ASQA (Rouge and MAUVE). Our approach also significantly outperforms a con"}, {"rank": 2, "score": 0.5964946, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 71, "text_snippet": " pre-trained language model could achieve strong performance across both discriminative and generative tasks. For further improvement, BART [ 32] and T5 [ 51,52] propose a single, pre-trained encoder-decoder model that leverages bi-directio"}, {"rank": 3, "score": 0.5859501, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 83, "text_snippet": ".49 28.22 5 20.18 17.3728.2419.6518.7127.08 820.5217.60 28.17 16.87 18.05 25.36 10 19.67 17.41 27.62 15.72 17.42 23.60 6 Related Works Retrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi- tec"}, {"rank": 4, "score": 0.58201146, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 69, "text_snippet": "documents by GTR-XXL (Ni et al., 2022) across all baselines for a fair comparison. 5https://github.com/princeton-nlp/ALCE 6We report numbers using the results reported in the paper as the implementations are not available. 7  Preprint. Tabl"}, {"rank": 5, "score": 0.5807001, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 70, "text_snippet": "re is capable of achieving strong performance across several tasks. 8  General-Purpose Architectures for NLP Prior work on general-purpose architectures for NLP tasks has shown great success without the use of retrieval. A single, pre-train"}]}
{"case_index": 105, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"[MASK], for each data data point, it contains s+o=Tnumber of tokens, which we use for CPT to prepare the model for downstream tasks utilizing chunk embeddings\"", "gold": "specifically", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.098, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.6075951, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 26, "text_snippet": "e detailed discussion 1REFRAGwithout cache means that we recompute the chunk embedding for the context and take this latency into account. 3  on empirical evaluation is in section A. 3 Methodology To align the encoder and decoder, we follow"}, {"rank": 2, "score": 0.5890364, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 147, "text_snippet": "ecoder Tokenizer &  Embedding  Decoder Input Text Token Embedding  Chunk  Embedding  RL-trained chunk expansion policy  Reward = - Log(Perplexity)  Donald Trump  Answer Figure 5A demonstration of selective token compression. For all chunks,"}, {"rank": 3, "score": 0.5888557, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 27, "text_snippet": "repare the model for downstream tasks utilizing chunk embeddings. To further enhance performance, we introduce selective compression via RL. After aligning the encoder and decoder through CPT, we apply supervised fine-tuning (SFT) to adapt "}, {"rank": 4, "score": 0.56325084, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 152, "text_snippet": "e downstream tasks (see section 5). REFRAG:Our approach is illustrated in figure 1. We use RoBERTa-large (Liu et al., 2019) as the encoder, feeding x1:stokens and evaluating the perplexity on the output tokens xs+1:s+o. We use REFRAG kto de"}, {"rank": 5, "score": 0.56301504, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 31, "text_snippet": "ts parametric memory during training. Once the encoder is aligned with the decoder through this reconstruction task, we initiate CPT byunfreezing the decoder. Curriculum learning.The training tasks described in the previous section may seem"}]}
{"case_index": 106, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"tion, one can use a subset of the corpus held out from [MASK] graph extraction and answer evaluation steps).\"", "gold": "subsequent", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 9.004, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.62210435, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 24, "text_snippet": "tion, one can use a subset of the corpus held out from subsequent graph extraction and answer evaluation steps). Adaptive benchmarking refers to the process of dynamically generating evaluation benchmarks tai- lored to specific domains or u"}, {"rank": 2, "score": 0.58517665, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 27, "text_snippet": "ns (Wang et al., 2023a; Zheng et al., 2024). Some prior work proposes criteria for having LLMs quantify the quality of 3  Source Documents Text Chunkstext extraction and chunking Entities & Relationshipsdomain-tailored summarization Knowled"}, {"rank": 3, "score": 0.5814055, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 29, "text_snippet": "lements (nodes, edges, covariates) that the LLM can summarize in parallel at both indexing time and query time. The “global answer” to a given query is produced using a final round of query-focused summarization over all community summaries"}, {"rank": 4, "score": 0.57823026, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 96, "text_snippet": "tigate these downstream risks for questions of a global nature, which might otherwise be answered by samples of retrieved facts falsely presented as global summaries. 7 Conclusion We have presented GraphRAG, a RAG approach that combines kno"}, {"rank": 5, "score": 0.5737939, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 23, "text_snippet": " this work, we propose an approach for generating a set of questions for evaluating global sensemaking over the entirety of the corpus. Our approach is related to LLM methods that use a corpus to generate questions whose answers would be su"}]}
{"case_index": 107, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"We summarize the key ideas for our two [MASK], extractive compressors and abstractive compressor here, and discuss their training schemes formally in Section 3.\"", "gold": "compressors", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.951, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.7047051, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 23, "text_snippet": "odels in the output should be faithful to the original input, yet the main goal is different. Instead of capturing salient information for humans readers, compressors aim to produce a concise text that are useful for a LM on an end task. In"}, {"rank": 2, "score": 0.7031914, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 19, "text_snippet": "ining compressors for conciseness and effectiveness. We summarize the key ideas for our two compressors, extractive compressors and abstractive compressor here, and discuss their training schemes formally in Section 3. 2Improving retriever "}, {"rank": 3, "score": 0.66675067, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 2, "text_snippet": "d an abstractive compressor which generates summaries by synthesizing information from multiple documents. Both compressors are trained to improve LMs’ performance on end tasks when the generated summaries are prepended to the LMs’ input, w"}, {"rank": 4, "score": 0.65034074, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 24, "text_snippet": "TRACTIVE COMPRESSION As we formulate extractive compression as a ranking problem, training extractive compressor re- sembles training a reranker for the retrieved documents4with two differences. First, our compressor considers a different g"}, {"rank": 5, "score": 0.6467813, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 22, "text_snippet": "y. Yet, using an extreme-scale model as the compressor is not desirable as we want the compressor to be substantially smaller than the LMs. Thus, we perform distillation (Hinton et al., 2015) of extreme-scale LMs to build a lightweight abst"}]}
{"case_index": 108, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"rieved evidence documents, and guide RALM to generate desired outputs when [MASK] to the input.\"", "gold": "prepended", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 9.855, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.6600958, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 6, "text_snippet": "further confuse LMs with irrelevant information, degrading model performances (Mallen et al., 2022; Shi et al., 2023a). To overcome such limitations, we propose RECOMP (Retrieve, Com press, Prepend), an inter- mediate step for RALMs which c"}, {"rank": 2, "score": 0.5941827, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 7, "text_snippet": "rieved evidence documents, and guide RALM to generate desired outputs when prepended to the input. To satisfy both efficiency and effectiveness constraints, our compressor strategically performs selective augmentation by generating an empty"}, {"rank": 3, "score": 0.58964396, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 0, "text_snippet": "RECOMP: I MPROVING RETRIEVAL -AUGMENTED LM S WITH COMPRESSION AND SELECTIVE AUGMENTATION Fangyuan Xu1, Weijia Shi2, Eunsol Choi1 Department of Computer Science 1The University of Texas at Austin 2University of Washington {fangyuan,eunsol }@"}, {"rank": 4, "score": 0.58725005, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 4, "text_snippet": "trieval-augmented language models (RALMs) (Khandelwal et al., 2019; Izacard et al., 2022; Lewis et al., 2020; Borgeaud et al., 2022) have shown impressive performance on knowledge-intensive tasks (Kwiatkowski et al., 2019; Petroni et al., 2"}, {"rank": 5, "score": 0.5860432, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 77, "text_snippet": "by compressing retrieved documents into a concise summary or an empty sequence, facilitating selective retrieval augmentation. Prompt Compression Recent work (Wingate et al., 2022; Chevalier et al., 2023; Mu et al., 2023) proposes compressi"}]}
{"case_index": 109, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"(y) Question Answering: Answer [MASK] pη (Non-Parametric) z 4 z3 z2 z 1d(z) Jeopardy Question Generation: Answer QueryFigure 1: Overview of our approach.\"", "gold": "generationretriever", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 20.498, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.5927747, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 35, "text_snippet": "y on parametric knowledge to generate reasonable responses. 3.3 Jeopardy Question Generation To evaluate RAG’s generation abilities in a non-QA setting, we study open-domain question gen- eration. Rather than use questions from standard ope"}, {"rank": 2, "score": 0.57190317, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 22, "text_snippet": "t prior probability pη(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [ 23]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index."}, {"rank": 3, "score": 0.569903, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 8, "text_snippet": " retriever ( Query Encoder +Document Index ) with a pre-trained seq2seq model ( Generator ) and ﬁne-tune end-to-end. For query x, we use Maximum Inner Product Search (MIPS) to ﬁnd the top-K documents zi. For ﬁnal prediction y, we treatzas a"}, {"rank": 4, "score": 0.55721176, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 14, "text_snippet": "active approaches. For knowledge-intensive generation, we experiment with MS-MARCO [ 1] and Jeopardy question generation, and we ﬁnd that our models generate responses that are more factual, speciﬁc, and diverse than a BART baseline. For FE"}, {"rank": 5, "score": 0.5570537, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 14, "text_snippet": " be trained using weak supervision in the form of question-answer pairs (Karpukhin et al., 2020), or pretrained using a cloze task and ﬁnetuned end-to- end (Guu et al., 2020; Lee et al., 2019). Generative question answering was mostly consi"}]}
{"case_index": 110, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"We use a training objective which prefers [MASK] documents that improve language model perplexity, while treating the LM as a frozen, black-box scoring function.\"", "gold": "retrieving", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 14.98, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.711449, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 9, "text_snippet": "22) that adapts language models to the retriever. We use a training objective which prefers retrieving documents that improve language model perplexity, while treating the LM as a frozen, black-box scoring function. Our experiments show tha"}, {"rank": 2, "score": 0.6210746, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 63, "text_snippet": " the performance of REPLUG andREPLUG LSR improved monotonically. How- ever, a small number of documents (e.g., 10) was sufficient to achieve large performance gains.  REPLUG: Retrieval-Augmented Black-Box Language Models Perplexity 14.0016."}, {"rank": 3, "score": 0.61988, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 29, "text_snippet": "triever inREPLUG by using the LM itself to provide supervision about which documents should be retrieved. Inspired by Sachan et al. (2022), our approach can be seen as adjusting the probabilities of the retrieved documents to match the prob"}, {"rank": 4, "score": 0.6152468, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 36, "text_snippet": " or losses. Indeed, for PDist and LOOP, the perplexity of the output may not vary much when conditioning on diﬀerent documents, especially in the case of long outputs. 2.3 Pretext tasks In this section, we describe pretext tasks that can be"}, {"rank": 5, "score": 0.6150925, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 1, "text_snippet": "mechanisms to en- code the retrieved text, REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design can be eas- ily applied to any existing retrieval and language models. Furthermore, we show t"}]}
{"case_index": 111, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"This preserves the [MASK] and semantic coherence of the text within each chunk.\"", "gold": "contextual", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.3333333333333333, "hit_rank": 3, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.869, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.55641115, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 35, "text_snippet": "ression, expanding important context chunks uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder a"}, {"rank": 2, "score": 0.55619144, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 3, "score": 0.5413798, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 23, "text_snippet": "es. If a sentence exceeds the 100-token limit, we move the entire sentence to the next chunk, rather than cutting it mid-sentence. This preserves the contextual and semantic coherence of the text within each chunk. These texts are then embe"}, {"rank": 4, "score": 0.534101, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 9, "text_snippet": "uestions at different levels. 1arXiv:2401.18059v1 [cs.CL] 31 Jan 2024  Published as a conference paper at ICLR 2024 2 3 4 5  1 1  2  3  3  4 5  5 6  8  7 Index #8  Text: summary of  nodes 2 and 3  Child Nodes: 2, 3  Text Embedding  Text chu"}, {"rank": 5, "score": 0.523933, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 22, "text_snippet": " RAPTOR addresses the issue of semantic depth and connection in reading by building a recursive tree structure that balances broader thematic comprehension with granular details and which allows nodes to be grouped based on semantic sim- il"}]}
{"case_index": 112, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"retrieved documents are prepended to the input context and fed into the black-box LM to make the final [MASK].\"", "gold": "prediction", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 9.145, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.7439543, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 7, "text_snippet": " retrieved documents are prepended to the input context and fed into the black-box LM to make the final prediction. Because the LM context length limits the number of documents that can be prepended, we also introduce a new ensemble scheme "}, {"rank": 2, "score": 0.6856743, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 21, "text_snippet": " (§3.1 Document Retrieval ). Then it prepends each document separately to the input context and ensembles output probabilities from different passes (§3.2 Input Reformulation ). and a training scheme to further adapt the retriever to large "}, {"rank": 3, "score": 0.6656153, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 1, "text_snippet": "mechanisms to en- code the retrieved text, REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design can be eas- ily applied to any existing retrieval and language models. Furthermore, we show t"}, {"rank": 4, "score": 0.6488022, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 6, "text_snippet": "PIs. Internal representations of such models are not exposed and fine-tuning is not supported. In this work, we introduce REPLUG (Retrieve and Plug ), a new retrieval-augmented LM framework where the lan- guage model is viewed as a black bo"}, {"rank": 5, "score": 0.632423, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 22, "text_snippet": " module. As shown in Figure 2, given an input context, REPLUG first retrieves a small set of relevant documents from an external corpus using a retriever (§3.1). Then we pass the concate- nation of each retrieved document with the input con"}]}
{"case_index": 113, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"We rely on the Fusion-in-Decoder modiﬁcation of [MASK] models, and process each document independently in the encoder (Izacard & Grave, 2020).\"", "gold": "sequence-to-sequence", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 11.606, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.7025794, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 9, "text_snippet": " strong few-shot learning, despite having lower parameter counts than other powerful recent few-shot learners. Atlasretrieves relevant documents based on the current context by using a general-purpose dense retriever using a dual-encoder ar"}, {"rank": 2, "score": 0.69794977, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 21, "text_snippet": "ce architecture (Raﬀel et al., 2019). We rely on the Fusion-in-Decoder modiﬁcation of sequence-to-sequence models, and process each document independently in the encoder (Izacard & Grave, 2020). We then concatenate the outputs of the encode"}, {"rank": 3, "score": 0.668368, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 20, "text_snippet": "eir corresponding embeddings. The Contriever model is pre-trained using the MoCo contrastive loss (He et al., 2020), and uses unsupervised data only. As shown in the following section, an advantage of dense retrievers is that both query and"}, {"rank": 4, "score": 0.65668094, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 22, "text_snippet": "ache.org 2spacy.io 3github.com/facebookresearch/faisstion over the concatenation of the resulting repre- sentations of all the retrieved passages. The model thus performs evidence fusion in the decoder only, and we refer to it as Fusion-in-"}, {"rank": 5, "score": 0.6447618, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 17, "text_snippet": "inal predic- tion. This style of retrieval can be added to both encoder- decoder (Yu, 2022; Izacard et al., 2022b) and decoder-only models (Khandelwal et al., 2020; Borgeaud et al., 2022; Shi et al., 2022; Rubin et al., 2022). For example, "}]}
{"case_index": 114, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"” Sensemaking tasks require reasoning over “ connections (which can be among people, places, and events) in order to anticipate their [MASK] and act effectively ” (Klein et al., 2006).\"", "gold": "trajectories", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 9.156, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6188044, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 7, "text_snippet": "plinary research over the past decade? ” Sensemaking tasks require reasoning over “ connections (which can be among people, places, and events) in order to anticipate their trajectories and act effectively ” (Klein et al., 2006). LLMs such "}, {"rank": 2, "score": 0.54606736, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 29, "text_snippet": "lements (nodes, edges, covariates) that the LLM can summarize in parallel at both indexing time and query time. The “global answer” to a given query is produced using a final round of query-focused summarization over all community summaries"}, {"rank": 3, "score": 0.5173822, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 8, "text_snippet": "nce analysis (Ranade and Joshi, 2023). Given a sensemaking query and a text with an implicit and interconnected set of concepts, an LLM can generate a summary that answers the query. The challenge, however, arises when the volume of data re"}, {"rank": 4, "score": 0.5129979, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 6, "text_snippet": "2006; Laskar et al., 2020; Yao et al., 2017). This conventional approach, which we collectively call vector RAG , works well for queries that can be answered with information localized within a small set of records. However, vector RAG appr"}, {"rank": 5, "score": 0.5025903, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 95, "text_snippet": "ontained in higher-level community summaries. Broader impacts . As a mechanism for question answering over large document collections, there are risks to downstream sensemaking and decision-making tasks if the generated answers do not accur"}]}
{"case_index": 115, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"To train the retriever and generator [MASK], we treat the retrieved document as a latent va\"", "gold": "end-to-end", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 8.419, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6048311, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 29, "text_snippet": "triever inREPLUG by using the LM itself to provide supervision about which documents should be retrieved. Inspired by Sachan et al. (2022), our approach can be seen as adjusting the probabilities of the retrieved documents to match the prob"}, {"rank": 2, "score": 0.60282123, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 57, "text_snippet": "nted generators using a second “informed” retriever with access to the output, which the test-time retriever can be distilled from, and Hofstätter et al. (2022) recently proposed a training set ﬁltering/weighting approach to train stronger "}, {"rank": 3, "score": 0.6004758, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 56, "text_snippet": " the reader and the retriever by leveraging the perplexity of the output generated by the reader. Sachan et al. (2021) and Lee et al. (2021a) both employ salient span masking to pre-train retrievers, leveraging the perplexity and attention "}, {"rank": 4, "score": 0.58956456, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 24, "text_snippet": "tasks and outperforms comparably-sized T5 models [32]. We refer to the BART generator parameters θas the parametric memory henceforth. 2.4 Training We jointly train the retriever and generator components without any direct supervision on wh"}, {"rank": 5, "score": 0.5870895, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 10, "text_snippet": "eural retriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The retriever (Dense Passage Retriever [ 26], henceforth DPR) provides latent documents conditioned on the input, and the seq2seq model (BART"}]}
{"case_index": 116, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"If the retrieved documents are irrelevant to the input or offer no additional information to LM, our compressor can return an empty string, [MASK] selective augmentation.\"", "gold": "implementing", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.612, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.6485591, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 2, "text_snippet": "d an abstractive compressor which generates summaries by synthesizing information from multiple documents. Both compressors are trained to improve LMs’ performance on end tasks when the generated summaries are prepended to the LMs’ input, w"}, {"rank": 2, "score": 0.63128126, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 77, "text_snippet": "by compressing retrieved documents into a concise summary or an empty sequence, facilitating selective retrieval augmentation. Prompt Compression Recent work (Wingate et al., 2022; Chevalier et al., 2023; Mu et al., 2023) proposes compressi"}, {"rank": 3, "score": 0.60330606, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 0, "text_snippet": "RECOMP: I MPROVING RETRIEVAL -AUGMENTED LM S WITH COMPRESSION AND SELECTIVE AUGMENTATION Fangyuan Xu1, Weijia Shi2, Eunsol Choi1 Department of Computer Science 1The University of Texas at Austin 2University of Washington {fangyuan,eunsol }@"}, {"rank": 4, "score": 0.59871626, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 18, "text_snippet": "cy. If the retrieved documents do not contain relevant information or retrieval augmentation is not necessary, scan be an empty sequence. (2) Effecive : when sis prepended to input sequence xand provided to LM Mas a prompt, LM should genera"}, {"rank": 5, "score": 0.56846875, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 6, "text_snippet": "further confuse LMs with irrelevant information, degrading model performances (Mallen et al., 2022; Shi et al., 2023a). To overcome such limitations, we propose RECOMP (Retrieve, Com press, Prepend), an inter- mediate step for RALMs which c"}]}
{"case_index": 117, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"Although we do not have human annotations to train this model, prior work (Goyal et al., 2022; Chen et al., 2023; Potluri et al., 2023) suggests that the extreme- scale LMs can generate good [MASK] summaries when prompted carefully.\"", "gold": "query-focused", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.3333333333333333, "hit_rank": 3, "matched_by": "doc_id+chunk_id", "retrieval_ms": 11.684, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.66130507, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 77, "text_snippet": "by compressing retrieved documents into a concise summary or an empty sequence, facilitating selective retrieval augmentation. Prompt Compression Recent work (Wingate et al., 2022; Chevalier et al., 2023; Mu et al., 2023) proposes compressi"}, {"rank": 2, "score": 0.6379167, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 51, "text_snippet": "abstractive approach, we consider summaries generated from different prompts ( {sj}n 1in Figure 3) and empty summary, and choose the one that leads to the best end task performance. As oracle compression is model dependent, we also report m"}, {"rank": 3, "score": 0.6349966, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 21, "text_snippet": "Compressor We train an encoder-decoder model encdec θto serve as an abstractive compressor, which takes the input sequence xand a concatenation of retrieved document set D [d1;d2;...dN]) and output a summary s. Although we do not have human"}, {"rank": 4, "score": 0.62519956, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 32, "text_snippet": "each sentence from the retrieved documents, we prepend the Wikipedia page title to it to for decontextualization. 3.2 A BSTRACTIVE COMPRESSION To train an abstractive compressor, we distill the query-focused summarization ability of extreme"}, {"rank": 5, "score": 0.6215201, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 84, "text_snippet": " and pre-training paradigm for generative LLMs, leveraging cross-attention and end-to-end pre-training with 9  retrieval from a trillion-token data store, achieving strong performance. Subsequent work by Shi et al. (2024) and Lin et al. (20"}]}
{"case_index": 118, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"In this framework, the input to models is augmented by [MASK] relevant documents that are retrieved from an external knowledge corpus (Guu et al., 2020).\"", "gold": "prepending", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.714, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.62756515, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 7, "text_snippet": "ieval-augmented generation (RAG) (Lewis et al., 2020). In this framework, the input to models is augmented by prepending relevant documents that are retrieved from an external knowledge corpus (Guu et al., 2020). While RAG serves as a pract"}, {"rank": 2, "score": 0.62587416, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 19, "text_snippet": " documents. It usually provides an extra knowledge source from a specific corpus, i.e., Wikipedia, which greatly improves the per- formance of LMs in a variety of tasks, especially in the knowledge-intensive ones. The proposed  methods gene"}, {"rank": 3, "score": 0.60196066, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 4, "text_snippet": "trieval-augmented language models (RALMs) (Khandelwal et al., 2019; Izacard et al., 2022; Lewis et al., 2020; Borgeaud et al., 2022) have shown impressive performance on knowledge-intensive tasks (Kwiatkowski et al., 2019; Petroni et al., 2"}, {"rank": 4, "score": 0.60147184, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 18, "text_snippet": "pact the LLMs and exacerbate the problems. Thus, it is obvious that the lack of accurate and specific knowledge can lead to misleading or even inaccurate generation, which will severely hurt the experience of users in most practical applica"}, {"rank": 5, "score": 0.5995863, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 6, "text_snippet": " The examples show that a low-quality retriever is prone to introducing a substantial amount of irrelevant information, impeding the generators from acquiring accurate knowledge and potentially misleading them. the parametric knowledge they"}]}
{"case_index": 119, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"The second approach, [MASK] , can predict each target token based on a different document.\"", "gold": "rag-token", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 11.146, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.6039637, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 2, "score": 0.58355707, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 17, "text_snippet": "rator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a distribution over generated text. In one approach, RAG-Sequence , the mo"}, {"rank": 3, "score": 0.58043814, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 75, "text_snippet": "le, enabling us to dynamically update the model’s memory by editing the document index. This approach has also been used in knowledge-intensive dialog, where generators have been conditioned on retrieved text directly, albeit obtained via T"}, {"rank": 4, "score": 0.5764788, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 19, "text_snippet": " documents. It usually provides an extra knowledge source from a specific corpus, i.e., Wikipedia, which greatly improves the per- formance of LMs in a variety of tasks, especially in the knowledge-intensive ones. The proposed  methods gene"}, {"rank": 5, "score": 0.57463825, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 11, "text_snippet": "e document is responsible for all tokens) or a per-token basis (where different documents are responsible for different tokens). Like T5 [ 51] or BART, RAG can be ﬁne-tuned on any seq2seq task, whereby both the generator and retriever are j"}]}
{"case_index": 120, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"These projected chunk [MASK] are then fed to the decoder model along with the token embeddings for the question to generate the answer y∼ M dec({e1, .\"", "gold": "embeddings", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 17.113, "llm_ms": 0.008, "top_contexts": [{"rank": 1, "score": 0.6574067, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 21, "text_snippet": "uery  Encoder Figure 1The main design ofREFRAG. The input context is chunked and processed by the light-weight encoder to produce chunk embeddings, which are precomputable for efficient reuse. A light-weight RL policy decide few chunks to e"}, {"rank": 2, "score": 0.6204587, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 147, "text_snippet": "ecoder Tokenizer &  Embedding  Decoder Input Text Token Embedding  Chunk  Embedding  RL-trained chunk expansion policy  Reward = - Log(Perplexity)  Donald Trump  Answer Figure 5A demonstration of selective token compression. For all chunks,"}, {"rank": 3, "score": 0.6199056, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 14, "text_snippet": "ortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now "}, {"rank": 4, "score": 0.6172621, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 18, "text_snippet": "f question and retrieval in this section. Model overview.Figure 1 shows the main architecture ofREFRAG. This model consists of a decoder-only foundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Rober"}, {"rank": 5, "score": 0.61227334, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 15, "text_snippet": "igure 1) while preserving the autoregressive nature of the decoder, thereby supporting multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight reinforcement learning (RL) policy that sel"}]}
{"case_index": 121, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"[MASK] Generation (RAG) augments the input space of LMs with retrieved text passages (Guu et al., 2020; Lewis et al., 2020), lea\"", "gold": "retrieval-augmented", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 7.93, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.7289335, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 7, "text_snippet": "ieval-augmented generation (RAG) (Lewis et al., 2020). In this framework, the input to models is augmented by prepending relevant documents that are retrieved from an external knowledge corpus (Guu et al., 2020). While RAG serves as a pract"}, {"rank": 2, "score": 0.7236019, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 2, "text_snippet": "ew framework called Self-Reflective Retrieval-Augmented Gen- eration ( SELF-RAG)that enhances an LM’s quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passage"}, {"rank": 3, "score": 0.72133493, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 6, "text_snippet": "y trained to leverage and follow facts from provided passages. This work introduces Self-Reflective Retrieval-augmented Generation ( SELF-RAG)to improve an LLM’s generation quality, including its factual accuracy without hurting its versati"}, {"rank": 4, "score": 0.7183406, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 96, "text_snippet": "ains an LM to learn to retrieve, generate, and critique text passages and its own generation by predicting the next tokens from its original vocabulary as well as newly added special tokens, called reflection tokens. SELF-RAGfurther enables"}, {"rank": 5, "score": 0.71563035, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 1, "text_snippet": "e reliance on the paramet- ric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and inco"}]}
{"case_index": 122, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"While being a [MASK] problem in natural lan- guage processing (V oorhees et al., 199\"", "gold": "longstanding", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.567, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.5820886, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 10, "text_snippet": " problem in natural lan- guage processing (V oorhees et al., 1999), this task has recently regained interest following the work by Chen et al. (2017). In that version of the prob- lem, strong supervision is available to the learning system,"}, {"rank": 2, "score": 0.54857, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 9, "text_snippet": "rticular, we show that the performance of our method signiﬁcantly improves when the number of retrieved passages increases. We believe that this is evidence that generative mod- els are good at combining evidence from multiple passages, com"}, {"rank": 3, "score": 0.5479559, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}, {"rank": 4, "score": 0.5414336, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 16, "text_snippet": "ous NLP tasks, including language mod- eling (Min et al., 2022; Borgeaud et al., 2022; Khandelwal et al., 2020) and open-domain question answering (Lewis et al., 2020; Izacard et al., 2022b; Hu et al., 2022). Specifi- cally, using the input"}, {"rank": 5, "score": 0.537282, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}]}
{"case_index": 123, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"These texts are then embedded using SBERT, a BERT-based encoder ( [MASK]1 ) (Reimers & Gurevych, 2019).\"", "gold": "multi-qa-mpnet-base-cos-v", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 8.214, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.593552, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 53, "text_snippet": "a speed-setting. We report accuracies for both the entire test set and the HARD subset. Controlled Baseline Comparisons We first present controlled comparisons using the UnifiedQA 3B as the reader, with SBERT (Reimers & Gurevych, 2019), BM2"}, {"rank": 2, "score": 0.59125865, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 10, "text_snippet": "eural retriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The retriever (Dense Passage Retriever [ 26], henceforth DPR) provides latent documents conditioned on the input, and the seq2seq model (BART"}, {"rank": 3, "score": 0.5838791, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 0, "text_snippet": "Published as a conference paper at ICLR 2024 RAPTOR: R ECURSIVE ABSTRACTIVE PROCESSING FOR TREE-ORGANIZED RETRIEVAL Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D. Manning Stanford University psarthi@cs."}, {"rank": 4, "score": 0.5699762, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 111, "text_snippet": ", 2021. URL https://arxiv.org/abs/2112.11446 . Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton- Brown, and Yoav Shoham. In-context retrieval-augmented language models. arXiv preprint arXiv:2302.00083 , 2023. "}, {"rank": 5, "score": 0.56443226, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 9, "text_snippet": "uestions at different levels. 1arXiv:2401.18059v1 [cs.CL] 31 Jan 2024  Published as a conference paper at ICLR 2024 2 3 4 5  1 1  2  3  3  4 5  5 6  8  7 Index #8  Text: summary of  nodes 2 and 3  Child Nodes: 2, 3  Text Embedding  Text chu"}]}
{"case_index": 124, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"A confidence degree is [MASK] based on which different knowledge retrieval actions of { Correct , Incorrect ,Ambiguous } can be triggered.\"", "gold": "quantified", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.292, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.60850465, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 39, "text_snippet": "ngly where the upper and lower thresholds are set. If the confidence score is higher than the upper threshold, the retrieved document is identified as Correct , while identified as Incorrect if below the lower threshold. Otherwise, a more s"}, {"rank": 2, "score": 0.60016924, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 11, "text_snippet": "lity of retrieved documents for a query. This serves as a crucial component in RAG, contributing to informative generation by reviewing and evaluating the relevance and reliability of the retrieved documents. A confidence degree is quantifi"}, {"rank": 3, "score": 0.5879363, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 42, "text_snippet": " fabricated facts. Therefore, we need to seek new sources of knowledge for correction. Here, web search is introduced to search from the Internet as elaborated in Section 4.5. This corrective action helps overcome the embarrassing challenge"}, {"rank": 4, "score": 0.57866657, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 37, "text_snippet": "D 2Confidence = Calculate and give a final judgment based on { score 1, score 2, ...score k} //Confidence has 3 optional values: [CORRECT], [INCORRECT] or [AMBIGUOUS] 3ifConfidence == [CORRECT] then 4 Internal_Knowledge = Knowledge_Refine( "}, {"rank": 5, "score": 0.5602997, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 27, "text_snippet": "or is constructed to estimate the relevance score of retrieved documents to the input query (Sec- tion 4.2). The relevance score is quantified into a total of three confidence degrees and then triggered the corresponding actions: { Correct "}]}
{"case_index": 125, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x)via a top-K [MASK].\"", "gold": "approximation", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 10.691, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.60035926, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 19, "text_snippet": "r produces the output sequence probability for each document, which are then marginalized, pRAG-Sequence (y|x)≈∑ z∈top-k(p(·|x))pη(z|x)pθ(y|x,z) =∑ z∈top-k(p(·|x))pη(z|x)N∏ ipθ(yi|x,z,y 1:i−1) RAG-Token Model In the RAG-Token model we can d"}, {"rank": 2, "score": 0.594793, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 3, "score": 0.5848094, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 10, "text_snippet": "eural retriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The retriever (Dense Passage Retriever [ 26], henceforth DPR) provides latent documents conditioned on the input, and the seq2seq model (BART"}, {"rank": 4, "score": 0.56092906, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 25, "text_snippet": "rator G. The retriever Raims to retrieve the top- Kdocuments D={dr1, ..., d rk}that are relevant to the input Xfrom the corpus C. Based on the input Xand the retrieved results D, the generator Gis responsible for generating the output Y. Th"}, {"rank": 5, "score": 0.55691165, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 25, "text_snippet": "beddings. 3.2. Input Reformulation The retrieved top- kdocuments provide rich information about the original input context xand can potentially help the LM to make a better prediction. One simple way to incorporate the retrieved documents a"}]}
{"case_index": 126, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"As shown inarXiv:2301.12652v4 [cs.CL] 24 May 2023 REPLUG: [MASK] Black-Box Language Models Figure 1, REPLUG is extremely flexible\"", "gold": "retrieval-augmented", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.828, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6862616, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 8, "text_snippet": "Black-Box Language Models Figure 1, REPLUG is extremely flexible and can be used with any existing black-box LM and retrieval model. We also introduce REPLUG LSR (REPLUG with LM- Supervised Retrieval), a training scheme that can further imp"}, {"rank": 2, "score": 0.67611897, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 7, "text_snippet": " retrieved documents are prepended to the input context and fed into the black-box LM to make the final prediction. Because the LM context length limits the number of documents that can be prepended, we also introduce a new ensemble scheme "}, {"rank": 3, "score": 0.64328146, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 71, "text_snippet": "a black box and augments it with a tuneable retrieval model. Our evaluation shows that REPLUG can be integrated with any existing language model to improve their performance  REPLUG: Retrieval-Augmented Black-Box Language Models on language"}, {"rank": 4, "score": 0.6260326, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 0, "text_snippet": "REPLUG: Retrieval-Augmented Black-Box Language Models Weijia Shi,1 *Sewon Min,1Michihiro Yasunaga,2Minjoon Seo,3Rich James,4Mike Lewis,4 Luke Zettlemoyer1 4Wen-tau Yih4 Abstract We introduce REPLUG, a retrieval-augmented lan- guage modeling"}, {"rank": 5, "score": 0.6179889, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 12, "text_snippet": "gnals, resulting in improved retrieval quality. •Evaluations on language modeling (§6), open-domain QA and MMLU demonstrate that REPLUG can im- prove the performance of various language models such as GPT, OPT and BLOOM, including very larg"}]}
{"case_index": 127, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"These reflection tokens (Table 1) signal the need for retrieval or confirm the output’s relevance, support, or [MASK].\"", "gold": "completeness", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 10.029, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6157661, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 7, "text_snippet": "rmittent special tokens (i.e., reflection tokens ). Reflection tokens are categorized into retrieval andcritique tokens to indicate the need for retrieval and its generation quality respectively (Figure 1 right). In particular, given an inp"}, {"rank": 2, "score": 0.5848683, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 31, "text_snippet": "tially supported, no support }All of the verification-worthy statement in y is supported by d. ISUSE x, y {5, 4, 3, 2, 1 } yis a useful response to x. Table 1: Four types of reflection tokens used in SELF-RAG. Each type uses several tokens "}, {"rank": 3, "score": 0.5690292, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 41, "text_snippet": "lding are unforgettable experience.No RetrievalNo Retrieval Retriever Figure 2: SELF-RAGtraining examples. The left example does not require retrieval while the right one requires retrieval; thus, passages are inserted. More examples are in"}, {"rank": 4, "score": 0.56372225, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 13, "text_snippet": "erifiability. consistently retrieves a fixed number of documents for generation regardless of the retrieval necessity (e.g., the bottom figure example does not require factual knowledge) and never second visits the generation quality. Moreo"}, {"rank": 5, "score": 0.56162846, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 6, "text_snippet": "y trained to leverage and follow facts from provided passages. This work introduces Self-Reflective Retrieval-augmented Generation ( SELF-RAG)to improve an LLM’s generation quality, including its factual accuracy without hurting its versati"}]}
{"case_index": 128, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"Born in Maida Vale, London…Where was Alan Turing born?[MASK]2seq modelMaida Vale, LondonFigure 1: A simple approach to open domain question answering.\"", "gold": "generativeseq", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.394, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6837826, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 4, "text_snippet": "external source of knowledge, such as Wikipedia. Retrieval based approaches were previously con- sidered in the context of open domain question answering with extractive models (Chen et al., 2017). In that case, systems start by retrieving "}, {"rank": 2, "score": 0.40493345, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 3, "score": 0.39074886, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 235, "text_snippet": "ull results for the cyclic-permutation-de-biased Atlas-XXL can be found in Table 18. A.2 Question answering A.2.1 Training Details For question answering, similarly to the MMLU experiments, we format the input using the following template: "}, {"rank": 4, "score": 0.38564968, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 6, "text_snippet": "rpreted. REALM [ 20] and ORQA [ 31], two recently introduced models that combine masked language models [ 8] with a differentiable retriever, have shown promising results,arXiv:2005.11401v4 [cs.CL] 12 Apr 2021  The Divine Comedy (x) qQuery "}, {"rank": 5, "score": 0.38543433, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 7, "text_snippet": "orgeaud et al., 2021, inter alia). However, retrieval-augmented models have yet to ∗equal contribution 1arXiv:2208.03299v3 [cs.CL] 16 Nov 2022  Fact checking:Bermuda Triangle is in the western part of the Himalayas.AtlasFalseMasked Language"}]}
{"case_index": 129, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"Background and Related Work Black-box Language Models Large language models (i.e., >100B), such as GPT-3 (Brown et al., 2020a), Codex (Chen et al., 2021a), and Yuan 1.0 (Wu et al., 2021), are not [MASK] due to commercial consi\"", "gold": "open-sourced", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.664, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.67824495, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 12, "text_snippet": "gnals, resulting in improved retrieval quality. •Evaluations on language modeling (§6), open-domain QA and MMLU demonstrate that REPLUG can im- prove the performance of various language models such as GPT, OPT and BLOOM, including very larg"}, {"rank": 2, "score": 0.670613, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 13, "text_snippet": " (Wu et al., 2021), are not open-sourced due to commercial considerations and are only available as black-box APIs, through which users can send queries and receive responses. On the other hand, even open sourced language models such as OPT"}, {"rank": 3, "score": 0.6411077, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 14, "text_snippet": "5k each (Younes Belkda, 2022)), making them inac- cessible to researchers and developers with limited re- sources. Traditionally, retrieval-augmented model frame- works (Khandelwal et al., 2020; Borgeaud et al., 2022; Yu, 2022; Izacard et a"}, {"rank": 4, "score": 0.63387775, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 5, "text_snippet": "ting the LM’s parameters, REPLUG treats the language model as a black box and augments it with a frozen or tunable retriever. This black-box assumption makes REPLUG applicable to large LMs (i.e., >100B parameters), which are often served vi"}, {"rank": 5, "score": 0.58364606, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 2, "text_snippet": "mproves the performance of GPT-3 (175B) on language modeling by 6.3%, as well as the perfor- mance of Codex on five-shot MMLU by 5.1%. 1. Introduction Large language models (LLMs) such as GPT-3 (Brown et al., 2020a) and Codex (Chen et al., "}]}
{"case_index": 130, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"Considering that retrieval is sometimes [MASK] for some queries, conversely, responses without retrieval are even more accurate in many situations.\"", "gold": "unnecessary", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 9.498, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.56789386, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 1, "text_snippet": "evitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Al- though retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heav"}, {"rank": 2, "score": 0.55368274, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 6, "text_snippet": " The examples show that a low-quality retriever is prone to introducing a substantial amount of irrelevant information, impeding the generators from acquiring accurate knowledge and potentially misleading them. the parametric knowledge they"}, {"rank": 3, "score": 0.55284584, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 23, "text_snippet": "t to retrieve in long-form generation. Compared with recent studies (Schick et al., 2023; Luo et al., 2023; Asai et al., 2024) that are the most relevant to our work, a main difference should be highlighted. These approaches target on explo"}, {"rank": 4, "score": 0.55211467, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 10, "text_snippet": " should not have been equally referred to and involved in RAG. On account of the above issues, this paper particularly studies the scenarios where the retriever returns inaccurate results. A method named Corrective Retrieval- Augmented Gene"}, {"rank": 5, "score": 0.54356855, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 41, "text_snippet": "lding are unforgettable experience.No RetrievalNo Retrieval Retriever Figure 2: SELF-RAGtraining examples. The left example does not require retrieval while the right one requires retrieval; thus, passages are inserted. More examples are in"}]}
{"case_index": 131, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"For the retrieval of support passages, we consider two methods: BM25 ([MASK] et al., 1995) and DPR (Karpukhin et al., 2020).\"", "gold": "robertson", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 11.159, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.74175906, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 19, "text_snippet": "den test set (right), competitions.codalab.org/competitions/17208#results ). Retrieval. For the retrieval of support passages, we consider two methods: BM25 (Robertson et al., 1995) and DPR (Karpukhin et al., 2020). In BM25, passages are re"}, {"rank": 2, "score": 0.6329607, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 114, "text_snippet": "eve Walker, Susan Jones, Micheline M Hancock-Beaulieu, Mike Gatford, et al. Okapi at TREC-3. Nist Special Publication Sp , 109:109, 1995. URL https://www. microsoft.com/en-us/research/publication/okapi-at-trec-3/ . Devendra Singh Sachan, Mi"}, {"rank": 3, "score": 0.63191926, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 20, "text_snippet": "es with SpaCy.2In DPR, passages and questions are represented as dense vector representations, computed using two BERT networks. The ranking function is the dot product between the query and passage represen- tations. Retrieval is performed"}, {"rank": 4, "score": 0.6313634, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 101, "text_snippet": "ov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906 , 2020. URL https://arxiv.org/abs/2004.04906 . [27] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, "}, {"rank": 5, "score": 0.62367713, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 51, "text_snippet": "answering (Voorhees et al., 1999), documents are often retrieved from Wikipedia (Chen et al., 2017). Recently, dense retrievers based on neural networks have become popular. These usually follow a dual-encoder architecture (Yih et al., 2011"}]}
{"case_index": 132, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"uestion answering systems (Chen et al., 2017; Yu et al., 2018), is to index large quantities of text, after splitting it into chunks (paragraphs), in a separate [MASK] retrieval system.\"", "gold": "information", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 1.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.902, "llm_ms": 0.004, "top_contexts": [{"rank": 1, "score": 0.70327437, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 5, "text_snippet": "uestion answering systems (Chen et al., 2017; Yu et al., 2018), is to index large quantities of text, after splitting it into chunks (paragraphs), in a separate information retrieval system. Retrieved information is then presented to the LL"}, {"rank": 2, "score": 0.6675186, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 8, "text_snippet": " indexing and retrieval system that uses a tree structure to capture both high-level and low-level details about a text. As shown in Figure 1, our system, RAPTOR, clusters chunks of text, generates text summaries of those clusters, and then"}, {"rank": 3, "score": 0.624892, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 7, "text_snippet": "re. This is particularly relevant for thematic questions that require integrating knowledge from multiple parts of a text, such as understanding an entire book, as in the NarrativeQA dataset (Ko ˇcisk`y et al., 2018). Consider the fairy tal"}, {"rank": 4, "score": 0.6149212, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 51, "text_snippet": "answering (Voorhees et al., 1999), documents are often retrieved from Wikipedia (Chen et al., 2017). Recently, dense retrievers based on neural networks have become popular. These usually follow a dual-encoder architecture (Yih et al., 2011"}, {"rank": 5, "score": 0.6083034, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 14, "text_snippet": "omponents: the retriever, the reader, and end-to-end system training. Retrieval methods have transitioned from traditional term-based techniques like TF-IDF (Sp¨arck Jones, 1972) and BM25 (Robertson et al., 1995; Roberts et al., 2020) to de"}]}
{"case_index": 133, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"3 2.2 Training [MASK] for the retriever In this section, we discuss four diﬀerent loss functions to train the retriever jointly with the langu\"", "gold": "objectives", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.814, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6992205, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 22, "text_snippet": "ther way to process the retrieved documents in the language model would be to concatenate the query and all the documents, and to use this long sequence as input of the model. Unfortunately, this approach does not scale with the number of d"}, {"rank": 2, "score": 0.65073395, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 23, "text_snippet": "ent loss functions to train the retriever jointly with the language model. We consider loss functions that leverage the language model to provide supervisory signal to train the retriever. In other words, if the language model ﬁnds a docume"}, {"rank": 3, "score": 0.62070686, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 77, "text_snippet": " warmup steps. We refresh the index every 1,000 steps. This means that the index is recomputed 10 times during the pre-training, leading to an overhead of around 30%, compared to training with a ﬁxed retriever. We set the number of retrieve"}, {"rank": 4, "score": 0.5986567, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 236, "text_snippet": "we retrieve 40 passages, and truncate the result of the concatenation between the query and the passages to 384 tokens. For few-shot ﬁne-tuning we train Atlasfor 30 steps using 64 random samples from the train sets. The retriever is trained"}, {"rank": 5, "score": 0.5978249, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 12, "text_snippet": "ntributions: •A thorough study on how to design and train retrieval-augmented language models, with a focus on downstream few-shot learning and sample eﬃciency. •The ﬁndings of this study lead to a retrieval-augmented language model, called"}]}
{"case_index": 134, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"But a [MASK] portion of the text within these retrieved documents is often non- essential for generation, which should not have been equally referred to and involved in RAG.\"", "gold": "considerable", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 9.478, "llm_ms": 0.004, "top_contexts": [{"rank": 1, "score": 0.6187606, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 9, "text_snippet": "nations (Zhang et al., 2023b). However, most conventional RAG ap- proaches indiscriminately incorporate the retrieved documents, regardless of whether these documents are relevant or not (Rony et al., 2022). Furthermore, current methods mos"}, {"rank": 2, "score": 0.561794, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 10, "text_snippet": " should not have been equally referred to and involved in RAG. On account of the above issues, this paper particularly studies the scenarios where the retriever returns inaccurate results. A method named Corrective Retrieval- Augmented Gene"}, {"rank": 3, "score": 0.56076676, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 4, "score": 0.55143243, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 6, "text_snippet": " The examples show that a low-quality retriever is prone to introducing a substantial amount of irrelevant information, impeding the generators from acquiring accurate knowledge and potentially misleading them. the parametric knowledge they"}, {"rank": 5, "score": 0.54484844, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 1, "text_snippet": "evitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Al- though retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heav"}]}
{"case_index": 135, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"Therefore, com- pared with the method of prepending all the retrieved docu- REPLUG: [MASK]\"", "gold": "retrieval-a", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 9.717, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6840168, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 7, "text_snippet": " retrieved documents are prepended to the input context and fed into the black-box LM to make the final prediction. Because the LM context length limits the number of documents that can be prepended, we also introduce a new ensemble scheme "}, {"rank": 2, "score": 0.66508865, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 1, "text_snippet": "mechanisms to en- code the retrieved text, REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design can be eas- ily applied to any existing retrieval and language models. Furthermore, we show t"}, {"rank": 3, "score": 0.6556057, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 21, "text_snippet": " (§3.1 Document Retrieval ). Then it prepends each document separately to the input context and ensembles output probabilities from different passes (§3.2 Input Reformulation ). and a training scheme to further adapt the retriever to large "}, {"rank": 4, "score": 0.6543146, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 6, "text_snippet": "PIs. Internal representations of such models are not exposed and fine-tuning is not supported. In this work, we introduce REPLUG (Retrieve and Plug ), a new retrieval-augmented LM framework where the lan- guage model is viewed as a black bo"}, {"rank": 5, "score": 0.6472943, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 63, "text_snippet": " the performance of REPLUG andREPLUG LSR improved monotonically. How- ever, a small number of documents (e.g., 10) was sufficient to achieve large performance gains.  REPLUG: Retrieval-Augmented Black-Box Language Models Perplexity 14.0016."}]}
{"case_index": 136, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"Early US models include X, S and PRO-4X, with a choice of 6-speed manual or 5-speed automatic [MASK], a choice of [...] moved from Smyrna, Tennessee, to Nissan's facility in Canton, Mississippi.\"", "gold": "transmissions", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.616, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.7885332, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 9, "text_snippet": "acility in Canton, Mississippi. Early US models include X, S and PRO-4X, with a choice of 6-speed manual or 5-speed automatic transmissions, a choice of [...] moved from Smyrna, Tennessee, to Nissan's facility in Canton, Mississippi. Early "}, {"rank": 2, "score": 0.76599866, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 8, "text_snippet": " moved from Smyrna, Tennessee, to Nissan's facility in Canton, Mississippi. Early US models include X, S and PRO-4X, with a choice of 6-speed manual or 5-speed automatic transmissions, a choice of [...] moved from Smyrna, Tennessee, to Niss"}, {"rank": 3, "score": 0.57666373, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 10, "text_snippet": "els include X, S and PRO-4X, with a choice of 6-speed manual…Retrieved documents DRECOMP (58 tokens)RetrieveCompressPrependNo retrieval (0 tokens)RALM (749 tokens)2010 ❌ ✅2015SummaryInput query xwhen did they stop making the nissan xterra?B"}, {"rank": 4, "score": 0.45492312, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 7, "text_snippet": "rieved evidence documents, and guide RALM to generate desired outputs when prepended to the input. To satisfy both efficiency and effectiveness constraints, our compressor strategically performs selective augmentation by generating an empty"}, {"rank": 5, "score": 0.33793014, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}]}
{"case_index": 137, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"We marginalize the latent documents with a top-K [MASK], either on a per-output basis (assuming the same document is responsible for all tokens) or a per-token basis (\"", "gold": "approximation", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 7.896, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6216924, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 2, "score": 0.5933928, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 19, "text_snippet": "r produces the output sequence probability for each document, which are then marginalized, pRAG-Sequence (y|x)≈∑ z∈top-k(p(·|x))pη(z|x)pθ(y|x,z) =∑ z∈top-k(p(·|x))pη(z|x)N∏ ipθ(yi|x,z,y 1:i−1) RAG-Token Model In the RAG-Token model we can d"}, {"rank": 3, "score": 0.5664563, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 37, "text_snippet": " To cast language modeling in the text-to-text framework, we consider a chunk of Nwords, and split this chunk in two sub-sequences of equal length N/2. Then, the ﬁrst sub-sequence is used as the query, and the second corresponds to the outp"}, {"rank": 4, "score": 0.5558727, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 26, "text_snippet": "t window size. To address this limitation, we adopt an ensemble strategy described as follows. Assume D′⊂ D consists of kmost relevant documents to x, ac- cording to the scoring function in Eq. (1). We prepend each document d∈ D′tox, pass t"}, {"rank": 5, "score": 0.5552065, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 25, "text_snippet": "beddings. 3.2. Input Reformulation The retrieved top- kdocuments provide rich information about the original input context xand can potentially help the LM to make a better prediction. One simple way to incorporate the retrieved documents a"}]}
{"case_index": 138, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"Both methods require updating the model [MASK] through gradient descent, which cannot be applied to black-box LMs.\"", "gold": "parameters", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 22.817, "llm_ms": 0.008, "top_contexts": [{"rank": 1, "score": 0.6155018, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 18, "text_snippet": "e to incorporate retrieved texts and pretrains the language model from scratch. Both methods require updating the model parameters through gradient descent, which cannot be applied to black-box LMs. Another line of retrieval-augmented LMs s"}, {"rank": 2, "score": 0.5836381, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 5, "text_snippet": "ting the LM’s parameters, REPLUG treats the language model as a black box and augments it with a frozen or tunable retriever. This black-box assumption makes REPLUG applicable to large LMs (i.e., >100B parameters), which are often served vi"}, {"rank": 3, "score": 0.56930494, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 8, "text_snippet": "Black-Box Language Models Figure 1, REPLUG is extremely flexible and can be used with any existing black-box LM and retrieval model. We also introduce REPLUG LSR (REPLUG with LM- Supervised Retrieval), a training scheme that can further imp"}, {"rank": 4, "score": 0.54953927, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 9, "text_snippet": "22) that adapts language models to the retriever. We use a training objective which prefers retrieving documents that improve language model perplexity, while treating the LM as a frozen, black-box scoring function. Our experiments show tha"}, {"rank": 5, "score": 0.5363096, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 71, "text_snippet": "a black box and augments it with a tuneable retrieval model. Our evaluation shows that REPLUG can be integrated with any existing language model to improve their performance  REPLUG: Retrieval-Augmented Black-Box Language Models on language"}]}
{"case_index": 139, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"By processing passages [MASK] in the en- coder, but jointly in the decoder, this method dif- fers from Min et al.\"", "gold": "independently", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.2, "hit_rank": 5, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.875, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.677938, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}, {"rank": 2, "score": 0.6450667, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 21, "text_snippet": "Raffel et al., 2019; Lewis et al., 2019). The model takes as input the question, as well as the support passages, and generates the answer. More precisely, each retrieved passage and its title are concatenated with the question, and process"}, {"rank": 3, "score": 0.6426946, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 8, "text_snippet": "3 Feb 2021  Question + Passage 1encoderQuestion + Passage 2encoderQuestion + Passage NencoderdecoderAnswerconcat… …… …Figure 2: Architecture of the Fusion-in-Decoder method. representations. Then, a sequence-to-sequence model generates the "}, {"rank": 4, "score": 0.6380284, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 16, "text_snippet": "las (Izacard et al., 2022), which fine-tunes an encoder- decoder model in conjunction with the retriever; REALM (Guu et al., 2020), a bidirectional, masked LM fine-tuned for open-domain question answering; and RAG (Retrieval-Augmented Gener"}, {"rank": 5, "score": 0.6328205, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 22, "text_snippet": "ache.org 2spacy.io 3github.com/facebookresearch/faisstion over the concatenation of the resulting repre- sentations of all the retrieved passages. The model thus performs evidence fusion in the decoder only, and we refer to it as Fusion-in-"}]}
{"case_index": 140, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"2021), and [MASK] questions based on medium-length passages (QuALITY , Pang et al.\"", "gold": "multiple-choice", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.481, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.7022209, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 52, "text_snippet": "ists of multiple-choice questions, each accompanied by context passages averaging approximately 5,000 tokens in length (Pang et al., 2022). This dataset calls for reasoning over the entire document for QA tasks, enabling us to measure the p"}, {"rank": 2, "score": 0.6725206, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 12, "text_snippet": "SPER, Dasigi et al. 2021), and multiple-choice questions based on medium-length passages (QuALITY , Pang et al. 2022).1 2 R ELATED WORK Why Retrieval? Recent advances in hardware and algorithms have indeed expanded the con- text lengths tha"}, {"rank": 3, "score": 0.6233939, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 108, "text_snippet": "nd Fok, Arman Cohan, and Kyle Lo. A controllable qa- based framework for decontextualization. arXiv preprint arXiv:2305.14772 , 2023. URL https: //arxiv.org/pdf/2305.14772.pdf . OpenAI. GPT-4 Technical Report. ArXiv , abs/2303.08774, 2023. "}, {"rank": 4, "score": 0.6199515, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 65, "text_snippet": "cks et al., 2021), BoolQ (Clark et al., 2019), SIQA (Sap et al., 2019), PIQA (Bisk et al., 2020), and Knowledge Intensive Language Tasks (KILT) (Petroni et al., 2020) (including HellaSwag, Winogrande, TQA, FEVER, NQ). We evaluate our perfor"}, {"rank": 5, "score": 0.6120495, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 53, "text_snippet": "a speed-setting. We report accuracies for both the entire test set and the HARD subset. Controlled Baseline Comparisons We first present controlled comparisons using the UnifiedQA 3B as the reader, with SBERT (Reimers & Gurevych, 2019), BM2"}]}
{"case_index": 141, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"er model encθwhich embeds sentence siand the input sequence xinto fixed- dimensional embeddings [MASK].\"", "gold": "respectively", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 9.133, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.6143702, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 20, "text_snippet": "er model encθwhich embeds sentence siand the input sequence xinto fixed- dimensional embeddings respectively. Their inner product represents how helpful it would be for the LMMto prepend sito the input xto generate y. The final summary sfro"}, {"rank": 2, "score": 0.61104345, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 26, "text_snippet": " 7: L ← L ∪ si 8: if|L|>0then 9: Ni←argTop5 sj∈L(⟨encθ(sj),encθ(xi)⟩) 10: T ← T ∪ { (xi,pi,Ni)} 11:encθ=Finetune (encθ,T) Figure 2: Learning an extractive compressor for lan- guage modeling task.Model We train a dual-encoder model encθwhich"}, {"rank": 3, "score": 0.55583334, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 19, "text_snippet": "ining compressors for conciseness and effectiveness. We summarize the key ideas for our two compressors, extractive compressors and abstractive compressor here, and discuss their training schemes formally in Section 3. 2Improving retriever "}, {"rank": 4, "score": 0.5436262, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 229, "text_snippet": "nsionality reduction on the word co-occurrence matrix, probabilistic models, explainable knowledge base method, and explicit representation in terms of the context in which words appear. Word and phrase embeddings, when used as the underlyi"}, {"rank": 5, "score": 0.5355728, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 0, "text_snippet": "RECOMP: I MPROVING RETRIEVAL -AUGMENTED LM S WITH COMPRESSION AND SELECTIVE AUGMENTATION Fangyuan Xu1, Weijia Shi2, Eunsol Choi1 Department of Computer Science 1The University of Texas at Austin 2University of Washington {fangyuan,eunsol }@"}]}
{"case_index": 142, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"(2019a) introduced a method based on hard expectation- [MASK] to tackle noisy supervision from this setting.\"", "gold": "maximization", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 9.198, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6055989, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 11, "text_snippet": " Different methods were pro- posed to tackle the setting where no gold spans are given to the system, but only the correct answer. Clark and Gardner (2018) proposed to use a global normalization over all the span corresponding to the answer"}, {"rank": 2, "score": 0.57244295, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 14, "text_snippet": " be trained using weak supervision in the form of question-answer pairs (Karpukhin et al., 2020), or pretrained using a cloze task and ﬁnetuned end-to- end (Guu et al., 2020; Lee et al., 2019). Generative question answering was mostly consi"}, {"rank": 3, "score": 0.57210326, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}, {"rank": 4, "score": 0.5525539, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 84, "text_snippet": " and pre-training paradigm for generative LLMs, leveraging cross-attention and end-to-end pre-training with 9  retrieval from a trillion-token data store, achieving strong performance. Subsequent work by Shi et al. (2024) and Lin et al. (20"}, {"rank": 5, "score": 0.5494224, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 41, "text_snippet": "lding are unforgettable experience.No RetrievalNo Retrieval Retriever Figure 2: SELF-RAGtraining examples. The left example does not require retrieval while the right one requires retrieval; thus, passages are inserted. More examples are in"}]}
{"case_index": 143, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"Without relying on external knowledge, this method obtained compet- itive results on several [MASK].\"", "gold": "benchmarks", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 9.42, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6301803, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 13, "text_snippet": "sive tasks —tasks that humans could not reasonably be expected to perform without access to an external knowledge source. Our RAG models achieve state-of-the-art results on open Natural Questions [ 29], WebQuestions [ 3] and CuratedTrec [ 2"}, {"rank": 2, "score": 0.62941694, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 3, "text_snippet": "s et al. (2020) introduced a generative model for open domain question answering. Without relying on external knowledge, this method obtained compet- itive results on several benchmarks. However, it requires models containing billions of pa"}, {"rank": 3, "score": 0.62300974, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 0, "text_snippet": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering Gautier Izacard1,2,3Edouard Grave1 1Facebook AI Research, Paris 2ENS, PSL University, Paris 3Inria, Paris gizacard|egrave@fb.com Abstract Generative mode"}, {"rank": 4, "score": 0.61142784, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 20, "text_snippet": "es with SpaCy.2In DPR, passages and questions are represented as dense vector representations, computed using two BERT networks. The ranking function is the dot product between the query and passage represen- tations. Retrieval is performed"}, {"rank": 5, "score": 0.6113287, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 31, "text_snippet": "tion answering. We observe that while conceptu- ally simple, this method outperforms existing work on the NaturalQuestion and TriviaQA benchmarks. In particular, generative models seem to perform well when evidence from multiple passages ne"}]}
{"case_index": 144, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"In this work, we in- [MASK] ways to improve large black-box language models with retrieval.\"", "gold": "vestigate", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 9.502, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6545423, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 71, "text_snippet": "a black box and augments it with a tuneable retrieval model. Our evaluation shows that REPLUG can be integrated with any existing language model to improve their performance  REPLUG: Retrieval-Augmented Black-Box Language Models on language"}, {"rank": 2, "score": 0.6374513, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 15, "text_snippet": "anguage models makes this approach infeasible. To ad- dress the challenges posed by large language models, we investigate retrieval-augmentation in the black-box setting , where users only have access to the model predictions and cannot acc"}, {"rank": 3, "score": 0.63310766, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 8, "text_snippet": "Black-Box Language Models Figure 1, REPLUG is extremely flexible and can be used with any existing black-box LM and retrieval model. We also introduce REPLUG LSR (REPLUG with LM- Supervised Retrieval), a training scheme that can further imp"}, {"rank": 4, "score": 0.6283922, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 1, "text_snippet": "mechanisms to en- code the retrieved text, REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design can be eas- ily applied to any existing retrieval and language models. Furthermore, we show t"}, {"rank": 5, "score": 0.6281522, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 11, "text_snippet": " model parameters), for both reducing LM perplexity and and im- proving in-context learning performance. We summarize our contributions as follows: •We introduce REPLUG (§3), the first retrieval- augmented language modeling framework for en"}]}
{"case_index": 145, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"Moreover, [MASK] citations for each segment with its self-assessment of whether the output is supported by the passage, leading to easier fact verification.\"", "gold": "self-ragprovides", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.806, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.60023427, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 13, "text_snippet": "erifiability. consistently retrieves a fixed number of documents for generation regardless of the retrieval necessity (e.g., the bottom figure example does not require factual knowledge) and never second visits the generation quality. Moreo"}, {"rank": 2, "score": 0.54742813, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 46, "text_snippet": "pervised data that precisely mimics the SELF- RAGinference-time process (Section 3.1). For each segment yt∈y, we run Cto assess whether additional passages could help to enhance generation. If retrieval is required, the retrieval special to"}, {"rank": 3, "score": 0.5359554, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 29, "text_snippet": "teness. In contrast, common RAG approaches retrieve passages indiscriminately, without ensuring complete support from cited sources. 3.1 P ROBLEM FORMALIZATION AND OVERVIEW Formally, given input x, we train Mto sequentially generate textual"}, {"rank": 4, "score": 0.53105354, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 8, "text_snippet": "on demand (Step 1). Subsequently, SELF-RAGconcurrently processes multiple retrieved passages, evaluating their relevance and then generating corresponding task outputs (Step 2). It then generates critique tokens to criticize its own output "}, {"rank": 5, "score": 0.53086144, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 23, "text_snippet": "ess retrieved passages before using them to prompt the LM to generate the output. SELF-RAGprocesses passages in parallel and filters out irrelevant ones through self-reflection, without relying on external models at inference. Moreover, our"}]}
{"case_index": 146, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"These models are typically trained on very large datasets and store a [MASK] amount of world or domain knowledge implicitly in their parameters.\"", "gold": "substantial", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 8.229, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.5937343, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 3, "text_snippet": "itly in their parameters. However, they are also prone to hallucination and cannot represent the full long tail of knowledge from the training corpus. Retrieval-augmented language models (Khandelwal et al., 2020; Borgeaud et al., 2022; Izac"}, {"rank": 2, "score": 0.58476967, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 5, "text_snippet": "ting the LM’s parameters, REPLUG treats the language model as a black box and augments it with a frozen or tunable retriever. This black-box assumption makes REPLUG applicable to large LMs (i.e., >100B parameters), which are often served vi"}, {"rank": 3, "score": 0.58389926, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 12, "text_snippet": "gnals, resulting in improved retrieval quality. •Evaluations on language modeling (§6), open-domain QA and MMLU demonstrate that REPLUG can im- prove the performance of various language models such as GPT, OPT and BLOOM, including very larg"}, {"rank": 4, "score": 0.58368945, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 4, "text_snippet": " tasks (Roberts et al., 2020). Nevertheless, even a large model does not contain sufficient domain- specific knowledge for particular tasks and the world continues to change, invalidating facts in the LLM. Updating the knowledge of these mo"}, {"rank": 5, "score": 0.582457, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 1, "text_snippet": "ameters, and achieve state-of-the-art results when ﬁne-tuned on down- stream NLP tasks. However, their ability to access and precisely manipulate knowl- edge is still limited, and hence on knowledge-intensive tasks, their performance lags b"}]}
{"case_index": 147, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"Both compressors are trained to improve LMs’ [MASK] on end tasks when the generated summaries are prepended to the LMs’ input, while keeping the summary concise.\"", "gold": "performance", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 9.13, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.72378993, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 2, "text_snippet": "d an abstractive compressor which generates summaries by synthesizing information from multiple documents. Both compressors are trained to improve LMs’ performance on end tasks when the generated summaries are prepended to the LMs’ input, w"}, {"rank": 2, "score": 0.6750155, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 77, "text_snippet": "by compressing retrieved documents into a concise summary or an empty sequence, facilitating selective retrieval augmentation. Prompt Compression Recent work (Wingate et al., 2022; Chevalier et al., 2023; Mu et al., 2023) proposes compressi"}, {"rank": 3, "score": 0.67467797, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 23, "text_snippet": "odels in the output should be faithful to the original input, yet the main goal is different. Instead of capturing salient information for humans readers, compressors aim to produce a concise text that are useful for a LM on an end task. In"}, {"rank": 4, "score": 0.6691333, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 3, "text_snippet": "n. We evaluate our approach on language modeling task and open domain question answering task. We achieve a compression rate of as low as 6% with minimal loss in performance for both tasks, significantly outperforming the off-the-shelf summ"}, {"rank": 5, "score": 0.64665115, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 12, "text_snippet": "ompressors implement multi-document query-focused summarization (Xu & Lapata, 2020), where we summarize retrieved evidence document set with respect to the input query. As we aim to enable RALM to generate correct output when summary is pre"}]}
{"case_index": 148, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"ting the LM’s [MASK], REPLUG treats the language model as a black box and augments it with a frozen or tunable retriever.\"", "gold": "parameters", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.999, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.75158465, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 5, "text_snippet": "ting the LM’s parameters, REPLUG treats the language model as a black box and augments it with a frozen or tunable retriever. This black-box assumption makes REPLUG applicable to large LMs (i.e., >100B parameters), which are often served vi"}, {"rank": 2, "score": 0.7324652, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 71, "text_snippet": "a black box and augments it with a tuneable retrieval model. Our evaluation shows that REPLUG can be integrated with any existing language model to improve their performance  REPLUG: Retrieval-Augmented Black-Box Language Models on language"}, {"rank": 3, "score": 0.72888327, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 8, "text_snippet": "Black-Box Language Models Figure 1, REPLUG is extremely flexible and can be used with any existing black-box LM and retrieval model. We also introduce REPLUG LSR (REPLUG with LM- Supervised Retrieval), a training scheme that can further imp"}, {"rank": 4, "score": 0.7183747, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 1, "text_snippet": "mechanisms to en- code the retrieved text, REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design can be eas- ily applied to any existing retrieval and language models. Furthermore, we show t"}, {"rank": 5, "score": 0.715779, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 6, "text_snippet": "PIs. Internal representations of such models are not exposed and fine-tuning is not supported. In this work, we introduce REPLUG (Retrieve and Plug ), a new retrieval-augmented LM framework where the lan- guage model is viewed as a black bo"}]}
{"case_index": 149, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"This is particularly relevant for thematic questions that require integrating knowledge from multiple parts of a text, such as [MASK] an entire book, as in the NarrativeQA dataset (Ko ˇcisk`y et al., 2018).\"", "gold": "understanding", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.242, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6678375, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 7, "text_snippet": "re. This is particularly relevant for thematic questions that require integrating knowledge from multiple parts of a text, such as understanding an entire book, as in the NarrativeQA dataset (Ko ˇcisk`y et al., 2018). Consider the fairy tal"}, {"rank": 2, "score": 0.6124424, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 147, "text_snippet": "ctly as a leaf node or indirectly as part of a summary from a higher layer. ”The first question we examine is “How does Cinderella find a happy ending?”, a multi-hop question best answered by synthesizing information from various text segme"}, {"rank": 3, "score": 0.60425556, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 150, "text_snippet": "estion that requires holistic understanding of the entire text. The text retrieved by RAPTOR and DPR for this question is shown in Table 13. The text retrieved by RAPTOR contains short descriptions of all the major parts of the story, where"}, {"rank": 4, "score": 0.59514105, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 50, "text_snippet": " the full texts of books and movie transcripts, totaling 1,572 documents (Ko ˇcisk`y et al., 2018; Wu et al., 2021). The NarrativeQA-Story task requires a comprehensive understanding of the entire narrative in order to accurately answer its"}, {"rank": 5, "score": 0.59464395, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 153, "text_snippet": "ecause the information that it retrieves is more relevant and exhaustive, allowing for better performance on downstream tasks. We also created a 2600-word story along with questions about its narrative and theme. An excerpt from the story i"}]}
{"case_index": 150, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"provide rigorous validation [MASK] diverse long-context tasks, including RAG, multi-turn conversations, and long document summarization, spanning a wide range of datasets.\"", "gold": "ofrefragacross", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.033, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.64419353, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 4, "text_snippet": " provide rigorous validation ofREFRAGacross diverse long-context tasks, including RAG, multi-turn conversations, and long document summarization, spanning a wide range of datasets. Experimental results confirm thatREFRAGdelivers substantial"}, {"rank": 2, "score": 0.6073777, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 57, "text_snippet": "en studied in RLHF (Touvron et al., 2023; Wu et al., 2023), which often requires training to change models’ behaviors. S ELF-RAGtailors an LM with no additional training. 4 E XPERIMENTS 4.1 T ASKS AND DATASETS We conduct evaluations of our "}, {"rank": 3, "score": 0.60722655, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 16, "text_snippet": "nsing most chunks for the query in RAG settings. We provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training and many real word long-context applications including RAG, multi-turn conversation with RA"}, {"rank": 4, "score": 0.59448826, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 13, "text_snippet": "sive tasks —tasks that humans could not reasonably be expected to perform without access to an external knowledge source. Our RAG models achieve state-of-the-art results on open Natural Questions [ 29], WebQuestions [ 3] and CuratedTrec [ 2"}, {"rank": 5, "score": 0.5874525, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 51, "text_snippet": "ts to extensively demon- strate CRAG ’s adaptability to RAG-based ap- proaches and its generalizability across both short- and long-form generation tasks. 5.1 Tasks, Datasets and Metrics CRAG was evaluated on four datasets, including PopQA "}]}
{"case_index": 151, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"of retrieved passages prepended to input, or pre-train a retriever and LM jointly, followed by few- shot [MASK] on task datasets (Izacard et al., 2022b).\"", "gold": "fine-tuning", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.062, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.7146468, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 19, "text_snippet": "rieved text passages (Guu et al., 2020; Lewis et al., 2020), leading to large improvements in knowledge-intensive tasks after fine-tuning or used with off-the-shelf LMs (Ram et al., 2023). A more recent work (Luo et al., 2023) instruction-t"}, {"rank": 2, "score": 0.6809741, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 22, "text_snippet": " the retriever and LM on instruction-tuning datasets in two steps. While we also train our model on diverse instruction-following datasets, SELF-RAGenables retrieval on demand and selection of the best possible model output via fine-grained"}, {"rank": 3, "score": 0.6653243, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 84, "text_snippet": " and pre-training paradigm for generative LLMs, leveraging cross-attention and end-to-end pre-training with 9  retrieval from a trillion-token data store, achieving strong performance. Subsequent work by Shi et al. (2024) and Lin et al. (20"}, {"rank": 4, "score": 0.66257, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 41, "text_snippet": "lding are unforgettable experience.No RetrievalNo Retrieval Retriever Figure 2: SELF-RAGtraining examples. The left example does not require retrieval while the right one requires retrieval; thus, passages are inserted. More examples are in"}, {"rank": 5, "score": 0.64472336, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 21, "text_snippet": " al., 2023a; Gao et al., 2023). We introduce a method to train an arbitrary LM to learn to use retrieval on-demand for diverse instruction-following queries and introduce controlled generation guided by reflections tokens to further improve"}]}
{"case_index": 152, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"This refinement operation involves knowledge decom- position, filter, and [MASK] (Section 4.4).\"", "gold": "recomposition", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 7.918, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.5351622, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 45, "text_snippet": "owledge Refinement Given a retrieved relevant document, a decompose- then-recompose knowledge refinement method is designed to further extract the most critical knowledge strips in it. To obtain fine-grained retrieval results, we segmented "}, {"rank": 2, "score": 0.5201925, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 1, "text_snippet": "e reliance on the paramet- ric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and inco"}, {"rank": 3, "score": 0.5186218, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 28, "text_snippet": "sition, filter, and recomposition (Section 4.4). If the action Incorrect is triggered, the retrieved documents will be discarded. Instead, web searches are resorted to and regarded as complementary knowledge sources for corrections (Section"}, {"rank": 4, "score": 0.51779836, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 5, "score": 0.5177747, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 44, "text_snippet": "ce. Discussion Preliminary experiments of employ- ing only the Correct andIncorrect actions show that the efficacy of CRAG was easily affected by the accuracy of the retrieval evaluator. The reason might be the distinct knowledge switch for"}]}
{"case_index": 153, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"This process differs from [MASK] RAG (Figure 1 left), which 1Our code and trained models are available at https://selfrag.github.io/ .\"", "gold": "conventional", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.457, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6829002, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 8, "text_snippet": "on demand (Step 1). Subsequently, SELF-RAGconcurrently processes multiple retrieved passages, evaluating their relevance and then generating corresponding task outputs (Step 2). It then generates critique tokens to criticize its own output "}, {"rank": 2, "score": 0.63863134, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 28, "text_snippet": "F-RAGis a framework that enhances the quality and factuality of an LLM through retrieval and self-reflection, without sacrificing LLM’s original creativity and versatility. Our end-to-end training lets an LM Mgenerate text informed by retri"}, {"rank": 3, "score": 0.63528657, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 27, "text_snippet": "n (reasoning path consistency) and without retrieval. Recent work on LLM refinement (Dhuliawala et al., 2023; Madaan et al., 2023; Paul et al., 2023) prompts a model to generate task output, natural language feedback and refined task output"}, {"rank": 4, "score": 0.62504053, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 80, "text_snippet": "rained on the same instruction-output pairs as SELF-RAGwithout retrieval or self-reflection and is retrieval-augmented at test time only, lags behind S ELF-RAG. This result indicates S ELF-RAGgains are not solely from training data and demo"}, {"rank": 5, "score": 0.60841876, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 3, "text_snippet": "e phase, enabling it to tailor its behavior to diverse task requirements. Experiments show that SELF- RAG(7B and 13B parameters) significantly outperforms state-of-the-art LLMs and retrieval-augmented models on a diverse set of tasks. Speci"}]}
{"case_index": 154, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"To address this, we design an indexing and retrieval system that uses a tree [MASK] to cap\"", "gold": "structure", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.2, "hit_rank": 5, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.707, "llm_ms": 0.008, "top_contexts": [{"rank": 1, "score": 0.6355249, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 8, "text_snippet": " indexing and retrieval system that uses a tree structure to capture both high-level and low-level details about a text. As shown in Figure 1, our system, RAPTOR, clusters chunks of text, generates text summaries of those clusters, and then"}, {"rank": 2, "score": 0.60341614, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 0, "text_snippet": "Published as a conference paper at ICLR 2024 RAPTOR: R ECURSIVE ABSTRACTIVE PROCESSING FOR TREE-ORGANIZED RETRIEVAL Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D. Manning Stanford University psarthi@cs."}, {"rank": 3, "score": 0.60122377, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 73, "text_snippet": "an effectively handle a wider range of questions, from higher-order thematic queries to detail-oriented questions. Detailed results for additional stories and an ablation study on layer contributions can be found in Appendix I. 5 C ONCLUSIO"}, {"rank": 4, "score": 0.58730626, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 36, "text_snippet": "by RAPTOR: tree traversal and collapsed tree. These methods offer unique ways of traversing the multi-layered RAPTOR tree to retrieve relevant information, each with its own advantages and trade-offs. We provide the pseudocode of both metho"}, {"rank": 5, "score": 0.58220696, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 7, "text_snippet": "re. This is particularly relevant for thematic questions that require integrating knowledge from multiple parts of a text, such as understanding an entire book, as in the NarrativeQA dataset (Ko ˇcisk`y et al., 2018). Consider the fairy tal"}]}
{"case_index": 155, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"Moreover, our [MASK] mechanism also evaluates other aspects of the model output quality including factuality.\"", "gold": "self-reflection", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 11.808, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.60131085, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 13, "text_snippet": "erifiability. consistently retrieves a fixed number of documents for generation regardless of the retrieval necessity (e.g., the bottom figure example does not require factual knowledge) and never second visits the generation quality. Moreo"}, {"rank": 2, "score": 0.5907979, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 30, "text_snippet": "evance”, “faithfulness”, and “answer relevance” (RAGAS, Es et al. 2023). Lacking a gold standard for evaluation, one can quantify relative performance for a given criterion by prompting the LLM to compare generations from two different comp"}, {"rank": 3, "score": 0.58878666, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 95, "text_snippet": "higher S&P scores on short-form PopQA, which is consistent with Menick et al. (2022). Human annotators also find ISRELand ISSUPreflection token predictions are mostly aligned with their assessments. Appendix Table 6 shows several annotated "}, {"rank": 4, "score": 0.5864486, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 27, "text_snippet": "n (reasoning path consistency) and without retrieval. Recent work on LLM refinement (Dhuliawala et al., 2023; Madaan et al., 2023; Paul et al., 2023) prompts a model to generate task output, natural language feedback and refined task output"}, {"rank": 5, "score": 0.5832149, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 6, "text_snippet": "y trained to leverage and follow facts from provided passages. This work introduces Self-Reflective Retrieval-augmented Generation ( SELF-RAG)to improve an LLM’s generation quality, including its factual accuracy without hurting its versati"}]}
{"case_index": 156, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"•Thorough downstream experiments in few-shot settings, demonstrating [MASK] results on few-shot NaturalQuestions (+2.8%), TriviaQA (+3.3%), FEVER (+5.1%), and results on par or stronger than models with 15 ×more parameters on MMLU.\"", "gold": "state-of-the-art", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.655, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.737183, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 13, "text_snippet": "ask at hand. •Thorough downstream experiments in few-shot settings, demonstrating state-of-the-art results on few-shot NaturalQuestions (+2.8%), TriviaQA (+3.3%), FEVER (+5.1%), and results on par or stronger than models with 15 ×more param"}, {"rank": 2, "score": 0.69680893, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 154, "text_snippet": "odel. By jointly pre-training the retriever module and the language model, we show that Atlashas strong few-shot learning capabilities on a wide range of knowledge intensive tasks, including NaturalQuestions, TriviaQA, FEVER, 8 KILT tasks a"}, {"rank": 3, "score": 0.6689347, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 1, "text_snippet": " Abstract Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter count"}, {"rank": 4, "score": 0.65166885, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 101, "text_snippet": "st likely of the 4 letters at test time. Full technical details can be found in appendix A.1. Performance vs Parameters. We start by comparing Atlasto closed-book models of diﬀerent sizes for 5-shot, 5-shot multitask and the full setting, a"}, {"rank": 5, "score": 0.6511343, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 12, "text_snippet": "ntributions: •A thorough study on how to design and train retrieval-augmented language models, with a focus on downstream few-shot learning and sample eﬃciency. •The ﬁndings of this study lead to a retrieval-augmented language model, called"}]}
{"case_index": 157, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"The relevance score is quantified into a total of three confidence degrees and then triggered the [MASK] actions: { Correct ,Incorrect , Ambiguous } (Section 4.3).\"", "gold": "corresponding", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 13.169, "llm_ms": 0.008, "top_contexts": [{"rank": 1, "score": 0.60975933, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 39, "text_snippet": "ngly where the upper and lower thresholds are set. If the confidence score is higher than the upper threshold, the retrieved document is identified as Correct , while identified as Incorrect if below the lower threshold. Otherwise, a more s"}, {"rank": 2, "score": 0.6039446, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 27, "text_snippet": "or is constructed to estimate the relevance score of retrieved documents to the input query (Sec- tion 4.2). The relevance score is quantified into a total of three confidence degrees and then triggered the corresponding actions: { Correct "}, {"rank": 3, "score": 0.57851017, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 42, "text_snippet": " fabricated facts. Therefore, we need to seek new sources of knowledge for correction. Here, web search is introduced to search from the Internet as elaborated in Section 4.5. This corrective action helps overcome the embarrassing challenge"}, {"rank": 4, "score": 0.56956625, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 28, "text_snippet": "sition, filter, and recomposition (Section 4.4). If the action Incorrect is triggered, the retrieved documents will be discarded. Instead, web searches are resorted to and regarded as complementary knowledge sources for corrections (Section"}, {"rank": 5, "score": 0.5589905, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 68, "text_snippet": "hange in terms of accuracy. Specifically, when the action Correct orIncorrect was removed, it was merged with Ambiguous so that the proportion that originally triggered Correct orIncorrect would trigger Ambiguous . On the other hand, when t"}]}
{"case_index": 158, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"The challenge, however, arises when the volume of data requires a RAG approach, since vector RAG approaches are unable to support [MASK] over an entire corpus.\"", "gold": "sensemaking", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 49.739, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.65373695, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 8, "text_snippet": "nce analysis (Ranade and Joshi, 2023). Given a sensemaking query and a text with an implicit and interconnected set of concepts, an LLM can generate a summary that answers the query. The challenge, however, arises when the volume of data re"}, {"rank": 2, "score": 0.652252, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 16, "text_snippet": " may use alternative retrieval mechanisms, we collectively refer to the family of conventional approaches as vector RAG . GraphRAG contrasts with vector RAG in its ability to answer queries that require global sensemaking over the entire da"}, {"rank": 3, "score": 0.649706, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 29, "text_snippet": "lements (nodes, edges, covariates) that the LLM can summarize in parallel at both indexing time and query time. The “global answer” to a given query is produced using a final round of query-focused summarization over all community summaries"}, {"rank": 4, "score": 0.6463717, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 6, "text_snippet": "2006; Laskar et al., 2020; Yao et al., 2017). This conventional approach, which we collectively call vector RAG , works well for queries that can be answered with information localized within a small set of records. However, vector RAG appr"}, {"rank": 5, "score": 0.6251701, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 96, "text_snippet": "tigate these downstream risks for questions of a global nature, which might otherwise be answered by samples of retrieved facts falsely presented as global summaries. 7 Conclusion We have presented GraphRAG, a RAG approach that combines kno"}]}
{"case_index": 159, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"Atlasachieves strong downstream performance in both few-shot and [MASK] settings.\"", "gold": "resource-rich", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 9.152, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.66749847, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 12, "text_snippet": "ntributions: •A thorough study on how to design and train retrieval-augmented language models, with a focus on downstream few-shot learning and sample eﬃciency. •The ﬁndings of this study lead to a retrieval-augmented language model, called"}, {"rank": 2, "score": 0.6614458, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 13, "text_snippet": "ask at hand. •Thorough downstream experiments in few-shot settings, demonstrating state-of-the-art results on few-shot NaturalQuestions (+2.8%), TriviaQA (+3.3%), FEVER (+5.1%), and results on par or stronger than models with 15 ×more param"}, {"rank": 3, "score": 0.6466036, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 154, "text_snippet": "odel. By jointly pre-training the retriever module and the language model, we show that Atlashas strong few-shot learning capabilities on a wide range of knowledge intensive tasks, including NaturalQuestions, TriviaQA, FEVER, 8 KILT tasks a"}, {"rank": 4, "score": 0.64610064, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 101, "text_snippet": "st likely of the 4 letters at test time. Full technical details can be found in appendix A.1. Performance vs Parameters. We start by comparing Atlasto closed-book models of diﬀerent sizes for 5-shot, 5-shot multitask and the full setting, a"}, {"rank": 5, "score": 0.6437572, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 1, "text_snippet": " Abstract Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter count"}]}
{"case_index": 160, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"hat retrieval with recursive summaries offers significant improvements over tra- ditional [MASK] LMs on several tasks.\"", "gold": "retrieval-augmented", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.591, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.7345942, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 2, "text_snippet": "hat retrieval with recursive summaries offers significant improvements over tra- ditional retrieval-augmented LMs on several tasks. On question-answering tasks that involve complex, multi-step reasoning, we show state-of-the-art results; fo"}, {"rank": 2, "score": 0.6558734, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 1, "text_snippet": "rom a retrieval corpus, limiting holistic under- standing of the overall document context. We introduce the novel approach of recursively embedding, clustering, and summarizing chunks of text, constructing a tree with differing levels of su"}, {"rank": 3, "score": 0.6396302, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 0, "text_snippet": "Published as a conference paper at ICLR 2024 RAPTOR: R ECURSIVE ABSTRACTIVE PROCESSING FOR TREE-ORGANIZED RETRIEVAL Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D. Manning Stanford University psarthi@cs."}, {"rank": 4, "score": 0.6317066, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 73, "text_snippet": "an effectively handle a wider range of questions, from higher-order thematic queries to detail-oriented questions. Detailed results for additional stories and an ablation study on layer contributions can be found in Appendix I. 5 C ONCLUSIO"}, {"rank": 5, "score": 0.60692376, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 67, "text_snippet": "ring approaches, which allows it to capture a range of information, from general themes to specific details, contributing to its overall strong performance. 4.1 C ONTRIBUTION OF THE TREE STRUCTURE We examine the contribution of each layer o"}]}
{"case_index": 161, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"(2020) in- troduced retrieval augmented [MASK] models for open domain question answering.\"", "gold": "generative", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.2, "hit_rank": 5, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.852, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.7009876, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 16, "text_snippet": "las (Izacard et al., 2022), which fine-tunes an encoder- decoder model in conjunction with the retriever; REALM (Guu et al., 2020), a bidirectional, masked LM fine-tuned for open-domain question answering; and RAG (Retrieval-Augmented Gener"}, {"rank": 2, "score": 0.69432306, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}, {"rank": 3, "score": 0.68792003, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 15, "text_snippet": " that answers do not correspond to spans in support documents, thus requiring ab- stractive models. Raffel et al. (2019) showed that generative models are competitive for reading com- prehension tasks such as SQuAD (Rajpurkar et al., 2016),"}, {"rank": 4, "score": 0.687558, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 0, "text_snippet": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering Gautier Izacard1,2,3Edouard Grave1 1Facebook AI Research, Paris 2ENS, PSL University, Paris 3Inria, Paris gizacard|egrave@fb.com Abstract Generative mode"}, {"rank": 5, "score": 0.6601148, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 16, "text_snippet": "0) and Lewis et al. (2020) in- troduced retrieval augmented generative models for open domain question answering. Our approach differs from these works by how the generative model processes the retrieved passages. This al- lows to scale to "}]}
{"case_index": 162, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"Given a question, each community summary is used to generate a partial response, before all partial responses are again [MASK] in a final response to the user.\"", "gold": "summarized", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 18.361, "llm_ms": 0.008, "top_contexts": [{"rank": 1, "score": 0.6507586, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 47, "text_snippet": "t within the context window. 3.1.6 Community Summaries →Community Answers →Global Answer Given a user query, the community summaries generated in the previous step can be used to generate a final answer in a multi-stage process. The hierarc"}, {"rank": 2, "score": 0.6241021, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 29, "text_snippet": "lements (nodes, edges, covariates) that the LLM can summarize in parallel at both indexing time and query time. The “global answer” to a given query is produced using a final round of query-focused summarization over all community summaries"}, {"rank": 3, "score": 0.61625224, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 48, "text_snippet": "mmunity structure offers the best balance of summary detail and scope for general sensemaking questions (evaluated in section 4). For a given community level, the global answer to any user query is generated as follows: •Prepare community s"}, {"rank": 4, "score": 0.6009959, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 49, "text_snippet": "lly lost) in a single context window. •Map community answers . Intermediate answers are generated in parallel. The LLM is also asked to generate a score between 0-100 indicating how helpful the generated answer is in answering the target qu"}, {"rank": 5, "score": 0.59546506, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 10, "text_snippet": "cted communities, with summaries at higher levels of the hierarchy recursively incorporating lower-level summaries. Together, these community summaries provide global descriptions and insights over the corpus. Finally, GraphRAG answers quer"}]}
{"case_index": 163, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"We propose compressors: (1) Extractive compressor which selects relevant sentences from retrieved document set; (2) Abstractive compressor which generates a summary [MASK] information from multiple retrieved documents.\"", "gold": "synthesizing", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.859, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.765903, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 2, "text_snippet": "d an abstractive compressor which generates summaries by synthesizing information from multiple documents. Both compressors are trained to improve LMs’ performance on end tasks when the generated summaries are prepended to the LMs’ input, w"}, {"rank": 2, "score": 0.7604885, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 11, "text_snippet": " prepending it as input to a language model at inference time. The compressed summary guides the LM to generate the correct answer, while significantly reducing the computation costs required to encode the documents. We propose compressors:"}, {"rank": 3, "score": 0.7381476, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 19, "text_snippet": "ining compressors for conciseness and effectiveness. We summarize the key ideas for our two compressors, extractive compressors and abstractive compressor here, and discuss their training schemes formally in Section 3. 2Improving retriever "}, {"rank": 4, "score": 0.719774, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 12, "text_snippet": "ompressors implement multi-document query-focused summarization (Xu & Lapata, 2020), where we summarize retrieved evidence document set with respect to the input query. As we aim to enable RALM to generate correct output when summary is pre"}, {"rank": 5, "score": 0.71419215, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 21, "text_snippet": "Compressor We train an encoder-decoder model encdec θto serve as an abstractive compressor, which takes the input sequence xand a concatenation of retrieved document set D [d1;d2;...dN]) and output a summary s. Although we do not have human"}]}
{"case_index": 164, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"For language generation tasks, we ﬁnd that RAG models generate more speciﬁc, diverse and factual language than a [MASK] parametric-only seq2seq baseline.\"", "gold": "state-of-the-art", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.587, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.73331785, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 4, "text_snippet": "architectures. For language generation tasks, we ﬁnd that RAG models generate more speciﬁc, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline. 1 Introduction Pre-trained neural language models have been s"}, {"rank": 2, "score": 0.6553498, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 3, "text_snippet": "cessed with a pre-trained neural retriever. We com- pare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. We ﬁne-tune and "}, {"rank": 3, "score": 0.65400696, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 4, "text_snippet": " for long-form generations relative to these models.1 1 I NTRODUCTION State-of-the-art LLMs continue to struggle with factual errors (Mallen et al., 2023; Min et al., 2023) despite their increased model and data scale (Ouyang et al., 2022)."}, {"rank": 4, "score": 0.65173477, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 9, "text_snippet": "y to the “workhorse of NLP,” i.e. sequence-to-sequence (seq2seq) models. We endow pre-trained, parametric-memory generation models with a non-parametric memory through a general-purpose ﬁne-tuning approach which we refer to as retrieval-aug"}, {"rank": 5, "score": 0.6485779, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 2, "text_snippet": "anism to explicit non-parametric memory have so far been only investigated for extractive downstream tasks. We explore a general-purpose ﬁne-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trained parametri"}]}
{"case_index": 165, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"of input sequence xi and candidate sentences sj, we measure 3Recent work (Zhang et al., 2022) shows that extractive approach does not always preserve [MASK], but such cases are still rare compared to abstractive approaches which can easily hallucinate.\"", "gold": "faithfulness", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 11.197, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.5860861, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 28, "text_snippet": " of input sequence xi and candidate sentences sj, we measure 3Recent work (Zhang et al., 2022) shows that extractive approach does not always preserve faithfulness, but such cases are still rare compared to abstractive approaches which can "}, {"rank": 2, "score": 0.57536644, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 61, "text_snippet": "extractive and abstractive, shows promising performances. On NQ and TQA, the abstractive approach is more effective. On NQ, it achieves a compression ratio of 5% tokens while losing 2 EM points compared to prepending full documents. On TQA,"}, {"rank": 3, "score": 0.5710481, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 71, "text_snippet": ". We also observe that model can be easily dis- tracted by irrelevant contexts, copying from a document span even when it does not contain gold answer, echoing findings from prior work (Shi et al., 2023a). Prepending top 5 documents has 8  "}, {"rank": 4, "score": 0.56359017, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 20, "text_snippet": "ets but can sometimes be a lossy means of compression. The recursive-abstractive summarization model by Wu et al. (2021) employs task decomposition to summarize smaller text chunks, which are later integrated to form summaries of larger sec"}, {"rank": 5, "score": 0.5607193, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 20, "text_snippet": "er model encθwhich embeds sentence siand the input sequence xinto fixed- dimensional embeddings respectively. Their inner product represents how helpful it would be for the LMMto prepend sito the input xto generate y. The final summary sfro"}]}
{"case_index": 166, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"Next, it partitions the graph into a hierarchy of communities of closely related entities, before using an LLM to generate [MASK] summaries.\"", "gold": "community-level", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 12.434, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.7198009, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 9, "text_snippet": "tirety of a large text corpus. GraphRAG first uses an LLM to construct a knowledge graph, where nodes correspond to key entities in the corpus and edges represent relationships between those entities. Next, it partitions the graph into a hi"}, {"rank": 2, "score": 0.6730876, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 44, "text_snippet": "ummaries at one level looking for general themes of interest, then read linked reports at a lower level that provide additional details for each subtopic. Here, however, we focus on their utility as part of a graph-based index used for answ"}, {"rank": 3, "score": 0.6663469, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 29, "text_snippet": "lements (nodes, edges, covariates) that the LLM can summarize in parallel at both indexing time and query time. The “global answer” to a given query is produced using a final round of query-focused summarization over all community summaries"}, {"rank": 4, "score": 0.6639417, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 10, "text_snippet": "cted communities, with summaries at higher levels of the hierarchy recursively incorporating lower-level summaries. Together, these community summaries provide global descriptions and insights over the corpus. Finally, GraphRAG answers quer"}, {"rank": 5, "score": 0.6584651, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 42, "text_snippet": "hical manner, recursively detecting sub-communities within each detected community until reaching leaf communities that can no longer be partitioned. 5  Each level of this hierarchy provides a community partition that covers the nodes of th"}]}
{"case_index": 167, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"We consider the sentence with the highest log [MASK] as a positive example pi(line 3).\"", "gold": "likelihood", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 11.652, "llm_ms": 0.008, "top_contexts": [{"rank": 1, "score": 0.5950476, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 29, "text_snippet": "g pM(y|[sj;xi]), log likelihood assigned to target output according to LM Mwhen candidate sentence is prepended to the input. We consider the sentence with the highest log likelihood as a positive example pi(line 3). To construct negative e"}, {"rank": 2, "score": 0.5077419, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 28, "text_snippet": " of input sequence xi and candidate sentences sj, we measure 3Recent work (Zhang et al., 2022) shows that extractive approach does not always preserve faithfulness, but such cases are still rare compared to abstractive approaches which can "}, {"rank": 3, "score": 0.4976372, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 45, "text_snippet": "e, with the highest scored document closest to the question (Si et al., 2022). We do not include the retrieved documents for in-context examples as it did not improve performance. An example input can be found in Appendix Table 7. 4.3 B ASE"}, {"rank": 4, "score": 0.48436067, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 25, "text_snippet": "b; Ram et al., 2023). Input: Base LM M, Compressor encθ, Training data {xi,Si,yi}T 1where xiis input, Si={sj}n 1is a set of candidate sentences from the retrieved documents for xi, yiis the target answer, and score threshold ϵ. Output: An u"}, {"rank": 5, "score": 0.47776562, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 220, "text_snippet": "f the correct answer: [MASK_0] {correct answer option letter} This format closely matches the format of MLM pre-training objective, aiding few-shot learning. When training, we permute the order of the answer options, i.e. shuﬄing which answ"}]}
{"case_index": 168, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"vector embeddings presents a challenge for traditional GMMs, as dis- tance metrics may behave poorly when used to measure similarity in [MASK] spaces (Ag- garwal et al., 2001).\"", "gold": "high-dimensional", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.918, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.6555733, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 29, "text_snippet": "vector embeddings presents a challenge for traditional GMMs, as dis- tance metrics may behave poorly when used to measure similarity in high-dimensional spaces (Ag- garwal et al., 2001). To mitigate this, we employ Uniform Manifold Approxim"}, {"rank": 2, "score": 0.57749975, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 28, "text_snippet": " Published as a conference paper at ICLR 2024 Given a set of Ntext segments, each represented as a d-dimensional dense vector embedding, the likelihood of a text vector, x, given its membership in the kthGaussian distribution, is denoted by"}, {"rank": 3, "score": 0.56885713, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 218, "text_snippet": "d to be measured. Evidence : Word embedding Word embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to v"}, {"rank": 4, "score": 0.5165316, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 228, "text_snippet": "e Word embedding Word embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers. Con"}, {"rank": 5, "score": 0.5101941, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 217, "text_snippet": "ify the output sentence, respond with [Retrieval]. Please provide explanations for your judgments. Instruction Explain the use of word embeddings in Natural Language Processing. Preceding sentences Word embeddings are one of the most powerf"}]}
{"case_index": 169, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"This flexibility is essen- tial because individual text segments often contain [MASK] relevant to various topics, thereby warranting their inclusion in multiple summaries.\"", "gold": "information", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 1.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 11.935, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.5509308, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 2, "score": 0.541201, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 27, "text_snippet": "uiring a fixed number of clusters. This flexibility is essen- tial because individual text segments often contain information relevant to various topics, thereby warranting their inclusion in multiple summaries. Our clustering algorithm is "}, {"rank": 3, "score": 0.5311118, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 29, "text_snippet": "lements (nodes, edges, covariates) that the LLM can summarize in parallel at both indexing time and query time. The “global answer” to a given query is produced using a final round of query-focused summarization over all community summaries"}, {"rank": 4, "score": 0.5282761, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 19, "text_snippet": "ortant context making them difficult to read or even misleading. (Cohan & Goharian, 2017; Newman et al., 2023; Zhang et al., 2023). Recursive summarization as Context Summarization techniques provide a condensed view of documents, enabling "}, {"rank": 5, "score": 0.52587026, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 72, "text_snippet": "e from the leaf nodes and each upper layer, as well as from different contiguous subsets of the layers. We show findings specific to one story in Table 8, revealing that a full-tree search, utilizing all layers, outperformed retrieval strat"}]}
{"case_index": 170, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"CRAG is plug-and-play and [MASK] implemented into RAG (Lewis et al., 2020) and Self-RAG (Asai et al., 2024) for demonstrating its adaptability to RAG-based approa\"", "gold": "experimentally", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 12.859, "llm_ms": 0.008, "top_contexts": [{"rank": 1, "score": 0.6528449, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 13, "text_snippet": "elpful for RAG, a decompose-then-recompose algorithm is meticulously crafted throughout the retrieval and utilization process. This algorithm ensures the refinement of retrieved information, optimizing the extraction of key insights and min"}, {"rank": 2, "score": 0.6334814, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 132, "text_snippet": "or: AsCRAG is a plug-and-play method, all generation models that can be uti- lized in RAG fit our approach as well. To be consistent with baselines for comparison, we adopted LLaMA2 (Touvron et al., 2023b) for the generation. We first intro"}, {"rank": 3, "score": 0.623495, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 1, "text_snippet": "evitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Al- though retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heav"}, {"rank": 4, "score": 0.6196132, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 133, "text_snippet": "nched model, SelfRAG-LLaMA2-7b , as a new generator to be consistent with their work and study the specific improvement of our method. Self-CRAG: To demonstrate that our plug-and- play approach can be utilized in other concurrent studies, w"}, {"rank": 5, "score": 0.611516, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 14, "text_snippet": "l., 2024) for demonstrating its adaptability to RAG-based approaches. Results onfour datasets of PopQA (Mallen et al., 2023), Biog- raphy (Min et al., 2023), Pub Health (Zhang et al., 2023a), and Arc-Challenge (Bhakthavatsalam et al., 2021)"}]}
{"case_index": 171, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"Experiments on four datasets covering short- and long-form generation tasks show that CRAG can [MASK] improve the performance of RAG-based approaches.1 1 Introduction Large language models (LLMs) have attracted increasing attention and exhibited impressive\"", "gold": "significantly", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.4, "llm_ms": 0.008, "top_contexts": [{"rank": 1, "score": 0.6825212, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 1, "text_snippet": "evitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Al- though retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heav"}, {"rank": 2, "score": 0.6666037, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 3, "text_snippet": " decompose-then- recompose algorithm is designed for retrieved documents to selectively focus on key infor- mation and filter out irrelevant information in them. CRAG is plug-and-play and can be seamlessly coupled with various RAG-based app"}, {"rank": 3, "score": 0.6651302, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 85, "text_snippet": "s well as general- izability across short- and long-form generation tasks. While we primarily proposed to improve the RAG framework from a corrective perspective and CRAG can be seamlessly coupled with various RAG-based approaches, fine-tun"}, {"rank": 4, "score": 0.6647708, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 4, "text_snippet": " for long-form generations relative to these models.1 1 I NTRODUCTION State-of-the-art LLMs continue to struggle with factual errors (Mallen et al., 2023; Min et al., 2023) despite their increased model and data scale (Ouyang et al., 2022)."}, {"rank": 5, "score": 0.6626236, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 16, "text_snippet": "cient utilization of retrieved documents. 3) Experimental results extensively demonstrate CRAG ’s adaptability to RAG-based approaches and its generalizability across short- and long-form generation tasks. 2 Related Work Hallucinations of L"}]}
{"case_index": 172, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"We com- pare two RAG [MASK], one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token.\"", "gold": "formulations", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 9.286, "llm_ms": 0.008, "top_contexts": [{"rank": 1, "score": 0.7342975, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 3, "text_snippet": "cessed with a pre-trained neural retriever. We com- pare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. We ﬁne-tune and "}, {"rank": 2, "score": 0.6681431, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 15, "text_snippet": " update the models’ knowledge as the world changes.1 2 Methods We explore RAG models, which use the input sequence xto retrieve text documents zand use them as additional context when generating the target sequence y. As shown in Figure 1, "}, {"rank": 3, "score": 0.6669534, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 29, "text_snippet": "teness. In contrast, common RAG approaches retrieve passages indiscriminately, without ensuring complete support from cited sources. 3.1 P ROBLEM FORMALIZATION AND OVERVIEW Formally, given input x, we train Mto sequentially generate textual"}, {"rank": 4, "score": 0.6649146, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 17, "text_snippet": "rator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a distribution over generated text. In one approach, RAG-Sequence , the mo"}, {"rank": 5, "score": 0.6609758, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 1, "text_snippet": "e reliance on the paramet- ric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and inco"}]}
{"case_index": 173, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"By exploiting this attention sparsity structure, we demonstrate a30 .85×the [MASK] acceleration (3 .75×improvement to previous work) without loss in perplexity.\"", "gold": "time-to-first-token", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 30.87, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.7116146, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 3, "text_snippet": " on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30 .85×the time-t"}, {"rank": 2, "score": 0.6324421, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 12, "text_snippet": "is information is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-a"}, {"rank": 3, "score": 0.6229122, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 96, "text_snippet": "ong document summarization, demonstrate thatREFRAG achieves up to30 .85×TTFT acceleration (3 .75×over previous state-of-the-art methods) without any loss in perplexity or downstream accuracy. Our results highlight the importance of speciali"}, {"rank": 4, "score": 0.6150219, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 134, "text_snippet": "ntext),REFRAG achieves16 .53×acceleration in TTFT with cache and8 .59×without cache. Both higher than CEPE (i.e., 2.01×and1 .04×acceleration respectively) while having better model performance (see table 1). With longer context, we are able"}, {"rank": 5, "score": 0.60661155, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 133, "text_snippet": "xt length s, we are able to achieve up to k2×acceleration in both TTFT and throughput. The details on the latency and throughput calculation are in section B.4. Empirical verification of latency/throughput improvement.Figure 2 shows the emp"}]}
{"case_index": 174, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"As a result, LLM inference throughput degrades with larger contexts, limiting their [MASK] in scenarios demanding high throughput and low latency, such as web-scale discovery.\"", "gold": "applicability", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 16.994, "llm_ms": 0.008, "top_contexts": [{"rank": 1, "score": 0.7138859, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 8, "text_snippet": "As a result, LLM inference throughput degrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low latency, such as web-scale discovery. Therefore, developing novel model architectures that opti"}, {"rank": 2, "score": 0.6801685, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 3, "text_snippet": " on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30 .85×the time-t"}, {"rank": 3, "score": 0.65935, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 10, "text_snippet": "G-based applications, such as web-scale search, with the goal of improving inference latency, specifically, the TTFT. We argue that specialized techniques exploiting the unique structure and sparsity inherent in RAG contexts can substantial"}, {"rank": 4, "score": 0.6575286, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 1, "text_snippet": "AG). However, processing long-context inputs introduces significant system latency and demands substantial memory for the key-value cache, resulting in reduced throughput and a fundamental trade-off between knowledge enrichment and system e"}, {"rank": 5, "score": 0.63996136, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 12, "text_snippet": "is information is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-a"}]}
{"case_index": 175, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"[MASK] passages in parallel and filters out irrelevant ones through self-reflection, without relying on external models at inference.\"", "gold": "self-ragprocesses", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 12.201, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.67632276, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 23, "text_snippet": "ess retrieved passages before using them to prompt the LM to generate the output. SELF-RAGprocesses passages in parallel and filters out irrelevant ones through self-reflection, without relying on external models at inference. Moreover, our"}, {"rank": 2, "score": 0.61517537, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 6, "text_snippet": "y trained to leverage and follow facts from provided passages. This work introduces Self-Reflective Retrieval-augmented Generation ( SELF-RAG)to improve an LLM’s generation quality, including its factual accuracy without hurting its versati"}, {"rank": 3, "score": 0.61115974, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 35, "text_snippet": "parallel and uses its own generated reflection tokens to enforce soft constraints (Section 3.3) or hard control (Algorithm 1) over the generated task output. For instance, in Figure 1 (right), the retrieved passages d1is selected at the fir"}, {"rank": 4, "score": 0.6039443, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 8, "text_snippet": "on demand (Step 1). Subsequently, SELF-RAGconcurrently processes multiple retrieved passages, evaluating their relevance and then generating corresponding task outputs (Step 2). It then generates critique tokens to criticize its own output "}, {"rank": 5, "score": 0.60071415, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 2, "text_snippet": "ew framework called Self-Reflective Retrieval-Augmented Gen- eration ( SELF-RAG)that enhances an LM’s quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passage"}]}
{"case_index": 176, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"[MASK] Generation (RAG)Ours: Self-reﬂective Retrieval-Augmented Genera\"", "gold": "retrieval-augmented", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 11.864, "llm_ms": 0.008, "top_contexts": [{"rank": 1, "score": 0.6735974, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 6, "text_snippet": "y trained to leverage and follow facts from provided passages. This work introduces Self-Reflective Retrieval-augmented Generation ( SELF-RAG)to improve an LLM’s generation quality, including its factual accuracy without hurting its versati"}, {"rank": 2, "score": 0.6619334, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 2, "text_snippet": "ew framework called Self-Reflective Retrieval-Augmented Gen- eration ( SELF-RAG)that enhances an LM’s quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passage"}, {"rank": 3, "score": 0.65743923, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 27, "text_snippet": "n (reasoning path consistency) and without retrieval. Recent work on LLM refinement (Dhuliawala et al., 2023; Madaan et al., 2023; Paul et al., 2023) prompts a model to generate task output, natural language feedback and refined task output"}, {"rank": 4, "score": 0.64790493, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 7, "text_snippet": "rmittent special tokens (i.e., reflection tokens ). Reflection tokens are categorized into retrieval andcritique tokens to indicate the need for retrieval and its generation quality respectively (Figure 1 right). In particular, given an inp"}, {"rank": 5, "score": 0.64184976, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 96, "text_snippet": "ains an LM to learn to retrieve, generate, and critique text passages and its own generation by predicting the next tokens from its original vocabulary as well as newly added special tokens, called reflection tokens. SELF-RAGfurther enables"}]}
{"case_index": 177, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"d as follows: the system gets a text query as input, and [MASK] a text output .\"", "gold": "generates", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.603, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.61880726, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 16, "text_snippet": "d as follows: the system gets a text query as input, and generates a text output . For example, in the case of question answering, the query corresponds to the question and the model needs to generate the answer. In the case of classiﬁcatio"}, {"rank": 2, "score": 0.56082743, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 18, "text_snippet": "y retrieving the top-k relevant documents from a large corpus of text with the retriever. Then, these documents are fed to the language model, along with the query, which in turns generates the output. Both the retriever and the language mo"}, {"rank": 3, "score": 0.55658174, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 17, "text_snippet": "chmark in Figure 2. As many natural language processing tasks require knowledge , our goal is to enhance standard text-to-text models with retrieval, which, as we hypothesise in the introduction, may be crucial to endow models with few-shot"}, {"rank": 4, "score": 0.5559726, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 5, "score": 0.55491126, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 37, "text_snippet": " To cast language modeling in the text-to-text framework, we consider a chunk of Nwords, and split this chunk in two sub-sequences of equal length N/2. Then, the ﬁrst sub-sequence is used as the query, and the second corresponds to the outp"}]}
{"case_index": 178, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"The tree traversal method traverses the tree [MASK], pruning and selecting the most relevant nodes at each level.\"", "gold": "layer-by-layer", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 14.279, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.6695886, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 36, "text_snippet": "by RAPTOR: tree traversal and collapsed tree. These methods offer unique ways of traversing the multi-layered RAPTOR tree to retrieve relevant information, each with its own advantages and trade-offs. We provide the pseudocode of both metho"}, {"rank": 2, "score": 0.6271461, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 40, "text_snippet": "between the query vector and the vector embeddings of these child nodes. 4. Select the top kchild nodes with the highest cosine similarity scores to the query, forming the set S2. 5. Continue this process recursively for dlayers, producing "}, {"rank": 3, "score": 0.59053755, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 38, "text_snippet": "dding and the embeddings of all nodes present at this initial layer. 2. Choose the top- knodes based on the highest cosine similarity scores, forming the set S1. 4  Published as a conference paper at ICLR 2024 Figure 2: Illustration of the "}, {"rank": 4, "score": 0.5667447, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 37, "text_snippet": "these selected nodes are considered at the next layer and the top-k nodes are selected from this pool again based on their cosine similarity to the query vector. This process is repeated until we reach the leaf nodes. Finally, the text from"}, {"rank": 5, "score": 0.56378806, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 43, "text_snippet": " set C. 3. Finally, pick the top- knodes that have the highest cosine similarity scores with the query. Keep adding nodes to the result set until you reach a predefined maximum number of tokens, ensuring you don’t exceed the model’s input l"}]}
{"case_index": 179, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \", n }do 5: L ← ∅ 6: if Score (M,yi,[sj;xi]) + ϵ < Score (M,yi,[pi;xi])then 7: L ← L ∪ si 8: if|L|>0then 9: Ni←[MASK]5 sj∈L(⟨encθ(sj),encθ(\"", "gold": "argtop", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.649, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6491314, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 25, "text_snippet": "b; Ram et al., 2023). Input: Base LM M, Compressor encθ, Training data {xi,Si,yi}T 1where xiis input, Si={sj}n 1is a set of candidate sentences from the retrieved documents for xi, yiis the target answer, and score threshold ϵ. Output: An u"}, {"rank": 2, "score": 0.6022385, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 124, "text_snippet": ": if Score (M,yi,[sj;xi])<Score (M,yi,[pi;xi])then 7: L ← L ∪ si 8: if|L|>0then 9: Ni←argTop5 sj∈L(⟨encθ(sj),encθ(xi)⟩) 10: T ← T ∪ { (xi,pi,Ni)} 11:encθ=Finetune (encθ,T) Figure 6: Learning an extractive compressor for QA task. The Score h"}, {"rank": 3, "score": 0.5994531, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 26, "text_snippet": " 7: L ← L ∪ si 8: if|L|>0then 9: Ni←argTop5 sj∈L(⟨encθ(sj),encθ(xi)⟩) 10: T ← T ∪ { (xi,pi,Ni)} 11:encθ=Finetune (encθ,T) Figure 2: Learning an extractive compressor for lan- guage modeling task.Model We train a dual-encoder model encθwhich"}, {"rank": 4, "score": 0.5922735, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 123, "text_snippet": "//huggingface.co/facebook/contriever 10https://huggingface.co/facebook/contriever-msmarco 15  Input: Base LM M, Compressor encoder encθ, Training data {xi,Si,yi}T 1where xiis input, Si={sj}n 1 is a set of candidate sentences from the retrie"}, {"rank": 5, "score": 0.5894544, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 29, "text_snippet": "g pM(y|[sj;xi]), log likelihood assigned to target output according to LM Mwhen candidate sentence is prepended to the input. We consider the sentence with the highest log likelihood as a positive example pi(line 3). To construct negative e"}]}
{"case_index": 180, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"GMMs assume that data points are generated from a mixture of several Gaussian [MASK].\"", "gold": "distributions", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.3333333333333333, "hit_rank": 3, "matched_by": "doc_id+chunk_id", "retrieval_ms": 9.659, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.57379484, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 28, "text_snippet": " Published as a conference paper at ICLR 2024 Given a set of Ntext segments, each represented as a d-dimensional dense vector embedding, the likelihood of a text vector, x, given its membership in the kthGaussian distribution, is denoted by"}, {"rank": 2, "score": 0.53562105, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 33, "text_snippet": "e Gaussian assumption in GMMs may not perfectly align with the nature of text data, which often exhibits a sparse and skewed distribution, our empirical observations suggest that it offers an effective model for our purpose. We run an ablat"}, {"rank": 3, "score": 0.5302248, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 27, "text_snippet": "uiring a fixed number of clusters. This flexibility is essen- tial because individual text segments often contain information relevant to various topics, thereby warranting their inclusion in multiple summaries. Our clustering algorithm is "}, {"rank": 4, "score": 0.50091565, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 32, "text_snippet": "oints), kis the number of model parameters, and ˆLis the maximized value of the likelihood function of the model. In the context of GMM, the number of parameters k is a function of the dimensionality of the input vectors and the number of c"}, {"rank": 5, "score": 0.48933253, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 29, "text_snippet": "vector embeddings presents a challenge for traditional GMMs, as dis- tance metrics may behave poorly when used to measure similarity in high-dimensional spaces (Ag- garwal et al., 2001). To mitigate this, we employ Uniform Manifold Approxim"}]}
{"case_index": 181, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"In this work, we de- sign criteria for evaluating [MASK] answers to global sensemaking questions and evaluate our results using the comparative approach.\"", "gold": "rag-generated", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.926, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.68440294, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 29, "text_snippet": "lements (nodes, edges, covariates) that the LLM can summarize in parallel at both indexing time and query time. The “global answer” to a given query is produced using a final round of query-focused summarization over all community summaries"}, {"rank": 2, "score": 0.6610726, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 30, "text_snippet": "evance”, “faithfulness”, and “answer relevance” (RAGAS, Es et al. 2023). Lacking a gold standard for evaluation, one can quantify relative performance for a given criterion by prompting the LLM to compare generations from two different comp"}, {"rank": 3, "score": 0.653459, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 11, "text_snippet": "swers are combined and used to generate a final global answer. The GraphRAG method and its ability to perform global sensemaking over an entire corpus form the main contribution of this work. To demonstrate this ability, we developed a nove"}, {"rank": 4, "score": 0.6429578, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 96, "text_snippet": "tigate these downstream risks for questions of a global nature, which might otherwise be answered by samples of retrieved facts falsely presented as global summaries. 7 Conclusion We have presented GraphRAG, a RAG approach that combines kno"}, {"rank": 5, "score": 0.62028915, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 23, "text_snippet": " this work, we propose an approach for generating a set of questions for evaluating global sensemaking over the entirety of the corpus. Our approach is related to LLM methods that use a corpus to generate questions whose answers would be su"}]}
{"case_index": 182, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"For [MASK] generation, we experiment with MS-MARCO [ 1] and Jeopardy question generation, and we ﬁnd that our models generate responses that are more factual, speciﬁc, and diverse than a BART baseline.\"", "gold": "knowledge-intensive", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.331, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.75147444, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 14, "text_snippet": "active approaches. For knowledge-intensive generation, we experiment with MS-MARCO [ 1] and Jeopardy question generation, and we ﬁnd that our models generate responses that are more factual, speciﬁc, and diverse than a BART baseline. For FE"}, {"rank": 2, "score": 0.6404643, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 36, "text_snippet": "tity from a fact about that entity. For example, “The World Cup” is the answer to the question “In 1986 Mexico scored as the ﬁrst country to host this international sports competition twice.” As Jeopardy questions are precise, factual state"}, {"rank": 3, "score": 0.5984701, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 13, "text_snippet": "sive tasks —tasks that humans could not reasonably be expected to perform without access to an external knowledge source. Our RAG models achieve state-of-the-art results on open Natural Questions [ 29], WebQuestions [ 3] and CuratedTrec [ 2"}, {"rank": 4, "score": 0.5922454, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 35, "text_snippet": "y on parametric knowledge to generate reasonable responses. 3.3 Jeopardy Question Generation To evaluate RAG’s generation abilities in a non-QA setting, we study open-domain question gen- eration. Rather than use questions from standard ope"}, {"rank": 5, "score": 0.5888328, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 108, "text_snippet": "s. arXiv preprint arXiv:2208.03299 , 2022b. URL https: //arxiv.org/abs/2208.03299 . Zhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. Active retrieval augmented gener"}]}
{"case_index": 183, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"Specifically, SELF-RAG outperforms ChatGPT and [MASK] Llama2-chat on Open-domain QA, reasoning and fact verification tasks, and it shows significant gains in improving factuality and citation accuracy for long-form generations relative to these models.1 1 I NTRODU\"", "gold": "retrieval-augmented", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.609, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.8121871, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 3, "text_snippet": "e phase, enabling it to tailor its behavior to diverse task requirements. Experiments show that SELF- RAG(7B and 13B parameters) significantly outperforms state-of-the-art LLMs and retrieval-augmented models on a diverse set of tasks. Speci"}, {"rank": 2, "score": 0.75551736, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 75, "text_snippet": "s Llama2 65Bto refine output. Comparison against baselines with retrieval. As shown in Tables 2 (bottom), our SELF-RAGalso outperforms existing RAG in many tasks, obtaining the best performance among non-proprietary LM-based models on all t"}, {"rank": 3, "score": 0.72848463, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 77, "text_snippet": "s except ChatGPT. Gao et al. (2023) found that ChatGPT consistently exhibits superior efficacy in this particular task, surpassing smaller LMs. Our SELF-RAGbridges this performance gap, even outperforming ChatGPT in citation precision, whic"}, {"rank": 4, "score": 0.7171481, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 96, "text_snippet": "ains an LM to learn to retrieve, generate, and critique text passages and its own generation by predicting the next tokens from its original vocabulary as well as newly added special tokens, called reflection tokens. SELF-RAGfurther enables"}, {"rank": 5, "score": 0.7082941, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 27, "text_snippet": "n (reasoning path consistency) and without retrieval. Recent work on LLM refinement (Dhuliawala et al., 2023; Madaan et al., 2023; Paul et al., 2023) prompts a model to generate task output, natural language feedback and refined task output"}]}
{"case_index": 184, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"xact Match NaturalQuestions 5 10 25 50 100 Number of passages54565860626466 TriviaQA 5 10 25 50 100 Number of passages343638404244464850 SQuADFigure 3: Performance of [MASK] (base) on valid sets as a function of the number of retrieved passages.\"", "gold": "fusion-in-decoder", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.3333333333333333, "hit_rank": 3, "matched_by": "doc_id+chunk_id", "retrieval_ms": 13.278, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.73047376, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 24, "text_snippet": " domain QA. Datasets. We consider the following datasets, and use the same setting as Lee et al. (2019): •NaturalQuestions (Kwiatkowski et al., 2019) contains questions corresponding to Google search queries. The open-domain version of this"}, {"rank": 2, "score": 0.7052618, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 33, "text_snippet": "es are competitive for knowl- edge retrieval tasks. Scaling with number of passages. In Figure 3, we report the performance with respect to the  NaturalQuestions TriviaQA Training Passages w/o ﬁnetuning w/ ﬁnetuning w/o ﬁnetuning w/ ﬁnetuni"}, {"rank": 3, "score": 0.689569, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 25, "text_snippet": "xact Match NaturalQuestions 5 10 25 50 100 Number of passages54565860626466 TriviaQA 5 10 25 50 100 Number of passages343638404244464850 SQuADFigure 3: Performance of Fusion-in-Decoder (base) on valid sets as a function of the number of ret"}, {"rank": 4, "score": 0.67232144, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 34, "text_snippet": "s are reported on dev sets. number of retrieved passages. In particular, we observe that increasing the number of passages from 10 to 100 leads to 6% improvement on Trivi- aQA and 3.5% improvement on NaturalQuestions. On the other hand, the"}, {"rank": 5, "score": 0.66112816, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}]}
{"case_index": 185, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"Our main contribution is the idea of using text [MASK] to allow retrieval augmentation of context at different scales, and to show its effectiveness in experiments on collections of long doc- uments.\"", "gold": "summarization", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.067, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6445603, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 1, "text_snippet": "rom a retrieval corpus, limiting holistic under- standing of the overall document context. We introduce the novel approach of recursively embedding, clustering, and summarizing chunks of text, constructing a tree with differing levels of su"}, {"rank": 2, "score": 0.6354785, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 10, "text_snippet": "rsively clusters chunks of text based on their vector embeddings and generates text summaries of those clusters, constructing a tree from the bottom up. Nodes clustered together are siblings; a parent node contains the text summary of that "}, {"rank": 3, "score": 0.63383734, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 0, "text_snippet": "Published as a conference paper at ICLR 2024 RAPTOR: R ECURSIVE ABSTRACTIVE PROCESSING FOR TREE-ORGANIZED RETRIEVAL Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D. Manning Stanford University psarthi@cs."}, {"rank": 4, "score": 0.6102066, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 73, "text_snippet": "an effectively handle a wider range of questions, from higher-order thematic queries to detail-oriented questions. Detailed results for additional stories and an ablation study on layer contributions can be found in Appendix I. 5 C ONCLUSIO"}, {"rank": 5, "score": 0.60992295, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 8, "text_snippet": " indexing and retrieval system that uses a tree structure to capture both high-level and low-level details about a text. As shown in Figure 1, our system, RAPTOR, clusters chunks of text, generates text summaries of those clusters, and then"}]}
{"case_index": 186, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"In addition, in some [MASK] generation tasks, external knowl- edge is needed more than once, and when to retrieve should be concerned.\"", "gold": "long-text", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.702, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.62068546, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 22, "text_snippet": "rrelevant context and improve robustness. SAIL (Luo et al., 2023) is tuned on instructions to insert retrieved documents before instructions. While Toolformer (Schick et al., 2023) is pre-trained for calling APIs such as Wikipedia. In addit"}, {"rank": 2, "score": 0.58326364, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 12, "text_snippet": "ck- augmented networks [ 25] and memory layers [ 30]. In contrast, we explore a setting where both parametric and non-parametric memory components are pre-trained and pre-loaded with extensive knowledge. Crucially, by using pre-trained acce"}, {"rank": 3, "score": 0.57479954, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 6, "text_snippet": " The examples show that a low-quality retriever is prone to introducing a substantial amount of irrelevant information, impeding the generators from acquiring accurate knowledge and potentially misleading them. the parametric knowledge they"}, {"rank": 4, "score": 0.56213504, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 13, "text_snippet": "sive tasks —tasks that humans could not reasonably be expected to perform without access to an external knowledge source. Our RAG models achieve state-of-the-art results on open Natural Questions [ 29], WebQuestions [ 3] and CuratedTrec [ 2"}, {"rank": 5, "score": 0.5586268, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 7, "text_snippet": "ieval-augmented generation (RAG) (Lewis et al., 2020). In this framework, the input to models is augmented by prepending relevant documents that are retrieved from an external knowledge corpus (Guu et al., 2020). While RAG serves as a pract"}]}
{"case_index": 187, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"(2018) introduced a supervised [MASK] to rerank paragraphs based on BiLSTM, while Wang et al.\"", "gold": "learningmethod", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.722, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.69358903, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 12, "text_snippet": "018b) described a technique to aggregate answers from different paragraphs, using conﬁdence and coverage scores. Passage retrieval is an important step in open domain question answering, and is an active area of research to improve QA syste"}, {"rank": 2, "score": 0.6522915, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 84, "text_snippet": " and pre-training paradigm for generative LLMs, leveraging cross-attention and end-to-end pre-training with 9  retrieval from a trillion-token data store, achieving strong performance. Subsequent work by Shi et al. (2024) and Lin et al. (20"}, {"rank": 3, "score": 0.63887274, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 16, "text_snippet": "las (Izacard et al., 2022), which fine-tunes an encoder- decoder model in conjunction with the retriever; REALM (Guu et al., 2020), a bidirectional, masked LM fine-tuned for open-domain question answering; and RAG (Retrieval-Augmented Gener"}, {"rank": 4, "score": 0.63745713, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}, {"rank": 5, "score": 0.6205239, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 20, "text_snippet": "es with SpaCy.2In DPR, passages and questions are represented as dense vector representations, computed using two BERT networks. The ranking function is the dot product between the query and passage represen- tations. Retrieval is performed"}]}
{"case_index": 188, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"The objective is to align any encoder–decoder combination so that the generations produced [MASK] contextclosely resemble those generated by the original decoder with access to the full context.\"", "gold": "withcompressed", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.065, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.521216, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 28, "text_snippet": " assist the decoder in predicting the next otokens xs+1:s+o. This task encourages the model to leverage contextual information for next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any en"}, {"rank": 2, "score": 0.5165668, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 30, "text_snippet": "ayer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress ktokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s to"}, {"rank": 3, "score": 0.51200217, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 1, "text_snippet": "evitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Al- though retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heav"}, {"rank": 4, "score": 0.5092001, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 3, "text_snippet": " on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30 .85×the time-t"}, {"rank": 5, "score": 0.50386626, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 48, "text_snippet": "gmented with reflection tokens Dgenusing the standard next token objective: max ME(x,y,r )∼DgenlogpM(y, r|x). (2) Unlike Ctraining (Eq. 1), Mlearns to predict the target output as well as the reflection tokens. During training, we mask out "}]}
{"case_index": 189, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"Our analysis demonstrates the effectiveness of training and inference with reflection tokens for overall performance improvements as well as test-time model [MASK] (e.g., balancing the trade-off between citation previsions and completeness).\"", "gold": "customizations", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.516, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.645669, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 18, "text_snippet": "at (Touvron et al., 2023) and Alpaca (Dubois et al., 2023) on all tasks. Our analysis demonstrates the effectiveness of training and inference with reflection tokens for overall performance improvements as well as test-time model customizat"}, {"rank": 2, "score": 0.64508384, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 36, "text_snippet": "text with reflection tokens by unifying them as next token predictions from the expanded model vocabulary (i.e., the original vocabulary plus reflection tokens). Specifically, we train the generator model Mon a curated corpus with interleav"}, {"rank": 3, "score": 0.6319641, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 48, "text_snippet": "gmented with reflection tokens Dgenusing the standard next token objective: max ME(x,y,r )∼DgenlogpM(y, r|x). (2) Unlike Ctraining (Eq. 1), Mlearns to predict the target output as well as the reflection tokens. During training, we mask out "}, {"rank": 4, "score": 0.6258808, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 15, "text_snippet": "st a critic model during training, reducing overhead. The critic model, in part, is supervised on a dataset of input, output, and corresponding reflection tokens collected by prompting a propriety LM (i.e., GPT-4; OpenAI 2023). While we dra"}, {"rank": 5, "score": 0.6196368, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 96, "text_snippet": "ains an LM to learn to retrieve, generate, and critique text passages and its own generation by predicting the next tokens from its original vocabulary as well as newly added special tokens, called reflection tokens. SELF-RAGfurther enables"}]}
{"case_index": 190, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"However, most existing methods target generic LLM tasks with long context and are largely [MASK] to our work.\"", "gold": "orthogonal", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.139, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6139091, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 9, "text_snippet": "f research, with approaches 1arXiv:2509.01092v2 [cs.CL] 12 Oct 2025  ranging from modifying the attention mechanism’s complexity (Beltagy et al., 2020) to sparsifying attention and context (Child et al., 2019; Xiao et al., 2024; Jiang et al"}, {"rank": 2, "score": 0.5703038, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 3, "text_snippet": " on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30 .85×the time-t"}, {"rank": 3, "score": 0.5640042, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 94, "text_snippet": "Complementary approaches rank or prune context by estimated informativeness, e.g.,Selective Contextuses self-information to drop low-value tokens, and sentence-level methods learn context-aware encoders for question-specific compression and"}, {"rank": 4, "score": 0.5625061, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 12, "text_snippet": "is information is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-a"}, {"rank": 5, "score": 0.55707276, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 2, "text_snippet": "ated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns th"}]}
{"case_index": 191, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"This is exactly the focus of this paper to improve the [MASK] of generation.\"", "gold": "robustness", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 8.167, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.5366997, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 1, "text_snippet": "evitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Al- though retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heav"}, {"rank": 2, "score": 0.525011, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 23, "text_snippet": "t to retrieve in long-form generation. Compared with recent studies (Schick et al., 2023; Luo et al., 2023; Asai et al., 2024) that are the most relevant to our work, a main difference should be highlighted. These approaches target on explo"}, {"rank": 3, "score": 0.5146607, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 0, "text_snippet": "Corrective Retrieval Augmented Generation Shi-Qi Yan1*, Jia-Chen Gu2*, Yun Zhu3, Zhen-Hua Ling1 1National Engineering Research Center of Speech and Language Information Processing, University of Science and Technology of China, Hefei, China"}, {"rank": 4, "score": 0.5001858, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 6, "text_snippet": "y trained to leverage and follow facts from provided passages. This work introduces Self-Reflective Retrieval-augmented Generation ( SELF-RAG)to improve an LLM’s generation quality, including its factual accuracy without hurting its versati"}, {"rank": 5, "score": 0.49970347, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 4, "text_snippet": " for long-form generations relative to these models.1 1 I NTRODUCTION State-of-the-art LLMs continue to struggle with factual errors (Mallen et al., 2023; Min et al., 2023) despite their increased model and data scale (Ouyang et al., 2022)."}]}
{"case_index": 192, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"2.3 Adaptive benchmarking for RAG Evaluation Many benchmark datasets for open-domain question answering exist, including HotPotQA (Yang et al., 2018), [MASK] (Tang and Yang, 2024), and MT-Bench (Zheng et al., 2024).\"", "gold": "multihop-rag", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.687, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.7782022, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 22, "text_snippet": "gly global summaries by using the LLM to create summaries spanning this community hierarchy. 2.3 Adaptive benchmarking for RAG Evaluation Many benchmark datasets for open-domain question answering exist, including HotPotQA (Yang et al., 201"}, {"rank": 2, "score": 0.6564958, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 32, "text_snippet": "ametric knowledge. We consider four popular open-domain QA datasets: Natural Questions (NQ) [ 29], TriviaQA (TQA) [ 24]. WebQuestions (WQ) [ 3] and CuratedTrec (CT) [ 2]. As CT and WQ are small, we follow DPR [ 26] by initializing CT and WQ"}, {"rank": 3, "score": 0.6562423, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 128, "text_snippet": "ng, R., and Derr, T. (2023b). Knowledge graph prompting for multi-document question answering. Xu, Y . and Lapata, M. (2021). Text summarization with latent queries. arXiv preprint arXiv:2106.00104 . Yang, Z., Qi, P., Zhang, S., Bengio, Y ."}, {"rank": 4, "score": 0.6283727, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 183, "text_snippet": "stions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics , 7:452–466, 2019. doi: 10.1162/tacl_a_00276. URL https://aclanthology.org/Q19-1026 . 7, 9 Guokun Lai, Qizhe Xie, Hanxiao Liu"}, {"rank": 5, "score": 0.62022114, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 42, "text_snippet": " state of the art (only on the T5-comparable split for TQA). RAG combines the generation ﬂexibility of the “closed-book” (parametric only) approaches and the performance of \"open-book\" retrieval-based approaches. Unlike REALM and T5+SSM, RA"}]}
{"case_index": 193, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"This model consists of a [MASK] foundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu et al., 2019)).\"", "gold": "decoder-only", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.491, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.6272198, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 18, "text_snippet": "f question and retrieval in this section. Model overview.Figure 1 shows the main architecture ofREFRAG. This model consists of a decoder-only foundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Rober"}, {"rank": 2, "score": 0.6088971, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 59, "text_snippet": "decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B as decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance varies with different encoder and decoder sizes. Figure 11 presents results for"}, {"rank": 3, "score": 0.5987368, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 39, "text_snippet": "MA-2-7B evaluated on xs+1:s+owith only output tokens as input.LLaMA-Full Context: LLaMA-2-7B evaluated on xs+1:s+owith the full sequence x1:Tas input.CEPE: Memory-efficient long-context model (Yen et al., 2024) a previous SOTA model which s"}, {"rank": 4, "score": 0.57437366, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 17, "text_snippet": "FRAG achieves better performance than LLaMA without incurring higher latency in the downstream applications. 2 Model Architecture We denote the decoder model as Mdecand the encoder model as Menc. Given an input with Ttokens x1, x2, . . . , "}, {"rank": 5, "score": 0.57108074, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 15, "text_snippet": "igure 1) while preserving the autoregressive nature of the decoder, thereby supporting multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight reinforcement learning (RL) policy that sel"}]}
{"case_index": 194, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"for long-form generations relative to these models.1 1 I NTRODUCTION [MASK] LLMs continue to struggle with factual errors (Mallen et al., 2023; Min et al., 2023) despite their increased model and data scale (Ouyang et al., 2022).\"", "gold": "state-of-the-art", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.185, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.7414205, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 4, "text_snippet": " for long-form generations relative to these models.1 1 I NTRODUCTION State-of-the-art LLMs continue to struggle with factual errors (Mallen et al., 2023; Min et al., 2023) despite their increased model and data scale (Ouyang et al., 2022)."}, {"rank": 2, "score": 0.6431837, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 6, "text_snippet": "y trained to leverage and follow facts from provided passages. This work introduces Self-Reflective Retrieval-augmented Generation ( SELF-RAG)to improve an LLM’s generation quality, including its factual accuracy without hurting its versati"}, {"rank": 3, "score": 0.6418101, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 61, "text_snippet": "s instead of strictly requiring exact matching, following Mallen et al. (2023); Schick et al. (2023). Long-form generation tasks include a biography generation task (Min et al., 2023) and a long-form QA task ALCE-ASQA Gao et al. (2023); Ste"}, {"rank": 4, "score": 0.63852364, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 4, "text_snippet": "Ms) have attracted increasing attention and exhibited impressive abili- ties to understand instructions and generate fluent language texts (Brown et al., 2020; Ouyang et al., 2022; Touvron et al., 2023a). Nevertheless, LLMs inevitably manif"}, {"rank": 5, "score": 0.629416, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 4, "text_snippet": " tasks (Roberts et al., 2020). Nevertheless, even a large model does not contain sufficient domain- specific knowledge for particular tasks and the world continues to change, invalidating facts in the LLM. Updating the knowledge of these mo"}]}
{"case_index": 195, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"cted communities, with summaries at higher levels of the hierarchy recursively [MASK] lower-level summaries.\"", "gold": "incorporating", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 26.061, "llm_ms": 0.008, "top_contexts": [{"rank": 1, "score": 0.60547423, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 45, "text_snippet": "ommunities are used to generate summaries for higher-level communities as follows: •Leaf-level communities . The element summaries of a leaf-level community are prioritized and then iteratively added to the LLM context window until the toke"}, {"rank": 2, "score": 0.5974146, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 10, "text_snippet": "cted communities, with summaries at higher levels of the hierarchy recursively incorporating lower-level summaries. Together, these community summaries provide global descriptions and insights over the corpus. Finally, GraphRAG answers quer"}, {"rank": 3, "score": 0.591989, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 42, "text_snippet": "hical manner, recursively detecting sub-communities within each detected community until reaching leaf communities that can no longer be partitioned. 5  Each level of this hierarchy provides a community partition that covers the nodes of th"}, {"rank": 4, "score": 0.58383656, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 47, "text_snippet": "t within the context window. 3.1.6 Community Summaries →Community Answers →Global Answer Given a user query, the community summaries generated in the previous step can be used to generate a final answer in a multi-stage process. The hierarc"}, {"rank": 5, "score": 0.582584, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 44, "text_snippet": "ummaries at one level looking for general themes of interest, then read linked reports at a lower level that provide additional details for each subtopic. Here, however, we focus on their utility as part of a graph-based index used for answ"}]}
{"case_index": 196, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"odels in the output should be faithful to the original input, yet the main goal is [MASK].\"", "gold": "different", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.868, "llm_ms": 0.008, "top_contexts": [{"rank": 1, "score": 0.5759373, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 22, "text_snippet": "y. Yet, using an extreme-scale model as the compressor is not desirable as we want the compressor to be substantially smaller than the LMs. Thus, we perform distillation (Hinton et al., 2015) of extreme-scale LMs to build a lightweight abst"}, {"rank": 2, "score": 0.57255226, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 23, "text_snippet": "odels in the output should be faithful to the original input, yet the main goal is different. Instead of capturing salient information for humans readers, compressors aim to produce a concise text that are useful for a LM on an end task. In"}, {"rank": 3, "score": 0.51796067, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 18, "text_snippet": "cy. If the retrieved documents do not contain relevant information or retrieval augmentation is not necessary, scan be an empty sequence. (2) Effecive : when sis prepended to input sequence xand provided to LM Mas a prompt, LM should genera"}, {"rank": 4, "score": 0.5126372, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 30, "text_snippet": "ayer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress ktokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s to"}, {"rank": 5, "score": 0.50545424, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 135, "text_snippet": "arXiv preprint arXiv:2305.00633 , 2023. URL https://arxiv.org/abs/2305.00633 . Fangyuan Xu, Weijia Shi, and Eunsol Choi. Recomp: Improving retrieval-augmented lms with compression and selective augmentation, 2023. URL https://arxiv.org/abs/"}]}
{"case_index": 197, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"We argue that specialized techniques exploiting the unique structure and sparsity inherent in RAG contexts can [MASK] reduce memory and computational overhead.\"", "gold": "substantially", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.25, "hit_rank": 4, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.559, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.66023064, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 3, "text_snippet": " on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30 .85×the time-t"}, {"rank": 2, "score": 0.6586811, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 95, "text_snippet": "oducedREFRAG, a novel and efficient decoding framework tailored for RAG applications. By leveraging the inherent sparsity and block-diagonal attention patterns present in RAG contexts,REFRAG compresses, senses, and expands context represent"}, {"rank": 3, "score": 0.64197874, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 12, "text_snippet": "is information is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-a"}, {"rank": 4, "score": 0.6295954, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 10, "text_snippet": "G-based applications, such as web-scale search, with the goal of improving inference latency, specifically, the TTFT. We argue that specialized techniques exploiting the unique structure and sparsity inherent in RAG contexts can substantial"}, {"rank": 5, "score": 0.61584884, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 94, "text_snippet": "Complementary approaches rank or prune context by estimated informativeness, e.g.,Selective Contextuses self-information to drop low-value tokens, and sentence-level methods learn context-aware encoders for question-specific compression and"}]}
{"case_index": 198, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"[MASK] For RAG-Sequence, the likelihood p(y|x)does not break into a conventional per- token likelihood, hence we cannot solve it with a single beam search.\"", "gold": "rag-sequence", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 15.015, "llm_ms": 0.008, "top_contexts": [{"rank": 1, "score": 0.6665311, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 26, "text_snippet": " arg maxyp(y|x). RAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera- tor with transition probability: p′ θ(yi|x,y 1:i−1) =∑ z∈top-k(p(·|x))pη(zi|x)pθ(yi|x,zi,y1:i−1)To decode, we can plug p′ θ(yi|x,y 1:i−"}, {"rank": 2, "score": 0.5934513, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 19, "text_snippet": "r produces the output sequence probability for each document, which are then marginalized, pRAG-Sequence (y|x)≈∑ z∈top-k(p(·|x))pη(z|x)pθ(y|x,z) =∑ z∈top-k(p(·|x))pη(z|x)N∏ ipθ(yi|x,z,y 1:i−1) RAG-Token Model In the RAG-Token model we can d"}, {"rank": 3, "score": 0.5700737, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 25, "text_snippet": "escent with Adam [ 28]. Updating the document encoder BERTdduring training is costly as it requires the document index to be periodically updated as REALM does during pre-training [ 20]. We do not ﬁnd this step necessary for strong performa"}, {"rank": 4, "score": 0.55919856, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 28, "text_snippet": "decoding procedure as “Thorough Decoding.” For longer output sequences,|Y|can become large, requiring many forward passes. For more efﬁcient decoding, we can make a further approximation that pθ(y|x,zi)≈0whereywas not generated during beam "}, {"rank": 5, "score": 0.5582679, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 17, "text_snippet": "rator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a distribution over generated text. In one approach, RAG-Sequence , the mo"}]}
{"case_index": 199, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"For language modelling, both trained compressors achieve a compression ratio of 25% with minimal [MASK] drop.\"", "gold": "performance", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 11.515, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6546248, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 15, "text_snippet": "6% and significantly outperforms prepending full documents. Our trained compressors also show promising results. For language modelling, both trained compressors achieve a compression ratio of 25% with minimal performance drop. When applied"}, {"rank": 2, "score": 0.62634337, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 3, "text_snippet": "n. We evaluate our approach on language modeling task and open domain question answering task. We achieve a compression rate of as low as 6% with minimal loss in performance for both tasks, significantly outperforming the off-the-shelf summ"}, {"rank": 3, "score": 0.60331094, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 58, "text_snippet": "ning task. We observe a performance regression as the compression rate increases; however, even at a compression rate of32, our model remains competitive (as shown in table 1). In contrast, a compression rate of64appears to be overly aggres"}, {"rank": 4, "score": 0.59991753, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 65, "text_snippet": "in Table 10 in the appendix. Overall, the performance is worse than the LM from which compressors are trained on, sometimes unable to outperform other compression baselines (e.g., no clear gain from using contriever vs. our trained contriev"}, {"rank": 5, "score": 0.5878141, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 2, "text_snippet": "d an abstractive compressor which generates summaries by synthesizing information from multiple documents. Both compressors are trained to improve LMs’ performance on end tasks when the generated summaries are prepended to the LMs’ input, w"}]}
{"case_index": 200, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"Despite this, the methods above usually ignore a [MASK], what if the retrieval goes wrong?\"", "gold": "question", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.3333333333333333, "hit_rank": 3, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.904, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.5513642, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 1, "text_snippet": "evitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Al- though retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heav"}, {"rank": 2, "score": 0.5424242, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 8, "text_snippet": "wledge raises significant concerns about the model’s behavior and performance in scenarios where retrieval may fail or return inaccu- rate results (Shi et al., 2023). As Figure 1 shows that a low-quality retriever is prone to introducingarX"}, {"rank": 3, "score": 0.53337467, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 20, "text_snippet": "model that specializes in response generation. Despite this, the methods above usually ignore a question, what if the retrieval goes wrong? Since the purpose of introducing a retrieval is to secure that generative LMs can obtain relevant an"}, {"rank": 4, "score": 0.53190964, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 6, "text_snippet": " The examples show that a low-quality retriever is prone to introducing a substantial amount of irrelevant information, impeding the generators from acquiring accurate knowledge and potentially misleading them. the parametric knowledge they"}, {"rank": 5, "score": 0.5290691, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 23, "text_snippet": "t to retrieve in long-form generation. Compared with recent studies (Schick et al., 2023; Luo et al., 2023; Asai et al., 2024) that are the most relevant to our work, a main difference should be highlighted. These approaches target on explo"}]}
{"case_index": 201, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"[MASK] nodes thus storing varying levels of detail, keeping granular details.\"", "gold": "ntermediate", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.442, "llm_ms": 0.008, "top_contexts": [{"rank": 1, "score": 0.5807509, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 9, "text_snippet": "uestions at different levels. 1arXiv:2401.18059v1 [cs.CL] 31 Jan 2024  Published as a conference paper at ICLR 2024 2 3 4 5  1 1  2  3  3  4 5  5 6  8  7 Index #8  Text: summary of  nodes 2 and 3  Child Nodes: 2, 3  Text Embedding  Text chu"}, {"rank": 2, "score": 0.5765058, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 21, "text_snippet": "ntermediate nodes thus storing varying levels of detail, keeping granular details. However, both methods, due to their reliance on adjacency for grouping or summarizing adjacent nodes, may still overlook distant interdependencies within the"}, {"rank": 3, "score": 0.56094635, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 8, "text_snippet": " indexing and retrieval system that uses a tree structure to capture both high-level and low-level details about a text. As shown in Figure 1, our system, RAPTOR, clusters chunks of text, generates text summaries of those clusters, and then"}, {"rank": 4, "score": 0.5502965, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 36, "text_snippet": "by RAPTOR: tree traversal and collapsed tree. These methods offer unique ways of traversing the multi-layered RAPTOR tree to retrieve relevant information, each with its own advantages and trade-offs. We provide the pseudocode of both metho"}, {"rank": 5, "score": 0.54874355, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 170, "text_snippet": " RAPTOR’s multi-layered tree structure. Notably, a significant percentage of the nodes retrieved by RAPTOR using the DPR retriever for the NarrativeQA dataset come from the first and second layers of the tree, as opposed to the leaf nodes. "}]}
{"case_index": 202, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"Like T5 [ 51] or BART, RAG can be ﬁne-tuned on any seq2seq task, whereby both the generator and [MASK] are jointly learned.\"", "gold": "retriever", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 11.063, "llm_ms": 0.008, "top_contexts": [{"rank": 1, "score": 0.63967866, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 71, "text_snippet": " pre-trained language model could achieve strong performance across both discriminative and generative tasks. For further improvement, BART [ 32] and T5 [ 51,52] propose a single, pre-trained encoder-decoder model that leverages bi-directio"}, {"rank": 2, "score": 0.62701404, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 11, "text_snippet": "e document is responsible for all tokens) or a per-token basis (where different documents are responsible for different tokens). Like T5 [ 51] or BART, RAG can be ﬁne-tuned on any seq2seq task, whereby both the generator and retriever are j"}, {"rank": 3, "score": 0.6170702, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 24, "text_snippet": "tasks and outperforms comparably-sized T5 models [32]. We refer to the BART generator parameters θas the parametric memory henceforth. 2.4 Training We jointly train the retriever and generator components without any direct supervision on wh"}, {"rank": 4, "score": 0.593327, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 23, "text_snippet": "T The generator component pθ(yi|x,z,y 1:i−1)could be modelled using any encoder-decoder. We use BART-large [ 32], a pre-trained seq2seq transformer [ 58] with 400M parameters. To combine the input xwith the retrieved content zwhen generatin"}, {"rank": 5, "score": 0.58356786, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 45, "text_snippet": "el can be any pre-trained LM, we use the same one as the generator LM (i.e., Llama 2-7B; Touvron et al. 2023) for Cinitialization. The critic achieves a higher than 90% agreement with GPT-4-based predictions on most reflection token categor"}]}
{"case_index": 203, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"Atlasretrieves relevant documents based on the current context by using a [MASK] dense retriever using a dual-encoder architecture, based on the Contriever (Izacard et al., 2022).\"", "gold": "general-purpose", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.715, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.7307429, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 9, "text_snippet": " strong few-shot learning, despite having lower parameter counts than other powerful recent few-shot learners. Atlasretrieves relevant documents based on the current context by using a general-purpose dense retriever using a dual-encoder ar"}, {"rank": 2, "score": 0.7159931, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 18, "text_snippet": "y retrieving the top-k relevant documents from a large corpus of text with the retriever. Then, these documents are fed to the language model, along with the query, which in turns generates the output. Both the retriever and the language mo"}, {"rank": 3, "score": 0.69591755, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 20, "text_snippet": "eir corresponding embeddings. The Contriever model is pre-trained using the MoCo contrastive loss (He et al., 2020), and uses unsupervised data only. As shown in the following section, an advantage of dense retrievers is that both query and"}, {"rank": 4, "score": 0.67795753, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 51, "text_snippet": "answering (Voorhees et al., 1999), documents are often retrieved from Wikipedia (Chen et al., 2017). Recently, dense retrievers based on neural networks have become popular. These usually follow a dual-encoder architecture (Yih et al., 2011"}, {"rank": 5, "score": 0.6632829, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 23, "text_snippet": "..dm}that are relevant to x. Following prior work (Qu et al., 2021; Izacard & Grave, 2021b; Ni et al., 2021), we use a dense retriever based on the dual encoder architecture, where an encoder is used to encode both the input context xand th"}]}
{"case_index": 204, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"ntributions: •A thorough study on how to design and train [MASK] language models, with a focus on downstream few-shot learning and sample eﬃciency.\"", "gold": "retrieval-augmented", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.879, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.76862574, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 12, "text_snippet": "ntributions: •A thorough study on how to design and train retrieval-augmented language models, with a focus on downstream few-shot learning and sample eﬃciency. •The ﬁndings of this study lead to a retrieval-augmented language model, called"}, {"rank": 2, "score": 0.6730852, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 0, "text_snippet": "Atlas: Few-shot Learning with Retrieval Augmented Language Models Gautier Izacard∗ ∗,♦,♣,♥gizacard@fb.com Patrick Lewis∗,♦plewis@fb.com Maria Lomeli♦marialomeli@fb.com Lucas Hosseini♦hoss@fb.com Fabio Petroni♦fabiopetroni@fb.com Timo Schick"}, {"rank": 3, "score": 0.6639817, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 1, "text_snippet": " Abstract Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter count"}, {"rank": 4, "score": 0.66318154, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 61, "text_snippet": " added to the prompt of a large language model performing in-context learning. 3.2 Few-shot learning Few-shot learning, the task of learning from very few examples, has been studied for decades (Thrun & Pratt, 1998; Fink, 2005; Vinyals et a"}, {"rank": 5, "score": 0.65392154, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 62, "text_snippet": "arge Language models. Providing language models with natural language descriptions of tasks, as proposed by Radford et al. (2019) has led to signiﬁcant developments in few-shot learning. GPT-3 (Brown et al., 2020) demonstrated the ability o"}]}
{"case_index": 205, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"Clark and Gardner (2018) proposed to use a global [MASK] over all the span corresponding to the answer, which was later applied to BERT based models (Wang et al., 2019).\"", "gold": "normalization", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 11.339, "llm_ms": 0.008, "top_contexts": [{"rank": 1, "score": 0.6521027, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 6, "text_snippet": "tech- niques have been considered, either using sparse representations based on TF/IDF or using dense embeddings (Guu et al., 2020; Karpukhin et al., 2020). The models which extract the answers are often based on contextualized word represe"}, {"rank": 2, "score": 0.63700384, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 11, "text_snippet": " Different methods were pro- posed to tackle the setting where no gold spans are given to the system, but only the correct answer. Clark and Gardner (2018) proposed to use a global normalization over all the span corresponding to the answer"}, {"rank": 3, "score": 0.6240051, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}, {"rank": 4, "score": 0.597836, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 5, "score": 0.59780985, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 21, "text_snippet": "Raffel et al., 2019; Lewis et al., 2019). The model takes as input the question, as well as the support passages, and generates the answer. More precisely, each retrieved passage and its title are concatenated with the question, and process"}]}
{"case_index": 206, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"Furthermore, current methods mostly treat complete documents as reference knowledge both during retrieval and [MASK].\"", "gold": "utilization", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 12.832, "llm_ms": 0.008, "top_contexts": [{"rank": 1, "score": 0.60334456, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 9, "text_snippet": "nations (Zhang et al., 2023b). However, most conventional RAG ap- proaches indiscriminately incorporate the retrieved documents, regardless of whether these documents are relevant or not (Rony et al., 2022). Furthermore, current methods mos"}, {"rank": 2, "score": 0.57808924, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 22, "text_snippet": "rrelevant context and improve robustness. SAIL (Luo et al., 2023) is tuned on instructions to insert retrieved documents before instructions. While Toolformer (Schick et al., 2023) is pre-trained for calling APIs such as Wikipedia. In addit"}, {"rank": 3, "score": 0.5751005, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 18, "text_snippet": "pact the LLMs and exacerbate the problems. Thus, it is obvious that the lack of accurate and specific knowledge can lead to misleading or even inaccurate generation, which will severely hurt the experience of users in most practical applica"}, {"rank": 4, "score": 0.57132566, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 19, "text_snippet": " documents. It usually provides an extra knowledge source from a specific corpus, i.e., Wikipedia, which greatly improves the per- formance of LMs in a variety of tasks, especially in the knowledge-intensive ones. The proposed  methods gene"}, {"rank": 5, "score": 0.57004684, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 6, "text_snippet": " The examples show that a low-quality retriever is prone to introducing a substantial amount of irrelevant information, impeding the generators from acquiring accurate knowledge and potentially misleading them. the parametric knowledge they"}]}
{"case_index": 207, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"Unlike prior methods (Yen et al., 2024),[MASK] compression of token chunks at arbitrary positions (see figure 1) while preserving the autoregressive nature of the decod\"", "gold": "refragsupports", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 12.529, "llm_ms": 0.008, "top_contexts": [{"rank": 1, "score": 0.7317564, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 14, "text_snippet": "ortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now "}, {"rank": 2, "score": 0.6973499, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 15, "text_snippet": "igure 1) while preserving the autoregressive nature of the decoder, thereby supporting multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight reinforcement learning (RL) policy that sel"}, {"rank": 3, "score": 0.65857846, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 12, "text_snippet": "is information is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-a"}, {"rank": 4, "score": 0.65159166, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 3, "text_snippet": " on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30 .85×the time-t"}, {"rank": 5, "score": 0.65110224, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 13, "text_snippet": "nd memory usage during decoding, allwithout requiring modificationsto the LLM architecture or introducing new decoder parameters. REFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved passag"}]}
{"case_index": 208, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"sations (Roller et al., 2021; Zhang et al., 2020), [MASK] historical dialogue into the context enables LLMs to respond more effectively to user queries.\"", "gold": "incorporating", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 11.411, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.6771023, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 6, "text_snippet": "sations (Roller et al., 2021; Zhang et al., 2020), incorporating historical dialogue into the context enables LLMs to respond more effectively to user queries. In retrieval-augmented generation (RAG) (Guu et al., 2020; Izacard et al., 2022)"}, {"rank": 2, "score": 0.62182784, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 5, "text_snippet": "l., 2024; Liu et al., 2023). In the canonical RAG setup, the system has access to a large external corpus of text records and retrieves a subset of records that are individually relevant to the query and collectively small enough to fit int"}, {"rank": 3, "score": 0.605894, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 0, "text_snippet": "REFRAG: Rethinking RAG based Decoding Xiaoqiang Lin1,2,∗,Aritra Ghosh1,Bryan Kian Hsiang Low2,Anshumali Shrivastava1,3,Vijai Mohan1 1Meta Superintelligence Labs,2National University of Singapore,3Rice University ∗Work done at Meta Large Lan"}, {"rank": 4, "score": 0.60557467, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 6, "text_snippet": "lar to some domain and enabling easy interpretability and provenance tracking, whereas the parametric knowledge of LLMs is opaque and difficult to trace back to its source (Akyurek et al., 2022). Nevertheless, existing retrieval-augmented a"}, {"rank": 5, "score": 0.6053161, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 5, "text_snippet": "urther enhances accuracy for popular applications. Date:October 14, 2025 Correspondence:Aritra Ghosh atarighosh@meta.com Code:Will be available athttps://github.com/facebookresearch/refrag 1 Introduction Large Language Models (LLMs) have de"}]}
{"case_index": 209, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"(3) Faithful :sshould be a faithful and [MASK] summary of the input document set (i.e., smust be entailed by the input document set ( [d1, d2, ...dN])).\"", "gold": "interpretable", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 21.552, "llm_ms": 0.008, "top_contexts": [{"rank": 1, "score": 0.68825144, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 18, "text_snippet": "cy. If the retrieved documents do not contain relevant information or retrieval augmentation is not necessary, scan be an empty sequence. (2) Effecive : when sis prepended to input sequence xand provided to LM Mas a prompt, LM should genera"}, {"rank": 2, "score": 0.56149566, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 72, "text_snippet": "nerates sentences that seemingly contains the answer. Our compressor successfully reduce such erroneous behavior to 39%. Is generated summary faithful and comprehensive? We (the authors) manually evaluate outputs of the abstractive compress"}, {"rank": 3, "score": 0.55521286, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 17, "text_snippet": "is work, we assume a blackbox LM and train the compressor. Given the set of retrieved Ndocuments ( [d1, d2, ...dN]) and the input sequence x, a compressor returns a token sequence s. We design our compressor to be substantially smaller than"}, {"rank": 4, "score": 0.5456356, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 16, "text_snippet": "oth its strength and weaknesses, thereby building foundation for future work. 2 P ROBLEM FORMULATION :RECOMP Given an input sequence x, a target output sequence yand a set of Nretrieved documents D ([d1, d2, ...dN]),2RECOMP compresses retri"}, {"rank": 5, "score": 0.54359317, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 77, "text_snippet": "by compressing retrieved documents into a concise summary or an empty sequence, facilitating selective retrieval augmentation. Prompt Compression Recent work (Wingate et al., 2022; Chevalier et al., 2023; Mu et al., 2023) proposes compressi"}]}
{"case_index": 210, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"Our algorithm varies [MASK] to create a hierar- chical clustering structure: it first identifies global clusters and then performs local clustering within these global clusters.\"", "gold": "nneighbors", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 14.21, "llm_ms": 0.008, "top_contexts": [{"rank": 1, "score": 0.6270032, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 30, "text_snippet": "he preservation of local and global structures. Our algorithm varies nneighbors to create a hierar- chical clustering structure: it first identifies global clusters and then performs local clustering within these global clusters. This two-s"}, {"rank": 2, "score": 0.5961477, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 26, "text_snippet": "d tree method evaluates nodes collectively across all layers to find the most relevant ones. Clustering Algorithm Clustering plays a key role in building the RAPTOR tree, organizing text segments into cohesive groups. This step groups relat"}, {"rank": 3, "score": 0.55744755, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 9, "text_snippet": "uestions at different levels. 1arXiv:2401.18059v1 [cs.CL] 31 Jan 2024  Published as a conference paper at ICLR 2024 2 3 4 5  1 1  2  3  3  4 5  5 6  8  7 Index #8  Text: summary of  nodes 2 and 3  Child Nodes: 2, 3  Text Embedding  Text chu"}, {"rank": 4, "score": 0.5380824, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 10, "text_snippet": "rsively clusters chunks of text based on their vector embeddings and generates text summaries of those clusters, constructing a tree from the bottom up. Nodes clustered together are siblings; a parent node contains the text summary of that "}, {"rank": 5, "score": 0.5367212, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 21, "text_snippet": "ntermediate nodes thus storing varying levels of detail, keeping granular details. However, both methods, due to their reliance on adjacency for grouping or summarizing adjacent nodes, may still overlook distant interdependencies within the"}]}
{"case_index": 211, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"In the canonical RAG setup, the system has access to a large external corpus of text records and retrieves a subset of records that are [MASK] relevant to the query and collectively small enough to fit into the context window of the LLM.\"", "gold": "individually", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 11.264, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.6380531, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 29, "text_snippet": "lements (nodes, edges, covariates) that the LLM can summarize in parallel at both indexing time and query time. The “global answer” to a given query is produced using a final round of query-focused summarization over all community summaries"}, {"rank": 2, "score": 0.62527126, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 5, "text_snippet": "l., 2024; Liu et al., 2023). In the canonical RAG setup, the system has access to a large external corpus of text records and retrieves a subset of records that are individually relevant to the query and collectively small enough to fit int"}, {"rank": 3, "score": 0.5939789, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 15, "text_snippet": "e LLM’s context window. In canonical RAG approaches, the retrieval process returns a set number of records that are seman- tically similar to the query and the generated answer uses only the information in those retrieved records. A common "}, {"rank": 4, "score": 0.5835934, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 27, "text_snippet": "ns (Wang et al., 2023a; Zheng et al., 2024). Some prior work proposes criteria for having LLMs quantify the quality of 3  Source Documents Text Chunkstext extraction and chunking Entities & Relationshipsdomain-tailored summarization Knowled"}, {"rank": 5, "score": 0.57384926, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 0, "text_snippet": "From Local to Global: A GraphRAG Approach to Query-Focused Summarization Darren Edge1†Ha Trinh1†Newman Cheng2Joshua Bradley2Alex Chao3 Apurva Mody3Steven Truitt2Dasha Metropolitansky1Robert Osazuwa Ness1 Jonathan Larson1 1Microsoft Research"}]}
{"case_index": 212, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"R EPLUG We introduce REPLUG (Retrieve and Plug ), a new retrieval- augmented LM paradigm where the language model is treated as black box and the retrieval component is added as a [MASK] tuneable module.\"", "gold": "potentially", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 10.742, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.7995827, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 71, "text_snippet": "a black box and augments it with a tuneable retrieval model. Our evaluation shows that REPLUG can be integrated with any existing language model to improve their performance  REPLUG: Retrieval-Augmented Black-Box Language Models on language"}, {"rank": 2, "score": 0.78777725, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 6, "text_snippet": "PIs. Internal representations of such models are not exposed and fine-tuning is not supported. In this work, we introduce REPLUG (Retrieve and Plug ), a new retrieval-augmented LM framework where the lan- guage model is viewed as a black bo"}, {"rank": 3, "score": 0.77576756, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 8, "text_snippet": "Black-Box Language Models Figure 1, REPLUG is extremely flexible and can be used with any existing black-box LM and retrieval model. We also introduce REPLUG LSR (REPLUG with LM- Supervised Retrieval), a training scheme that can further imp"}, {"rank": 4, "score": 0.7524109, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 0, "text_snippet": "REPLUG: Retrieval-Augmented Black-Box Language Models Weijia Shi,1 *Sewon Min,1Michihiro Yasunaga,2Minjoon Seo,3Rich James,4Mike Lewis,4 Luke Zettlemoyer1 4Wen-tau Yih4 Abstract We introduce REPLUG, a retrieval-augmented lan- guage modeling"}, {"rank": 5, "score": 0.75071496, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 1, "text_snippet": "mechanisms to en- code the retrieved text, REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design can be eas- ily applied to any existing retrieval and language models. Furthermore, we show t"}]}
{"case_index": 213, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"Our RAG models achieve [MASK] results on open Natural Questions [ 29], WebQuestions [ 3] and CuratedTrec [ 2] and strongly outperform recent approaches that use specialised pre-training objectives on TriviaQA [ 24].\"", "gold": "state-of-the-art", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.932, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.7799676, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 13, "text_snippet": "sive tasks —tasks that humans could not reasonably be expected to perform without access to an external knowledge source. Our RAG models achieve state-of-the-art results on open Natural Questions [ 29], WebQuestions [ 3] and CuratedTrec [ 2"}, {"rank": 2, "score": 0.7060741, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 32, "text_snippet": "ametric knowledge. We consider four popular open-domain QA datasets: Natural Questions (NQ) [ 29], TriviaQA (TQA) [ 24]. WebQuestions (WQ) [ 3] and CuratedTrec (CT) [ 2]. As CT and WQ are small, we follow DPR [ 26] by initializing CT and WQ"}, {"rank": 3, "score": 0.68976134, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 3, "text_snippet": "cessed with a pre-trained neural retriever. We com- pare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. We ﬁne-tune and "}, {"rank": 4, "score": 0.6790546, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 41, "text_snippet": "o not require such supervision will be applicable to a wider range of tasks. We explore two variants: the standard 3-way classiﬁcation task (supports/refutes/not enough info) and the 2-way (supports/refutes) task studied in Thorne and Vlach"}, {"rank": 5, "score": 0.6603441, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 47, "text_snippet": "AG approaches state-of-the-art model performance, which is impressive given that (i) those models access gold passages with speciﬁc information required to generate the reference answer , (ii) many questions are unanswerable without the gol"}]}
{"case_index": 214, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"2 Published as a conference paper at ICLR 2024 Despite a diversity in methods, the [MASK] compon\"", "gold": "retrieving", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.891, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.6208394, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 0, "text_snippet": "Published as a conference paper at ICLR 2024 RAPTOR: R ECURSIVE ABSTRACTIVE PROCESSING FOR TREE-ORGANIZED RETRIEVAL Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D. Manning Stanford University psarthi@cs."}, {"rank": 2, "score": 0.5835346, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 17, "text_snippet": "gorithm to handle passage diversity and relevance in multi-answer retrieval. Dense Hi- erarchical Retrieval (DHR) andHybrid Hierarchical Retrieval (HHR) represent advancements in retrieval accuracy by combining document and passage level re"}, {"rank": 3, "score": 0.5750822, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 9, "text_snippet": "uestions at different levels. 1arXiv:2401.18059v1 [cs.CL] 31 Jan 2024  Published as a conference paper at ICLR 2024 2 3 4 5  1 1  2  3  3  4 5  5 6  8  7 Index #8  Text: summary of  nodes 2 and 3  Child Nodes: 2, 3  Text Embedding  Text chu"}, {"rank": 4, "score": 0.57203525, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 18, "text_snippet": " ICLR 2024 Despite a diversity in methods, the retrieving components of models predominantly rely on stan- dard approaches, i.e., chunking corpora and encoding with BERT-based retrievers. Although this approach is widely adopted, Nair et al"}, {"rank": 5, "score": 0.56991065, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 67, "text_snippet": "ring approaches, which allows it to capture a range of information, from general themes to specific details, contributing to its overall strong performance. 4.1 C ONTRIBUTION OF THE TREE STRUCTURE We examine the contribution of each layer o"}]}
{"case_index": 215, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"CRAG is [MASK] and can be seamlessly coupled with various RAG-based approaches.\"", "gold": "plug-and-play", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.25, "hit_rank": 4, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.816, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6800165, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 1, "text_snippet": "evitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Al- though retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heav"}, {"rank": 2, "score": 0.64559853, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 10, "text_snippet": " should not have been equally referred to and involved in RAG. On account of the above issues, this paper particularly studies the scenarios where the retriever returns inaccurate results. A method named Corrective Retrieval- Augmented Gene"}, {"rank": 3, "score": 0.63668776, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 84, "text_snippet": "ed to improve the robustness of generation. Essentially, a lightweight retrieval evaluator is to estimate and trigger three knowledge retrieval actions discriminately. With the further leverage of web search and optimized knowledge utilizat"}, {"rank": 4, "score": 0.6182693, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 3, "text_snippet": " decompose-then- recompose algorithm is designed for retrieved documents to selectively focus on key infor- mation and filter out irrelevant information in them. CRAG is plug-and-play and can be seamlessly coupled with various RAG-based app"}, {"rank": 5, "score": 0.6167543, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 53, "text_snippet": "s in that our motivation is to improve the retrieval quality by correcting the retrieval results that the system judges to be of low quality. This can be analogous to RAG’s augmentation to standalone parameterized language models and we fur"}]}
{"case_index": 216, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the [MASK] memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever.\"", "gold": "non-parametric", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 19.225, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.74868226, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 9, "text_snippet": "y to the “workhorse of NLP,” i.e. sequence-to-sequence (seq2seq) models. We endow pre-trained, parametric-memory generation models with a non-parametric memory through a general-purpose ﬁne-tuning approach which we refer to as retrieval-aug"}, {"rank": 2, "score": 0.7309594, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 2, "text_snippet": "anism to explicit non-parametric memory have so far been only investigated for extractive downstream tasks. We explore a general-purpose ﬁne-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trained parametri"}, {"rank": 3, "score": 0.6204093, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 11, "text_snippet": "e document is responsible for all tokens) or a per-token basis (where different documents are responsible for different tokens). Like T5 [ 51] or BART, RAG can be ﬁne-tuned on any seq2seq task, whereby both the generator and retriever are j"}, {"rank": 4, "score": 0.6170503, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 4, "text_snippet": "architectures. For language generation tasks, we ﬁnd that RAG models generate more speciﬁc, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline. 1 Introduction Pre-trained neural language models have been s"}, {"rank": 5, "score": 0.6101202, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 6, "text_snippet": "eval-augmented architecture. These models employ a non-parametric memory, e.g. a neural retriever over a large, external, potentially non-static knowledge source to enhance a parametric language model. In addition to their memorisation abil"}]}
{"case_index": 217, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"In ad- dition, versions of the GraphRAG approach are also available as extensions to multiple open- source libraries, including LangChain (LangChain, 2024), LlamaIndex (LlamaIndex, 2024), Nebu- laGraph ([MASK], 2024), and Neo4J (Neo4J, 2024).\"", "gold": "nebulagraph", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.776, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.71797323, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 13, "text_snippet": "ps://github .com/microsoft/graphrag. In ad- dition, versions of the GraphRAG approach are also available as extensions to multiple open- source libraries, including LangChain (LangChain, 2024), LlamaIndex (LlamaIndex, 2024), Nebu- laGraph ("}, {"rank": 2, "score": 0.6630422, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 19, "text_snippet": "2016; Mooney and Bunescu, 2005; Yates et al., 2007). GraphRAG falls into a more recent body of research that use of LLMs for knowledge graph extraction (Ban et al., 2023; Melnyk et al., 2022; OpenAI, 2023; Tan et al., 2017; Trajanoska et al"}, {"rank": 3, "score": 0.6564602, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 17, "text_snippet": " are later used to answer queries as in Mao et al. 2020). These summaries are generated in parallel and iteratively aggregated into global summaries, similar to prior techniques (Feng et al., 2023; Gao et al., 2023; Khattab et al., 2022; Sh"}, {"rank": 4, "score": 0.65116864, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 27, "text_snippet": "ns (Wang et al., 2023a; Zheng et al., 2024). Some prior work proposes criteria for having LLMs quantify the quality of 3  Source Documents Text Chunkstext extraction and chunking Entities & Relationshipsdomain-tailored summarization Knowled"}, {"rank": 5, "score": 0.6419444, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 117, "text_snippet": "ph rag: Retrieval-augmented genera- tion with llm based on knowledge graphs. https://www .nebula-graph .io/posts/graph-RAG. Neo4J (2024). Get started with graphrag: Neo4j’s ecosystem tools. https://neo4j .com/developer- blog/graphrag-ecosys"}]}
{"case_index": 218, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"3 M ETHODS Overview of RAPTOR Building on the idea that long texts often present subtopics and hierarchi- cal [MASK] (Cao & Wang, 2022; Dong et al., 2023b), RAPTOR addresses the issue of semantic depth and connection in\"", "gold": "structures", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.997, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.7629088, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 21, "text_snippet": "ntermediate nodes thus storing varying levels of detail, keeping granular details. However, both methods, due to their reliance on adjacency for grouping or summarizing adjacent nodes, may still overlook distant interdependencies within the"}, {"rank": 2, "score": 0.7025336, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 22, "text_snippet": " RAPTOR addresses the issue of semantic depth and connection in reading by building a recursive tree structure that balances broader thematic comprehension with granular details and which allows nodes to be grouped based on semantic sim- il"}, {"rank": 3, "score": 0.6748193, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 9, "text_snippet": "uestions at different levels. 1arXiv:2401.18059v1 [cs.CL] 31 Jan 2024  Published as a conference paper at ICLR 2024 2 3 4 5  1 1  2  3  3  4 5  5 6  8  7 Index #8  Text: summary of  nodes 2 and 3  Child Nodes: 2, 3  Text Embedding  Text chu"}, {"rank": 4, "score": 0.67248213, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 8, "text_snippet": " indexing and retrieval system that uses a tree structure to capture both high-level and low-level details about a text. As shown in Figure 1, our system, RAPTOR, clusters chunks of text, generates text summaries of those clusters, and then"}, {"rank": 5, "score": 0.6491847, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 0, "text_snippet": "Published as a conference paper at ICLR 2024 RAPTOR: R ECURSIVE ABSTRACTIVE PROCESSING FOR TREE-ORGANIZED RETRIEVAL Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D. Manning Stanford University psarthi@cs."}]}
{"case_index": 219, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"Here, this loss is only used to optimize the [MASK] of the retriever, and not the language model.\"", "gold": "parameters", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.2, "hit_rank": 5, "matched_by": "doc_id+chunk_id", "retrieval_ms": 12.038, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.63791823, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 23, "text_snippet": "ent loss functions to train the retriever jointly with the language model. We consider loss functions that leverage the language model to provide supervisory signal to train the retriever. In other words, if the language model ﬁnds a docume"}, {"rank": 2, "score": 0.6194073, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 22, "text_snippet": "ther way to process the retrieved documents in the language model would be to concatenate the query and all the documents, and to use this long sequence as input of the model. Unfortunately, this approach does not scale with the number of d"}, {"rank": 3, "score": 0.5966951, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 29, "text_snippet": "triever inREPLUG by using the LM itself to provide supervision about which documents should be retrieved. Inspired by Sachan et al. (2022), our approach can be seen as adjusting the probabilities of the retrieved documents to match the prob"}, {"rank": 4, "score": 0.592174, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 77, "text_snippet": " warmup steps. We refresh the index every 1,000 steps. This means that the index is recomputed 10 times during the pre-training, leading to an overhead of around 30%, compared to training with a ﬁxed retriever. We set the number of retrieve"}, {"rank": 5, "score": 0.58667004, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 29, "text_snippet": "om the retriever deﬁned in Equation 1: KL(pattn∥pretr) =K∑ k=1pattn(dk) log(pattn(dk) pretr(dk)) . Here, this loss is only used to optimize the parameters of the retriever, and not the language model. When using recent deep learning framewo"}]}
{"case_index": 220, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"To [MASK] others to reproduce our results, we will publish all source code l\"", "gold": "facilitate", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.3333333333333333, "hit_rank": 3, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.548, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.4678272, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 15, "text_snippet": "hers to reproduce our results, we will publish all source code later. In summary, our contributions in this paper are three-fold: 1) This paper studies the scenarios where the retriever returns inaccurate results and, to the best of our kno"}, {"rank": 2, "score": 0.4515607, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 83, "text_snippet": " NSF grant (IIS-2312948). ETHICS STATEMENT We use commercial language model to generate training data for our compressors, which might include factual error. We conduct careful human evaluation on the data generated and present our analysis"}, {"rank": 3, "score": 0.43973082, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 14, "text_snippet": "l., 2024) for demonstrating its adaptability to RAG-based approaches. Results onfour datasets of PopQA (Mallen et al., 2023), Biog- raphy (Min et al., 2023), Pub Health (Zhang et al., 2023a), and Arc-Challenge (Bhakthavatsalam et al., 2021)"}, {"rank": 4, "score": 0.41421086, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 13, "text_snippet": " (Wu et al., 2021), are not open-sourced due to commercial considerations and are only available as black-box APIs, through which users can send queries and receive responses. On the other hand, even open sourced language models such as OPT"}, {"rank": 5, "score": 0.41209975, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 23, "text_snippet": "t to retrieve in long-form generation. Compared with recent studies (Schick et al., 2023; Luo et al., 2023; Asai et al., 2024) that are the most relevant to our work, a main difference should be highlighted. These approaches target on explo"}]}
{"case_index": 221, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"In addition, our optimization framework for large context [MASK] extend the context size of LLMs by16 ×.\"", "gold": "enablesrefragto", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.161, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.65869725, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 3, "text_snippet": " on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30 .85×the time-t"}, {"rank": 2, "score": 0.63027686, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 42, "text_snippet": "xt lengths s∈ { 4096,8192,16384}. Although our model is trained on s+o= 6144, both REFRAG 8andREFRAG 16maintain superior performance at longer contexts. The original Llama-2-7B supports only a4k context window, whereas our approach enables "}, {"rank": 3, "score": 0.6299959, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 39, "text_snippet": "MA-2-7B evaluated on xs+1:s+owith only output tokens as input.LLaMA-Full Context: LLaMA-2-7B evaluated on xs+1:s+owith the full sequence x1:Tas input.CEPE: Memory-efficient long-context model (Yen et al., 2024) a previous SOTA model which s"}, {"rank": 4, "score": 0.6128478, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 69, "text_snippet": "r, due to the compression, we are able to have more context information and hence achieve better performance. Surprisingly, REFRAG 16andREFRAG 32both outperform the LLaMA FTmodel despite having2 ×and4×fewer tokens in the decoder (i.e., lowe"}, {"rank": 5, "score": 0.6113343, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 94, "text_snippet": "Complementary approaches rank or prune context by estimated informativeness, e.g.,Selective Contextuses self-information to drop low-value tokens, and sentence-level methods learn context-aware encoders for question-specific compression and"}]}
{"case_index": 222, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"As this approach is extractive, we assume the faithfulness criteria is mostly satisfied.3 Abstractive Compressor We train an [MASK] model encdec θto serve as\"", "gold": "encoder-decoder", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 13.031, "llm_ms": 0.008, "top_contexts": [{"rank": 1, "score": 0.7176726, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 22, "text_snippet": "y. Yet, using an extreme-scale model as the compressor is not desirable as we want the compressor to be substantially smaller than the LMs. Thus, we perform distillation (Hinton et al., 2015) of extreme-scale LMs to build a lightweight abst"}, {"rank": 2, "score": 0.7060571, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 20, "text_snippet": "er model encθwhich embeds sentence siand the input sequence xinto fixed- dimensional embeddings respectively. Their inner product represents how helpful it would be for the LMMto prepend sito the input xto generate y. The final summary sfro"}, {"rank": 3, "score": 0.687559, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 21, "text_snippet": "Compressor We train an encoder-decoder model encdec θto serve as an abstractive compressor, which takes the input sequence xand a concatenation of retrieved document set D [d1;d2;...dN]) and output a summary s. Although we do not have human"}, {"rank": 4, "score": 0.68586165, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 23, "text_snippet": "odels in the output should be faithful to the original input, yet the main goal is different. Instead of capturing salient information for humans readers, compressors aim to produce a concise text that are useful for a LM on an end task. In"}, {"rank": 5, "score": 0.6608542, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 2, "text_snippet": "d an abstractive compressor which generates summaries by synthesizing information from multiple documents. Both compressors are trained to improve LMs’ performance on end tasks when the generated summaries are prepended to the LMs’ input, w"}]}
{"case_index": 223, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"We use a [MASK] bi-encoder from DPR to initialize our retriever and to build the document index.\"", "gold": "pre-trained", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 11.806, "llm_ms": 0.008, "top_contexts": [{"rank": 1, "score": 0.62455964, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 10, "text_snippet": "eural retriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The retriever (Dense Passage Retriever [ 26], henceforth DPR) provides latent documents conditioned on the input, and the seq2seq model (BART"}, {"rank": 2, "score": 0.6039914, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 18, "text_snippet": "y retrieving the top-k relevant documents from a large corpus of text with the retriever. Then, these documents are fed to the language model, along with the query, which in turns generates the output. Both the retriever and the language mo"}, {"rank": 3, "score": 0.60268664, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 51, "text_snippet": "answering (Voorhees et al., 1999), documents are often retrieved from Wikipedia (Chen et al., 2017). Recently, dense retrievers based on neural networks have become popular. These usually follow a dual-encoder architecture (Yih et al., 2011"}, {"rank": 4, "score": 0.6006572, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 20, "text_snippet": "eir corresponding embeddings. The Contriever model is pre-trained using the MoCo contrastive loss (He et al., 2020), and uses unsupervised data only. As shown in the following section, an advantage of dense retrievers is that both query and"}, {"rank": 5, "score": 0.58760786, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 40, "text_snippet": "ent retriever ﬁne-tuning Retrieval is facilitated by using a document index, which is a pre-computed collection of the document embeddings for all the documents in the retrieval corpus. When jointly training the retriever and language model"}]}
{"case_index": 224, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"They are able to learn new tasks with very few examples or even from [MASK] alone.\"", "gold": "instructions", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.814, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.62174577, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 61, "text_snippet": " added to the prompt of a large language model performing in-context learning. 3.2 Few-shot learning Few-shot learning, the task of learning from very few examples, has been studied for decades (Thrun & Pratt, 1998; Fink, 2005; Vinyals et a"}, {"rank": 2, "score": 0.59072864, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 3, "text_snippet": "xamples, outperforming a 540B parameters model by 3% despite having 50x fewer parameters. 1 Introduction Large language models (LLMs) are impressive few-shot learners (Brown et al., 2020; Rae et al., 2021; Hoﬀmann et al., 2022; Chowdhery et"}, {"rank": 3, "score": 0.5791936, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 2, "text_snippet": "ings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including"}, {"rank": 4, "score": 0.57775027, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 64, "text_snippet": " learning. The above models perform few-shot learning with in-context instructions without training the parameters of the language model. Few-shot learning can also be accomplished by combining textual templates (“prompts”) and various form"}, {"rank": 5, "score": 0.5699775, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 0, "text_snippet": "Atlas: Few-shot Learning with Retrieval Augmented Language Models Gautier Izacard∗ ∗,♦,♣,♥gizacard@fb.com Patrick Lewis∗,♦plewis@fb.com Maria Lomeli♦marialomeli@fb.com Lucas Hosseini♦hoss@fb.com Fabio Petroni♦fabiopetroni@fb.com Timo Schick"}]}
{"case_index": 225, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"This al- lows to scale to large numbers of [MASK], and to beneﬁt from this large amount of evidence.\"", "gold": "documents", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 9.474, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6350561, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 1, "text_snippet": "x- pensive to train and query. In this paper, we investigate how much these models can ben- eﬁt from retrieving text passages, potentially containing evidence. We obtain state-of-the- art results on the Natural Questions and Triv- iaQA open"}, {"rank": 2, "score": 0.6265528, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 2, "text_snippet": "s a ﬂexible framework to efﬁciently aggregate and com- bine evidence from multiple passages. 1 Introduction Recently, several works have shown that factual information can be extracted from large scale language models trained on vast quanti"}, {"rank": 3, "score": 0.6200626, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 31, "text_snippet": "tion answering. We observe that while conceptu- ally simple, this method outperforms existing work on the NaturalQuestion and TriviaQA benchmarks. In particular, generative models seem to perform well when evidence from multiple passages ne"}, {"rank": 4, "score": 0.6156244, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 9, "text_snippet": "rticular, we show that the performance of our method signiﬁcantly improves when the number of retrieved passages increases. We believe that this is evidence that generative mod- els are good at combining evidence from multiple passages, com"}, {"rank": 5, "score": 0.5897395, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 0, "text_snippet": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering Gautier Izacard1,2,3Edouard Grave1 1Facebook AI Research, Paris 2ENS, PSL University, Paris 3Inria, Paris gizacard|egrave@fb.com Abstract Generative mode"}]}
{"case_index": 226, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"For this [MASK] ability to emerge, the key ingredients are scaling both the parameter count of the model, and the size of the training data.\"", "gold": "generalisation", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.3333333333333333, "hit_rank": 3, "matched_by": "doc_id+chunk_id", "retrieval_ms": 19.916, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.65502846, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 4, "text_snippet": " model, and the size of the training data. Large language models owe this improvement to both a larger computational budget, enabling more complex reasoning, and the ability to memorize more information related to downstream tasks from the "}, {"rank": 2, "score": 0.63213193, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 63, "text_snippet": "s learning ability, leading to the further development of large models (Lieber et al., 2021; Rae et al., 2021; Smith et al., 2022; Chowdhery et al., 2022; Smith et al., 2022). Hoﬀmann et al. (2022) revisited the scaling law from Kaplan et a"}, {"rank": 3, "score": 0.6263436, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 3, "text_snippet": "xamples, outperforming a 540B parameters model by 3% despite having 50x fewer parameters. 1 Introduction Large language models (LLMs) are impressive few-shot learners (Brown et al., 2020; Rae et al., 2021; Hoﬀmann et al., 2022; Chowdhery et"}, {"rank": 4, "score": 0.61924887, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 1, "text_snippet": " Abstract Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter count"}, {"rank": 5, "score": 0.61348355, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 62, "text_snippet": "arge Language models. Providing language models with natural language descriptions of tasks, as proposed by Radford et al. (2019) has led to signiﬁcant developments in few-shot learning. GPT-3 (Brown et al., 2020) demonstrated the ability o"}]}
{"case_index": 227, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"f the input (i.e., s≫q) and hence the overall input to the decoder will be [MASK] by a factor of ≃k.\"", "gold": "reduced", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.469, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6275003, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 22, "text_snippet": "f the input (i.e., s≫q) and hence the overall input to the decoder will be reduced by a factor of ≃k. This architectural design leads to significant reductions in both latency and memory usage, primarily due to the shortened input sequence."}, {"rank": 2, "score": 0.5050647, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 21, "text_snippet": "uery  Encoder Figure 1The main design ofREFRAG. The input context is chunked and processed by the light-weight encoder to produce chunk embeddings, which are precomputable for efficient reuse. A light-weight RL policy decide few chunks to e"}, {"rank": 3, "score": 0.4974459, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 12, "text_snippet": "is information is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-a"}, {"rank": 4, "score": 0.49601766, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 3, "text_snippet": " on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30 .85×the time-t"}, {"rank": 5, "score": 0.49191242, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 15, "text_snippet": "igure 1) while preserving the autoregressive nature of the decoder, thereby supporting multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight reinforcement learning (RL) policy that sel"}]}
{"case_index": 228, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"Here, we propose an alternative which gives slightly stronger results, which relies on the following [MASK].\"", "gold": "observation", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 7.461, "llm_ms": 0.013, "top_contexts": [{"rank": 1, "score": 0.58059883, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 12, "text_snippet": "ntributions: •A thorough study on how to design and train retrieval-augmented language models, with a focus on downstream few-shot learning and sample eﬃciency. •The ﬁndings of this study lead to a retrieval-augmented language model, called"}, {"rank": 2, "score": 0.57188964, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 1, "text_snippet": " Abstract Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter count"}, {"rank": 3, "score": 0.5576065, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 0, "text_snippet": "Atlas: Few-shot Learning with Retrieval Augmented Language Models Gautier Izacard∗ ∗,♦,♣,♥gizacard@fb.com Patrick Lewis∗,♦plewis@fb.com Maria Lomeli♦marialomeli@fb.com Lucas Hosseini♦hoss@fb.com Fabio Petroni♦fabiopetroni@fb.com Timo Schick"}, {"rank": 4, "score": 0.556458, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 57, "text_snippet": "nted generators using a second “informed” retriever with access to the output, which the test-time retriever can be distilled from, and Hofstätter et al. (2022) recently proposed a training set ﬁltering/weighting approach to train stronger "}, {"rank": 5, "score": 0.5556357, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 90, "text_snippet": "ctives. We thus decide adopt use Perplexity Distillation for subsequent experiments, as it tends to be more stable than EMDR2or ADist, and more computationally eﬃcient than LOOP. Next, we compare the diﬀerent self-supervised pretext tasks i"}]}
{"case_index": 229, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"Recent work on LLM refinement (Dhuliawala et al., 2023; Madaan et al., 2023; Paul et al., 2023) prompts a model to generate task output, natural language feedback and refined task output [MASK], but at the cost of inference efficiency.\"", "gold": "iteratively", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.926, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.719457, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 27, "text_snippet": "n (reasoning path consistency) and without retrieval. Recent work on LLM refinement (Dhuliawala et al., 2023; Madaan et al., 2023; Paul et al., 2023) prompts a model to generate task output, natural language feedback and refined task output"}, {"rank": 2, "score": 0.68646, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 135, "text_snippet": "ns con- sumed and generated by the model. Self-reflection is a prompt engineering technique where the LLM generates an answer, and is then prompted to evaluate its output for correctness, clarity, or completeness, then finally generate an i"}, {"rank": 3, "score": 0.6605253, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 21, "text_snippet": " al., 2023a; Gao et al., 2023). We introduce a method to train an arbitrary LM to learn to use retrieval on-demand for diverse instruction-following queries and introduce controlled generation guided by reflections tokens to further improve"}, {"rank": 4, "score": 0.65837276, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 15, "text_snippet": "st a critic model during training, reducing overhead. The critic model, in part, is supervised on a dataset of input, output, and corresponding reflection tokens collected by prompting a propriety LM (i.e., GPT-4; OpenAI 2023). While we dra"}, {"rank": 5, "score": 0.65767366, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 23, "text_snippet": "ess retrieved passages before using them to prompt the LM to generate the output. SELF-RAGprocesses passages in parallel and filters out irrelevant ones through self-reflection, without relying on external models at inference. Moreover, our"}]}
{"case_index": 230, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"This means that all the tasks are framed as follows: the system gets a text query as input, and [MASK]\"", "gold": "generat", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 8.874, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.62752414, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 16, "text_snippet": "d as follows: the system gets a text query as input, and generates a text output . For example, in the case of question answering, the query corresponds to the question and the model needs to generate the answer. In the case of classiﬁcatio"}, {"rank": 2, "score": 0.5747462, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 2, "text_snippet": "ings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including"}, {"rank": 3, "score": 0.55150586, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}, {"rank": 4, "score": 0.55099237, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 154, "text_snippet": "odel. By jointly pre-training the retriever module and the language model, we show that Atlashas strong few-shot learning capabilities on a wide range of knowledge intensive tasks, including NaturalQuestions, TriviaQA, FEVER, 8 KILT tasks a"}, {"rank": 5, "score": 0.5498355, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 17, "text_snippet": "chmark in Figure 2. As many natural language processing tasks require knowledge , our goal is to enhance standard text-to-text models with retrieval, which, as we hypothesise in the introduction, may be crucial to endow models with few-shot"}]}
{"case_index": 231, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"that answers do not [MASK] to spans in support documents, thus requiring ab- stractive models.\"", "gold": "correspond", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 12.849, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6169106, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 15, "text_snippet": " that answers do not correspond to spans in support documents, thus requiring ab- stractive models. Raffel et al. (2019) showed that generative models are competitive for reading com- prehension tasks such as SQuAD (Rajpurkar et al., 2016),"}, {"rank": 2, "score": 0.5704075, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 6, "text_snippet": "tech- niques have been considered, either using sparse representations based on TF/IDF or using dense embeddings (Guu et al., 2020; Karpukhin et al., 2020). The models which extract the answers are often based on contextualized word represe"}, {"rank": 3, "score": 0.5615986, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 5, "text_snippet": "t, it retrieves support text passages from an external source of knowledge such as Wikipedia. Then, a generative encoder-decoder model produces the answer, conditioned on the question and the re- trieved passages. This approach scales well "}, {"rank": 4, "score": 0.5445931, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 10, "text_snippet": " problem in natural lan- guage processing (V oorhees et al., 1999), this task has recently regained interest following the work by Chen et al. (2017). In that version of the prob- lem, strong supervision is available to the learning system,"}, {"rank": 5, "score": 0.5348813, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}]}
{"case_index": 232, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"Retrieval Methods [MASK] language models (RALMs) have seen improvements in various components: the retriever, the reader, and end-to-end system trai\"", "gold": "retrieval-augmented", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 14.0, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6944492, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 13, "text_snippet": "nd to underutilize long-range context and see diminishing performance as con- text length increases, especially when pertinent information is embedded within a lengthy context. Moreover, practically, use of long contexts is expensive and sl"}, {"rank": 2, "score": 0.69334036, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 4, "text_snippet": "trieval-augmented language models (RALMs) (Khandelwal et al., 2019; Izacard et al., 2022; Lewis et al., 2020; Borgeaud et al., 2022) have shown impressive performance on knowledge-intensive tasks (Kwiatkowski et al., 2019; Petroni et al., 2"}, {"rank": 3, "score": 0.6845447, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 14, "text_snippet": "omponents: the retriever, the reader, and end-to-end system training. Retrieval methods have transitioned from traditional term-based techniques like TF-IDF (Sp¨arck Jones, 1972) and BM25 (Robertson et al., 1995; Roberts et al., 2020) to de"}, {"rank": 4, "score": 0.64444315, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 2, "text_snippet": "hat retrieval with recursive summaries offers significant improvements over tra- ditional retrieval-augmented LMs on several tasks. On question-answering tasks that involve complex, multi-step reasoning, we show state-of-the-art results; fo"}, {"rank": 5, "score": 0.6321045, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 0, "text_snippet": "Published as a conference paper at ICLR 2024 RAPTOR: R ECURSIVE ABSTRACTIVE PROCESSING FOR TREE-ORGANIZED RETRIEVAL Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D. Manning Stanford University psarthi@cs."}]}
{"case_index": 233, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"We achieve a compression rate of as low as 6% with minimal loss in performance for both tasks, significantly outperforming the off-the-shelf [MASK] models.\"", "gold": "summarization", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.371, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6444074, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 3, "text_snippet": "n. We evaluate our approach on language modeling task and open domain question answering task. We achieve a compression rate of as low as 6% with minimal loss in performance for both tasks, significantly outperforming the off-the-shelf summ"}, {"rank": 2, "score": 0.6443941, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 58, "text_snippet": "ning task. We observe a performance regression as the compression rate increases; however, even at a compression rate of32, our model remains competitive (as shown in table 1). In contrast, a compression rate of64appears to be overly aggres"}, {"rank": 3, "score": 0.642957, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 15, "text_snippet": "6% and significantly outperforms prepending full documents. Our trained compressors also show promising results. For language modelling, both trained compressors achieve a compression ratio of 25% with minimal performance drop. When applied"}, {"rank": 4, "score": 0.5987706, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 14, "text_snippet": " significantly fewer tokens compared to RALM without compression. We present two oracle compression methods – an extractive oracle which selects a sentence in evidence documents that leads to the best task performance and an abstractive ora"}, {"rank": 5, "score": 0.5964477, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 45, "text_snippet": " with minimal information loss. Ideally, this approach should outperform random selection, which is indeed observed in figure 3. The RL-based selective compression policy consistently achieves superior performance across varying compression"}]}
{"case_index": 234, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"If the retrieved documents do not contain relevant information or retrieval [MASK] is not necessary, scan be an empty sequence.\"", "gold": "augmentation", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.503, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.6277948, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 18, "text_snippet": "cy. If the retrieved documents do not contain relevant information or retrieval augmentation is not necessary, scan be an empty sequence. (2) Effecive : when sis prepended to input sequence xand provided to LM Mas a prompt, LM should genera"}, {"rank": 2, "score": 0.5569801, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 77, "text_snippet": "by compressing retrieved documents into a concise summary or an empty sequence, facilitating selective retrieval augmentation. Prompt Compression Recent work (Wingate et al., 2022; Chevalier et al., 2023; Mu et al., 2023) proposes compressi"}, {"rank": 3, "score": 0.5473406, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 0, "text_snippet": "RECOMP: I MPROVING RETRIEVAL -AUGMENTED LM S WITH COMPRESSION AND SELECTIVE AUGMENTATION Fangyuan Xu1, Weijia Shi2, Eunsol Choi1 Department of Computer Science 1The University of Texas at Austin 2University of Washington {fangyuan,eunsol }@"}, {"rank": 4, "score": 0.5326617, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 41, "text_snippet": "r the full collection of documents regularly during training to keep the index fresh, which can be computationally expensive for large indices. This is particularly true at ﬁne-tuning time, where the number of training examples could be sma"}, {"rank": 5, "score": 0.52922314, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}]}
{"case_index": 235, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"GraphRAG contrasts with these approaches by generating a graph index from the source data, then applying graph-based community detection to create a thematic [MASK] of the data.\"", "gold": "partitioning", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 9.824, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.67114854, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 44, "text_snippet": "ummaries at one level looking for general themes of interest, then read linked reports at a lower level that provide additional details for each subtopic. Here, however, we focus on their utility as part of a graph-based index used for answ"}, {"rank": 2, "score": 0.65732133, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 29, "text_snippet": "lements (nodes, edges, covariates) that the LLM can summarize in parallel at both indexing time and query time. The “global answer” to a given query is produced using a final round of query-focused summarization over all community summaries"}, {"rank": 3, "score": 0.65271103, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 9, "text_snippet": "tirety of a large text corpus. GraphRAG first uses an LLM to construct a knowledge graph, where nodes correspond to key entities in the corpus and edges represent relationships between those entities. Next, it partitions the graph into a hi"}, {"rank": 4, "score": 0.65015525, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 10, "text_snippet": "cted communities, with summaries at higher levels of the hierarchy recursively incorporating lower-level summaries. Together, these community summaries provide global descriptions and insights over the corpus. Finally, GraphRAG answers quer"}, {"rank": 5, "score": 0.63369876, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 17, "text_snippet": " are later used to answer queries as in Mao et al. 2020). These summaries are generated in parallel and iteratively aggregated into global summaries, similar to prior techniques (Feng et al., 2023; Gao et al., 2023; Khattab et al., 2022; Sh"}]}
{"case_index": 236, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"igure 1) while preserving the [MASK] nature of the decoder, thereby supporting multi-turn and agentic applications.\"", "gold": "autoregressive", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 15.143, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.6615042, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 15, "text_snippet": "igure 1) while preserving the autoregressive nature of the decoder, thereby supporting multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight reinforcement learning (RL) policy that sel"}, {"rank": 2, "score": 0.5881254, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 17, "text_snippet": "FRAG achieves better performance than LLaMA without incurring higher latency in the downstream applications. 2 Model Architecture We denote the decoder model as Mdecand the encoder model as Menc. Given an input with Ttokens x1, x2, . . . , "}, {"rank": 3, "score": 0.57069457, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 12, "text_snippet": "is information is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-a"}, {"rank": 4, "score": 0.5645081, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 3, "text_snippet": " on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30 .85×the time-t"}, {"rank": 5, "score": 0.5640061, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 21, "text_snippet": "uery  Encoder Figure 1The main design ofREFRAG. The input context is chunked and processed by the light-weight encoder to produce chunk embeddings, which are precomputable for efficient reuse. A light-weight RL policy decide few chunks to e"}]}
{"case_index": 237, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"Reflection tokens, inspired by reward models used in [MASK] learning (Ziegler et al., 2019; Ouyang et al., 2022), are inserted offline into the original corpus by a trained critic model.\"", "gold": "reinforcement", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 12.846, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.72635543, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 14, "text_snippet": "t with reflection tokens by unifying them as the next token prediction from the expanded model vocabulary. We train our generator LM on a diverse collection of text interleaved with reflection tokens and retrieved passages. Reflection token"}, {"rank": 2, "score": 0.7245899, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 15, "text_snippet": "st a critic model during training, reducing overhead. The critic model, in part, is supervised on a dataset of input, output, and corresponding reflection tokens collected by prompting a propriety LM (i.e., GPT-4; OpenAI 2023). While we dra"}, {"rank": 3, "score": 0.6848371, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 36, "text_snippet": "text with reflection tokens by unifying them as next token predictions from the expanded model vocabulary (i.e., the original vocabulary plus reflection tokens). Specifically, we train the generator model Mon a curated corpus with interleav"}, {"rank": 4, "score": 0.6662933, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 48, "text_snippet": "gmented with reflection tokens Dgenusing the standard next token objective: max ME(x,y,r )∼DgenlogpM(y, r|x). (2) Unlike Ctraining (Eq. 1), Mlearns to predict the target output as well as the reflection tokens. During training, we mask out "}, {"rank": 5, "score": 0.6635171, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 149, "text_snippet": "tuned reward model shows high prediction matching with GPT-4 predicted feedback. 17  Preprint. Algorithm 2 SELF-RAGTraining 1:Input input-output data D={X, Y}, generator M,Cθ 2:Initialize Cwith a pre-trained LM 3:Sample data {Xsample, Ysamp"}]}
{"case_index": 238, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"3)Unusually Structured and Sparse Attention.Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero [MASK] between chunks (see figure 7).\"", "gold": "cross-attention", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.402, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.72729135, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 12, "text_snippet": "is information is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-a"}, {"rank": 2, "score": 0.6386298, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 2, "text_snippet": "ated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns th"}, {"rank": 3, "score": 0.61915004, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 14, "text_snippet": "ortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now "}, {"rank": 4, "score": 0.600419, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 9, "text_snippet": "f research, with approaches 1arXiv:2509.01092v2 [cs.CL] 12 Oct 2025  ranging from modifying the attention mechanism’s complexity (Beltagy et al., 2020) to sparsifying attention and context (Child et al., 2019; Xiao et al., 2024; Jiang et al"}, {"rank": 5, "score": 0.58313644, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 3, "text_snippet": " on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30 .85×the time-t"}]}
{"case_index": 239, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"rticular, we show that the [MASK] of our method signiﬁcantly improves when the number of retrieved passages increases.\"", "gold": "performance", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 9.935, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.68880635, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 9, "text_snippet": "rticular, we show that the performance of our method signiﬁcantly improves when the number of retrieved passages increases. We believe that this is evidence that generative mod- els are good at combining evidence from multiple passages, com"}, {"rank": 2, "score": 0.6771329, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}, {"rank": 3, "score": 0.6737824, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 31, "text_snippet": "tion answering. We observe that while conceptu- ally simple, this method outperforms existing work on the NaturalQuestion and TriviaQA benchmarks. In particular, generative models seem to perform well when evidence from multiple passages ne"}, {"rank": 4, "score": 0.6631467, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 37, "text_snippet": "nswering, which relies on retriev- ing support passages before processing them with a generative model. We show that while conceptually simple, this approach is competitive with existing methods, and that it scales well with the number of r"}, {"rank": 5, "score": 0.6389221, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 1, "text_snippet": "x- pensive to train and query. In this paper, we investigate how much these models can ben- eﬁt from retrieving text passages, potentially containing evidence. We obtain state-of-the- art results on the Natural Questions and Triv- iaQA open"}]}
{"case_index": 240, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG [MASK].\"", "gold": "applications", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 11.82, "llm_ms": 0.008, "top_contexts": [{"rank": 1, "score": 0.78675914, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 3, "text_snippet": " on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30 .85×the time-t"}, {"rank": 2, "score": 0.7287458, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 95, "text_snippet": "oducedREFRAG, a novel and efficient decoding framework tailored for RAG applications. By leveraging the inherent sparsity and block-diagonal attention patterns present in RAG contexts,REFRAG compresses, senses, and expands context represent"}, {"rank": 3, "score": 0.72702837, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 12, "text_snippet": "is information is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-a"}, {"rank": 4, "score": 0.7081809, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 94, "text_snippet": "Complementary approaches rank or prune context by estimated informativeness, e.g.,Selective Contextuses self-information to drop low-value tokens, and sentence-level methods learn context-aware encoders for question-specific compression and"}, {"rank": 5, "score": 0.6956347, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 15, "text_snippet": "igure 1) while preserving the autoregressive nature of the decoder, thereby supporting multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight reinforcement learning (RL) policy that sel"}]}
{"case_index": 241, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"Nevertheless, LLMs inevitably manifest [MASK] (Ji et al., 2023) due to their struggle with factual errors (Mallen et al., 2023; Min et al., 2023) and inability to secure the accuracy of generated texts solely by *Equal contribution.\"", "gold": "hallucinations", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.3333333333333333, "hit_rank": 3, "matched_by": "doc_id+chunk_id", "retrieval_ms": 17.543, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.6606813, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 4, "text_snippet": " for long-form generations relative to these models.1 1 I NTRODUCTION State-of-the-art LLMs continue to struggle with factual errors (Mallen et al., 2023; Min et al., 2023) despite their increased model and data scale (Ouyang et al., 2022)."}, {"rank": 2, "score": 0.635822, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 1, "text_snippet": "evitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Al- though retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heav"}, {"rank": 3, "score": 0.6278508, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 4, "text_snippet": "Ms) have attracted increasing attention and exhibited impressive abili- ties to understand instructions and generate fluent language texts (Brown et al., 2020; Ouyang et al., 2022; Touvron et al., 2023a). Nevertheless, LLMs inevitably manif"}, {"rank": 4, "score": 0.6187635, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 5, "text_snippet": "ks (Ram et al., 2023; Asai et al., 2023a). However, these methods may hinder the versatility of LLMs or introduce unnecessary or off-topic passages that lead to low-quality generations (Shi et al., 2023) since they retrieve passages indiscr"}, {"rank": 5, "score": 0.6013919, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 97, "text_snippet": "meters or with conventional retrieval-augmented generation approaches. 10  Preprint. ETHICAL CONCERNS This work aims to improve the factuality of LLM outputs, the lack of which continues to cause nu- merous real-world problems (e.g., spread"}]}
{"case_index": 242, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform- ers Library [ 66] and can be found at https://github.com/huggingface/[MASK]/blob/master/ examples/rag/ .\"", "gold": "transformers", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 37.623, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.6974519, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 16, "text_snippet": "Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform- ers Library [ 66] and can be found at https://github.com/huggingface/transformers/blob/master/ examples/rag/ . An interactive demo of RAG models ca"}, {"rank": 2, "score": 0.6743401, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 139, "text_snippet": "Wikipedia. After submission, We have ported our code to HuggingFace Transformers [ 66]3, which achieves equivalent performance to the previous version but is a cleaner and easier to use implementation. This version is also open-sourced. We "}, {"rank": 3, "score": 0.57509774, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 132, "text_snippet": "doi: 10.18653/v1/W18-5713. URL https://www.aclweb.org/anthology/W18-5713 . 15  [66] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Daviso"}, {"rank": 4, "score": 0.5430466, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 15, "text_snippet": " update the models’ knowledge as the world changes.1 2 Methods We explore RAG models, which use the input sequence xto retrieve text documents zand use them as additional context when generating the target sequence y. As shown in Figure 1, "}, {"rank": 5, "score": 0.5358925, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 113, "text_snippet": ", Rockt ¨aschel, T., et al. (2020). Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems , 33:9459–9474. Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni,"}]}
{"case_index": 243, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"(2021), which is inspired by the [MASK] algorithm, treating re\"", "gold": "expectation-maximization", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 8.182, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6274162, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 0, "text_snippet": "Atlas: Few-shot Learning with Retrieval Augmented Language Models Gautier Izacard∗ ∗,♦,♣,♥gizacard@fb.com Patrick Lewis∗,♦plewis@fb.com Maria Lomeli♦marialomeli@fb.com Lucas Hosseini♦hoss@fb.com Fabio Petroni♦fabiopetroni@fb.com Timo Schick"}, {"rank": 2, "score": 0.61362696, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 12, "text_snippet": "ntributions: •A thorough study on how to design and train retrieval-augmented language models, with a focus on downstream few-shot learning and sample eﬃciency. •The ﬁndings of this study lead to a retrieval-augmented language model, called"}, {"rank": 3, "score": 0.60376143, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 61, "text_snippet": " added to the prompt of a large language model performing in-context learning. 3.2 Few-shot learning Few-shot learning, the task of learning from very few examples, has been studied for decades (Thrun & Pratt, 1998; Fink, 2005; Vinyals et a"}, {"rank": 4, "score": 0.6005843, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 110, "text_snippet": "s of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 874–880, Online, April 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.eacl-main.74. https://aclant"}, {"rank": 5, "score": 0.5884104, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 1, "text_snippet": " Abstract Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter count"}]}
{"case_index": 244, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"is exciting, such models do have down- sides: They cannot easily expand or revise their memory, can’t [MASK] provide insight into their predictions, and may produce “hallucinations” [ 38].\"", "gold": "straightforwardly", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.639, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.67043674, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 5, "text_snippet": " is exciting, such models do have down- sides: They cannot easily expand or revise their memory, can’t straightforwardly provide insight into their predictions, and may produce “hallucinations” [ 38]. Hybrid models that combine parametric m"}, {"rank": 2, "score": 0.58053285, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 3, "text_snippet": "itly in their parameters. However, they are also prone to hallucination and cannot represent the full long tail of knowledge from the training corpus. Retrieval-augmented language models (Khandelwal et al., 2020; Borgeaud et al., 2022; Izac"}, {"rank": 3, "score": 0.5682189, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 1, "text_snippet": "ameters, and achieve state-of-the-art results when ﬁne-tuned on down- stream NLP tasks. However, their ability to access and precisely manipulate knowl- edge is still limited, and hence on knowledge-intensive tasks, their performance lags b"}, {"rank": 4, "score": 0.5612726, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 1, "text_snippet": "evitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Al- though retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heav"}, {"rank": 5, "score": 0.55059975, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 12, "text_snippet": "ck- augmented networks [ 25] and memory layers [ 30]. In contrast, we explore a setting where both parametric and non-parametric memory components are pre-trained and pre-loaded with extensive knowledge. Crucially, by using pre-trained acce"}]}
{"case_index": 245, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"In other words, we would like the [MASK] to find documents that result in lower perplex- ity scores.\"", "gold": "retriever", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.528, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6200015, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 9, "text_snippet": "22) that adapts language models to the retriever. We use a training objective which prefers retrieving documents that improve language model perplexity, while treating the LM as a frozen, black-box scoring function. Our experiments show tha"}, {"rank": 2, "score": 0.6131958, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 29, "text_snippet": "triever inREPLUG by using the LM itself to provide supervision about which documents should be retrieved. Inspired by Sachan et al. (2022), our approach can be seen as adjusting the probabilities of the retrieved documents to match the prob"}, {"rank": 3, "score": 0.60839254, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 63, "text_snippet": " the performance of REPLUG andREPLUG LSR improved monotonically. How- ever, a small number of documents (e.g., 10) was sufficient to achieve large performance gains.  REPLUG: Retrieval-Augmented Black-Box Language Models Perplexity 14.0016."}, {"rank": 4, "score": 0.573618, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 7, "text_snippet": " retrieved documents are prepended to the input context and fed into the black-box LM to make the final prediction. Because the LM context length limits the number of documents that can be prepended, we also introduce a new ensemble scheme "}, {"rank": 5, "score": 0.5652467, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 1, "text_snippet": "mechanisms to en- code the retrieved text, REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design can be eas- ily applied to any existing retrieval and language models. Furthermore, we show t"}]}
{"case_index": 246, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"3.1.1 Source [MASK] →Text Chunks To start, the documents in the corpus are split into text chunks.\"", "gold": "documents", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.3333333333333333, "hit_rank": 3, "matched_by": "doc_id+chunk_id", "retrieval_ms": 9.936, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.61084557, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 2, "score": 0.5925461, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 37, "text_snippet": " To cast language modeling in the text-to-text framework, we consider a chunk of Nwords, and split this chunk in two sub-sequences of equal length N/2. Then, the ﬁrst sub-sequence is used as the query, and the second corresponds to the outp"}, {"rank": 3, "score": 0.5882426, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 31, "text_snippet": "oach. We also validate results using statistics derived from LLM-extracted statements of verifiable facts, or “claims.” 3 Methods 3.1 GraphRAG Workflow Figure 1 illustrates the high-level data flow of the GraphRAG approach and pipeline. In "}, {"rank": 4, "score": 0.57721716, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 27, "text_snippet": "ns (Wang et al., 2023a; Zheng et al., 2024). Some prior work proposes criteria for having LLMs quantify the quality of 3  Source Documents Text Chunkstext extraction and chunking Entities & Relationshipsdomain-tailored summarization Knowled"}, {"rank": 5, "score": 0.57247186, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 32, "text_snippet": " The LLM extracts information from each chunk for downstream processing. Selecting the size of the chunk is a fundamental design decision; longer text chunks require fewer LLM calls for such extraction (which reduces cost) but suffer from d"}]}
{"case_index": 247, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"First, it increases computational costs as LMs now encode [MASK] more tokens.\"", "gold": "substantially", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.075, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.6139819, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 5, "text_snippet": " but such approach comes with limitations. First, it increases computational costs as LMs now encode substantially more tokens. Second, even if we manage to adapt LMs to efficiently incorporate longer context (Beltagy et al., 2020; Zaheer e"}, {"rank": 2, "score": 0.603611, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 93, "text_snippet": "uce input token length to lower latency and cost while maintaining task performance. A prominent approach isLLMLingua(Jiang et al., 2023),which employs coarse-to-fine, budget-controlled compression with token-level iterative refinement, ach"}, {"rank": 3, "score": 0.5863805, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 0, "text_snippet": "RECOMP: I MPROVING RETRIEVAL -AUGMENTED LM S WITH COMPRESSION AND SELECTIVE AUGMENTATION Fangyuan Xu1, Weijia Shi2, Eunsol Choi1 Department of Computer Science 1The University of Texas at Austin 2University of Washington {fangyuan,eunsol }@"}, {"rank": 4, "score": 0.578432, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 7, "text_snippet": "at increasing prompt length for contextual learning leads to higher latency and greater memory consumption during inference (Yen et al., 2024). Specifically, longer prompts require additional memory for the key-value (KV) cache, which scale"}, {"rank": 5, "score": 0.55885047, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 77, "text_snippet": "by compressing retrieved documents into a concise summary or an empty sequence, facilitating selective retrieval augmentation. Prompt Compression Recent work (Wingate et al., 2022; Chevalier et al., 2023; Mu et al., 2023) proposes compressi"}]}
{"case_index": 248, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token [MASK] ef\"", "gold": "allocation", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.3333333333333333, "hit_rank": 3, "matched_by": "doc_id+chunk_id", "retrieval_ms": 16.318, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.6635801, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 14, "text_snippet": "ortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now "}, {"rank": 2, "score": 0.6552016, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 3, "text_snippet": " on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30 .85×the time-t"}, {"rank": 3, "score": 0.6487698, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 13, "text_snippet": "nd memory usage during decoding, allwithout requiring modificationsto the LLM architecture or introducing new decoder parameters. REFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved passag"}, {"rank": 4, "score": 0.6300699, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 15, "text_snippet": "igure 1) while preserving the autoregressive nature of the decoder, thereby supporting multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight reinforcement learning (RL) policy that sel"}, {"rank": 5, "score": 0.62742996, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 12, "text_snippet": "is information is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-a"}]}
{"case_index": 249, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"lements (nodes, edges, [MASK]) that the LLM can summarize in parallel at both indexing time and query time.\"", "gold": "covariates", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.695, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.6807841, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 29, "text_snippet": "lements (nodes, edges, covariates) that the LLM can summarize in parallel at both indexing time and query time. The “global answer” to a given query is produced using a final round of query-focused summarization over all community summaries"}, {"rank": 2, "score": 0.6407647, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 27, "text_snippet": "ns (Wang et al., 2023a; Zheng et al., 2024). Some prior work proposes criteria for having LLMs quantify the quality of 3  Source Documents Text Chunkstext extraction and chunking Entities & Relationshipsdomain-tailored summarization Knowled"}, {"rank": 3, "score": 0.633117, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 28, "text_snippet": "n Indexing Time Query Time Pipeline Stage Figure 1: Graph RAG pipeline using an LLM-derived graph index of source document text. This graph index spans nodes (e.g., entities), edges (e.g., relationships), and covariates (e.g., claims) that "}, {"rank": 4, "score": 0.59397954, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 9, "text_snippet": "tirety of a large text corpus. GraphRAG first uses an LLM to construct a knowledge graph, where nodes correspond to key entities in the corpus and edges represent relationships between those entities. Next, it partitions the graph into a hi"}, {"rank": 5, "score": 0.5900249, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 20, "text_snippet": "elements of the graph, or properties of the graph structure directly in the prompt (Baek et al., 2023; He et al., 2024; Zhang, 2023) or as factual grounding for generated outputs (Kang et al., 2023; Ranade and Joshi, 2023). Other techniques"}]}
{"case_index": 250, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"In BM25, passages are [MASK] as bag of words, and the ranking function is based on term and inverse doc- ument frequencies.\"", "gold": "represented", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 15.645, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.53334594, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 19, "text_snippet": "den test set (right), competitions.codalab.org/competitions/17208#results ). Retrieval. For the retrieval of support passages, we consider two methods: BM25 (Robertson et al., 1995) and DPR (Karpukhin et al., 2020). In BM25, passages are re"}, {"rank": 2, "score": 0.52025026, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 3, "score": 0.4946847, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 37, "text_snippet": " To cast language modeling in the text-to-text framework, we consider a chunk of Nwords, and split this chunk in two sub-sequences of equal length N/2. Then, the ﬁrst sub-sequence is used as the query, and the second corresponds to the outp"}, {"rank": 4, "score": 0.49059772, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 12, "text_snippet": "018b) described a technique to aggregate answers from different paragraphs, using conﬁdence and coverage scores. Passage retrieval is an important step in open domain question answering, and is an active area of research to improve QA syste"}, {"rank": 5, "score": 0.48995996, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 61, "text_snippet": "riever to a word overlap-based BM25 retriever [ 53]. Here, we replace RAG’s retriever with a ﬁxed BM25 system, and use BM25 retrieval scores as logits when calculating p(z|x). Table 6 shows the results. For FEVER, BM25 performs best, perhap"}]}
{"case_index": 251, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"at increasing prompt length for contextual learning leads to higher latency and greater memory [MASK] during inference (Yen et al., 2024).\"", "gold": "consumption", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 16.892, "llm_ms": 0.014, "top_contexts": [{"rank": 1, "score": 0.7358141, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 7, "text_snippet": "at increasing prompt length for contextual learning leads to higher latency and greater memory consumption during inference (Yen et al., 2024). Specifically, longer prompts require additional memory for the key-value (KV) cache, which scale"}, {"rank": 2, "score": 0.64531523, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 3, "text_snippet": " on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30 .85×the time-t"}, {"rank": 3, "score": 0.6199095, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 12, "text_snippet": "is information is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-a"}, {"rank": 4, "score": 0.61582416, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 8, "text_snippet": "As a result, LLM inference throughput degrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low latency, such as web-scale discovery. Therefore, developing novel model architectures that opti"}, {"rank": 5, "score": 0.6136702, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 94, "text_snippet": "Complementary approaches rank or prune context by estimated informativeness, e.g.,Selective Contextuses self-information to drop low-value tokens, and sentence-level methods learn context-aware encoders for question-specific compression and"}]}
{"case_index": 252, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"t prior probability pη(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be [MASK] solved in sub-linear time [ 23].\"", "gold": "approximately", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 15.062, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.48911732, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 8, "text_snippet": " retriever ( Query Encoder +Document Index ) with a pre-trained seq2seq model ( Generator ) and ﬁne-tune end-to-end. For query x, we use Maximum Inner Product Search (MIPS) to ﬁnd the top-K documents zi. For ﬁnal prediction y, we treatzas a"}, {"rank": 2, "score": 0.48397416, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 22, "text_snippet": "t prior probability pη(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [ 23]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index."}, {"rank": 3, "score": 0.4617676, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 4, "score": 0.46006483, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 31, "text_snippet": "′⊂ D with the highest simi- larity scores from a corpus Dgiven an input context x, as described in §3.1. We then compute the retrieval likelihood of each retrieved document d: PR(d|x) =es(d,x)/γ P d∈D′es(d,x)/γ where γis a hyperparameter th"}, {"rank": 5, "score": 0.44202822, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 59, "text_snippet": " memory by leveraging approximate nearest neighbors search (Grave et al., 2017a). The related kNN-LM model (Khandelwal et al., 2020) replaced LSTMs by transformer networks, and scaled the memory to billions of tokens, leading to strong perf"}]}
{"case_index": 253, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"Then, these documents are fed to the language model, along with the query, which in turns [MASK] the output.\"", "gold": "generates", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 13.03, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.5998986, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 18, "text_snippet": "y retrieving the top-k relevant documents from a large corpus of text with the retriever. Then, these documents are fed to the language model, along with the query, which in turns generates the output. Both the retriever and the language mo"}, {"rank": 2, "score": 0.5862602, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 37, "text_snippet": " To cast language modeling in the text-to-text framework, we consider a chunk of Nwords, and split this chunk in two sub-sequences of equal length N/2. Then, the ﬁrst sub-sequence is used as the query, and the second corresponds to the outp"}, {"rank": 3, "score": 0.57680964, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 22, "text_snippet": "ther way to process the retrieved documents in the language model would be to concatenate the query and all the documents, and to use this long sequence as input of the model. Unfortunately, this approach does not scale with the number of d"}, {"rank": 4, "score": 0.57058656, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 5, "score": 0.56351095, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 60, "text_snippet": "ut. Retrieval-Augmentation with Search Engines. Recently, diﬀerent works have proposed to train large language models to interact with a search engine, by generating text queries, and using the retrieved documents as additional context (Nak"}]}
{"case_index": 254, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"n improve GPT- 3 performance on open-domain question answering, we approach the problem in a more general setting, including language modeling and [MASK] tasks.\"", "gold": "understanding", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.662, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.70682156, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 20, "text_snippet": "n improve GPT- 3 performance on open-domain question answering, we approach the problem in a more general setting, including language modeling and understanding tasks. We also pro- pose an ensemble method to incorporate more documents  REPL"}, {"rank": 2, "score": 0.6698595, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 9, "text_snippet": "22) that adapts language models to the retriever. We use a training objective which prefers retrieving documents that improve language model perplexity, while treating the LM as a frozen, black-box scoring function. Our experiments show tha"}, {"rank": 3, "score": 0.6674975, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 12, "text_snippet": "gnals, resulting in improved retrieval quality. •Evaluations on language modeling (§6), open-domain QA and MMLU demonstrate that REPLUG can im- prove the performance of various language models such as GPT, OPT and BLOOM, including very larg"}, {"rank": 4, "score": 0.63448304, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 11, "text_snippet": " model parameters), for both reducing LM perplexity and and im- proving in-context learning performance. We summarize our contributions as follows: •We introduce REPLUG (§3), the first retrieval- augmented language modeling framework for en"}, {"rank": 5, "score": 0.63278556, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 71, "text_snippet": "a black box and augments it with a tuneable retrieval model. Our evaluation shows that REPLUG can be integrated with any existing language model to improve their performance  REPLUG: Retrieval-Augmented Black-Box Language Models on language"}]}
{"case_index": 255, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"Controlled experiments show that retrieval with recursive summaries offers [MASK] improv\"", "gold": "significant", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.042, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.66638416, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 2, "text_snippet": "hat retrieval with recursive summaries offers significant improvements over tra- ditional retrieval-augmented LMs on several tasks. On question-answering tasks that involve complex, multi-step reasoning, we show state-of-the-art results; fo"}, {"rank": 2, "score": 0.631986, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 1, "text_snippet": "rom a retrieval corpus, limiting holistic under- standing of the overall document context. We introduce the novel approach of recursively embedding, clustering, and summarizing chunks of text, constructing a tree with differing levels of su"}, {"rank": 3, "score": 0.605636, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 129, "text_snippet": "esented in table 9. The results from this ablation study clearly indicate an improvement in accuracy when employing RAPTOR’s clustering mechanism over the recency-based tree approach. This finding substantiates our hypothesis that the clust"}, {"rank": 4, "score": 0.59656626, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 73, "text_snippet": "an effectively handle a wider range of questions, from higher-order thematic queries to detail-oriented questions. Detailed results for additional stories and an ablation study on layer contributions can be found in Appendix I. 5 C ONCLUSIO"}, {"rank": 5, "score": 0.5883995, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 67, "text_snippet": "ring approaches, which allows it to capture a range of information, from general themes to specific details, contributing to its overall strong performance. 4.1 C ONTRIBUTION OF THE TREE STRUCTURE We examine the contribution of each layer o"}]}
{"case_index": 256, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"This yields a set of [MASK] Y, some of which may not have appeared in the beams of all documents.\"", "gold": "hypotheses", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 9.206, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.5445869, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 2, "score": 0.53908384, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 27, "text_snippet": "n beam search for each document z, scoring each hypothesis using pθ(yi|x,z,y 1:i−1). This yields a set of hypotheses Y, some of which may not have appeared in the beams of all documents. To estimate the probability of an hypothesis ywe run "}, {"rank": 3, "score": 0.5143809, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 25, "text_snippet": "rator G. The retriever Raims to retrieve the top- Kdocuments D={dr1, ..., d rk}that are relevant to the input Xfrom the corpus C. Based on the input Xand the retrieved results D, the generator Gis responsible for generating the output Y. Th"}, {"rank": 4, "score": 0.4801213, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 25, "text_snippet": "beddings. 3.2. Input Reformulation The retrieved top- kdocuments provide rich information about the original input context xand can potentially help the LM to make a better prediction. One simple way to incorporate the retrieved documents a"}, {"rank": 5, "score": 0.47771868, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 19, "text_snippet": "r produces the output sequence probability for each document, which are then marginalized, pRAG-Sequence (y|x)≈∑ z∈top-k(p(·|x))pη(z|x)pθ(y|x,z) =∑ z∈top-k(p(·|x))pη(z|x)N∏ ipθ(yi|x,z,y 1:i−1) RAG-Token Model In the RAG-Token model we can d"}]}
{"case_index": 257, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"Earlier studies adopt either sparse or dense retrievers at the front end of a pre- trained language model that [MASK] in response generation.\"", "gold": "specializes", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.3333333333333333, "hit_rank": 3, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.84, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.6320544, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 56, "text_snippet": " the reader and the retriever by leveraging the perplexity of the output generated by the reader. Sachan et al. (2021) and Lee et al. (2021a) both employ salient span masking to pre-train retrievers, leveraging the perplexity and attention "}, {"rank": 2, "score": 0.614005, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 57, "text_snippet": "nted generators using a second “informed” retriever with access to the output, which the test-time retriever can be distilled from, and Hofstätter et al. (2022) recently proposed a training set ﬁltering/weighting approach to train stronger "}, {"rank": 3, "score": 0.61354774, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 19, "text_snippet": " documents. It usually provides an extra knowledge source from a specific corpus, i.e., Wikipedia, which greatly improves the per- formance of LMs in a variety of tasks, especially in the knowledge-intensive ones. The proposed  methods gene"}, {"rank": 4, "score": 0.61146283, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 17, "text_snippet": "inal predic- tion. This style of retrieval can be added to both encoder- decoder (Yu, 2022; Izacard et al., 2022b) and decoder-only models (Khandelwal et al., 2020; Borgeaud et al., 2022; Shi et al., 2022; Rubin et al., 2022). For example, "}, {"rank": 5, "score": 0.6046927, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 84, "text_snippet": " and pre-training paradigm for generative LLMs, leveraging cross-attention and end-to-end pre-training with 9  retrieval from a trillion-token data store, achieving strong performance. Subsequent work by Shi et al. (2024) and Lin et al. (20"}]}
{"case_index": 258, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"RAG is ideal when the total number of [MASK] in a data source is too large to include in a single prompt to the LLM, i.e.\"", "gold": "records", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 9.209, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.65073645, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 29, "text_snippet": "lements (nodes, edges, covariates) that the LLM can summarize in parallel at both indexing time and query time. The “global answer” to a given query is produced using a final round of query-focused summarization over all community summaries"}, {"rank": 2, "score": 0.6249234, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 14, "text_snippet": "es, whereupon this information is incorporated into the generation of a response to the query by an LLM (or other generative AI model, such as a multi-media model). The query and retrieved records populate a prompt template, which is then p"}, {"rank": 3, "score": 0.62209404, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 27, "text_snippet": "ns (Wang et al., 2023a; Zheng et al., 2024). Some prior work proposes criteria for having LLMs quantify the quality of 3  Source Documents Text Chunkstext extraction and chunking Entities & Relationshipsdomain-tailored summarization Knowled"}, {"rank": 4, "score": 0.60772586, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 1, "text_snippet": "ibuted equally to this work Abstract The use of retrieval-augmented generation (RAG) to retrieve relevant informa- tion from an external knowledge source enables large language models (LLMs) to answer questions over private and/or previousl"}, {"rank": 5, "score": 0.6009531, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 8, "text_snippet": "nce analysis (Ranade and Joshi, 2023). Given a sensemaking query and a text with an implicit and interconnected set of concepts, an LLM can generate a summary that answers the query. The challenge, however, arises when the volume of data re"}]}
{"case_index": 259, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"We ﬁne-tune and evaluate our models on a wide range of knowledge- intensive NLP tasks and set the state of the art on three open domain QA tasks, outperforming parametric seq2seq models and task-speciﬁc [MASK] architectures.\"", "gold": "retrieve-and-extract", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.845, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.68141705, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 3, "text_snippet": "cessed with a pre-trained neural retriever. We com- pare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. We ﬁne-tune and "}, {"rank": 2, "score": 0.67006063, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 13, "text_snippet": "sive tasks —tasks that humans could not reasonably be expected to perform without access to an external knowledge source. Our RAG models achieve state-of-the-art results on open Natural Questions [ 29], WebQuestions [ 3] and CuratedTrec [ 2"}, {"rank": 3, "score": 0.65023875, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 69, "text_snippet": "ance across a variety of NLP tasks when considered in isolation. Such tasks include open-domain question answering [ 5,29], fact checking [ 56], fact completion [ 48], long-form question answering [ 12], Wikipedia article generation [ 36], "}, {"rank": 4, "score": 0.64668864, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 14, "text_snippet": "active approaches. For knowledge-intensive generation, we experiment with MS-MARCO [ 1] and Jeopardy question generation, and we ﬁnd that our models generate responses that are more factual, speciﬁc, and diverse than a BART baseline. For FE"}, {"rank": 5, "score": 0.63677645, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 1, "text_snippet": "ameters, and achieve state-of-the-art results when ﬁne-tuned on down- stream NLP tasks. However, their ability to access and precisely manipulate knowl- edge is still limited, and hence on knowledge-intensive tasks, their performance lags b"}]}
{"case_index": 260, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"[MASK] Generation RAG (Lewis et al., 2020; Guu et al., 2020) is regarded as a useful method to address the issues above, which enhances the input questions of generative LMs with retrieved documents.\"", "gold": "retrieval-augmented", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 11.663, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.75587845, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 18, "text_snippet": "pact the LLMs and exacerbate the problems. Thus, it is obvious that the lack of accurate and specific knowledge can lead to misleading or even inaccurate generation, which will severely hurt the experience of users in most practical applica"}, {"rank": 2, "score": 0.72349715, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 7, "text_snippet": "ieval-augmented generation (RAG) (Lewis et al., 2020). In this framework, the input to models is augmented by prepending relevant documents that are retrieved from an external knowledge corpus (Guu et al., 2020). While RAG serves as a pract"}, {"rank": 3, "score": 0.71105677, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 20, "text_snippet": "model that specializes in response generation. Despite this, the methods above usually ignore a question, what if the retrieval goes wrong? Since the purpose of introducing a retrieval is to secure that generative LMs can obtain relevant an"}, {"rank": 4, "score": 0.69861454, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 4, "text_snippet": " for long-form generations relative to these models.1 1 I NTRODUCTION State-of-the-art LLMs continue to struggle with factual errors (Mallen et al., 2023; Min et al., 2023) despite their increased model and data scale (Ouyang et al., 2022)."}, {"rank": 5, "score": 0.6856777, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 4, "text_snippet": "o substantial improvements over a conventional RAG baseline for both the comprehensiveness and diversity of generated answers. 1 Introduction Retrieval augmented generation (RAG) (Lewis et al., 2020) is an established approach to using LLMs"}]}
{"case_index": 261, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"We endow pre-trained, parametric-memory generation models with a non-parametric memory through a general-purpose ﬁne-tuning approach which we refer to as [MASK] generation (RAG).\"", "gold": "retrieval-augmented", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 12.072, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.7525391, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 9, "text_snippet": "y to the “workhorse of NLP,” i.e. sequence-to-sequence (seq2seq) models. We endow pre-trained, parametric-memory generation models with a non-parametric memory through a general-purpose ﬁne-tuning approach which we refer to as retrieval-aug"}, {"rank": 2, "score": 0.75246894, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 2, "text_snippet": "anism to explicit non-parametric memory have so far been only investigated for extractive downstream tasks. We explore a general-purpose ﬁne-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trained parametri"}, {"rank": 3, "score": 0.66483766, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 3, "text_snippet": "cessed with a pre-trained neural retriever. We com- pare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. We ﬁne-tune and "}, {"rank": 4, "score": 0.66226584, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 1, "text_snippet": "e reliance on the paramet- ric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and inco"}, {"rank": 5, "score": 0.65870965, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 7, "text_snippet": "ieval-augmented generation (RAG) (Lewis et al., 2020). In this framework, the input to models is augmented by prepending relevant documents that are retrieved from an external knowledge corpus (Guu et al., 2020). While RAG serves as a pract"}]}
{"case_index": 262, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"It usually provides an extra knowledge source from a specific corpus, i.e., Wikipedia, which greatly improves the per- formance of LMs in a variety of tasks, especially in the [MASK] ones.\"", "gold": "knowledge-intensive", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 13.942, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.61766535, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 19, "text_snippet": " documents. It usually provides an extra knowledge source from a specific corpus, i.e., Wikipedia, which greatly improves the per- formance of LMs in a variety of tasks, especially in the knowledge-intensive ones. The proposed  methods gene"}, {"rank": 2, "score": 0.58003265, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 69, "text_snippet": "ance across a variety of NLP tasks when considered in isolation. Such tasks include open-domain question answering [ 5,29], fact checking [ 56], fact completion [ 48], long-form question answering [ 12], Wikipedia article generation [ 36], "}, {"rank": 3, "score": 0.5731625, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 164, "text_snippet": "intensive tasks for open-ended generation (e.g., instruction following). Recent or concurrent work studies instruction-tuning of retrieval systems (Asai et al., 2023b) or joint training of retrieval and LM components (Lin et al., 2023), whi"}, {"rank": 4, "score": 0.5631803, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 4, "text_snippet": " tasks (Roberts et al., 2020). Nevertheless, even a large model does not contain sufficient domain- specific knowledge for particular tasks and the world continues to change, invalidating facts in the LLM. Updating the knowledge of these mo"}, {"rank": 5, "score": 0.56220335, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 13, "text_snippet": "sive tasks —tasks that humans could not reasonably be expected to perform without access to an external knowledge source. Our RAG models achieve state-of-the-art results on open Natural Questions [ 29], WebQuestions [ 3] and CuratedTrec [ 2"}]}
{"case_index": 263, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"(2023) uses [MASK] and snippets of passages, which improves correctness on most datasets but can sometimes be a lossy means of compression.\"", "gold": "summarizations", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 9.066, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.59467643, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 61, "text_snippet": "extractive and abstractive, shows promising performances. On NQ and TQA, the abstractive approach is more effective. On NQ, it achieves a compression ratio of 5% tokens while losing 2 EM points compared to prepending full documents. On TQA,"}, {"rank": 2, "score": 0.5925324, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 3, "score": 0.5925213, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 77, "text_snippet": "by compressing retrieved documents into a concise summary or an empty sequence, facilitating selective retrieval augmentation. Prompt Compression Recent work (Wingate et al., 2022; Chevalier et al., 2023; Mu et al., 2023) proposes compressi"}, {"rank": 4, "score": 0.5892762, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 32, "text_snippet": "each sentence from the retrieved documents, we prepend the Wikipedia page title to it to for decontextualization. 3.2 A BSTRACTIVE COMPRESSION To train an abstractive compressor, we distill the query-focused summarization ability of extreme"}, {"rank": 5, "score": 0.5862939, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 0, "text_snippet": "Published as a conference paper at ICLR 2024 RAPTOR: R ECURSIVE ABSTRACTIVE PROCESSING FOR TREE-ORGANIZED RETRIEVAL Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D. Manning Stanford University psarthi@cs."}]}
{"case_index": 264, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"We refer to this decoding [MASK] as “Thorough Decoding.” For longer output seq\"", "gold": "procedure", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.902, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.5612557, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 53, "text_snippet": "old, we trigger retrieval (details in Appendix Section A.3). Tree-decoding with critique tokens. At each segment step t, when retrieval is required, based either on hard or soft conditions, Rretrieves Kpassages, and the generator Mprocesses"}, {"rank": 2, "score": 0.55133307, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 27, "text_snippet": "n beam search for each document z, scoring each hypothesis using pθ(yi|x,z,y 1:i−1). This yields a set of hypotheses Y, some of which may not have appeared in the beams of all documents. To estimate the probability of an hypothesis ywe run "}, {"rank": 3, "score": 0.53873026, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 4, "score": 0.53160137, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 28, "text_snippet": "decoding procedure as “Thorough Decoding.” For longer output sequences,|Y|can become large, requiring many forward passes. For more efﬁcient decoding, we can make a further approximation that pθ(y|x,zi)≈0whereywas not generated during beam "}, {"rank": 5, "score": 0.5026595, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}]}
{"case_index": 265, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"If so, it outputs a retrieval token that calls a [MASK] model on demand (Step 1).\"", "gold": "retriever", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 9.881, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.639874, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 7, "text_snippet": "rmittent special tokens (i.e., reflection tokens ). Reflection tokens are categorized into retrieval andcritique tokens to indicate the need for retrieval and its generation quality respectively (Figure 1 right). In particular, given an inp"}, {"rank": 2, "score": 0.61851966, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 36, "text_snippet": "text with reflection tokens by unifying them as next token predictions from the expanded model vocabulary (i.e., the original vocabulary plus reflection tokens). Specifically, we train the generator model Mon a curated corpus with interleav"}, {"rank": 3, "score": 0.6163256, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 34, "text_snippet": "l predicts the next output segment, as it does in a standard LM. If retrieval is needed, the model generates: a critique token to evaluate the retrieved passage’s relevance, the next response segment, and a critique token to evaluate if the"}, {"rank": 4, "score": 0.6122525, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 22, "text_snippet": " the retriever and LM on instruction-tuning datasets in two steps. While we also train our model on diverse instruction-following datasets, SELF-RAGenables retrieval on demand and selection of the best possible model output via fine-grained"}, {"rank": 5, "score": 0.6108056, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 41, "text_snippet": "lding are unforgettable experience.No RetrievalNo Retrieval Retriever Figure 2: SELF-RAGtraining examples. The left example does not require retrieval while the right one requires retrieval; thus, passages are inserted. More examples are in"}]}
{"case_index": 266, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"However, in order to produce a fair evaluation, our method avoids generating the questions directly from the corpus itself (as an alternative [MASK], one can use a subset of the corpus held out from subsequen\"", "gold": "implementation", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.565, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.65528226, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 23, "text_snippet": " this work, we propose an approach for generating a set of questions for evaluating global sensemaking over the entirety of the corpus. Our approach is related to LLM methods that use a corpus to generate questions whose answers would be su"}, {"rank": 2, "score": 0.5971359, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 24, "text_snippet": "tion, one can use a subset of the corpus held out from subsequent graph extraction and answer evaluation steps). Adaptive benchmarking refers to the process of dynamically generating evaluation benchmarks tai- lored to specific domains or u"}, {"rank": 3, "score": 0.5573239, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 1, "text_snippet": "ibuted equally to this work Abstract The use of retrieval-augmented generation (RAG) to retrieve relevant informa- tion from an external knowledge source enables large language models (LLMs) to answer questions over private and/or previousl"}, {"rank": 4, "score": 0.55575746, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 5, "score": 0.5454123, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 22, "text_snippet": "gly global summaries by using the LLM to create summaries spanning this community hierarchy. 2.3 Adaptive benchmarking for RAG Evaluation Many benchmark datasets for open-domain question answering exist, including HotPotQA (Yang et al., 201"}]}
{"case_index": 267, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"FRAG achieves better performance than LLaMA without incurring higher latency in the downstream [MASK].\"", "gold": "applications", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 8.644, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.70972127, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 3, "text_snippet": " on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30 .85×the time-t"}, {"rank": 2, "score": 0.6526835, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 56, "text_snippet": "ance Performance vs. Latency Llama REFRAG 8× compressionFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a strong retriever scenario (right).REFRAGperform similarly to LLaMA model"}, {"rank": 3, "score": 0.6494395, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 12, "text_snippet": "is information is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-a"}, {"rank": 4, "score": 0.6456781, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 69, "text_snippet": "r, due to the compression, we are able to have more context information and hence achieve better performance. Surprisingly, REFRAG 16andREFRAG 32both outperform the LLaMA FTmodel despite having2 ×and4×fewer tokens in the decoder (i.e., lowe"}, {"rank": 5, "score": 0.6392559, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 94, "text_snippet": "Complementary approaches rank or prune context by estimated informativeness, e.g.,Selective Contextuses self-information to drop low-value tokens, and sentence-level methods learn context-aware encoders for question-specific compression and"}]}
{"case_index": 268, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"xamples, [MASK] a 540B parameters model by 3% despite having 50x fewer parameters.\"", "gold": "outperforming", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 11.325, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6817771, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 3, "text_snippet": "xamples, outperforming a 540B parameters model by 3% despite having 50x fewer parameters. 1 Introduction Large language models (LLMs) are impressive few-shot learners (Brown et al., 2020; Rae et al., 2021; Hoﬀmann et al., 2022; Chowdhery et"}, {"rank": 2, "score": 0.62208784, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 103, "text_snippet": "0M parameters, whereas the equivalent Atlasachieves around 40%, signiﬁcantly better than random, despite its small size. All models improve with more data, but interestingly, the 770M models do not beneﬁt as much from few-shot multitask lea"}, {"rank": 3, "score": 0.57453734, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 63, "text_snippet": "s learning ability, leading to the further development of large models (Lieber et al., 2021; Rae et al., 2021; Smith et al., 2022; Chowdhery et al., 2022; Smith et al., 2022). Hoﬀmann et al. (2022) revisited the scaling law from Kaplan et a"}, {"rank": 4, "score": 0.5730333, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 101, "text_snippet": "st likely of the 4 letters at test time. Full technical details can be found in appendix A.1. Performance vs Parameters. We start by comparing Atlasto closed-book models of diﬀerent sizes for 5-shot, 5-shot multitask and the full setting, a"}, {"rank": 5, "score": 0.57212293, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 11, "text_snippet": " performance in both few-shot and resource-rich settings. For example, with only 11B parameters, Atlasachieves an accuracy of 42.4% on NaturalQuestions using 64 training examples (45.1% with a Wikipedia-only index), outperforming PaLM (Chow"}]}
{"case_index": 269, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"generated texts such as “fluency” (Wang et al., 2023a) Some of these criteria are generic to vector RAG systems and not relevant to global sensemaking, such as “context relevance”, “[MASK]”, and “answer relevance” (RAGAS, Es et al\"", "gold": "faithfulness", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 13.883, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.79098946, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 29, "text_snippet": "lements (nodes, edges, covariates) that the LLM can summarize in parallel at both indexing time and query time. The “global answer” to a given query is produced using a final round of query-focused summarization over all community summaries"}, {"rank": 2, "score": 0.6898378, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 30, "text_snippet": "evance”, “faithfulness”, and “answer relevance” (RAGAS, Es et al. 2023). Lacking a gold standard for evaluation, one can quantify relative performance for a given criterion by prompting the LLM to compare generations from two different comp"}, {"rank": 3, "score": 0.62656844, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 96, "text_snippet": "tigate these downstream risks for questions of a global nature, which might otherwise be answered by samples of retrieved facts falsely presented as global summaries. 7 Conclusion We have presented GraphRAG, a RAG approach that combines kno"}, {"rank": 4, "score": 0.62056243, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 95, "text_snippet": "ontained in higher-level community summaries. Broader impacts . As a mechanism for question answering over large document collections, there are risks to downstream sensemaking and decision-making tasks if the generated answers do not accur"}, {"rank": 5, "score": 0.6137304, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 50, "text_snippet": "il the token limit is reached. This final context is used to generate the global answer returned to the user. 3.2 Global Sensemaking Question Generation To evaluate the effectiveness of RAG systems for global sensemaking tasks, we use an LL"}]}
{"case_index": 270, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"We use this approach to compare GraphRAG to vector RAG on two [MASK] real-world text datasets.\"", "gold": "representative", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 33.18, "llm_ms": 0.008, "top_contexts": [{"rank": 1, "score": 0.6290941, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 12, "text_snippet": "a diverse set of global sensemaking questions based on corpus-specific use cases, before using a second LLM to judge the answers of two different RAG systems using predefined criteria (defined in Section 3.3). We use this approach to compar"}, {"rank": 2, "score": 0.6213232, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 22, "text_snippet": "gly global summaries by using the LLM to create summaries spanning this community hierarchy. 2.3 Adaptive benchmarking for RAG Evaluation Many benchmark datasets for open-domain question answering exist, including HotPotQA (Yang et al., 201"}, {"rank": 3, "score": 0.61290133, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 27, "text_snippet": "ns (Wang et al., 2023a; Zheng et al., 2024). Some prior work proposes criteria for having LLMs quantify the quality of 3  Source Documents Text Chunkstext extraction and chunking Entities & Relationshipsdomain-tailored summarization Knowled"}, {"rank": 4, "score": 0.61249864, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 63, "text_snippet": "siness, sports, technology, health, and science (Tang and Yang, 2024). The corpus is divided into 3197 ×600-token text chunks, with 100-token overlaps between chunks ( ∼1.7 million tokens). 4.1.2 Conditions We compared six conditions includ"}, {"rank": 5, "score": 0.60980785, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 96, "text_snippet": "tigate these downstream risks for questions of a global nature, which might otherwise be answered by samples of retrieved facts falsely presented as global summaries. 7 Conclusion We have presented GraphRAG, a RAG approach that combines kno"}]}
{"case_index": 271, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"These datasets were gen- erated in a way that answers do not [MASK] to spans in support documents, t\"", "gold": "correspond", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 22.308, "llm_ms": 0.008, "top_contexts": [{"rank": 1, "score": 0.59251416, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 15, "text_snippet": " that answers do not correspond to spans in support documents, thus requiring ab- stractive models. Raffel et al. (2019) showed that generative models are competitive for reading com- prehension tasks such as SQuAD (Rajpurkar et al., 2016),"}, {"rank": 2, "score": 0.5666534, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 14, "text_snippet": " be trained using weak supervision in the form of question-answer pairs (Karpukhin et al., 2020), or pretrained using a cloze task and ﬁnetuned end-to- end (Guu et al., 2020; Lee et al., 2019). Generative question answering was mostly consi"}, {"rank": 3, "score": 0.56429535, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 5, "text_snippet": "t, it retrieves support text passages from an external source of knowledge such as Wikipedia. Then, a generative encoder-decoder model produces the answer, conditioned on the question and the re- trieved passages. This approach scales well "}, {"rank": 4, "score": 0.55085605, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 10, "text_snippet": " problem in natural lan- guage processing (V oorhees et al., 1999), this task has recently regained interest following the work by Chen et al. (2017). In that version of the prob- lem, strong supervision is available to the learning system,"}, {"rank": 5, "score": 0.54348207, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 6, "text_snippet": "tech- niques have been considered, either using sparse representations based on TF/IDF or using dense embeddings (Guu et al., 2020; Karpukhin et al., 2020). The models which extract the answers are often based on contextualized word represe"}]}
{"case_index": 272, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"While Toolformer (Schick et al., 2023) is [MASK] for calling APIs such as Wikipedia.\"", "gold": "pre-trained", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 25.901, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.6151566, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 22, "text_snippet": "rrelevant context and improve robustness. SAIL (Luo et al., 2023) is tuned on instructions to insert retrieved documents before instructions. While Toolformer (Schick et al., 2023) is pre-trained for calling APIs such as Wikipedia. In addit"}, {"rank": 2, "score": 0.5571878, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 20, "text_snippet": " prior work often retrieves only once at the beginning, Jiang et al. (2023) propose to adaptively retrieve passages for generation on top of a proprietary LLM or Schick et al. (2023) train an LM to generate API calls for named entities. Yet"}, {"rank": 3, "score": 0.5519023, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 106, "text_snippet": ". , 21:140:1–140:67. Md. Rashad Al Hasan Rony, Ricardo Usbeck, and Jens Lehmann. 2022. Dialokg: Knowledge-structure aware task-oriented dialogue generation. In Findings of the Association for Computational Linguistics:NAACL 2022, Seattle, W"}, {"rank": 4, "score": 0.549622, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 164, "text_snippet": "intensive tasks for open-ended generation (e.g., instruction following). Recent or concurrent work studies instruction-tuning of retrieval systems (Asai et al., 2023b) or joint training of retrieval and LM components (Lin et al., 2023), whi"}, {"rank": 5, "score": 0.51916385, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 129, "text_snippet": "ola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761 , 2023. URL https://arxiv.org/abs/2302. 04761 . John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, a"}]}
{"case_index": 273, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"Type Input Output Definitions Retrieve x/x, y {yes, no, continue } Decides when to retrieve with R ISREL x, d {relevant , irrelevant } dprovides useful [MASK] to solve x.\"", "gold": "information", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 1.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 16.478, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.57792366, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 30, "text_snippet": "ection tokens (Table 1). 2All work is arXived within a week of this preprint. 3In this paper, we treat one sentence as a segment in our experiments, but our framework is applicable to any segment unit (i.e., sub-sentence). 3  Preprint. Type"}, {"rank": 2, "score": 0.57205546, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 158, "text_snippet": "ility ( t=Tonly) 10: Sample d 11: else if Retrieve is not predicted then 12: Cpredicts ISUSEgiven x, y Add augmented (x, y, d, r )toDgen Training examples. Table 4 show several training examples used for Mtraining. A.3 S ELF-RAGINFERENCE De"}, {"rank": 3, "score": 0.5689735, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 141, "text_snippet": "ires factual grounding. Noindicates retrieval is unnecessary as the sequence does not require factual grounding or may not be enhanced by knowledge retrieval, Yes indicates retrieval is necessary. We additionally have continue to use eviden"}, {"rank": 4, "score": 0.5493791, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 46, "text_snippet": "pervised data that precisely mimics the SELF- RAGinference-time process (Section 3.1). For each segment yt∈y, we run Cto assess whether additional passages could help to enhance generation. If retrieval is required, the retrieval special to"}, {"rank": 5, "score": 0.54364157, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 41, "text_snippet": "lding are unforgettable experience.No RetrievalNo Retrieval Retriever Figure 2: SELF-RAGtraining examples. The left example does not require retrieval while the right one requires retrieval; thus, passages are inserted. More examples are in"}]}
{"case_index": 274, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"der allows to scale to large number of contexts, as it only [MASK] self attention over one context at a time.\"", "gold": "performs", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 14.318, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.59248364, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 9, "text_snippet": "f research, with approaches 1arXiv:2509.01092v2 [cs.CL] 12 Oct 2025  ranging from modifying the attention mechanism’s complexity (Beltagy et al., 2020) to sparsifying attention and context (Child et al., 2019; Xiao et al., 2024; Jiang et al"}, {"rank": 2, "score": 0.5817976, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 23, "text_snippet": "der allows to scale to large number of contexts, as it only performs self attention over one context at a time. This means that the computation time of the model grows linearly with the number of passages, instead of quadratically. On the o"}, {"rank": 3, "score": 0.57654315, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 85, "text_snippet": ", which uses an encoder to process each passage in parallel and concatenates the hidden states for generation via a decoder. This approach accelerates attention computation by removing cross-document attention, but does not apply compressio"}, {"rank": 4, "score": 0.5569756, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 2, "text_snippet": "ated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns th"}, {"rank": 5, "score": 0.55155206, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 84, "text_snippet": " and pre-training paradigm for generative LLMs, leveraging cross-attention and end-to-end pre-training with 9  retrieval from a trillion-token data store, achieving strong performance. Subsequent work by Shi et al. (2024) and Lin et al. (20"}]}
{"case_index": 275, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"Average pooling is applied over the outputs of the last layer to obtain one vector [MASK] per query or document.\"", "gold": "representation", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 8.664, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.5338296, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 2, "score": 0.51077926, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 22, "text_snippet": "ther way to process the retrieved documents in the language model would be to concatenate the query and all the documents, and to use this long sequence as input of the model. Unfortunately, this approach does not scale with the number of d"}, {"rank": 3, "score": 0.5066779, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 20, "text_snippet": "eir corresponding embeddings. The Contriever model is pre-trained using the MoCo contrastive loss (He et al., 2020), and uses unsupervised data only. As shown in the following section, an advantage of dense retrievers is that both query and"}, {"rank": 4, "score": 0.50656134, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 18, "text_snippet": "y retrieving the top-k relevant documents from a large corpus of text with the retriever. Then, these documents are fed to the language model, along with the query, which in turns generates the output. Both the retriever and the language mo"}, {"rank": 5, "score": 0.5027295, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 220, "text_snippet": "f the correct answer: [MASK_0] {correct answer option letter} This format closely matches the format of MLM pre-training objective, aiding few-shot learning. When training, we permute the order of the answer options, i.e. shuﬄing which answ"}]}
{"case_index": 276, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"Step 3: Critique outputs and select best [MASK] in a 16th-century novel Las Sergas de Esplandián.\"", "gold": "segmentorigins", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 8.423, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.50237685, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 8, "text_snippet": "on demand (Step 1). Subsequently, SELF-RAGconcurrently processes multiple retrieved passages, evaluating their relevance and then generating corresponding task outputs (Step 2). It then generates critique tokens to criticize its own output "}, {"rank": 2, "score": 0.48133337, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 34, "text_snippet": "l predicts the next output segment, as it does in a standard LM. If retrieval is needed, the model generates: a critique token to evaluate the retrieved passage’s relevance, the next response segment, and a critique token to evaluate if the"}, {"rank": 3, "score": 0.46956524, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 15, "text_snippet": "st a critic model during training, reducing overhead. The critic model, in part, is supervised on a dataset of input, output, and corresponding reflection tokens collected by prompting a propriety LM (i.e., GPT-4; OpenAI 2023). While we dra"}, {"rank": 4, "score": 0.46015114, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 38, "text_snippet": "ing of two models, the critic C(Section 3.2.1) and the generator M(Section 3.2.2). 3.2.1 T RAINING THE CRITIC MODEL Data collection for critic model. Manual annotation of reflection tokens for each segment is expensive (Wu et al., 2023). A "}, {"rank": 5, "score": 0.45661616, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 12, "text_snippet": "ding Utah.  Prompt: Write an essay of your best summer vacation Prompt: Write an essay of your best summer vacation No RetrievalMy best summer vacation is when my family and I embarked on a road trip along …My best…  >Repeat.… No informatio"}]}
{"case_index": 277, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"One simple way to [MASK] the retrieved documents as part of the input to the LM is to prepend xwith all kdocuments.\"", "gold": "incorporate", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.015, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6842091, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 7, "text_snippet": " retrieved documents are prepended to the input context and fed into the black-box LM to make the final prediction. Because the LM context length limits the number of documents that can be prepended, we also introduce a new ensemble scheme "}, {"rank": 2, "score": 0.6744739, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 25, "text_snippet": "beddings. 3.2. Input Reformulation The retrieved top- kdocuments provide rich information about the original input context xand can potentially help the LM to make a better prediction. One simple way to incorporate the retrieved documents a"}, {"rank": 3, "score": 0.65025467, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 21, "text_snippet": " (§3.1 Document Retrieval ). Then it prepends each document separately to the input context and ensembles output probabilities from different passes (§3.2 Input Reformulation ). and a training scheme to further adapt the retriever to large "}, {"rank": 4, "score": 0.64870405, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 1, "text_snippet": "mechanisms to en- code the retrieved text, REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design can be eas- ily applied to any existing retrieval and language models. Furthermore, we show t"}, {"rank": 5, "score": 0.6192287, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 6, "text_snippet": "PIs. Internal representations of such models are not exposed and fine-tuning is not supported. In this work, we introduce REPLUG (Retrieve and Plug ), a new retrieval-augmented LM framework where the lan- guage model is viewed as a black bo"}]}
{"case_index": 278, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"During CPT, we input the first stokens x1:sinto the encoder and use its output to assist the decoder in [MASK] the next otokens xs+1:s+o.\"", "gold": "predicting", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.288, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.6271243, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 27, "text_snippet": "repare the model for downstream tasks utilizing chunk embeddings. To further enhance performance, we introduce selective compression via RL. After aligning the encoder and decoder through CPT, we apply supervised fine-tuning (SFT) to adapt "}, {"rank": 2, "score": 0.6099923, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 28, "text_snippet": " assist the decoder in predicting the next otokens xs+1:s+o. This task encourages the model to leverage contextual information for next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any en"}, {"rank": 3, "score": 0.5814639, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 31, "text_snippet": "ts parametric memory during training. Once the encoder is aligned with the decoder through this reconstruction task, we initiate CPT byunfreezing the decoder. Curriculum learning.The training tasks described in the previous section may seem"}, {"rank": 4, "score": 0.5668347, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 152, "text_snippet": "e downstream tasks (see section 5). REFRAG:Our approach is illustrated in figure 1. We use RoBERTa-large (Liu et al., 2019) as the encoder, feeding x1:stokens and evaluating the perplexity on the output tokens xs+1:s+o. We use REFRAG kto de"}, {"rank": 5, "score": 0.5497129, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 29, "text_snippet": "ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task and a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor achieving strong CPT perform"}]}
{"case_index": 279, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"The ﬁrst loss that we consider is based on the attention scores of the language model, and is heavily [MASK] by Izacard & Grave (2021).\"", "gold": "inspired", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 17.205, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.6325736, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 25, "text_snippet": "n Distillation (ADist). The ﬁrst loss that we consider is based on the attention scores of the language model, and is heavily inspired by Izacard & Grave (2021). The main idea is that the cross-attention scores between the input documents a"}, {"rank": 2, "score": 0.5836469, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 0, "text_snippet": "Atlas: Few-shot Learning with Retrieval Augmented Language Models Gautier Izacard∗ ∗,♦,♣,♥gizacard@fb.com Patrick Lewis∗,♦plewis@fb.com Maria Lomeli♦marialomeli@fb.com Lucas Hosseini♦hoss@fb.com Fabio Petroni♦fabiopetroni@fb.com Timo Schick"}, {"rank": 3, "score": 0.5765992, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 23, "text_snippet": "ent loss functions to train the retriever jointly with the language model. We consider loss functions that leverage the language model to provide supervisory signal to train the retriever. In other words, if the language model ﬁnds a docume"}, {"rank": 4, "score": 0.57617664, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 24, "text_snippet": "erest, without relying on document annotations. For example, in the case of fact checking, a model only requires pairs of claims and corresponding verdicts but no documents containing the evidence to back up the verdict. In practice, we can"}, {"rank": 5, "score": 0.5675166, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 9, "text_snippet": "f research, with approaches 1arXiv:2509.01092v2 [cs.CL] 12 Oct 2025  ranging from modifying the attention mechanism’s complexity (Beltagy et al., 2020) to sparsifying attention and context (Child et al., 2019; Xiao et al., 2024; Jiang et al"}]}
{"case_index": 280, "query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"ous NLP tasks, including language mod- eling (Min et al., 2022; Borgeaud et al., 2022; Khandelwal et al., 2020) and [MASK] question answering (Lewis et al., 2020; Izacard et al., 2022b; Hu et al., 2022).\"", "gold": "open-domain", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 21.807, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.7557249, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 16, "text_snippet": "ous NLP tasks, including language mod- eling (Min et al., 2022; Borgeaud et al., 2022; Khandelwal et al., 2020) and open-domain question answering (Lewis et al., 2020; Izacard et al., 2022b; Hu et al., 2022). Specifi- cally, using the input"}, {"rank": 2, "score": 0.66904604, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 94, "text_snippet": "iedel, S. Question and an- swer test-train overlap in open-domain question answer- ing datasets. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume , pp. 1000–1008, 20"}, {"rank": 3, "score": 0.66646105, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 85, "text_snippet": "Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A., Welbl, J., Clark, A., et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556 , 2022. Hu, Y ., Hua, H., Ya"}, {"rank": 4, "score": 0.65791726, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 12, "text_snippet": "gnals, resulting in improved retrieval quality. •Evaluations on language modeling (§6), open-domain QA and MMLU demonstrate that REPLUG can im- prove the performance of various language models such as GPT, OPT and BLOOM, including very larg"}, {"rank": 5, "score": 0.6478184, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 83, "text_snippet": "sser, S., and Leahy, C. The Pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027 , 2020.  REPLUG: Retrieval-Augmented Black-Box Language Models Goyal, A., Friesen, A., Banino, A., Weber, T., Ke, N. R"}]}
{"case_index": 281, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"The Contriever uses a dual-encoder architecture, where the query and documents are embedded [MASK] by a transformer encoder (Huang et al., 2013; Karpukhin et al., 2020).\"", "gold": "independently", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.3333333333333333, "hit_rank": 3, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.006, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.70545846, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 20, "text_snippet": "eir corresponding embeddings. The Contriever model is pre-trained using the MoCo contrastive loss (He et al., 2020), and uses unsupervised data only. As shown in the following section, an advantage of dense retrievers is that both query and"}, {"rank": 2, "score": 0.6891161, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 18, "text_snippet": "y retrieving the top-k relevant documents from a large corpus of text with the retriever. Then, these documents are fed to the language model, along with the query, which in turns generates the output. Both the retriever and the language mo"}, {"rank": 3, "score": 0.6727726, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 19, "text_snippet": " based on continuous dense embeddings. The Contriever uses a dual-encoder architecture, where the query and documents are embedded independently by a transformer encoder (Huang et al., 2013; Karpukhin et al., 2020). Average pooling is appli"}, {"rank": 4, "score": 0.63382715, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 9, "text_snippet": " strong few-shot learning, despite having lower parameter counts than other powerful recent few-shot learners. Atlasretrieves relevant documents based on the current context by using a general-purpose dense retriever using a dual-encoder ar"}, {"rank": 5, "score": 0.61051255, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 23, "text_snippet": "..dm}that are relevant to x. Following prior work (Qu et al., 2021; Izacard & Grave, 2021b; Ni et al., 2021), we use a dense retriever based on the dual encoder architecture, where an encoder is used to encode both the input context xand th"}]}
{"case_index": 282, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"Recently, multiple works show that retrieval systems entirely based on dense [MASK] and approximate nearest neighbors were competi- tive with traditional approaches.\"", "gold": "representation", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 8.217, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.651289, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 51, "text_snippet": "answering (Voorhees et al., 1999), documents are often retrieved from Wikipedia (Chen et al., 2017). Recently, dense retrievers based on neural networks have become popular. These usually follow a dual-encoder architecture (Yih et al., 2011"}, {"rank": 2, "score": 0.6163968, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 14, "text_snippet": "omponents: the retriever, the reader, and end-to-end system training. Retrieval methods have transitioned from traditional term-based techniques like TF-IDF (Sp¨arck Jones, 1972) and BM25 (Robertson et al., 1995; Roberts et al., 2020) to de"}, {"rank": 3, "score": 0.6074755, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 101, "text_snippet": "ov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906 , 2020. URL https://arxiv.org/abs/2004.04906 . [27] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, "}, {"rank": 4, "score": 0.60648614, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 20, "text_snippet": "es with SpaCy.2In DPR, passages and questions are represented as dense vector representations, computed using two BERT networks. The ranking function is the dot product between the query and passage represen- tations. Retrieval is performed"}, {"rank": 5, "score": 0.6057056, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}]}
{"case_index": 283, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"Both the retriever and the language model are based on [MASK] transformer networks, which we describe in more detail below.\"", "gold": "pre-trained", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.995, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.6428603, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 18, "text_snippet": "y retrieving the top-k relevant documents from a large corpus of text with the retriever. Then, these documents are fed to the language model, along with the query, which in turns generates the output. Both the retriever and the language mo"}, {"rank": 2, "score": 0.6346428, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 20, "text_snippet": "eir corresponding embeddings. The Contriever model is pre-trained using the MoCo contrastive loss (He et al., 2020), and uses unsupervised data only. As shown in the following section, an advantage of dense retrievers is that both query and"}, {"rank": 3, "score": 0.60601467, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 17, "text_snippet": "chmark in Figure 2. As many natural language processing tasks require knowledge , our goal is to enhance standard text-to-text models with retrieval, which, as we hypothesise in the introduction, may be crucial to endow models with few-shot"}, {"rank": 4, "score": 0.5951838, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 12, "text_snippet": "ntributions: •A thorough study on how to design and train retrieval-augmented language models, with a focus on downstream few-shot learning and sample eﬃciency. •The ﬁndings of this study lead to a retrieval-augmented language model, called"}, {"rank": 5, "score": 0.594866, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 71, "text_snippet": " pre-trained language model could achieve strong performance across both discriminative and generative tasks. For further improvement, BART [ 32] and T5 [ 51,52] propose a single, pre-trained encoder-decoder model that leverages bi-directio"}]}
{"case_index": 284, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"Ms) have attracted increasing attention and exhibited impressive abili- ties to understand [MASK] and generate fluent language texts (Brown et al., 2020; Ouyang et al., 2022; Touvron et al., 2023a).\"", "gold": "instructions", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.362, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.65632725, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 4, "text_snippet": "Ms) have attracted increasing attention and exhibited impressive abili- ties to understand instructions and generate fluent language texts (Brown et al., 2020; Ouyang et al., 2022; Touvron et al., 2023a). Nevertheless, LLMs inevitably manif"}, {"rank": 2, "score": 0.5840389, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 4, "text_snippet": " for long-form generations relative to these models.1 1 I NTRODUCTION State-of-the-art LLMs continue to struggle with factual errors (Mallen et al., 2023; Min et al., 2023) despite their increased model and data scale (Ouyang et al., 2022)."}, {"rank": 3, "score": 0.5790223, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 84, "text_snippet": " and pre-training paradigm for generative LLMs, leveraging cross-attention and end-to-end pre-training with 9  retrieval from a trillion-token data store, achieving strong performance. Subsequent work by Shi et al. (2024) and Lin et al. (20"}, {"rank": 4, "score": 0.5788653, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 83, "text_snippet": ".49 28.22 5 20.18 17.3728.2419.6518.7127.08 820.5217.60 28.17 16.87 18.05 25.36 10 19.67 17.41 27.62 15.72 17.42 23.60 6 Related Works Retrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi- tec"}, {"rank": 5, "score": 0.5767892, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 15, "text_snippet": " that answers do not correspond to spans in support documents, thus requiring ab- stractive models. Raffel et al. (2019) showed that generative models are competitive for reading com- prehension tasks such as SQuAD (Rajpurkar et al., 2016),"}]}
{"case_index": 285, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"Experiments show that SELF- RAG(7B and 13B parameters) significantly outperforms state-of-the-art LLMs and [MASK] models on a diverse set of tasks.\"", "gold": "retrieval-augmented", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.796, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.73330975, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 3, "text_snippet": "e phase, enabling it to tailor its behavior to diverse task requirements. Experiments show that SELF- RAG(7B and 13B parameters) significantly outperforms state-of-the-art LLMs and retrieval-augmented models on a diverse set of tasks. Speci"}, {"rank": 2, "score": 0.7307248, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 96, "text_snippet": "ains an LM to learn to retrieve, generate, and critique text passages and its own generation by predicting the next tokens from its original vocabulary as well as newly added special tokens, called reflection tokens. SELF-RAGfurther enables"}, {"rank": 3, "score": 0.6732081, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 75, "text_snippet": "s Llama2 65Bto refine output. Comparison against baselines with retrieval. As shown in Tables 2 (bottom), our SELF-RAGalso outperforms existing RAG in many tasks, obtaining the best performance among non-proprietary LM-based models on all t"}, {"rank": 4, "score": 0.67047447, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 80, "text_snippet": "rained on the same instruction-output pairs as SELF-RAGwithout retrieval or self-reflection and is retrieval-augmented at test time only, lags behind S ELF-RAG. This result indicates S ELF-RAGgains are not solely from training data and demo"}, {"rank": 5, "score": 0.65789384, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 24, "text_snippet": "ering tasks and to generate with tree search, guided by LM-generated value scores. While their value function simply indicates an overall score of each generation, SELF-RAGtrains to an arbitrary LM to learn to generate fine-grained self-ref"}]}
{"case_index": 286, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"GraphRAG leverages summaries over large sections of the data source as a form of ”[MASK]” (described in Cheng et al.\"", "gold": "self-memory", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.117, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.69873977, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 17, "text_snippet": " are later used to answer queries as in Mao et al. 2020). These summaries are generated in parallel and iteratively aggregated into global summaries, similar to prior techniques (Feng et al., 2023; Gao et al., 2023; Khattab et al., 2022; Sh"}, {"rank": 2, "score": 0.6915313, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 16, "text_snippet": " may use alternative retrieval mechanisms, we collectively refer to the family of conventional approaches as vector RAG . GraphRAG contrasts with vector RAG in its ability to answer queries that require global sensemaking over the entire da"}, {"rank": 3, "score": 0.6890288, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 0, "text_snippet": "From Local to Global: A GraphRAG Approach to Query-Focused Summarization Darren Edge1†Ha Trinh1†Newman Cheng2Joshua Bradley2Alex Chao3 Apurva Mody3Steven Truitt2Dasha Metropolitansky1Robert Osazuwa Ness1 Jonathan Larson1 1Microsoft Research"}, {"rank": 4, "score": 0.66777676, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 29, "text_snippet": "lements (nodes, edges, covariates) that the LLM can summarize in parallel at both indexing time and query time. The “global answer” to a given query is produced using a final round of query-focused summarization over all community summaries"}, {"rank": 5, "score": 0.6650442, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 96, "text_snippet": "tigate these downstream risks for questions of a global nature, which might otherwise be answered by samples of retrieved facts falsely presented as global summaries. 7 Conclusion We have presented GraphRAG, a RAG approach that combines kno"}]}
{"case_index": 287, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"Training a [MASK] for QA task works similarly, but scoring will evaluate whether the LM will generate t\"", "gold": "compressor", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.3333333333333333, "hit_rank": 3, "matched_by": "doc_id+chunk_id", "retrieval_ms": 12.899, "llm_ms": 0.008, "top_contexts": [{"rank": 1, "score": 0.6248753, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 30, "text_snippet": "ilarly, but scoring will evaluate whether the LM will generate the correct answer with summary prepended (change in line 6). Pseudo code for the QA tasks is in Figure 6 the Appendix. We train our encoder with a contrastive loss (Karpukhin e"}, {"rank": 2, "score": 0.5778898, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 60, "text_snippet": "ut. Retrieval-Augmentation with Search Engines. Recently, diﬀerent works have proposed to train large language models to interact with a search engine, by generating text queries, and using the retrieved documents as additional context (Nak"}, {"rank": 3, "score": 0.5738535, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 29, "text_snippet": "g pM(y|[sj;xi]), log likelihood assigned to target output according to LM Mwhen candidate sentence is prepended to the input. We consider the sentence with the highest log likelihood as a positive example pi(line 3). To construct negative e"}, {"rank": 4, "score": 0.57320154, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 5, "text_snippet": "uestion answering systems (Chen et al., 2017; Yu et al., 2018), is to index large quantities of text, after splitting it into chunks (paragraphs), in a separate information retrieval system. Retrieved information is then presented to the LL"}, {"rank": 5, "score": 0.5699644, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 265, "text_snippet": "er module. For these experiments the index contains the December 2018 Wikipedia dump. ModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW acc acc acc acc em em em f1 Atlas64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4 Atlasfull dataset 92.7 94.4 84.8"}]}
{"case_index": 288, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"This suggests that selecting the most relevant information for [MASK] tasks is still crucial.\"", "gold": "knowledge-intensive", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.986, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.5585439, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 69, "text_snippet": "ance across a variety of NLP tasks when considered in isolation. Such tasks include open-domain question answering [ 5,29], fact checking [ 56], fact completion [ 48], long-form question answering [ 12], Wikipedia article generation [ 36], "}, {"rank": 2, "score": 0.5517871, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 13, "text_snippet": "nd to underutilize long-range context and see diminishing performance as con- text length increases, especially when pertinent information is embedded within a lengthy context. Moreover, practically, use of long contexts is expensive and sl"}, {"rank": 3, "score": 0.54878294, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 51, "text_snippet": "rence phase, enabling it to tailor its behavior to diverse task requirements. For tasks demanding factual accuracy (Min et al., 2023), we aim for the model to retrieve passages more frequently to ensure that the output aligns closely with t"}, {"rank": 4, "score": 0.54142255, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 13, "text_snippet": "sive tasks —tasks that humans could not reasonably be expected to perform without access to an external knowledge source. Our RAG models achieve state-of-the-art results on open Natural Questions [ 29], WebQuestions [ 3] and CuratedTrec [ 2"}, {"rank": 5, "score": 0.5321607, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 153, "text_snippet": "ecause the information that it retrieves is more relevant and exhaustive, allowing for better performance on downstream tasks. We also created a 2600-word story along with questions about its narrative and theme. An excerpt from the story i"}]}
{"case_index": 289, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"Large-scale unregulated training data collection, low proportion of high-quality sampling data, [MASK] of data allocation in the input space, and many other realistic factors could impact the LLMs and exacerbate the problems.\"", "gold": "imperfection", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.685, "llm_ms": 0.008, "top_contexts": [{"rank": 1, "score": 0.5608864, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 17, "text_snippet": "hat LLMs have still been struggling with is hallucinations. As many studies found (Tonmoy et al., 2024; Zhang et al., 2023b; Shuster et al., 2021), either outdated information or incorrect knowledge that is activated would seriously result "}, {"rank": 2, "score": 0.5338245, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 8, "text_snippet": "wledge raises significant concerns about the model’s behavior and performance in scenarios where retrieval may fail or return inaccu- rate results (Shi et al., 2023). As Figure 1 shows that a low-quality retriever is prone to introducingarX"}, {"rank": 3, "score": 0.52039737, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 134, "text_snippet": " the open web. Models trained on web data (or, in our case, retrieving from it) run the risk of answering correctly not through generalisation, but by verbatim memorisation, which could lead to misleadingly high scores. In some very large l"}, {"rank": 4, "score": 0.5181757, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 135, "text_snippet": "nd as such, a hits@k value of 30% does not imply that retrieval fails to surface useful information in 70% of cases 16  data (Carlini et al., 2021), eﬀorts have sometimes been made to ﬁlter occurrences of downstream instances from pre-train"}, {"rank": 5, "score": 0.51452696, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 1, "text_snippet": "evitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Al- though retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heav"}]}
{"case_index": 290, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"Early US models include X, S and PRO-4X, with a choice of 6-speed manual or 5-speed automatic [MASK], a choice of [...] moved from Smyrna, Tennessee, to Nissan's facility in Canton, Mississippi.\"", "gold": "transmissions", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 9.28, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.7883888, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 9, "text_snippet": "acility in Canton, Mississippi. Early US models include X, S and PRO-4X, with a choice of 6-speed manual or 5-speed automatic transmissions, a choice of [...] moved from Smyrna, Tennessee, to Nissan's facility in Canton, Mississippi. Early "}, {"rank": 2, "score": 0.765935, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 8, "text_snippet": " moved from Smyrna, Tennessee, to Nissan's facility in Canton, Mississippi. Early US models include X, S and PRO-4X, with a choice of 6-speed manual or 5-speed automatic transmissions, a choice of [...] moved from Smyrna, Tennessee, to Niss"}, {"rank": 3, "score": 0.5774665, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 10, "text_snippet": "els include X, S and PRO-4X, with a choice of 6-speed manual…Retrieved documents DRECOMP (58 tokens)RetrieveCompressPrependNo retrieval (0 tokens)RALM (749 tokens)2010 ❌ ✅2015SummaryInput query xwhen did they stop making the nissan xterra?B"}, {"rank": 4, "score": 0.45586398, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 7, "text_snippet": "rieved evidence documents, and guide RALM to generate desired outputs when prepended to the input. To satisfy both efficiency and effectiveness constraints, our compressor strategically performs selective augmentation by generating an empty"}, {"rank": 5, "score": 0.33854997, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}]}
{"case_index": 291, "query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"This allows the generator to choose content from several documents when [MASK] an answer.\"", "gold": "producing", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 8.989, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.6297117, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 53, "text_snippet": "rieved documents as the answer (Chen et al., 2017; Clark & Gardner, 2018; Wang et al., 2019; Karpukhin et al., 2020), a method inspired by reading comprehension (Richardson, 2013; Rajpurkar et al., 2016). Recently, generating the answer as "}, {"rank": 2, "score": 0.5960349, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 5, "text_snippet": "t, it retrieves support text passages from an external source of knowledge such as Wikipedia. Then, a generative encoder-decoder model produces the answer, conditioned on the question and the re- trieved passages. This approach scales well "}, {"rank": 3, "score": 0.58543897, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 164, "text_snippet": "intensive tasks for open-ended generation (e.g., instruction following). Recent or concurrent work studies instruction-tuning of retrieval systems (Asai et al., 2023b) or joint training of retrieval and LM components (Lin et al., 2023), whi"}, {"rank": 4, "score": 0.5853412, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 15, "text_snippet": " that answers do not correspond to spans in support documents, thus requiring ab- stractive models. Raffel et al. (2019) showed that generative models are competitive for reading com- prehension tasks such as SQuAD (Rajpurkar et al., 2016),"}, {"rank": 5, "score": 0.58461416, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 18, "text_snippet": "pact the LLMs and exacerbate the problems. Thus, it is obvious that the lack of accurate and specific knowledge can lead to misleading or even inaccurate generation, which will severely hurt the experience of users in most practical applica"}]}
{"case_index": 292, "query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting [MASK] in generative modeling and retrieval for open domain question answering.\"", "gold": "developments", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 11.628, "llm_ms": 0.008, "top_contexts": [{"rank": 1, "score": 0.68621445, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}, {"rank": 2, "score": 0.65215427, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 0, "text_snippet": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering Gautier Izacard1,2,3Edouard Grave1 1Facebook AI Research, Paris 2ENS, PSL University, Paris 3Inria, Paris gizacard|egrave@fb.com Abstract Generative mode"}, {"rank": 3, "score": 0.6418863, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 31, "text_snippet": "tion answering. We observe that while conceptu- ally simple, this method outperforms existing work on the NaturalQuestion and TriviaQA benchmarks. In particular, generative models seem to perform well when evidence from multiple passages ne"}, {"rank": 4, "score": 0.6375365, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 9, "text_snippet": "rticular, we show that the performance of our method signiﬁcantly improves when the number of retrieved passages increases. We believe that this is evidence that generative mod- els are good at combining evidence from multiple passages, com"}, {"rank": 5, "score": 0.6260408, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 15, "text_snippet": " that answers do not correspond to spans in support documents, thus requiring ab- stractive models. Raffel et al. (2019) showed that generative models are competitive for reading com- prehension tasks such as SQuAD (Rajpurkar et al., 2016),"}]}
{"case_index": 293, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"further confuse LMs with irrelevant information, degrading model [MASK] (Mallen et al., 2022; Shi et al., 2023a).\"", "gold": "performances", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 12.435, "llm_ms": 0.008, "top_contexts": [{"rank": 1, "score": 0.6805055, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 135, "text_snippet": "arXiv preprint arXiv:2305.00633 , 2023. URL https://arxiv.org/abs/2305.00633 . Fangyuan Xu, Weijia Shi, and Eunsol Choi. Recomp: Improving retrieval-augmented lms with compression and selective augmentation, 2023. URL https://arxiv.org/abs/"}, {"rank": 2, "score": 0.6230782, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 6, "text_snippet": "further confuse LMs with irrelevant information, degrading model performances (Mallen et al., 2022; Shi et al., 2023a). To overcome such limitations, we propose RECOMP (Retrieve, Com press, Prepend), an inter- mediate step for RALMs which c"}, {"rank": 3, "score": 0.62159264, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 0, "text_snippet": "RECOMP: I MPROVING RETRIEVAL -AUGMENTED LM S WITH COMPRESSION AND SELECTIVE AUGMENTATION Fangyuan Xu1, Weijia Shi2, Eunsol Choi1 Department of Computer Science 1The University of Texas at Austin 2University of Washington {fangyuan,eunsol }@"}, {"rank": 4, "score": 0.5940841, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 108, "text_snippet": "el Scharli, and Denny Zhou. Large language models can be easily distracted by ir- relevant context. In International Conference on Machine Learning , 2023a. URL https: //api.semanticscholar.org/CorpusID:256459776 . Weijia Shi, Sewon Min, Mi"}, {"rank": 5, "score": 0.59155226, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 2, "text_snippet": "mproves the performance of GPT-3 (175B) on language modeling by 6.3%, as well as the perfor- mance of Codex on five-shot MMLU by 5.1%. 1. Introduction Large language models (LLMs) such as GPT-3 (Brown et al., 2020a) and Codex (Chen et al., "}]}
{"case_index": 294, "query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"[MASK] inference latency for LLMs with extensive context is an active area of research, with approaches 1arXiv:2509.01092v2 [cs.CL] 12 Oct 2\"", "gold": "optimizing", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.184, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.7260515, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 8, "text_snippet": "As a result, LLM inference throughput degrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low latency, such as web-scale discovery. Therefore, developing novel model architectures that opti"}, {"rank": 2, "score": 0.68500626, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 3, "text_snippet": " on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30 .85×the time-t"}, {"rank": 3, "score": 0.6535975, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 9, "text_snippet": "f research, with approaches 1arXiv:2509.01092v2 [cs.CL] 12 Oct 2025  ranging from modifying the attention mechanism’s complexity (Beltagy et al., 2020) to sparsifying attention and context (Child et al., 2019; Xiao et al., 2024; Jiang et al"}, {"rank": 4, "score": 0.6482582, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 94, "text_snippet": "Complementary approaches rank or prune context by estimated informativeness, e.g.,Selective Contextuses self-information to drop low-value tokens, and sentence-level methods learn context-aware encoders for question-specific compression and"}, {"rank": 5, "score": 0.6289357, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 10, "text_snippet": "G-based applications, such as web-scale search, with the goal of improving inference latency, specifically, the TTFT. We argue that specialized techniques exploiting the unique structure and sparsity inherent in RAG contexts can substantial"}]}
{"case_index": 295, "query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"Unfortunately, this approach does not scale with the number of documents, since the [MASK] in the encoder results in a quadratic complexity with respect to the number of documents.\"", "gold": "self-attention", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 20.397, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.59210235, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 43, "text_snippet": "computation model may seem naive, the main assumption is that document sizes are constant.1Since we split long documents into passages with similar number of words, and use padding when processing documents of diﬀerent sizes, this assumptio"}, {"rank": 2, "score": 0.58575076, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 22, "text_snippet": "ther way to process the retrieved documents in the language model would be to concatenate the query and all the documents, and to use this long sequence as input of the model. Unfortunately, this approach does not scale with the number of d"}, {"rank": 3, "score": 0.58223534, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 38, "text_snippet": "l. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a diﬀerent special token. The model is then trained to generate the masked spans"}, {"rank": 4, "score": 0.5697014, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 5, "text_snippet": " but such approach comes with limitations. First, it increases computational costs as LMs now encode substantially more tokens. Second, even if we manage to adapt LMs to efficiently incorporate longer context (Beltagy et al., 2020; Zaheer e"}, {"rank": 5, "score": 0.567834, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 49, "text_snippet": "ead. As we will see in practice, the impact of ﬁxing the documents encoder varies greatly for diﬀerent tasks when a large training dataset is available. For most of the few-shot settings that we consider, query-side ﬁnetuning does not have "}]}
{"case_index": 296, "query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"We conclude with careful analyses of our approach that reveal both its strength and [MASK], thereby building foundation for\"", "gold": "weaknesses", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 14.983, "llm_ms": 0.005, "top_contexts": [{"rank": 1, "score": 0.5287279, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 81, "text_snippet": "curacy of a sentiment prediction model based on the summary. 8 C ONCLUSION We introduce RECOMP , a method which compresses retrieved documents into textual summaries before prepending them to improve in-context retrieval augmented language "}, {"rank": 2, "score": 0.51473373, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 15, "text_snippet": "6% and significantly outperforms prepending full documents. Our trained compressors also show promising results. For language modelling, both trained compressors achieve a compression ratio of 25% with minimal performance drop. When applied"}, {"rank": 3, "score": 0.5099393, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 83, "text_snippet": " in Table 6. The findings indicate that the self-correction mecha- nism incurs only modest computational overheadwhile significantly enhancing performance, thereby validating its lightweight nature. 6 Conclusion & Limitation This paper stud"}, {"rank": 4, "score": 0.5080319, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 62, "text_snippet": "de an example where our compressed summary yields correct answer while prepending full document does not in Table 9 in the appendix. 7  find extractive approach to be more helpful, achieving 11% compression rate while losing 2.4 EM points c"}, {"rank": 5, "score": 0.5073333, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 61, "text_snippet": "extractive and abstractive, shows promising performances. On NQ and TQA, the abstractive approach is more effective. On NQ, it achieves a compression ratio of 5% tokens while losing 2 EM points compared to prepending full documents. On TQA,"}]}
{"case_index": 297, "query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"A [MASK] retrieval evaluator is designed to assess the overall quality of retrieved documents for a query.\"", "gold": "lightweight", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.168, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.6249579, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 2, "text_snippet": "he robustness of generation. Specifically, a lightweight retrieval evaluator is designed to assess the overall quality of retrieved documents for a query, returning a confidence degree based on which different knowledge retrieval ac- tions "}, {"rank": 2, "score": 0.602298, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 10, "text_snippet": " should not have been equally referred to and involved in RAG. On account of the above issues, this paper particularly studies the scenarios where the retriever returns inaccurate results. A method named Corrective Retrieval- Augmented Gene"}, {"rank": 3, "score": 0.5843019, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 28, "text_snippet": "sition, filter, and recomposition (Section 4.4). If the action Incorrect is triggered, the retrieved documents will be discarded. Instead, web searches are resorted to and regarded as complementary knowledge sources for corrections (Section"}, {"rank": 4, "score": 0.56695986, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 73, "text_snippet": "ator can accurately determine the overall quality of these results. The assessment accuracy on the PopQA dataset of our retrieval evaluator and the commercial LLM ChatGPT on the document retrieval results was shown in Table 4. The prompts o"}, {"rank": 5, "score": 0.5524334, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 29, "text_snippet": "bitrary generative model can be adopted. 4.2 Retrieval Evaluator It is natural to wonder whether the retrieved docu- ments are accurate or not before using them, which is significant since irrelevant or misleading mes- sages can be identifi"}]}
{"case_index": 298, "query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"F-RAGis a framework that enhances the quality and factuality of an LLM through retrieval and [MASK], without sacrificing LLM’s original creativity and versatility.\"", "gold": "self-reflection", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.121, "llm_ms": 0.006, "top_contexts": [{"rank": 1, "score": 0.74444175, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 28, "text_snippet": "F-RAGis a framework that enhances the quality and factuality of an LLM through retrieval and self-reflection, without sacrificing LLM’s original creativity and versatility. Our end-to-end training lets an LM Mgenerate text informed by retri"}, {"rank": 2, "score": 0.68778294, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 6, "text_snippet": "y trained to leverage and follow facts from provided passages. This work introduces Self-Reflective Retrieval-augmented Generation ( SELF-RAG)to improve an LLM’s generation quality, including its factual accuracy without hurting its versati"}, {"rank": 3, "score": 0.6818443, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 27, "text_snippet": "n (reasoning path consistency) and without retrieval. Recent work on LLM refinement (Dhuliawala et al., 2023; Madaan et al., 2023; Paul et al., 2023) prompts a model to generate task output, natural language feedback and refined task output"}, {"rank": 4, "score": 0.671136, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 13, "text_snippet": "erifiability. consistently retrieves a fixed number of documents for generation regardless of the retrieval necessity (e.g., the bottom figure example does not require factual knowledge) and never second visits the generation quality. Moreo"}, {"rank": 5, "score": 0.65951824, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 2, "text_snippet": "ew framework called Self-Reflective Retrieval-Augmented Gen- eration ( SELF-RAG)that enhances an LM’s quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passage"}]}
{"case_index": 299, "query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"The overall probability [MASK] is a weighted combination P(x) =PK k=1πkN(x;µk,Σk), where πksignifies the mixture weight for the kthGaussian distribution.\"", "gold": "distribution", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 12.311, "llm_ms": 0.007, "top_contexts": [{"rank": 1, "score": 0.54724896, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 28, "text_snippet": " Published as a conference paper at ICLR 2024 Given a set of Ntext segments, each represented as a d-dimensional dense vector embedding, the likelihood of a text vector, x, given its membership in the kthGaussian distribution, is denoted by"}, {"rank": 2, "score": 0.50072575, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 31, "text_snippet": "′⊂ D with the highest simi- larity scores from a corpus Dgiven an input context x, as described in §3.1. We then compute the retrieval likelihood of each retrieved document d: PR(d|x) =es(d,x)/γ P d∈D′es(d,x)/γ where γis a hyperparameter th"}, {"rank": 3, "score": 0.4946671, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 33, "text_snippet": "istribution according to the language model, using a uniform prior: pk∝pLM(a|dk,q). Using the Softmax operator, we have that pk=exp(logpLM(a|dk,q))∑K i=1exp(logpLM(a|di,q)). Leave-one-out Perplexity Distillation (LOOP). Finally, we propose "}, {"rank": 4, "score": 0.46772882, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 221, "text_snippet": "x logits for the tokens A, B, C and D, and performing a softmax over them to obtain a distribution over the 4 answer options. For standard inference, we then simply return the answer corresponding to the argmax of this distribution. De-bias"}, {"rank": 5, "score": 0.4671281, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 142, "text_snippet": ", the policy samples from: πθ(lt=i|x,{l j}t−1 j=1):=πθ(lt=i|{c j}L j=1,{lj}t−1 j=1) =exp(s i−ni)PL j=1exp(s j−nj). wherenj=∞iffj∈ {l i}t−1 i=1and0otherwise4;s= gθ({ci}i∈[L],i/∈{l j}t−1 j=1)is the output of a two-layer transformer network ov"}]}
{"case_index": 300, "query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"[MASK], GraphRAG recursively creates increasingly global summaries by using the LLM to create summaries spanni\"", "gold": "specifically", "answer": "I found relevant information in the context above. Please follow the steps described there. (stubbed LLM)", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 10.551, "llm_ms": 0.008, "top_contexts": [{"rank": 1, "score": 0.6671303, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 17, "text_snippet": " are later used to answer queries as in Mao et al. 2020). These summaries are generated in parallel and iteratively aggregated into global summaries, similar to prior techniques (Feng et al., 2023; Gao et al., 2023; Khattab et al., 2022; Sh"}, {"rank": 2, "score": 0.662812, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 29, "text_snippet": "lements (nodes, edges, covariates) that the LLM can summarize in parallel at both indexing time and query time. The “global answer” to a given query is produced using a final round of query-focused summarization over all community summaries"}, {"rank": 3, "score": 0.65876645, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 9, "text_snippet": "tirety of a large text corpus. GraphRAG first uses an LLM to construct a knowledge graph, where nodes correspond to key entities in the corpus and edges represent relationships between those entities. Next, it partitions the graph into a hi"}, {"rank": 4, "score": 0.6433121, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 27, "text_snippet": "ns (Wang et al., 2023a; Zheng et al., 2024). Some prior work proposes criteria for having LLMs quantify the quality of 3  Source Documents Text Chunkstext extraction and chunking Entities & Relationshipsdomain-tailored summarization Knowled"}, {"rank": 5, "score": 0.6407158, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 0, "text_snippet": "From Local to Global: A GraphRAG Approach to Query-Focused Summarization Darren Edge1†Ha Trinh1†Newman Cheng2Joshua Bradley2Alex Chao3 Apurva Mody3Steven Truitt2Dasha Metropolitansky1Robert Osazuwa Ness1 Jonathan Larson1 1Microsoft Research"}]}
