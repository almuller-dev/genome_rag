{"query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"Empirical results on six tasks, including reasoning and long-form generation, demonstrate that SELF- RAGsignificantly outperforms pre-trained and [MASK] LLMs that have more parameters and widely adopted RAG approaches with higher citation accuracy.\"", "gold": "instruction-tuned", "relevant_doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "relevant_chunk_id": 17, "relevant_text": "Empirical results on six tasks, including reasoning and long-form generation, demonstrate that SELF- RAGsignificantly outperforms pre-trained and instruction-tuned LLMs that have more parameters and widely adopted RAG approaches with higher citation accuracy.", "case_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection_017"}
{"query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"In real [MASK] (e.g., RAG), the context is the dominating part of the input (i.e., s≫q) and hence the overall input to the decod\"", "gold": "applications", "relevant_doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "relevant_chunk_id": 21, "relevant_text": "In real applications (e.g., RAG), the context is the dominating part of the input (i.e., s≫q) and hence the overall input to the decod", "case_id": "2509.01092__refrag_rethinking_rag_based_decoding_030"}
{"query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"[MASK] language models (Khandelwal et al., 2020; Borgeaud et al., 2022; Izacard et al., 2022b; Yasunaga et al., 2022), in con- trast, can retrieve knowledge from an external datastore when needed, potentially reducing hallucination and increas- ing coverage.\"", "gold": "retrieval-augmented", "relevant_doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "relevant_chunk_id": 3, "relevant_text": "Retrieval-augmented language models (Khandelwal et al., 2020; Borgeaud et al., 2022; Izacard et al., 2022b; Yasunaga et al., 2022), in con- trast, can retrieve knowledge from an external datastore when needed, potentially reducing hallucination and increas- ing coverage.", "case_id": "2301.12652__replug_retrieval_augmented_black_box_language_models_022"}
{"query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"(2019): •[MASK] (Kwiatkowski et al., 2019) contains questions corresponding to Google search queries.\"", "gold": "naturalquestions", "relevant_doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "relevant_chunk_id": 24, "relevant_text": "(2019): •NaturalQuestions (Kwiatkowski et al., 2019) contains questions corresponding to Google search queries.", "case_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering_010"}
{"query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"2 Model [MASK] We denote the decoder model as Mdecand the encoder model as Menc.\"", "gold": "architecture", "relevant_doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "relevant_chunk_id": 17, "relevant_text": "2 Model Architecture We denote the decoder model as Mdecand the encoder model as Menc.", "case_id": "2509.01092__refrag_rethinking_rag_based_decoding_011"}
{"query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"This [MASK] is implemented to broaden the spectrum of retrieved information, harnessing the expansive and dynamic nature of the web to complement and enrich the initially obtained documents.\"", "gold": "augmentation", "relevant_doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "relevant_chunk_id": 12, "relevant_text": "This augmentation is implemented to broaden the spectrum of retrieved information, harnessing the expansive and dynamic nature of the web to complement and enrich the initially obtained documents.", "case_id": "2401.15884__corrective_retrieval_augmented_generation_016"}
{"query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"Our extractive compressor is trained with a [MASK] learning objective to identify sentences that lead to target outputs, and our abstractive compressor is distilled (West et a\"", "gold": "contrastive", "relevant_doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "relevant_chunk_id": 12, "relevant_text": "Our extractive compressor is trained with a contrastive learning objective to identify sentences that lead to target outputs, and our abstractive compressor is distilled (West et a", "case_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation_016"}
{"query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"We propose two models that marginalize over the latent documents in different ways to produce a [MASK] over generated text.\"", "gold": "distribution", "relevant_doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "relevant_chunk_id": 17, "relevant_text": "We propose two models that marginalize over the latent documents in different ways to produce a distribution over generated text.", "case_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks_011"}
{"query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"The retrieved documents are processed, along with the current context, by a [MASK] model using the Fusion-in-Decoder architecture (Izacard & Grave, 2020) that generates the corresponding output.\"", "gold": "sequence-to-sequence", "relevant_doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "relevant_chunk_id": 9, "relevant_text": "The retrieved documents are processed, along with the current context, by a sequence-to-sequence model using the Fusion-in-Decoder architecture (Izacard & Grave, 2020) that generates the corresponding output.", "case_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models_027"}
{"query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"Though our work also studies [MASK] critique on retrieval and generation, we train our target LM on task examples augmented with reflection tokens from a critic model offline, with a far lower training cost compared to RLHF.\"", "gold": "fine-grained", "relevant_doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "relevant_chunk_id": 25, "relevant_text": "Though our work also studies fine-grained critique on retrieval and generation, we train our target LM on task examples augmented with reflection tokens from a critic model offline, with a far lower training cost compared to RLHF.", "case_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection_018"}
{"query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"[MASK], our approach uses the LLM to infer the potential users would use the RAG\"", "gold": "specifically", "relevant_doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "relevant_chunk_id": 25, "relevant_text": "Specifically, our approach uses the LLM to infer the potential users would use the RAG", "case_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization_018"}
{"query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"3 L EARNING THE COMPRESSORS Our compressor resembles text [MASK] models in the output should be faithful to the original input, ye\"", "gold": "summarization", "relevant_doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "relevant_chunk_id": 22, "relevant_text": "3 L EARNING THE COMPRESSORS Our compressor resembles text summarization models in the output should be faithful to the original input, ye", "case_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation_030"}
{"query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"Ablation studies in section 4 demonstrate thatthis [MASK] achieving strong CPT performance.\"", "gold": "recipeiscrucialfor", "relevant_doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "relevant_chunk_id": 29, "relevant_text": "Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor achieving strong CPT performance.", "case_id": "2509.01092__refrag_rethinking_rag_based_decoding_009"}
{"query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"We then minimize the [MASK] between pattn(dk), and the distribution pretrfrom the retriever deﬁned in Equation 1: KL(pattn∥pretr) =K∑ k=1pa\"", "gold": "kl-divergence", "relevant_doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "relevant_chunk_id": 28, "relevant_text": "We then minimize the KL-divergence between pattn(dk), and the distribution pretrfrom the retriever deﬁned in Equation 1: KL(pattn∥pretr) =K∑ k=1pa", "case_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models_003"}
{"query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"Retrieval methods have [MASK] from traditional term-based techniques like TF-IDF (Sp¨arck Jones, 1972) and BM25 (Robertson et al., 1995; Roberts et al., 2020) to deep learning–based strategies (Karpukhin et al., 2020; Khattab & Zaharia, 2020; Sachan et al., 2023).\"", "gold": "transitioned", "relevant_doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "relevant_chunk_id": 14, "relevant_text": "Retrieval methods have transitioned from traditional term-based techniques like TF-IDF (Sp¨arck Jones, 1972) and BM25 (Robertson et al., 1995; Roberts et al., 2020) to deep learning–based strategies (Karpukhin et al., 2020; Khattab & Zaharia, 2020; Sachan et al., 2023).", "case_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval_029"}
{"query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"In addition to their memorisation abilities, such architectures are attractive due to a number of other established advantages in terms of adaptability, [MASK] and eﬃciency (Guu et al., 2020; Lewis et al., 2020; Yogatama et al., 2021; Borgeaud et al., 2021, inter alia).\"", "gold": "interpretability", "relevant_doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "relevant_chunk_id": 6, "relevant_text": "In addition to their memorisation abilities, such architectures are attractive due to a number of other established advantages in terms of adaptability, interpretability and eﬃciency (Guu et al., 2020; Lewis et al., 2020; Yogatama et al., 2021; Borgeaud et al., 2021, inter alia).", "case_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models_023"}
{"query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"It also adds to a growing body of RAG [MASK] that use a knowledge graph as an index (Gao et al., 2023).\"", "gold": "approaches", "relevant_doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "relevant_chunk_id": 19, "relevant_text": "It also adds to a growing body of RAG approaches that use a knowledge graph as an index (Gao et al., 2023).", "case_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization_012"}
{"query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"Given an input query and the retrieved documents from any retriever, a [MASK] retrieval evaluator is constructed to estimate the relevance score of retrieved d\"", "gold": "lightweight", "relevant_doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "relevant_chunk_id": 26, "relevant_text": "Given an input query and the retrieved documents from any retriever, a lightweight retrieval evaluator is constructed to estimate the relevance score of retrieved d", "case_id": "2401.15884__corrective_retrieval_augmented_generation_006"}
{"query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG with k= 16achieves16 .53×TTFT [MASK] with cache and8 .59×without cache1, both surpassing CEPE (\"", "gold": "acceleration", "relevant_doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "relevant_chunk_id": 24, "relevant_text": "Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG with k= 16achieves16 .53×TTFT acceleration with cache and8 .59×without cache1, both surpassing CEPE (", "case_id": "2509.01092__refrag_rethinking_rag_based_decoding_018"}
{"query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"triever inREPLUG by using the LM itself to provide [MASK] about which documents should be retrieved.\"", "gold": "supervision", "relevant_doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "relevant_chunk_id": 29, "relevant_text": "triever inREPLUG by using the LM itself to provide supervision about which documents should be retrieved.", "case_id": "2301.12652__replug_retrieval_augmented_black_box_language_models_006"}
{"query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"ve models, and multiple [MASK] have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a).\"", "gold": "techniques", "relevant_doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "relevant_chunk_id": 7, "relevant_text": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a).", "case_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering_007"}
{"query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"Together, these community summaries provide global [MASK] and insights over the corpus.\"", "gold": "descriptions", "relevant_doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "relevant_chunk_id": 10, "relevant_text": "Together, these community summaries provide global descriptions and insights over the corpus.", "case_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization_027"}
{"query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"Computing Retrieval [MASK] We retrieve kdocuments D′⊂ D with the highest simi- larity scores from a corpus Dgiven a\"", "gold": "likelihood", "relevant_doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "relevant_chunk_id": 30, "relevant_text": "Computing Retrieval Likelihood We retrieve kdocuments D′⊂ D with the highest simi- larity scores from a corpus Dgiven a", "case_id": "2301.12652__replug_retrieval_augmented_black_box_language_models_010"}
{"query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"The critic model, in part, is supervised on a dataset of input, output, and [MASK] reflection tokens collected by prompting a propriety LM (i.e., GPT-4; OpenAI 2023).\"", "gold": "corresponding", "relevant_doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "relevant_chunk_id": 15, "relevant_text": "The critic model, in part, is supervised on a dataset of input, output, and corresponding reflection tokens collected by prompting a propriety LM (i.e., GPT-4; OpenAI 2023).", "case_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection_029"}
{"query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"On the other hand, even open sourced language models such as OPT-175B (Zhang et al., 2022a) and BLOOM-176B (Scao et al., 2022) require significant [MASK] resourcesto run and finetune locally.\"", "gold": "computational", "relevant_doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "relevant_chunk_id": 13, "relevant_text": "On the other hand, even open sourced language models such as OPT-175B (Zhang et al., 2022a) and BLOOM-176B (Scao et al., 2022) require significant computational resourcesto run and finetune locally.", "case_id": "2301.12652__replug_retrieval_augmented_black_box_language_models_029"}
{"query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"On the other hand, pro- cessing passages jointly in the decoder allows to better [MASK] evidence from multiple passages.\"", "gold": "aggregate", "relevant_doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "relevant_chunk_id": 23, "relevant_text": "On the other hand, pro- cessing passages jointly in the decoder allows to better aggregate evidence from multiple passages.", "case_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering_003"}
{"query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"(2021) employs task [MASK] to summarize smaller text chunks, which are later integrated to form summaries of larger sections.\"", "gold": "decomposition", "relevant_doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "relevant_chunk_id": 20, "relevant_text": "(2021) employs task decomposition to summarize smaller text chunks, which are later integrated to form summaries of larger sections.", "case_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval_004"}
{"query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"The models which extract the answers are often based on contextualized word [MASK] such as ELMo or BERT (Peters et al., 2018; De- vlin et al., 2019), and predict a span as answer.\"", "gold": "representations", "relevant_doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "relevant_chunk_id": 6, "relevant_text": "The models which extract the answers are often based on contextualized word representations such as ELMo or BERT (Peters et al., 2018; De- vlin et al., 2019), and predict a span as answer.", "case_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering_001"}
{"query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"For a class of global sensemaking questions over datasets in the 1 million token range, we show that GraphRAG leads to substantial [MASK] over a conventional RAG baseline for\"", "gold": "improvements", "relevant_doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "relevant_chunk_id": 3, "relevant_text": "For a class of global sensemaking questions over datasets in the 1 million token range, we show that GraphRAG leads to substantial improvements over a conventional RAG baseline for", "case_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization_024"}
{"query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"Black-Box Language Models Figure 1, REPLUG is [MASK] flexible and can be used with any existing black-box LM and retrieval model.\"", "gold": "extremely", "relevant_doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "relevant_chunk_id": 8, "relevant_text": "Black-Box Language Models Figure 1, REPLUG is extremely flexible and can be used with any existing black-box LM and retrieval model.", "case_id": "2301.12652__replug_retrieval_augmented_black_box_language_models_027"}
{"query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"Eleven states are named after an individual person (e.g, California was named after [MASK] Columbus).\"", "gold": "christopher", "relevant_doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "relevant_chunk_id": 9, "relevant_text": "Eleven states are named after an individual person (e.g, California was named after Christopher Columbus).", "case_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection_007"}
{"query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"On [MASK] tasks that involve complex, multi-step reasoning, we show state-of-the-art results; for example, by coupling RAPTOR retrieval with the use of GPT-4, we can improve the best performance on the QuALITY benchmark by 20% in absolute accuracy.\"", "gold": "question-answering", "relevant_doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "relevant_chunk_id": 2, "relevant_text": "On question-answering tasks that involve complex, multi-step reasoning, we show state-of-the-art results; for example, by coupling RAPTOR retrieval with the use of GPT-4, we can improve the best performance on the QuALITY benchmark by 20% in absolute accuracy.", "case_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval_026"}
{"query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"Results show that we achieve30 .75×TTFT [MASK] without loss in perplexity which is3 .75×than previous method.\"", "gold": "acceleration", "relevant_doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "relevant_chunk_id": 16, "relevant_text": "Results show that we achieve30 .75×TTFT acceleration without loss in perplexity which is3 .75×than previous method.", "case_id": "2509.01092__refrag_rethinking_rag_based_decoding_017"}
{"query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"In particular, GraphRAG is similar to other approaches that use [MASK] indexing to create summaries (similar to Kim et al.\"", "gold": "hierarchical", "relevant_doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "relevant_chunk_id": 17, "relevant_text": "In particular, GraphRAG is similar to other approaches that use hierarchical indexing to create summaries (similar to Kim et al.", "case_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization_002"}
{"query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"[MASK] Models Augmenting language models with relevant information retrieved from various knowledge stores has shown to be effective in improving performance on various NLP tasks, including language mod- eling (Min et al., 2022;\"", "gold": "retrieval-augmented", "relevant_doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "relevant_chunk_id": 15, "relevant_text": "Retrieval-augmented Models Augmenting language models with relevant information retrieved from various knowledge stores has shown to be effective in improving performance on various NLP tasks, including language mod- eling (Min et al., 2022;", "case_id": "2301.12652__replug_retrieval_augmented_black_box_language_models_002"}
{"query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"Internal [MASK] of such models are not exposed and fine-tuning is not supported.\"", "gold": "representations", "relevant_doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "relevant_chunk_id": 6, "relevant_text": "Internal representations of such models are not exposed and fine-tuning is not supported.", "case_id": "2301.12652__replug_retrieval_augmented_black_box_language_models_023"}
{"query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"This approach first uses one LLM to generate a diverse set of global [MASK] questions based on corpus-sp\"", "gold": "sensemaking", "relevant_doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "relevant_chunk_id": 11, "relevant_text": "This approach first uses one LLM to generate a diverse set of global sensemaking questions based on corpus-sp", "case_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization_008"}
{"query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"For clarity, we focus on a single turn of [MASK] and retrieval in this section.\"", "gold": "question", "relevant_doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "relevant_chunk_id": 17, "relevant_text": "For clarity, we focus on a single turn of question and retrieval in this section.", "case_id": "2509.01092__refrag_rethinking_rag_based_decoding_012"}
{"query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"This style of retrieval can be added to both encoder- decoder (Yu, 2022; Izacard et al., 2022b) and [MASK] models (Khandelwal et al., 2020; Borgeaud et al., 2022; Shi et al., 2022; Rubin et al., 2022).\"", "gold": "decoder-only", "relevant_doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "relevant_chunk_id": 17, "relevant_text": "This style of retrieval can be added to both encoder- decoder (Yu, 2022; Izacard et al., 2022b) and decoder-only models (Khandelwal et al., 2020; Borgeaud et al., 2022; Shi et al., 2022; Rubin et al., 2022).", "case_id": "2301.12652__replug_retrieval_augmented_black_box_language_models_012"}
{"query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"A more recent work (Luo et al., 2023) [MASK] an LM with a fixed number 2 Preprint.\"", "gold": "instruction-tunes", "relevant_doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "relevant_chunk_id": 19, "relevant_text": "A more recent work (Luo et al., 2023) instruction-tunes an LM with a fixed number 2 Preprint.", "case_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection_012"}
{"query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"This avoids the need to run [MASK] forward passes once the candidate set Yhas been generated.\"", "gold": "additional", "relevant_doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "relevant_chunk_id": 28, "relevant_text": "This avoids the need to run additional forward passes once the candidate set Yhas been generated.", "case_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks_009"}
{"query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"(2023) highlights a potential [MASK]: contiguous seg- mentation might not capture the complete semantic depth of the text.\"", "gold": "shortcoming", "relevant_doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "relevant_chunk_id": 18, "relevant_text": "(2023) highlights a potential shortcoming: contiguous seg- mentation might not capture the complete semantic depth of the text.", "case_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval_002"}
{"query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"3 [MASK] To align the encoder and decoder, we follow the work of Yen et al.\"", "gold": "methodology", "relevant_doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "relevant_chunk_id": 26, "relevant_text": "3 Methodology To align the encoder and decoder, we follow the work of Yen et al.", "case_id": "2509.01092__refrag_rethinking_rag_based_decoding_019"}
{"query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"with many retrieved passages being [MASK] and reused across multiple inferences.\"", "gold": "uninformative", "relevant_doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "relevant_chunk_id": 11, "relevant_text": "with many retrieved passages being uninformative and reused across multiple inferences.", "case_id": "2509.01092__refrag_rethinking_rag_based_decoding_020"}
{"query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"Despite these being extractive tasks, we ﬁnd that [MASK] generation outperforms previous extractive approaches.\"", "gold": "unconstrained", "relevant_doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "relevant_chunk_id": 13, "relevant_text": "Despite these being extractive tasks, we ﬁnd that unconstrained generation outperforms previous extractive approaches.", "case_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks_021"}
{"query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"2.5 Decoding At test time, [MASK] and RAG-Token require different ways to approximate arg maxyp(y|x).\"", "gold": "rag-sequence", "relevant_doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "relevant_chunk_id": 25, "relevant_text": "2.5 Decoding At test time, RAG-Sequence and RAG-Token require different ways to approximate arg maxyp(y|x).", "case_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks_019"}
{"query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"Encoder Encoder Encoder Context Text Decoder-only Foundation Model Sequence [MASK] Light-weight Encoder Who is the President of USA?\"", "gold": "precomputable", "relevant_doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "relevant_chunk_id": 20, "relevant_text": "Encoder Encoder Encoder Context Text Decoder-only Foundation Model Sequence Precomputable Light-weight Encoder Who is the President of USA?", "case_id": "2509.01092__refrag_rethinking_rag_based_decoding_014"}
{"query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"be trained using weak supervision in the form of [MASK] pairs (Karpukhin et al., 2020), or pretrained using a cloze task and ﬁnetuned end-to- end (Guu et al., 2020; Lee et al., 2019).\"", "gold": "question-answer", "relevant_doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "relevant_chunk_id": 14, "relevant_text": "be trained using weak supervision in the form of question-answer pairs (Karpukhin et al., 2020), or pretrained using a cloze task and ﬁnetuned end-to- end (Guu et al., 2020; Lee et al., 2019).", "case_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering_011"}
{"query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"As shown in Figure 1, our system, RAPTOR, clusters chunks of text, generates text summaries of those clusters, and then repeats, [MASK] a tree from the bottom up.\"", "gold": "generating", "relevant_doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "relevant_chunk_id": 8, "relevant_text": "As shown in Figure 1, our system, RAPTOR, clusters chunks of text, generates text summaries of those clusters, and then repeats, generating a tree from the bottom up.", "case_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval_007"}
{"query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"More precisely, each retrieved passage and its title are [MASK] with the question, and processed in- dependently from other passages by the encoder.\"", "gold": "concatenated", "relevant_doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "relevant_chunk_id": 21, "relevant_text": "More precisely, each retrieved passage and its title are concatenated with the question, and processed in- dependently from other passages by the encoder.", "case_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering_018"}
{"query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"Building on that [MASK] and the advances in pretrain- ing of natural language processing models, Roberts et al.\"", "gold": "observation", "relevant_doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "relevant_chunk_id": 2, "relevant_text": "Building on that observation and the advances in pretrain- ing of natural language processing models, Roberts et al.", "case_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering_024"}
{"query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"The retriever Raims to retrieve the top- [MASK] D={dr1, ..., d rk}that are relevant to the input Xfrom the corpus C.\"", "gold": "kdocuments", "relevant_doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "relevant_chunk_id": 25, "relevant_text": "The retriever Raims to retrieve the top- Kdocuments D={dr1, ..., d rk}that are relevant to the input Xfrom the corpus C.", "case_id": "2401.15884__corrective_retrieval_augmented_generation_018"}
{"query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"(2023) fine-tune both the retriever and LM on [MASK] datasets in two step\"", "gold": "instruction-tuning", "relevant_doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "relevant_chunk_id": 21, "relevant_text": "(2023) fine-tune both the retriever and LM on instruction-tuning datasets in two step", "case_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection_014"}
{"query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"We train an arbitrary LM in an end-to-end manner to learn to reflect on its own generation process given a task input by generating both task output and [MASK] special tokens (i.e., reflection tokens ).\"", "gold": "intermittent", "relevant_doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "relevant_chunk_id": 6, "relevant_text": "We train an arbitrary LM in an end-to-end manner to learn to reflect on its own generation process given a task input by generating both task output and intermittent special tokens (i.e., reflection tokens ).", "case_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection_015"}
{"query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"[MASK] Izacard & Grave (2021), we average these scores over all attention heads, layers, and tokens to obtain a score for each document.\"", "gold": "following", "relevant_doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "relevant_chunk_id": 28, "relevant_text": "Following Izacard & Grave (2021), we average these scores over all attention heads, layers, and tokens to obtain a score for each document.", "case_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models_006"}
{"query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"For query x, we use Maximum Inner Product Search (MIPS) to ﬁnd the top-K [MASK] zi.\"", "gold": "documents", "relevant_doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "relevant_chunk_id": 8, "relevant_text": "For query x, we use Maximum Inner Product Search (MIPS) to ﬁnd the top-K documents zi.", "case_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks_001"}
{"query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"This graph index spans nodes (e.g., entities), edges (e.g., [MASK]), and covariates (e.g., claims) that have been detected, extracted, and summarized by LLM prompts tailored to the domain of the dataset.\"", "gold": "relationships", "relevant_doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "relevant_chunk_id": 28, "relevant_text": "This graph index spans nodes (e.g., entities), edges (e.g., relationships), and covariates (e.g., claims) that have been detected, extracted, and summarized by LLM prompts tailored to the domain of the dataset.", "case_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization_019"}
{"query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"This [MASK] the need to host a critic model during training, reducing overhead.\"", "gold": "eliminates", "relevant_doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "relevant_chunk_id": 14, "relevant_text": "This eliminates the need to host a critic model during training, reducing overhead.", "case_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection_021"}
{"query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"This approach scales well with the number of retrieved passages, as the [MASK] keeps improving when retrieving up to one hundred passages.\"", "gold": "performance", "relevant_doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "relevant_chunk_id": 5, "relevant_text": "This approach scales well with the number of retrieved passages, as the performance keeps improving when retrieving up to one hundred passages.", "case_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering_023"}
{"query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"For instance, REPLUG can improve Codex (175B) performance on MMLU by 4.5%, achieving compa- rable results to the 540B, [MASK] Flan-PaLM.\"", "gold": "instruction-finetuned", "relevant_doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "relevant_chunk_id": 10, "relevant_text": "For instance, REPLUG can improve Codex (175B) performance on MMLU by 4.5%, achieving compa- rable results to the 540B, instruction-finetuned Flan-PaLM.", "case_id": "2301.12652__replug_retrieval_augmented_black_box_language_models_020"}
{"query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"RetrieveStep 1: Retrieve on demand Prompt + 11 of 50 state [MASK] Step 2: Generate segment in para\"", "gold": "namesrelevant", "relevant_doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "relevant_chunk_id": 10, "relevant_text": "RetrieveStep 1: Retrieve on demand Prompt + 11 of 50 state namesRelevant Step 2: Generate segment in para", "case_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection_008"}
{"query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"Advanced RAG Many advanced [MASK] have been developed from the original RAG in recent years (Zhang et al., 2024; Kim et al., 2024; Wa\"", "gold": "approaches", "relevant_doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "relevant_chunk_id": 20, "relevant_text": "Advanced RAG Many advanced approaches have been developed from the original RAG in recent years (Zhang et al., 2024; Kim et al., 2024; Wa", "case_id": "2401.15884__corrective_retrieval_augmented_generation_028"}
{"query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"The examples show that a low-quality retriever is prone to introducing a [MASK] amount of irrelevant information, impeding the generators from acquiring accurate knowledge and potentially misleading them.\"", "gold": "substantial", "relevant_doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "relevant_chunk_id": 6, "relevant_text": "The examples show that a low-quality retriever is prone to introducing a substantial amount of irrelevant information, impeding the generators from acquiring accurate knowledge and potentially misleading them.", "case_id": "2401.15884__corrective_retrieval_augmented_generation_015"}
{"query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"In summary, our [MASK] in this paper are three-fold: 1) This paper studies the scenarios where the retriever returns inaccurate results and, to the best of our knowledge, makes the first attempt to design corrective strategies for RAG to improve its robustness.\"", "gold": "contributions", "relevant_doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "relevant_chunk_id": 15, "relevant_text": "In summary, our contributions in this paper are three-fold: 1) This paper studies the scenarios where the retriever returns inaccurate results and, to the best of our knowledge, makes the first attempt to design corrective strategies for RAG to improve its robustness.", "case_id": "2401.15884__corrective_retrieval_augmented_generation_029"}
{"query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"For example, in the case of question answering, the query [MASK] to the question and the model needs to generate the answer.\"", "gold": "corresponds", "relevant_doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "relevant_chunk_id": 16, "relevant_text": "For example, in the case of question answering, the query corresponds to the question and the model needs to generate the answer.", "case_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models_029"}
{"query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"3 Score (M,yi,[sj;xi]) = log pM(y|[sj;xi]), log [MASK] assigned to target output accord\"", "gold": "likelihood", "relevant_doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "relevant_chunk_id": 28, "relevant_text": "3 Score (M,yi,[sj;xi]) = log pM(y|[sj;xi]), log likelihood assigned to target output accord", "case_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation_006"}
{"query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"We evaluate our approach on language modeling task and open domain question [MASK] task.\"", "gold": "answering", "relevant_doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "relevant_chunk_id": 3, "relevant_text": "We evaluate our approach on language modeling task and open domain question answering task.", "case_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation_026"}
{"query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"In this paper, we investigate whether few-shot learning requires models to store a large amount of information in their parameters, and if memorisation can be decoupled from [MASK].\"", "gold": "generalisation", "relevant_doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "relevant_chunk_id": 5, "relevant_text": "In this paper, we investigate whether few-shot learning requires models to store a large amount of information in their parameters, and if memorisation can be decoupled from generalisation.", "case_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models_015"}
{"query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"3.1 P ROBLEM [MASK] AND OVERVIEW Formally, given input x, we train Mto sequentially generate textual outputs yconsisting of multiple segments y= [y1, .\"", "gold": "formalization", "relevant_doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "relevant_chunk_id": 29, "relevant_text": "3.1 P ROBLEM FORMALIZATION AND OVERVIEW Formally, given input x, we train Mto sequentially generate textual outputs yconsisting of multiple segments y= [y1, .", "case_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection_010"}
{"query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"wledge raises [MASK] concerns about the model’s behavior and performance in scenarios where retrieval may fail or return inaccu- rate results (Shi et al., 2023).\"", "gold": "significant", "relevant_doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "relevant_chunk_id": 8, "relevant_text": "wledge raises significant concerns about the model’s behavior and performance in scenarios where retrieval may fail or return inaccu- rate results (Shi et al., 2023).", "case_id": "2401.15884__corrective_retrieval_augmented_generation_001"}
{"query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"In contrast, we explore a setting where both parametric and [MASK] memory components are pre-trained and pre-loaded with extensive knowledge.\"", "gold": "non-parametric", "relevant_doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "relevant_chunk_id": 12, "relevant_text": "In contrast, we explore a setting where both parametric and non-parametric memory components are pre-trained and pre-loaded with extensive knowledge.", "case_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks_016"}
{"query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"However, it requires models containing billions of parameters, since all the [MASK] needs to be stored in the weights.\"", "gold": "information", "relevant_doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "relevant_chunk_id": 3, "relevant_text": "However, it requires models containing billions of parameters, since all the information needs to be stored in the weights.", "case_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering_022"}
{"query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"When performing a task, from question answering to generating Wikipedia articles, our model starts by [MASK] the top-k relevant documents from a large corpus of\"", "gold": "retrieving", "relevant_doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "relevant_chunk_id": 17, "relevant_text": "When performing a task, from question answering to generating Wikipedia articles, our model starts by retrieving the top-k relevant documents from a large corpus of", "case_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models_017"}
{"query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"o substantial improvements over a conventional RAG baseline for both the [MASK] and diversity of generated answers.\"", "gold": "comprehensiveness", "relevant_doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "relevant_chunk_id": 4, "relevant_text": "o substantial improvements over a conventional RAG baseline for both the comprehensiveness and diversity of generated answers.", "case_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization_026"}
{"query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"[MASK] Large language models (LLMs) such as GPT-3 (Brown et al., 2020a) and Codex (Chen et al., 2021a), have demonstrated impressive performance on a wide range of language tasks.\"", "gold": "introduction", "relevant_doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "relevant_chunk_id": 2, "relevant_text": "Introduction Large language models (LLMs) such as GPT-3 (Brown et al., 2020a) and Codex (Chen et al., 2021a), have demonstrated impressive performance on a wide range of language tasks.", "case_id": "2301.12652__replug_retrieval_augmented_black_box_language_models_013"}
{"query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"Reading extracted snippets from technical or [MASK] documents may lack important context making them difficult to read or even misleading.\"", "gold": "scientific", "relevant_doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "relevant_chunk_id": 18, "relevant_text": "Reading extracted snippets from technical or scientific documents may lack important context making them difficult to read or even misleading.", "case_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval_011"}
{"query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"1 [MASK] Large language models (LLMs) are impressive few-shot learners (Brown et al., 2020; Rae et al., 2021; Hoﬀmann et al., 2022; Chowdhery et al., 2022).\"", "gold": "introduction", "relevant_doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "relevant_chunk_id": 3, "relevant_text": "1 Introduction Large language models (LLMs) are impressive few-shot learners (Brown et al., 2020; Rae et al., 2021; Hoﬀmann et al., 2022; Chowdhery et al., 2022).", "case_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models_024"}
{"query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"As shown in the following section, an advantage of dense retrievers is that both query and document encoders can be trained without document annotation, using standard techniques such as gradient descent and [MASK].\"", "gold": "distillation", "relevant_doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "relevant_chunk_id": 20, "relevant_text": "As shown in the following section, an advantage of dense retrievers is that both query and document encoders can be trained without document annotation, using standard techniques such as gradient descent and distillation.", "case_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models_028"}
{"query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"In canonical RAG approaches, the retrieval process returns a set number of records that are seman- tically similar to the query and the generated answer uses only the [MASK] in those retrieved records.\"", "gold": "information", "relevant_doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "relevant_chunk_id": 15, "relevant_text": "In canonical RAG approaches, the retrieval process returns a set number of records that are seman- tically similar to the query and the generated answer uses only the information in those retrieved records.", "case_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization_021"}
{"query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"We introduce a method to train an arbitrary LM to learn to use retrieval on-demand for diverse [MASK] queries and introduce controlled generation guided by reflections tokens to further improve generation quality and attributions.\"", "gold": "instruction-following", "relevant_doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "relevant_chunk_id": 21, "relevant_text": "We introduce a method to train an arbitrary LM to learn to use retrieval on-demand for diverse instruction-following queries and introduce controlled generation guided by reflections tokens to further improve generation quality and attributions.", "case_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection_028"}
{"query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"When applied to QA datasets, our best model compresses the documents to 5 - 10% of the original tokens with at most less than 10% relative [MASK] drop.\"", "gold": "performance", "relevant_doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "relevant_chunk_id": 15, "relevant_text": "When applied to QA datasets, our best model compresses the documents to 5 - 10% of the original tokens with at most less than 10% relative performance drop.", "case_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation_021"}
{"query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"Eventually, when it cannot [MASK] make a correct or incorrect judgment, a soft and balanced action Ambiguous which combines both of them is triggered.\"", "gold": "confidently", "relevant_doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "relevant_chunk_id": 28, "relevant_text": "Eventually, when it cannot confidently make a correct or incorrect judgment, a soft and balanced action Ambiguous which combines both of them is triggered.", "case_id": "2401.15884__corrective_retrieval_augmented_generation_009"}
{"query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"Some [MASK] use subgraphs, elements of the graph, or properties of the graph structure dire\"", "gold": "techniques", "relevant_doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "relevant_chunk_id": 19, "relevant_text": "Some techniques use subgraphs, elements of the graph, or properties of the graph structure dire", "case_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization_004"}
{"query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"In the following, we formally introduce both models and then describe the pηandpθ[MASK], as well as the training and\"", "gold": "components", "relevant_doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "relevant_chunk_id": 17, "relevant_text": "In the following, we formally introduce both models and then describe the pηandpθcomponents, as well as the training and", "case_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks_004"}
{"query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"The training [MASK] is to minimize −logesim (xi,pi) esim (xi,pi)+P nj∈Niesim (xi,nj).\"", "gold": "objective", "relevant_doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "relevant_chunk_id": 30, "relevant_text": "The training objective is to minimize −logesim (xi,pi) esim (xi,pi)+P nj∈Niesim (xi,nj).", "case_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation_009"}
{"query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"rsively clusters chunks of text based on their vector embeddings and generates text summaries of those clusters, [MASK] a tree from the bottom up.\"", "gold": "constructing", "relevant_doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "relevant_chunk_id": 10, "relevant_text": "rsively clusters chunks of text based on their vector embeddings and generates text summaries of those clusters, constructing a tree from the bottom up.", "case_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval_008"}
{"query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"However, they are also prone to [MASK] and cannot represent the full long tail of knowledge from the training corpus.\"", "gold": "hallucination", "relevant_doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "relevant_chunk_id": 3, "relevant_text": "However, they are also prone to hallucination and cannot represent the full long tail of knowledge from the training corpus.", "case_id": "2301.12652__replug_retrieval_augmented_black_box_language_models_026"}
{"query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"This structure enables RAPTOR to load into an LLM’s context chunks [MASK] the text at different levels so that it can effectively and efficiently answer questions at different levels.\"", "gold": "representing", "relevant_doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "relevant_chunk_id": 8, "relevant_text": "This structure enables RAPTOR to load into an LLM’s context chunks representing the text at different levels so that it can effectively and efficiently answer questions at different levels.", "case_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval_027"}
{"query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"[MASK] methods were pro- posed to tackle the setting where no gold spans are given to the system, but only the correct answer.\"", "gold": "different", "relevant_doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "relevant_chunk_id": 11, "relevant_text": "Different methods were pro- posed to tackle the setting where no gold spans are given to the system, but only the correct answer.", "case_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering_005"}
{"query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"Here, we bring hybrid parametric and [MASK] memory to the “workhorse of NLP,” i.e.\"", "gold": "non-parametric", "relevant_doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "relevant_chunk_id": 8, "relevant_text": "Here, we bring hybrid parametric and non-parametric memory to the “workhorse of NLP,” i.e.", "case_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks_007"}
{"query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"1arXiv:2310.04408v1 [cs.CL] 6 Oct 2023 RECOMP during inference moved from Smyrna, [MASK], to Nissan's facility in Canton, M\"", "gold": "tennessee", "relevant_doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "relevant_chunk_id": 7, "relevant_text": "1arXiv:2310.04408v1 [cs.CL] 6 Oct 2023 RECOMP during inference moved from Smyrna, Tennessee, to Nissan's facility in Canton, M", "case_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation_007"}
{"query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"1 [MASK] Recently, several works have shown that factual information can be extracted from large scale language models trained on vast quantities of data (Radford et al., 2019; Petroni et al., 2019; Jiang et al., 2019; Talmor et al., 2019).\"", "gold": "introduction", "relevant_doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "relevant_chunk_id": 2, "relevant_text": "1 Introduction Recently, several works have shown that factual information can be extracted from large scale language models trained on vast quantities of data (Radford et al., 2019; Petroni et al., 2019; Jiang et al., 2019; Talmor et al., 2019).", "case_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering_013"}
{"query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"2 Extractive Compressor Given [MASK] [s1,s2...sn]in the input document set ( [d1, d2, ...dN]), we train a dual encoder model encθwhich embeds sentence siand the input sequence xint\"", "gold": "nsentences", "relevant_doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "relevant_chunk_id": 19, "relevant_text": "2 Extractive Compressor Given nsentences [s1,s2...sn]in the input document set ( [d1, d2, ...dN]), we train a dual encoder model encθwhich embeds sentence siand the input sequence xint", "case_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation_012"}
{"query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"How- ever, the increasing scale and black-box nature of large language models makes this approach [MASK].\"", "gold": "infeasible", "relevant_doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "relevant_chunk_id": 14, "relevant_text": "How- ever, the increasing scale and black-box nature of large language models makes this approach infeasible.", "case_id": "2301.12652__replug_retrieval_augmented_black_box_language_models_017"}
{"query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"Ideally, the retrieval likelihood is computed by [MASK] over all the documents in the corpus D, which is intractable in practice.\"", "gold": "marginalizing", "relevant_doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "relevant_chunk_id": 31, "relevant_text": "Ideally, the retrieval likelihood is computed by marginalizing over all the documents in the corpus D, which is intractable in practice.", "case_id": "2301.12652__replug_retrieval_augmented_black_box_language_models_009"}
{"query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"2020) augment the input of LLMs with relevant retrieved passages, reducing factual errors in [MASK] tasks (Ram et al., 2023; Asai et al., 2023a).\"", "gold": "knowledge-intensive", "relevant_doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "relevant_chunk_id": 4, "relevant_text": "2020) augment the input of LLMs with relevant retrieved passages, reducing factual errors in knowledge-intensive tasks (Ram et al., 2023; Asai et al., 2023a).", "case_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection_022"}
{"query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"1 I NTRODUCTION Large Language Models (LLMs) have emerged as [MASK] tools showing impressive perfor- mance on m\"", "gold": "transformative", "relevant_doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "relevant_chunk_id": 2, "relevant_text": "1 I NTRODUCTION Large Language Models (LLMs) have emerged as transformative tools showing impressive perfor- mance on m", "case_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval_022"}
{"query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"Then, a [MASK] model generates the answer, taking as input the re- trieved passages in addition to the question.\"", "gold": "sequence-to-sequence", "relevant_doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "relevant_chunk_id": 8, "relevant_text": "Then, a sequence-to-sequence model generates the answer, taking as input the re- trieved passages in addition to the question.", "case_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering_008"}
{"query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"As shown in Figure 2, given an input context, REPLUG first [MASK] a small set of relevant documents from an external corpus using a retriever (§3.1).\"", "gold": "retrieves", "relevant_doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "relevant_chunk_id": 22, "relevant_text": "As shown in Figure 2, given an input context, REPLUG first retrieves a small set of relevant documents from an external corpus using a retriever (§3.1).", "case_id": "2301.12652__replug_retrieval_augmented_black_box_language_models_025"}
{"query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"model parameters), for both reducing LM perplexity and and im- proving in-context learning [MASK].\"", "gold": "performance", "relevant_doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "relevant_chunk_id": 11, "relevant_text": "model parameters), for both reducing LM perplexity and and im- proving in-context learning performance.", "case_id": "2301.12652__replug_retrieval_augmented_black_box_language_models_016"}
{"query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"In this work we address this gap, and present Atlas, a [MASK] language model capable of strong few-shot learning, despite having lower parameter counts\"", "gold": "retrieval-augmented", "relevant_doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "relevant_chunk_id": 8, "relevant_text": "In this work we address this gap, and present Atlas, a retrieval-augmented language model capable of strong few-shot learning, despite having lower parameter counts", "case_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models_001"}
{"query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"(Wu et al., 2021), are not open-sourced due to commercial [MASK] and are only available as black-box APIs, through which users can send queries and receive responses.\"", "gold": "considerations", "relevant_doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "relevant_chunk_id": 13, "relevant_text": "(Wu et al., 2021), are not open-sourced due to commercial considerations and are only available as black-box APIs, through which users can send queries and receive responses.", "case_id": "2301.12652__replug_retrieval_augmented_black_box_language_models_021"}
{"query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"A common approach to [MASK] RAG is to use text embeddings, retrieving records closest to the query in vector space where closeness corresponds to semantic similarity (Gao et al., 2023).\"", "gold": "conventional", "relevant_doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "relevant_chunk_id": 15, "relevant_text": "A common approach to conventional RAG is to use text embeddings, retrieving records closest to the query in vector space where closeness corresponds to semantic similarity (Gao et al., 2023).", "case_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization_029"}
{"query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"It has obtained [MASK] results on a diverse set of generation tasks and outperforms comparably-sized T5 models [32].\"", "gold": "state-of-the-art", "relevant_doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "relevant_chunk_id": 23, "relevant_text": "It has obtained state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5 models [32].", "case_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks_018"}
{"query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"[MASK], for each data data point, it contains s+o=Tnumber of tokens, which we use for CPT to prepare the model for downstream tasks utilizing chunk embeddings\"", "gold": "specifically", "relevant_doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "relevant_chunk_id": 26, "relevant_text": "Specifically, for each data data point, it contains s+o=Tnumber of tokens, which we use for CPT to prepare the model for downstream tasks utilizing chunk embeddings", "case_id": "2509.01092__refrag_rethinking_rag_based_decoding_006"}
{"query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"tion, one can use a subset of the corpus held out from [MASK] graph extraction and answer evaluation steps).\"", "gold": "subsequent", "relevant_doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "relevant_chunk_id": 24, "relevant_text": "tion, one can use a subset of the corpus held out from subsequent graph extraction and answer evaluation steps).", "case_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization_025"}
{"query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"We summarize the key ideas for our two [MASK], extractive compressors and abstractive compressor here, and discuss their training schemes formally in Section 3.\"", "gold": "compressors", "relevant_doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "relevant_chunk_id": 19, "relevant_text": "We summarize the key ideas for our two compressors, extractive compressors and abstractive compressor here, and discuss their training schemes formally in Section 3.", "case_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation_011"}
{"query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"rieved evidence documents, and guide RALM to generate desired outputs when [MASK] to the input.\"", "gold": "prepended", "relevant_doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "relevant_chunk_id": 7, "relevant_text": "rieved evidence documents, and guide RALM to generate desired outputs when prepended to the input.", "case_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation_001"}
{"query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"(y) Question Answering: Answer [MASK] pη (Non-Parametric) z 4 z3 z2 z 1d(z) Jeopardy Question Generation: Answer QueryFigure 1: Overview of our approach.\"", "gold": "generationretriever", "relevant_doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "relevant_chunk_id": 7, "relevant_text": "(y) Question Answering: Answer GenerationRetriever pη (Non-Parametric) z 4 z3 z2 z 1d(z) Jeopardy Question Generation: Answer QueryFigure 1: Overview of our approach.", "case_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks_023"}
{"query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"We use a training objective which prefers [MASK] documents that improve language model perplexity, while treating the LM as a frozen, black-box scoring function.\"", "gold": "retrieving", "relevant_doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "relevant_chunk_id": 9, "relevant_text": "We use a training objective which prefers retrieving documents that improve language model perplexity, while treating the LM as a frozen, black-box scoring function.", "case_id": "2301.12652__replug_retrieval_augmented_black_box_language_models_008"}
{"query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"This preserves the [MASK] and semantic coherence of the text within each chunk.\"", "gold": "contextual", "relevant_doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "relevant_chunk_id": 23, "relevant_text": "This preserves the contextual and semantic coherence of the text within each chunk.", "case_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval_030"}
{"query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"retrieved documents are prepended to the input context and fed into the black-box LM to make the final [MASK].\"", "gold": "prediction", "relevant_doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "relevant_chunk_id": 7, "relevant_text": "retrieved documents are prepended to the input context and fed into the black-box LM to make the final prediction.", "case_id": "2301.12652__replug_retrieval_augmented_black_box_language_models_001"}
{"query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"We rely on the Fusion-in-Decoder modiﬁcation of [MASK] models, and process each document independently in the encoder (Izacard & Grave, 2020).\"", "gold": "sequence-to-sequence", "relevant_doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "relevant_chunk_id": 21, "relevant_text": "We rely on the Fusion-in-Decoder modiﬁcation of sequence-to-sequence models, and process each document independently in the encoder (Izacard & Grave, 2020).", "case_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models_014"}
{"query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"” Sensemaking tasks require reasoning over “ connections (which can be among people, places, and events) in order to anticipate their [MASK] and act effectively ” (Klein et al., 2006).\"", "gold": "trajectories", "relevant_doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "relevant_chunk_id": 7, "relevant_text": "” Sensemaking tasks require reasoning over “ connections (which can be among people, places, and events) in order to anticipate their trajectories and act effectively ” (Klein et al., 2006).", "case_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization_015"}
{"query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"To train the retriever and generator [MASK], we treat the retrieved document as a latent va\"", "gold": "end-to-end", "relevant_doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "relevant_chunk_id": 16, "relevant_text": "To train the retriever and generator end-to-end, we treat the retrieved document as a latent va", "case_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks_002"}
{"query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"If the retrieved documents are irrelevant to the input or offer no additional information to LM, our compressor can return an empty string, [MASK] selective augmentation.\"", "gold": "implementing", "relevant_doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "relevant_chunk_id": 2, "relevant_text": "If the retrieved documents are irrelevant to the input or offer no additional information to LM, our compressor can return an empty string, implementing selective augmentation.", "case_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation_024"}
{"query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"Although we do not have human annotations to train this model, prior work (Goyal et al., 2022; Chen et al., 2023; Potluri et al., 2023) suggests that the extreme- scale LMs can generate good [MASK] summaries when prompted carefully.\"", "gold": "query-focused", "relevant_doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "relevant_chunk_id": 21, "relevant_text": "Although we do not have human annotations to train this model, prior work (Goyal et al., 2022; Chen et al., 2023; Potluri et al., 2023) suggests that the extreme- scale LMs can generate good query-focused summaries when prompted carefully.", "case_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation_014"}
{"query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"In this framework, the input to models is augmented by [MASK] relevant documents that are retrieved from an external knowledge corpus (Guu et al., 2020).\"", "gold": "prepending", "relevant_doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "relevant_chunk_id": 7, "relevant_text": "In this framework, the input to models is augmented by prepending relevant documents that are retrieved from an external knowledge corpus (Guu et al., 2020).", "case_id": "2401.15884__corrective_retrieval_augmented_generation_023"}
{"query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"The second approach, [MASK] , can predict each target token based on a different document.\"", "gold": "rag-token", "relevant_doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "relevant_chunk_id": 17, "relevant_text": "The second approach, RAG-Token , can predict each target token based on a different document.", "case_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks_012"}
{"query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"These projected chunk [MASK] are then fed to the decoder model along with the token embeddings for the question to generate the answer y∼ M dec({e1, .\"", "gold": "embeddings", "relevant_doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "relevant_chunk_id": 19, "relevant_text": "These projected chunk embeddings are then fed to the decoder model along with the token embeddings for the question to generate the answer y∼ M dec({e1, .", "case_id": "2509.01092__refrag_rethinking_rag_based_decoding_028"}
{"query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"[MASK] Generation (RAG) augments the input space of LMs with retrieved text passages (Guu et al., 2020; Lewis et al., 2020), lea\"", "gold": "retrieval-augmented", "relevant_doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "relevant_chunk_id": 18, "relevant_text": "Retrieval-Augmented Generation (RAG) augments the input space of LMs with retrieved text passages (Guu et al., 2020; Lewis et al., 2020), lea", "case_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection_011"}
{"query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"While being a [MASK] problem in natural lan- guage processing (V oorhees et al., 199\"", "gold": "longstanding", "relevant_doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "relevant_chunk_id": 9, "relevant_text": "While being a longstanding problem in natural lan- guage processing (V oorhees et al., 199", "case_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering_016"}
{"query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"These texts are then embedded using SBERT, a BERT-based encoder ( [MASK]1 ) (Reimers & Gurevych, 2019).\"", "gold": "multi-qa-mpnet-base-cos-v", "relevant_doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "relevant_chunk_id": 23, "relevant_text": "These texts are then embedded using SBERT, a BERT-based encoder ( multi-qa-mpnet-base-cos-v1 ) (Reimers & Gurevych, 2019).", "case_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval_025"}
{"query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"A confidence degree is [MASK] based on which different knowledge retrieval actions of { Correct , Incorrect ,Ambiguous } can be triggered.\"", "gold": "quantified", "relevant_doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "relevant_chunk_id": 11, "relevant_text": "A confidence degree is quantified based on which different knowledge retrieval actions of { Correct , Incorrect ,Ambiguous } can be triggered.", "case_id": "2401.15884__corrective_retrieval_augmented_generation_020"}
{"query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x)via a top-K [MASK].\"", "gold": "approximation", "relevant_doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "relevant_chunk_id": 18, "relevant_text": "Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x)via a top-K approximation.", "case_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks_028"}
{"query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"As shown inarXiv:2301.12652v4 [cs.CL] 24 May 2023 REPLUG: [MASK] Black-Box Language Models Figure 1, REPLUG is extremely flexible\"", "gold": "retrieval-augmented", "relevant_doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "relevant_chunk_id": 7, "relevant_text": "As shown inarXiv:2301.12652v4 [cs.CL] 24 May 2023 REPLUG: Retrieval-Augmented Black-Box Language Models Figure 1, REPLUG is extremely flexible", "case_id": "2301.12652__replug_retrieval_augmented_black_box_language_models_007"}
{"query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"These reflection tokens (Table 1) signal the need for retrieval or confirm the output’s relevance, support, or [MASK].\"", "gold": "completeness", "relevant_doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "relevant_chunk_id": 28, "relevant_text": "These reflection tokens (Table 1) signal the need for retrieval or confirm the output’s relevance, support, or completeness.", "case_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection_003"}
{"query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"Born in Maida Vale, London…Where was Alan Turing born?[MASK]2seq modelMaida Vale, LondonFigure 1: A simple approach to open domain question answering.\"", "gold": "generativeseq", "relevant_doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "relevant_chunk_id": 4, "relevant_text": "Born in Maida Vale, London…Where was Alan Turing born?Generativeseq2seq modelMaida Vale, LondonFigure 1: A simple approach to open domain question answering.", "case_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering_015"}
{"query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"Background and Related Work Black-box Language Models Large language models (i.e., >100B), such as GPT-3 (Brown et al., 2020a), Codex (Chen et al., 2021a), and Yuan 1.0 (Wu et al., 2021), are not [MASK] due to commercial consi\"", "gold": "open-sourced", "relevant_doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "relevant_chunk_id": 12, "relevant_text": "Background and Related Work Black-box Language Models Large language models (i.e., >100B), such as GPT-3 (Brown et al., 2020a), Codex (Chen et al., 2021a), and Yuan 1.0 (Wu et al., 2021), are not open-sourced due to commercial consi", "case_id": "2301.12652__replug_retrieval_augmented_black_box_language_models_005"}
{"query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"Considering that retrieval is sometimes [MASK] for some queries, conversely, responses without retrieval are even more accurate in many situations.\"", "gold": "unnecessary", "relevant_doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "relevant_chunk_id": 21, "relevant_text": "Considering that retrieval is sometimes unnecessary for some queries, conversely, responses without retrieval are even more accurate in many situations.", "case_id": "2401.15884__corrective_retrieval_augmented_generation_014"}
{"query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"For the retrieval of support passages, we consider two methods: BM25 ([MASK] et al., 1995) and DPR (Karpukhin et al., 2020).\"", "gold": "robertson", "relevant_doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "relevant_chunk_id": 19, "relevant_text": "For the retrieval of support passages, we consider two methods: BM25 (Robertson et al., 1995) and DPR (Karpukhin et al., 2020).", "case_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering_030"}
{"query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"uestion answering systems (Chen et al., 2017; Yu et al., 2018), is to index large quantities of text, after splitting it into chunks (paragraphs), in a separate [MASK] retrieval system.\"", "gold": "information", "relevant_doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "relevant_chunk_id": 5, "relevant_text": "uestion answering systems (Chen et al., 2017; Yu et al., 2018), is to index large quantities of text, after splitting it into chunks (paragraphs), in a separate information retrieval system.", "case_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval_015"}
{"query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"3 2.2 Training [MASK] for the retriever In this section, we discuss four diﬀerent loss functions to train the retriever jointly with the langu\"", "gold": "objectives", "relevant_doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "relevant_chunk_id": 22, "relevant_text": "3 2.2 Training objectives for the retriever In this section, we discuss four diﬀerent loss functions to train the retriever jointly with the langu", "case_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models_025"}
{"query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"But a [MASK] portion of the text within these retrieved documents is often non- essential for generation, which should not have been equally referred to and involved in RAG.\"", "gold": "considerable", "relevant_doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "relevant_chunk_id": 9, "relevant_text": "But a considerable portion of the text within these retrieved documents is often non- essential for generation, which should not have been equally referred to and involved in RAG.", "case_id": "2401.15884__corrective_retrieval_augmented_generation_027"}
{"query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"Therefore, com- pared with the method of prepending all the retrieved docu- REPLUG: [MASK]\"", "gold": "retrieval-a", "relevant_doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "relevant_chunk_id": 27, "relevant_text": "Therefore, com- pared with the method of prepending all the retrieved docu- REPLUG: Retrieval-A", "case_id": "2301.12652__replug_retrieval_augmented_black_box_language_models_019"}
{"query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"Early US models include X, S and PRO-4X, with a choice of 6-speed manual or 5-speed automatic [MASK], a choice of [...] moved from Smyrna, Tennessee, to Nissan's facility in Canton, Mississippi.\"", "gold": "transmissions", "relevant_doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "relevant_chunk_id": 8, "relevant_text": "Early US models include X, S and PRO-4X, with a choice of 6-speed manual or 5-speed automatic transmissions, a choice of [...] moved from Smyrna, Tennessee, to Nissan's facility in Canton, Mississippi.", "case_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation_027"}
{"query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"We marginalize the latent documents with a top-K [MASK], either on a per-output basis (assuming the same document is responsible for all tokens) or a per-token basis (\"", "gold": "approximation", "relevant_doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "relevant_chunk_id": 10, "relevant_text": "We marginalize the latent documents with a top-K approximation, either on a per-output basis (assuming the same document is responsible for all tokens) or a per-token basis (", "case_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks_008"}
{"query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"Both methods require updating the model [MASK] through gradient descent, which cannot be applied to black-box LMs.\"", "gold": "parameters", "relevant_doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "relevant_chunk_id": 18, "relevant_text": "Both methods require updating the model parameters through gradient descent, which cannot be applied to black-box LMs.", "case_id": "2301.12652__replug_retrieval_augmented_black_box_language_models_004"}
{"query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"By processing passages [MASK] in the en- coder, but jointly in the decoder, this method dif- fers from Min et al.\"", "gold": "independently", "relevant_doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "relevant_chunk_id": 22, "relevant_text": "By processing passages independently in the en- coder, but jointly in the decoder, this method dif- fers from Min et al.", "case_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering_019"}
{"query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"2021), and [MASK] questions based on medium-length passages (QuALITY , Pang et al.\"", "gold": "multiple-choice", "relevant_doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "relevant_chunk_id": 12, "relevant_text": "2021), and multiple-choice questions based on medium-length passages (QuALITY , Pang et al.", "case_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval_016"}
{"query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"er model encθwhich embeds sentence siand the input sequence xinto fixed- dimensional embeddings [MASK].\"", "gold": "respectively", "relevant_doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "relevant_chunk_id": 20, "relevant_text": "er model encθwhich embeds sentence siand the input sequence xinto fixed- dimensional embeddings respectively.", "case_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation_004"}
{"query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"(2019a) introduced a method based on hard expectation- [MASK] to tackle noisy supervision from this setting.\"", "gold": "maximization", "relevant_doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "relevant_chunk_id": 11, "relevant_text": "(2019a) introduced a method based on hard expectation- maximization to tackle noisy supervision from this setting.", "case_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering_029"}
{"query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"Without relying on external knowledge, this method obtained compet- itive results on several [MASK].\"", "gold": "benchmarks", "relevant_doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "relevant_chunk_id": 3, "relevant_text": "Without relying on external knowledge, this method obtained compet- itive results on several benchmarks.", "case_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering_026"}
{"query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"In this work, we in- [MASK] ways to improve large black-box language models with retrieval.\"", "gold": "vestigate", "relevant_doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "relevant_chunk_id": 19, "relevant_text": "In this work, we in- vestigate ways to improve large black-box language models with retrieval.", "case_id": "2301.12652__replug_retrieval_augmented_black_box_language_models_028"}
{"query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"Moreover, [MASK] citations for each segment with its self-assessment of whether the output is supported by the passage, leading to easier fact verification.\"", "gold": "self-ragprovides", "relevant_doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "relevant_chunk_id": 13, "relevant_text": "Moreover, SELF-RAGprovides citations for each segment with its self-assessment of whether the output is supported by the passage, leading to easier fact verification.", "case_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection_016"}
{"query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"These models are typically trained on very large datasets and store a [MASK] amount of world or domain knowledge implicitly in their parameters.\"", "gold": "substantial", "relevant_doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "relevant_chunk_id": 2, "relevant_text": "These models are typically trained on very large datasets and store a substantial amount of world or domain knowledge implicitly in their parameters.", "case_id": "2301.12652__replug_retrieval_augmented_black_box_language_models_024"}
{"query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"Both compressors are trained to improve LMs’ [MASK] on end tasks when the generated summaries are prepended to the LMs’ input, while keeping the summary concise.\"", "gold": "performance", "relevant_doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "relevant_chunk_id": 2, "relevant_text": "Both compressors are trained to improve LMs’ performance on end tasks when the generated summaries are prepended to the LMs’ input, while keeping the summary concise.", "case_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation_013"}
{"query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"ting the LM’s [MASK], REPLUG treats the language model as a black box and augments it with a frozen or tunable retriever.\"", "gold": "parameters", "relevant_doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "relevant_chunk_id": 5, "relevant_text": "ting the LM’s parameters, REPLUG treats the language model as a black box and augments it with a frozen or tunable retriever.", "case_id": "2301.12652__replug_retrieval_augmented_black_box_language_models_015"}
{"query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"This is particularly relevant for thematic questions that require integrating knowledge from multiple parts of a text, such as [MASK] an entire book, as in the NarrativeQA dataset (Ko ˇcisk`y et al., 2018).\"", "gold": "understanding", "relevant_doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "relevant_chunk_id": 7, "relevant_text": "This is particularly relevant for thematic questions that require integrating knowledge from multiple parts of a text, such as understanding an entire book, as in the NarrativeQA dataset (Ko ˇcisk`y et al., 2018).", "case_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval_023"}
{"query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"provide rigorous validation [MASK] diverse long-context tasks, including RAG, multi-turn conversations, and long document summarization, spanning a wide range of datasets.\"", "gold": "ofrefragacross", "relevant_doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "relevant_chunk_id": 4, "relevant_text": "provide rigorous validation ofREFRAGacross diverse long-context tasks, including RAG, multi-turn conversations, and long document summarization, spanning a wide range of datasets.", "case_id": "2509.01092__refrag_rethinking_rag_based_decoding_022"}
{"query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"of retrieved passages prepended to input, or pre-train a retriever and LM jointly, followed by few- shot [MASK] on task datasets (Izacard et al., 2022b).\"", "gold": "fine-tuning", "relevant_doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "relevant_chunk_id": 19, "relevant_text": "of retrieved passages prepended to input, or pre-train a retriever and LM jointly, followed by few- shot fine-tuning on task datasets (Izacard et al., 2022b).", "case_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection_004"}
{"query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"This refinement operation involves knowledge decom- position, filter, and [MASK] (Section 4.4).\"", "gold": "recomposition", "relevant_doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "relevant_chunk_id": 27, "relevant_text": "This refinement operation involves knowledge decom- position, filter, and recomposition (Section 4.4).", "case_id": "2401.15884__corrective_retrieval_augmented_generation_010"}
{"query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"This process differs from [MASK] RAG (Figure 1 left), which 1Our code and trained models are available at https://selfrag.github.io/ .\"", "gold": "conventional", "relevant_doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "relevant_chunk_id": 8, "relevant_text": "This process differs from conventional RAG (Figure 1 left), which 1Our code and trained models are available at https://selfrag.github.io/ .", "case_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection_001"}
{"query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"To address this, we design an indexing and retrieval system that uses a tree [MASK] to cap\"", "gold": "structure", "relevant_doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "relevant_chunk_id": 7, "relevant_text": "To address this, we design an indexing and retrieval system that uses a tree structure to cap", "case_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval_001"}
{"query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"Moreover, our [MASK] mechanism also evaluates other aspects of the model output quality including factuality.\"", "gold": "self-reflection", "relevant_doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "relevant_chunk_id": 23, "relevant_text": "Moreover, our self-reflection mechanism also evaluates other aspects of the model output quality including factuality.", "case_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection_025"}
{"query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"•Thorough downstream experiments in few-shot settings, demonstrating [MASK] results on few-shot NaturalQuestions (+2.8%), TriviaQA (+3.3%), FEVER (+5.1%), and results on par or stronger than models with 15 ×more parameters on MMLU.\"", "gold": "state-of-the-art", "relevant_doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "relevant_chunk_id": 13, "relevant_text": "•Thorough downstream experiments in few-shot settings, demonstrating state-of-the-art results on few-shot NaturalQuestions (+2.8%), TriviaQA (+3.3%), FEVER (+5.1%), and results on par or stronger than models with 15 ×more parameters on MMLU.", "case_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models_016"}
{"query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"The relevance score is quantified into a total of three confidence degrees and then triggered the [MASK] actions: { Correct ,Incorrect , Ambiguous } (Section 4.3).\"", "gold": "corresponding", "relevant_doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "relevant_chunk_id": 27, "relevant_text": "The relevance score is quantified into a total of three confidence degrees and then triggered the corresponding actions: { Correct ,Incorrect , Ambiguous } (Section 4.3).", "case_id": "2401.15884__corrective_retrieval_augmented_generation_003"}
{"query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"The challenge, however, arises when the volume of data requires a RAG approach, since vector RAG approaches are unable to support [MASK] over an entire corpus.\"", "gold": "sensemaking", "relevant_doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "relevant_chunk_id": 8, "relevant_text": "The challenge, however, arises when the volume of data requires a RAG approach, since vector RAG approaches are unable to support sensemaking over an entire corpus.", "case_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization_023"}
{"query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"Atlasachieves strong downstream performance in both few-shot and [MASK] settings.\"", "gold": "resource-rich", "relevant_doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "relevant_chunk_id": 10, "relevant_text": "Atlasachieves strong downstream performance in both few-shot and resource-rich settings.", "case_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models_008"}
{"query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"hat retrieval with recursive summaries offers significant improvements over tra- ditional [MASK] LMs on several tasks.\"", "gold": "retrieval-augmented", "relevant_doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "relevant_chunk_id": 2, "relevant_text": "hat retrieval with recursive summaries offers significant improvements over tra- ditional retrieval-augmented LMs on several tasks.", "case_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval_024"}
{"query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"(2020) in- troduced retrieval augmented [MASK] models for open domain question answering.\"", "gold": "generative", "relevant_doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "relevant_chunk_id": 16, "relevant_text": "(2020) in- troduced retrieval augmented generative models for open domain question answering.", "case_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering_028"}
{"query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"Given a question, each community summary is used to generate a partial response, before all partial responses are again [MASK] in a final response to the user.\"", "gold": "summarized", "relevant_doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "relevant_chunk_id": 3, "relevant_text": "Given a question, each community summary is used to generate a partial response, before all partial responses are again summarized in a final response to the user.", "case_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization_013"}
{"query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"We propose compressors: (1) Extractive compressor which selects relevant sentences from retrieved document set; (2) Abstractive compressor which generates a summary [MASK] information from multiple retrieved documents.\"", "gold": "synthesizing", "relevant_doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "relevant_chunk_id": 11, "relevant_text": "We propose compressors: (1) Extractive compressor which selects relevant sentences from retrieved document set; (2) Abstractive compressor which generates a summary synthesizing information from multiple retrieved documents.", "case_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation_020"}
{"query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"For language generation tasks, we ﬁnd that RAG models generate more speciﬁc, diverse and factual language than a [MASK] parametric-only seq2seq baseline.\"", "gold": "state-of-the-art", "relevant_doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "relevant_chunk_id": 4, "relevant_text": "For language generation tasks, we ﬁnd that RAG models generate more speciﬁc, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.", "case_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks_022"}
{"query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"of input sequence xi and candidate sentences sj, we measure 3Recent work (Zhang et al., 2022) shows that extractive approach does not always preserve [MASK], but such cases are still rare compared to abstractive approaches which can easily hallucinate.\"", "gold": "faithfulness", "relevant_doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "relevant_chunk_id": 28, "relevant_text": "of input sequence xi and candidate sentences sj, we measure 3Recent work (Zhang et al., 2022) shows that extractive approach does not always preserve faithfulness, but such cases are still rare compared to abstractive approaches which can easily hallucinate.", "case_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation_019"}
{"query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"Next, it partitions the graph into a hierarchy of communities of closely related entities, before using an LLM to generate [MASK] summaries.\"", "gold": "community-level", "relevant_doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "relevant_chunk_id": 9, "relevant_text": "Next, it partitions the graph into a hierarchy of communities of closely related entities, before using an LLM to generate community-level summaries.", "case_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization_001"}
{"query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"We consider the sentence with the highest log [MASK] as a positive example pi(line 3).\"", "gold": "likelihood", "relevant_doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "relevant_chunk_id": 29, "relevant_text": "We consider the sentence with the highest log likelihood as a positive example pi(line 3).", "case_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation_003"}
{"query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"vector embeddings presents a challenge for traditional GMMs, as dis- tance metrics may behave poorly when used to measure similarity in [MASK] spaces (Ag- garwal et al., 2001).\"", "gold": "high-dimensional", "relevant_doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "relevant_chunk_id": 29, "relevant_text": "vector embeddings presents a challenge for traditional GMMs, as dis- tance metrics may behave poorly when used to measure similarity in high-dimensional spaces (Ag- garwal et al., 2001).", "case_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval_010"}
{"query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"This flexibility is essen- tial because individual text segments often contain [MASK] relevant to various topics, thereby warranting their inclusion in multiple summaries.\"", "gold": "information", "relevant_doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "relevant_chunk_id": 27, "relevant_text": "This flexibility is essen- tial because individual text segments often contain information relevant to various topics, thereby warranting their inclusion in multiple summaries.", "case_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval_019"}
{"query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"CRAG is plug-and-play and [MASK] implemented into RAG (Lewis et al., 2020) and Self-RAG (Asai et al., 2024) for demonstrating its adaptability to RAG-based approa\"", "gold": "experimentally", "relevant_doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "relevant_chunk_id": 13, "relevant_text": "CRAG is plug-and-play and experimentally implemented into RAG (Lewis et al., 2020) and Self-RAG (Asai et al., 2024) for demonstrating its adaptability to RAG-based approa", "case_id": "2401.15884__corrective_retrieval_augmented_generation_005"}
{"query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"Experiments on four datasets covering short- and long-form generation tasks show that CRAG can [MASK] improve the performance of RAG-based approaches.1 1 Introduction Large language models (LLMs) have attracted increasing attention and exhibited impressive\"", "gold": "significantly", "relevant_doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "relevant_chunk_id": 3, "relevant_text": "Experiments on four datasets covering short- and long-form generation tasks show that CRAG can significantly improve the performance of RAG-based approaches.1 1 Introduction Large language models (LLMs) have attracted increasing attention and exhibited impressive", "case_id": "2401.15884__corrective_retrieval_augmented_generation_024"}
{"query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"We com- pare two RAG [MASK], one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token.\"", "gold": "formulations", "relevant_doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "relevant_chunk_id": 3, "relevant_text": "We com- pare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token.", "case_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks_024"}
{"query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"By exploiting this attention sparsity structure, we demonstrate a30 .85×the [MASK] acceleration (3 .75×improvement to previous work) without loss in perplexity.\"", "gold": "time-to-first-token", "relevant_doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "relevant_chunk_id": 3, "relevant_text": "By exploiting this attention sparsity structure, we demonstrate a30 .85×the time-to-first-token acceleration (3 .75×improvement to previous work) without loss in perplexity.", "case_id": "2509.01092__refrag_rethinking_rag_based_decoding_024"}
{"query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"As a result, LLM inference throughput degrades with larger contexts, limiting their [MASK] in scenarios demanding high throughput and low latency, such as web-scale discovery.\"", "gold": "applicability", "relevant_doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "relevant_chunk_id": 8, "relevant_text": "As a result, LLM inference throughput degrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low latency, such as web-scale discovery.", "case_id": "2509.01092__refrag_rethinking_rag_based_decoding_001"}
{"query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"[MASK] passages in parallel and filters out irrelevant ones through self-reflection, without relying on external models at inference.\"", "gold": "self-ragprocesses", "relevant_doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "relevant_chunk_id": 23, "relevant_text": "SELF-RAGprocesses passages in parallel and filters out irrelevant ones through self-reflection, without relying on external models at inference.", "case_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection_030"}
{"query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"[MASK] Generation (RAG)Ours: Self-reﬂective Retrieval-Augmented Genera\"", "gold": "retrieval-augmented", "relevant_doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "relevant_chunk_id": 9, "relevant_text": "Retrieval-Augmented Generation (RAG)Ours: Self-reﬂective Retrieval-Augmented Genera", "case_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection_027"}
{"query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"d as follows: the system gets a text query as input, and [MASK] a text output .\"", "gold": "generates", "relevant_doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "relevant_chunk_id": 16, "relevant_text": "d as follows: the system gets a text query as input, and generates a text output .", "case_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models_021"}
{"query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"The tree traversal method traverses the tree [MASK], pruning and selecting the most relevant nodes at each level.\"", "gold": "layer-by-layer", "relevant_doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "relevant_chunk_id": 25, "relevant_text": "The tree traversal method traverses the tree layer-by-layer, pruning and selecting the most relevant nodes at each level.", "case_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval_018"}
{"query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \", n }do 5: L ← ∅ 6: if Score (M,yi,[sj;xi]) + ϵ < Score (M,yi,[pi;xi])then 7: L ← L ∪ si 8: if|L|>0then 9: Ni←[MASK]5 sj∈L(⟨encθ(sj),encθ(\"", "gold": "argtop", "relevant_doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "relevant_chunk_id": 25, "relevant_text": ", n }do 5: L ← ∅ 6: if Score (M,yi,[sj;xi]) + ϵ < Score (M,yi,[pi;xi])then 7: L ← L ∪ si 8: if|L|>0then 9: Ni←argTop5 sj∈L(⟨encθ(sj),encθ(", "case_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation_018"}
{"query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"GMMs assume that data points are generated from a mixture of several Gaussian [MASK].\"", "gold": "distributions", "relevant_doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "relevant_chunk_id": 27, "relevant_text": "GMMs assume that data points are generated from a mixture of several Gaussian distributions.", "case_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval_006"}
{"query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"In this work, we de- sign criteria for evaluating [MASK] answers to global sensemaking questions and evaluate our results using the comparative approach.\"", "gold": "rag-generated", "relevant_doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "relevant_chunk_id": 30, "relevant_text": "In this work, we de- sign criteria for evaluating RAG-generated answers to global sensemaking questions and evaluate our results using the comparative approach.", "case_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization_010"}
{"query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"For [MASK] generation, we experiment with MS-MARCO [ 1] and Jeopardy question generation, and we ﬁnd that our models generate responses that are more factual, speciﬁc, and diverse than a BART baseline.\"", "gold": "knowledge-intensive", "relevant_doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "relevant_chunk_id": 14, "relevant_text": "For knowledge-intensive generation, we experiment with MS-MARCO [ 1] and Jeopardy question generation, and we ﬁnd that our models generate responses that are more factual, speciﬁc, and diverse than a BART baseline.", "case_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks_029"}
{"query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"Specifically, SELF-RAG outperforms ChatGPT and [MASK] Llama2-chat on Open-domain QA, reasoning and fact verification tasks, and it shows significant gains in improving factuality and citation accuracy for long-form generations relative to these models.1 1 I NTRODU\"", "gold": "retrieval-augmented", "relevant_doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "relevant_chunk_id": 3, "relevant_text": "Specifically, SELF-RAG outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA, reasoning and fact verification tasks, and it shows significant gains in improving factuality and citation accuracy for long-form generations relative to these models.1 1 I NTRODU", "case_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection_024"}
{"query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"xact Match NaturalQuestions 5 10 25 50 100 Number of passages54565860626466 TriviaQA 5 10 25 50 100 Number of passages343638404244464850 SQuADFigure 3: Performance of [MASK] (base) on valid sets as a function of the number of retrieved passages.\"", "gold": "fusion-in-decoder", "relevant_doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "relevant_chunk_id": 25, "relevant_text": "xact Match NaturalQuestions 5 10 25 50 100 Number of passages54565860626466 TriviaQA 5 10 25 50 100 Number of passages343638404244464850 SQuADFigure 3: Performance of Fusion-in-Decoder (base) on valid sets as a function of the number of retrieved passages.", "case_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering_009"}
{"query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"Our main contribution is the idea of using text [MASK] to allow retrieval augmentation of context at different scales, and to show its effectiveness in experiments on collections of long doc- uments.\"", "gold": "summarization", "relevant_doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "relevant_chunk_id": 10, "relevant_text": "Our main contribution is the idea of using text summarization to allow retrieval augmentation of context at different scales, and to show its effectiveness in experiments on collections of long doc- uments.", "case_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval_020"}
{"query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"In addition, in some [MASK] generation tasks, external knowl- edge is needed more than once, and when to retrieve should be concerned.\"", "gold": "long-text", "relevant_doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "relevant_chunk_id": 22, "relevant_text": "In addition, in some long-text generation tasks, external knowl- edge is needed more than once, and when to retrieve should be concerned.", "case_id": "2401.15884__corrective_retrieval_augmented_generation_025"}
{"query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"(2018) introduced a supervised [MASK] to rerank paragraphs based on BiLSTM, while Wang et al.\"", "gold": "learningmethod", "relevant_doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "relevant_chunk_id": 12, "relevant_text": "(2018) introduced a supervised learningmethod to rerank paragraphs based on BiLSTM, while Wang et al.", "case_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering_017"}
{"query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"The objective is to align any encoder–decoder combination so that the generations produced [MASK] contextclosely resemble those generated by the original decoder with access to the full context.\"", "gold": "withcompressed", "relevant_doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "relevant_chunk_id": 28, "relevant_text": "The objective is to align any encoder–decoder combination so that the generations produced withcompressed contextclosely resemble those generated by the original decoder with access to the full context.", "case_id": "2509.01092__refrag_rethinking_rag_based_decoding_010"}
{"query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"Our analysis demonstrates the effectiveness of training and inference with reflection tokens for overall performance improvements as well as test-time model [MASK] (e.g., balancing the trade-off between citation previsions and completeness).\"", "gold": "customizations", "relevant_doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "relevant_chunk_id": 18, "relevant_text": "Our analysis demonstrates the effectiveness of training and inference with reflection tokens for overall performance improvements as well as test-time model customizations (e.g., balancing the trade-off between citation previsions and completeness).", "case_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection_002"}
{"query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"However, most existing methods target generic LLM tasks with long context and are largely [MASK] to our work.\"", "gold": "orthogonal", "relevant_doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "relevant_chunk_id": 9, "relevant_text": "However, most existing methods target generic LLM tasks with long context and are largely orthogonal to our work.", "case_id": "2509.01092__refrag_rethinking_rag_based_decoding_027"}
{"query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"This is exactly the focus of this paper to improve the [MASK] of generation.\"", "gold": "robustness", "relevant_doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "relevant_chunk_id": 26, "relevant_text": "This is exactly the focus of this paper to improve the robustness of generation.", "case_id": "2401.15884__corrective_retrieval_augmented_generation_019"}
{"query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"2.3 Adaptive benchmarking for RAG Evaluation Many benchmark datasets for open-domain question answering exist, including HotPotQA (Yang et al., 2018), [MASK] (Tang and Yang, 2024), and MT-Bench (Zheng et al., 2024).\"", "gold": "multihop-rag", "relevant_doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "relevant_chunk_id": 22, "relevant_text": "2.3 Adaptive benchmarking for RAG Evaluation Many benchmark datasets for open-domain question answering exist, including HotPotQA (Yang et al., 2018), MultiHop-RAG (Tang and Yang, 2024), and MT-Bench (Zheng et al., 2024).", "case_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization_014"}
{"query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"This model consists of a [MASK] foundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu et al., 2019)).\"", "gold": "decoder-only", "relevant_doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "relevant_chunk_id": 18, "relevant_text": "This model consists of a decoder-only foundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu et al., 2019)).", "case_id": "2509.01092__refrag_rethinking_rag_based_decoding_004"}
{"query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"for long-form generations relative to these models.1 1 I NTRODUCTION [MASK] LLMs continue to struggle with factual errors (Mallen et al., 2023; Min et al., 2023) despite their increased model and data scale (Ouyang et al., 2022).\"", "gold": "state-of-the-art", "relevant_doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "relevant_chunk_id": 4, "relevant_text": "for long-form generations relative to these models.1 1 I NTRODUCTION State-of-the-art LLMs continue to struggle with factual errors (Mallen et al., 2023; Min et al., 2023) despite their increased model and data scale (Ouyang et al., 2022).", "case_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection_026"}
{"query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"cted communities, with summaries at higher levels of the hierarchy recursively [MASK] lower-level summaries.\"", "gold": "incorporating", "relevant_doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "relevant_chunk_id": 10, "relevant_text": "cted communities, with summaries at higher levels of the hierarchy recursively incorporating lower-level summaries.", "case_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization_007"}
{"query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"odels in the output should be faithful to the original input, yet the main goal is [MASK].\"", "gold": "different", "relevant_doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "relevant_chunk_id": 23, "relevant_text": "odels in the output should be faithful to the original input, yet the main goal is different.", "case_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation_025"}
{"query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"We argue that specialized techniques exploiting the unique structure and sparsity inherent in RAG contexts can [MASK] reduce memory and computational overhead.\"", "gold": "substantially", "relevant_doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "relevant_chunk_id": 10, "relevant_text": "We argue that specialized techniques exploiting the unique structure and sparsity inherent in RAG contexts can substantially reduce memory and computational overhead.", "case_id": "2509.01092__refrag_rethinking_rag_based_decoding_008"}
{"query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"[MASK] For RAG-Sequence, the likelihood p(y|x)does not break into a conventional per- token likelihood, hence we cannot solve it with a single beam search.\"", "gold": "rag-sequence", "relevant_doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "relevant_chunk_id": 26, "relevant_text": "RAG-Sequence For RAG-Sequence, the likelihood p(y|x)does not break into a conventional per- token likelihood, hence we cannot solve it with a single beam search.", "case_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks_006"}
{"query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"For language modelling, both trained compressors achieve a compression ratio of 25% with minimal [MASK] drop.\"", "gold": "performance", "relevant_doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "relevant_chunk_id": 15, "relevant_text": "For language modelling, both trained compressors achieve a compression ratio of 25% with minimal performance drop.", "case_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation_005"}
{"query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"Despite this, the methods above usually ignore a [MASK], what if the retrieval goes wrong?\"", "gold": "question", "relevant_doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "relevant_chunk_id": 20, "relevant_text": "Despite this, the methods above usually ignore a question, what if the retrieval goes wrong?", "case_id": "2401.15884__corrective_retrieval_augmented_generation_004"}
{"query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"[MASK] nodes thus storing varying levels of detail, keeping granular details.\"", "gold": "ntermediate", "relevant_doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "relevant_chunk_id": 21, "relevant_text": "ntermediate nodes thus storing varying levels of detail, keeping granular details.", "case_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval_028"}
{"query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"Like T5 [ 51] or BART, RAG can be ﬁne-tuned on any seq2seq task, whereby both the generator and [MASK] are jointly learned.\"", "gold": "retriever", "relevant_doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "relevant_chunk_id": 11, "relevant_text": "Like T5 [ 51] or BART, RAG can be ﬁne-tuned on any seq2seq task, whereby both the generator and retriever are jointly learned.", "case_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks_020"}
{"query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"Atlasretrieves relevant documents based on the current context by using a [MASK] dense retriever using a dual-encoder architecture, based on the Contriever (Izacard et al., 2022).\"", "gold": "general-purpose", "relevant_doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "relevant_chunk_id": 9, "relevant_text": "Atlasretrieves relevant documents based on the current context by using a general-purpose dense retriever using a dual-encoder architecture, based on the Contriever (Izacard et al., 2022).", "case_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models_007"}
{"query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"ntributions: •A thorough study on how to design and train [MASK] language models, with a focus on downstream few-shot learning and sample eﬃciency.\"", "gold": "retrieval-augmented", "relevant_doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "relevant_chunk_id": 12, "relevant_text": "ntributions: •A thorough study on how to design and train retrieval-augmented language models, with a focus on downstream few-shot learning and sample eﬃciency.", "case_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models_020"}
{"query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"Clark and Gardner (2018) proposed to use a global [MASK] over all the span corresponding to the answer, which was later applied to BERT based models (Wang et al., 2019).\"", "gold": "normalization", "relevant_doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "relevant_chunk_id": 11, "relevant_text": "Clark and Gardner (2018) proposed to use a global normalization over all the span corresponding to the answer, which was later applied to BERT based models (Wang et al., 2019).", "case_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering_021"}
{"query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"Furthermore, current methods mostly treat complete documents as reference knowledge both during retrieval and [MASK].\"", "gold": "utilization", "relevant_doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "relevant_chunk_id": 9, "relevant_text": "Furthermore, current methods mostly treat complete documents as reference knowledge both during retrieval and utilization.", "case_id": "2401.15884__corrective_retrieval_augmented_generation_007"}
{"query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"Unlike prior methods (Yen et al., 2024),[MASK] compression of token chunks at arbitrary positions (see figure 1) while preserving the autoregressive nature of the decod\"", "gold": "refragsupports", "relevant_doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "relevant_chunk_id": 14, "relevant_text": "Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at arbitrary positions (see figure 1) while preserving the autoregressive nature of the decod", "case_id": "2509.01092__refrag_rethinking_rag_based_decoding_021"}
{"query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"sations (Roller et al., 2021; Zhang et al., 2020), [MASK] historical dialogue into the context enables LLMs to respond more effectively to user queries.\"", "gold": "incorporating", "relevant_doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "relevant_chunk_id": 6, "relevant_text": "sations (Roller et al., 2021; Zhang et al., 2020), incorporating historical dialogue into the context enables LLMs to respond more effectively to user queries.", "case_id": "2509.01092__refrag_rethinking_rag_based_decoding_015"}
{"query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"(3) Faithful :sshould be a faithful and [MASK] summary of the input document set (i.e., smust be entailed by the input document set ( [d1, d2, ...dN])).\"", "gold": "interpretable", "relevant_doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "relevant_chunk_id": 18, "relevant_text": "(3) Faithful :sshould be a faithful and interpretable summary of the input document set (i.e., smust be entailed by the input document set ( [d1, d2, ...dN])).", "case_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation_002"}
{"query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"Our algorithm varies [MASK] to create a hierar- chical clustering structure: it first identifies global clusters and then performs local clustering within these global clusters.\"", "gold": "nneighbors", "relevant_doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "relevant_chunk_id": 30, "relevant_text": "Our algorithm varies nneighbors to create a hierar- chical clustering structure: it first identifies global clusters and then performs local clustering within these global clusters.", "case_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval_009"}
{"query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"In the canonical RAG setup, the system has access to a large external corpus of text records and retrieves a subset of records that are [MASK] relevant to the query and collectively small enough to fit into the context window of the LLM.\"", "gold": "individually", "relevant_doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "relevant_chunk_id": 5, "relevant_text": "In the canonical RAG setup, the system has access to a large external corpus of text records and retrieves a subset of records that are individually relevant to the query and collectively small enough to fit into the context window of the LLM.", "case_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization_022"}
{"query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"R EPLUG We introduce REPLUG (Retrieve and Plug ), a new retrieval- augmented LM paradigm where the language model is treated as black box and the retrieval component is added as a [MASK] tuneable module.\"", "gold": "potentially", "relevant_doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "relevant_chunk_id": 21, "relevant_text": "R EPLUG We introduce REPLUG (Retrieve and Plug ), a new retrieval- augmented LM paradigm where the language model is treated as black box and the retrieval component is added as a potentially tuneable module.", "case_id": "2301.12652__replug_retrieval_augmented_black_box_language_models_030"}
{"query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"Our RAG models achieve [MASK] results on open Natural Questions [ 29], WebQuestions [ 3] and CuratedTrec [ 2] and strongly outperform recent approaches that use specialised pre-training objectives on TriviaQA [ 24].\"", "gold": "state-of-the-art", "relevant_doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "relevant_chunk_id": 13, "relevant_text": "Our RAG models achieve state-of-the-art results on open Natural Questions [ 29], WebQuestions [ 3] and CuratedTrec [ 2] and strongly outperform recent approaches that use specialised pre-training objectives on TriviaQA [ 24].", "case_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks_005"}
{"query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"2 Published as a conference paper at ICLR 2024 Despite a diversity in methods, the [MASK] compon\"", "gold": "retrieving", "relevant_doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "relevant_chunk_id": 17, "relevant_text": "2 Published as a conference paper at ICLR 2024 Despite a diversity in methods, the retrieving compon", "case_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval_017"}
{"query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"CRAG is [MASK] and can be seamlessly coupled with various RAG-based approaches.\"", "gold": "plug-and-play", "relevant_doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "relevant_chunk_id": 3, "relevant_text": "CRAG is plug-and-play and can be seamlessly coupled with various RAG-based approaches.", "case_id": "2401.15884__corrective_retrieval_augmented_generation_013"}
{"query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the [MASK] memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever.\"", "gold": "non-parametric", "relevant_doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "relevant_chunk_id": 2, "relevant_text": "We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever.", "case_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks_013"}
{"query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"In ad- dition, versions of the GraphRAG approach are also available as extensions to multiple open- source libraries, including LangChain (LangChain, 2024), LlamaIndex (LlamaIndex, 2024), Nebu- laGraph ([MASK], 2024), and Neo4J (Neo4J, 2024).\"", "gold": "nebulagraph", "relevant_doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "relevant_chunk_id": 13, "relevant_text": "In ad- dition, versions of the GraphRAG approach are also available as extensions to multiple open- source libraries, including LangChain (LangChain, 2024), LlamaIndex (LlamaIndex, 2024), Nebu- laGraph (NebulaGraph, 2024), and Neo4J (Neo4J, 2024).", "case_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization_016"}
{"query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"3 M ETHODS Overview of RAPTOR Building on the idea that long texts often present subtopics and hierarchi- cal [MASK] (Cao & Wang, 2022; Dong et al., 2023b), RAPTOR addresses the issue of semantic depth and connection in\"", "gold": "structures", "relevant_doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "relevant_chunk_id": 21, "relevant_text": "3 M ETHODS Overview of RAPTOR Building on the idea that long texts often present subtopics and hierarchi- cal structures (Cao & Wang, 2022; Dong et al., 2023b), RAPTOR addresses the issue of semantic depth and connection in", "case_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval_014"}
{"query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"Here, this loss is only used to optimize the [MASK] of the retriever, and not the language model.\"", "gold": "parameters", "relevant_doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "relevant_chunk_id": 29, "relevant_text": "Here, this loss is only used to optimize the parameters of the retriever, and not the language model.", "case_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models_010"}
{"query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"To [MASK] others to reproduce our results, we will publish all source code l\"", "gold": "facilitate", "relevant_doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "relevant_chunk_id": 14, "relevant_text": "To facilitate others to reproduce our results, we will publish all source code l", "case_id": "2401.15884__corrective_retrieval_augmented_generation_021"}
{"query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"In addition, our optimization framework for large context [MASK] extend the context size of LLMs by16 ×.\"", "gold": "enablesrefragto", "relevant_doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "relevant_chunk_id": 3, "relevant_text": "In addition, our optimization framework for large context enablesREFRAGto extend the context size of LLMs by16 ×.", "case_id": "2509.01092__refrag_rethinking_rag_based_decoding_026"}
{"query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"As this approach is extractive, we assume the faithfulness criteria is mostly satisfied.3 Abstractive Compressor We train an [MASK] model encdec θto serve as\"", "gold": "encoder-decoder", "relevant_doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "relevant_chunk_id": 20, "relevant_text": "As this approach is extractive, we assume the faithfulness criteria is mostly satisfied.3 Abstractive Compressor We train an encoder-decoder model encdec θto serve as", "case_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation_028"}
{"query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"We use a [MASK] bi-encoder from DPR to initialize our retriever and to build the document index.\"", "gold": "pre-trained", "relevant_doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "relevant_chunk_id": 22, "relevant_text": "We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index.", "case_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks_025"}
{"query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"They are able to learn new tasks with very few examples or even from [MASK] alone.\"", "gold": "instructions", "relevant_doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "relevant_chunk_id": 3, "relevant_text": "They are able to learn new tasks with very few examples or even from instructions alone.", "case_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models_026"}
{"query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"This al- lows to scale to large numbers of [MASK], and to beneﬁt from this large amount of evidence.\"", "gold": "documents", "relevant_doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "relevant_chunk_id": 16, "relevant_text": "This al- lows to scale to large numbers of documents, and to beneﬁt from this large amount of evidence.", "case_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering_014"}
{"query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"For this [MASK] ability to emerge, the key ingredients are scaling both the parameter count of the model, and the size of the training data.\"", "gold": "generalisation", "relevant_doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "relevant_chunk_id": 3, "relevant_text": "For this generalisation ability to emerge, the key ingredients are scaling both the parameter count of the model, and the size of the training data.", "case_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models_022"}
{"query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"f the input (i.e., s≫q) and hence the overall input to the decoder will be [MASK] by a factor of ≃k.\"", "gold": "reduced", "relevant_doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "relevant_chunk_id": 22, "relevant_text": "f the input (i.e., s≫q) and hence the overall input to the decoder will be reduced by a factor of ≃k.", "case_id": "2509.01092__refrag_rethinking_rag_based_decoding_025"}
{"query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"Here, we propose an alternative which gives slightly stronger results, which relies on the following [MASK].\"", "gold": "observation", "relevant_doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "relevant_chunk_id": 27, "relevant_text": "Here, we propose an alternative which gives slightly stronger results, which relies on the following observation.", "case_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models_019"}
{"query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"Recent work on LLM refinement (Dhuliawala et al., 2023; Madaan et al., 2023; Paul et al., 2023) prompts a model to generate task output, natural language feedback and refined task output [MASK], but at the cost of inference efficiency.\"", "gold": "iteratively", "relevant_doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "relevant_chunk_id": 27, "relevant_text": "Recent work on LLM refinement (Dhuliawala et al., 2023; Madaan et al., 2023; Paul et al., 2023) prompts a model to generate task output, natural language feedback and refined task output iteratively, but at the cost of inference efficiency.", "case_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection_019"}
{"query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"This means that all the tasks are framed as follows: the system gets a text query as input, and [MASK]\"", "gold": "generat", "relevant_doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "relevant_chunk_id": 15, "relevant_text": "This means that all the tasks are framed as follows: the system gets a text query as input, and generat", "case_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models_005"}
{"query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"that answers do not [MASK] to spans in support documents, thus requiring ab- stractive models.\"", "gold": "correspond", "relevant_doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "relevant_chunk_id": 15, "relevant_text": "that answers do not correspond to spans in support documents, thus requiring ab- stractive models.", "case_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering_004"}
{"query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"Retrieval Methods [MASK] language models (RALMs) have seen improvements in various components: the retriever, the reader, and end-to-end system trai\"", "gold": "retrieval-augmented", "relevant_doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "relevant_chunk_id": 13, "relevant_text": "Retrieval Methods Retrieval-augmented language models (RALMs) have seen improvements in various components: the retriever, the reader, and end-to-end system trai", "case_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval_021"}
{"query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"We achieve a compression rate of as low as 6% with minimal loss in performance for both tasks, significantly outperforming the off-the-shelf [MASK] models.\"", "gold": "summarization", "relevant_doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "relevant_chunk_id": 3, "relevant_text": "We achieve a compression rate of as low as 6% with minimal loss in performance for both tasks, significantly outperforming the off-the-shelf summarization models.", "case_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation_022"}
{"query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"If the retrieved documents do not contain relevant information or retrieval [MASK] is not necessary, scan be an empty sequence.\"", "gold": "augmentation", "relevant_doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "relevant_chunk_id": 18, "relevant_text": "If the retrieved documents do not contain relevant information or retrieval augmentation is not necessary, scan be an empty sequence.", "case_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation_017"}
{"query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"GraphRAG contrasts with these approaches by generating a graph index from the source data, then applying graph-based community detection to create a thematic [MASK] of the data.\"", "gold": "partitioning", "relevant_doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "relevant_chunk_id": 18, "relevant_text": "GraphRAG contrasts with these approaches by generating a graph index from the source data, then applying graph-based community detection to create a thematic partitioning of the data.", "case_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization_011"}
{"query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"igure 1) while preserving the [MASK] nature of the decoder, thereby supporting multi-turn and agentic applications.\"", "gold": "autoregressive", "relevant_doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "relevant_chunk_id": 15, "relevant_text": "igure 1) while preserving the autoregressive nature of the decoder, thereby supporting multi-turn and agentic applications.", "case_id": "2509.01092__refrag_rethinking_rag_based_decoding_029"}
{"query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"Reflection tokens, inspired by reward models used in [MASK] learning (Ziegler et al., 2019; Ouyang et al., 2022), are inserted offline into the original corpus by a trained critic model.\"", "gold": "reinforcement", "relevant_doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "relevant_chunk_id": 14, "relevant_text": "Reflection tokens, inspired by reward models used in reinforcement learning (Ziegler et al., 2019; Ouyang et al., 2022), are inserted offline into the original corpus by a trained critic model.", "case_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection_005"}
{"query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"3)Unusually Structured and Sparse Attention.Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero [MASK] between chunks (see figure 7).\"", "gold": "cross-attention", "relevant_doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "relevant_chunk_id": 12, "relevant_text": "3)Unusually Structured and Sparse Attention.Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).", "case_id": "2509.01092__refrag_rethinking_rag_based_decoding_016"}
{"query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"rticular, we show that the [MASK] of our method signiﬁcantly improves when the number of retrieved passages increases.\"", "gold": "performance", "relevant_doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "relevant_chunk_id": 9, "relevant_text": "rticular, we show that the performance of our method signiﬁcantly improves when the number of retrieved passages increases.", "case_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering_020"}
{"query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG [MASK].\"", "gold": "applications", "relevant_doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "relevant_chunk_id": 3, "relevant_text": "To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications.", "case_id": "2509.01092__refrag_rethinking_rag_based_decoding_013"}
{"query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"Nevertheless, LLMs inevitably manifest [MASK] (Ji et al., 2023) due to their struggle with factual errors (Mallen et al., 2023; Min et al., 2023) and inability to secure the accuracy of generated texts solely by *Equal contribution.\"", "gold": "hallucinations", "relevant_doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "relevant_chunk_id": 4, "relevant_text": "Nevertheless, LLMs inevitably manifest hallucinations (Ji et al., 2023) due to their struggle with factual errors (Mallen et al., 2023; Min et al., 2023) and inability to secure the accuracy of generated texts solely by *Equal contribution.", "case_id": "2401.15884__corrective_retrieval_augmented_generation_022"}
{"query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform- ers Library [ 66] and can be found at https://github.com/huggingface/[MASK]/blob/master/ examples/rag/ .\"", "gold": "transformers", "relevant_doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "relevant_chunk_id": 16, "relevant_text": "Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform- ers Library [ 66] and can be found at https://github.com/huggingface/transformers/blob/master/ examples/rag/ .", "case_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks_017"}
{"query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"(2021), which is inspired by the [MASK] algorithm, treating re\"", "gold": "expectation-maximization", "relevant_doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "relevant_chunk_id": 29, "relevant_text": "(2021), which is inspired by the expectation-maximization algorithm, treating re", "case_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models_009"}
{"query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"is exciting, such models do have down- sides: They cannot easily expand or revise their memory, can’t [MASK] provide insight into their predictions, and may produce “hallucinations” [ 38].\"", "gold": "straightforwardly", "relevant_doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "relevant_chunk_id": 5, "relevant_text": "is exciting, such models do have down- sides: They cannot easily expand or revise their memory, can’t straightforwardly provide insight into their predictions, and may produce “hallucinations” [ 38].", "case_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks_015"}
{"query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"In other words, we would like the [MASK] to find documents that result in lower perplex- ity scores.\"", "gold": "retriever", "relevant_doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "relevant_chunk_id": 29, "relevant_text": "In other words, we would like the retriever to find documents that result in lower perplex- ity scores.", "case_id": "2301.12652__replug_retrieval_augmented_black_box_language_models_003"}
{"query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"3.1.1 Source [MASK] →Text Chunks To start, the documents in the corpus are split into text chunks.\"", "gold": "documents", "relevant_doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "relevant_chunk_id": 31, "relevant_text": "3.1.1 Source Documents →Text Chunks To start, the documents in the corpus are split into text chunks.", "case_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization_009"}
{"query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"First, it increases computational costs as LMs now encode [MASK] more tokens.\"", "gold": "substantially", "relevant_doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "relevant_chunk_id": 5, "relevant_text": "First, it increases computational costs as LMs now encode substantially more tokens.", "case_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation_015"}
{"query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token [MASK] ef\"", "gold": "allocation", "relevant_doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "relevant_chunk_id": 13, "relevant_text": "This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation ef", "case_id": "2509.01092__refrag_rethinking_rag_based_decoding_005"}
{"query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"lements (nodes, edges, [MASK]) that the LLM can summarize in parallel at both indexing time and query time.\"", "gold": "covariates", "relevant_doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "relevant_chunk_id": 29, "relevant_text": "lements (nodes, edges, covariates) that the LLM can summarize in parallel at both indexing time and query time.", "case_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization_006"}
{"query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"In BM25, passages are [MASK] as bag of words, and the ranking function is based on term and inverse doc- ument frequencies.\"", "gold": "represented", "relevant_doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "relevant_chunk_id": 19, "relevant_text": "In BM25, passages are represented as bag of words, and the ranking function is based on term and inverse doc- ument frequencies.", "case_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering_025"}
{"query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"at increasing prompt length for contextual learning leads to higher latency and greater memory [MASK] during inference (Yen et al., 2024).\"", "gold": "consumption", "relevant_doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "relevant_chunk_id": 7, "relevant_text": "at increasing prompt length for contextual learning leads to higher latency and greater memory consumption during inference (Yen et al., 2024).", "case_id": "2509.01092__refrag_rethinking_rag_based_decoding_023"}
{"query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"t prior probability pη(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be [MASK] solved in sub-linear time [ 23].\"", "gold": "approximately", "relevant_doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "relevant_chunk_id": 22, "relevant_text": "t prior probability pη(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [ 23].", "case_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks_030"}
{"query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"Then, these documents are fed to the language model, along with the query, which in turns [MASK] the output.\"", "gold": "generates", "relevant_doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "relevant_chunk_id": 18, "relevant_text": "Then, these documents are fed to the language model, along with the query, which in turns generates the output.", "case_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models_002"}
{"query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"n improve GPT- 3 performance on open-domain question answering, we approach the problem in a more general setting, including language modeling and [MASK] tasks.\"", "gold": "understanding", "relevant_doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "relevant_chunk_id": 20, "relevant_text": "n improve GPT- 3 performance on open-domain question answering, we approach the problem in a more general setting, including language modeling and understanding tasks.", "case_id": "2301.12652__replug_retrieval_augmented_black_box_language_models_014"}
{"query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"Controlled experiments show that retrieval with recursive summaries offers [MASK] improv\"", "gold": "significant", "relevant_doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "relevant_chunk_id": 1, "relevant_text": "Controlled experiments show that retrieval with recursive summaries offers significant improv", "case_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval_013"}
{"query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"This yields a set of [MASK] Y, some of which may not have appeared in the beams of all documents.\"", "gold": "hypotheses", "relevant_doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "relevant_chunk_id": 27, "relevant_text": "This yields a set of hypotheses Y, some of which may not have appeared in the beams of all documents.", "case_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks_003"}
{"query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"Earlier studies adopt either sparse or dense retrievers at the front end of a pre- trained language model that [MASK] in response generation.\"", "gold": "specializes", "relevant_doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "relevant_chunk_id": 19, "relevant_text": "Earlier studies adopt either sparse or dense retrievers at the front end of a pre- trained language model that specializes in response generation.", "case_id": "2401.15884__corrective_retrieval_augmented_generation_012"}
{"query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"RAG is ideal when the total number of [MASK] in a data source is too large to include in a single prompt to the LLM, i.e.\"", "gold": "records", "relevant_doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "relevant_chunk_id": 14, "relevant_text": "RAG is ideal when the total number of records in a data source is too large to include in a single prompt to the LLM, i.e.", "case_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization_005"}
{"query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"We ﬁne-tune and evaluate our models on a wide range of knowledge- intensive NLP tasks and set the state of the art on three open domain QA tasks, outperforming parametric seq2seq models and task-speciﬁc [MASK] architectures.\"", "gold": "retrieve-and-extract", "relevant_doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "relevant_chunk_id": 3, "relevant_text": "We ﬁne-tune and evaluate our models on a wide range of knowledge- intensive NLP tasks and set the state of the art on three open domain QA tasks, outperforming parametric seq2seq models and task-speciﬁc retrieve-and-extract architectures.", "case_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks_026"}
{"query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"[MASK] Generation RAG (Lewis et al., 2020; Guu et al., 2020) is regarded as a useful method to address the issues above, which enhances the input questions of generative LMs with retrieved documents.\"", "gold": "retrieval-augmented", "relevant_doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "relevant_chunk_id": 18, "relevant_text": "Retrieval-Augmented Generation RAG (Lewis et al., 2020; Guu et al., 2020) is regarded as a useful method to address the issues above, which enhances the input questions of generative LMs with retrieved documents.", "case_id": "2401.15884__corrective_retrieval_augmented_generation_002"}
{"query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"We endow pre-trained, parametric-memory generation models with a non-parametric memory through a general-purpose ﬁne-tuning approach which we refer to as [MASK] generation (RAG).\"", "gold": "retrieval-augmented", "relevant_doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "relevant_chunk_id": 9, "relevant_text": "We endow pre-trained, parametric-memory generation models with a non-parametric memory through a general-purpose ﬁne-tuning approach which we refer to as retrieval-augmented generation (RAG).", "case_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks_027"}
{"query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"It usually provides an extra knowledge source from a specific corpus, i.e., Wikipedia, which greatly improves the per- formance of LMs in a variety of tasks, especially in the [MASK] ones.\"", "gold": "knowledge-intensive", "relevant_doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "relevant_chunk_id": 19, "relevant_text": "It usually provides an extra knowledge source from a specific corpus, i.e., Wikipedia, which greatly improves the per- formance of LMs in a variety of tasks, especially in the knowledge-intensive ones.", "case_id": "2401.15884__corrective_retrieval_augmented_generation_011"}
{"query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"(2023) uses [MASK] and snippets of passages, which improves correctness on most datasets but can sometimes be a lossy means of compression.\"", "gold": "summarizations", "relevant_doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "relevant_chunk_id": 19, "relevant_text": "(2023) uses summarizations and snippets of passages, which improves correctness on most datasets but can sometimes be a lossy means of compression.", "case_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval_012"}
{"query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"We refer to this decoding [MASK] as “Thorough Decoding.” For longer output seq\"", "gold": "procedure", "relevant_doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "relevant_chunk_id": 27, "relevant_text": "We refer to this decoding procedure as “Thorough Decoding.” For longer output seq", "case_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks_010"}
{"query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"If so, it outputs a retrieval token that calls a [MASK] model on demand (Step 1).\"", "gold": "retriever", "relevant_doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "relevant_chunk_id": 7, "relevant_text": "If so, it outputs a retrieval token that calls a retriever model on demand (Step 1).", "case_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection_023"}
{"query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"However, in order to produce a fair evaluation, our method avoids generating the questions directly from the corpus itself (as an alternative [MASK], one can use a subset of the corpus held out from subsequen\"", "gold": "implementation", "relevant_doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "relevant_chunk_id": 23, "relevant_text": "However, in order to produce a fair evaluation, our method avoids generating the questions directly from the corpus itself (as an alternative implementation, one can use a subset of the corpus held out from subsequen", "case_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization_030"}
{"query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"FRAG achieves better performance than LLaMA without incurring higher latency in the downstream [MASK].\"", "gold": "applications", "relevant_doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "relevant_chunk_id": 17, "relevant_text": "FRAG achieves better performance than LLaMA without incurring higher latency in the downstream applications.", "case_id": "2509.01092__refrag_rethinking_rag_based_decoding_002"}
{"query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"xamples, [MASK] a 540B parameters model by 3% despite having 50x fewer parameters.\"", "gold": "outperforming", "relevant_doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "relevant_chunk_id": 3, "relevant_text": "xamples, outperforming a 540B parameters model by 3% despite having 50x fewer parameters.", "case_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models_013"}
{"query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"generated texts such as “fluency” (Wang et al., 2023a) Some of these criteria are generic to vector RAG systems and not relevant to global sensemaking, such as “context relevance”, “[MASK]”, and “answer relevance” (RAGAS, Es et al\"", "gold": "faithfulness", "relevant_doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "relevant_chunk_id": 29, "relevant_text": "generated texts such as “fluency” (Wang et al., 2023a) Some of these criteria are generic to vector RAG systems and not relevant to global sensemaking, such as “context relevance”, “faithfulness”, and “answer relevance” (RAGAS, Es et al", "case_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization_003"}
{"query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"We use this approach to compare GraphRAG to vector RAG on two [MASK] real-world text datasets.\"", "gold": "representative", "relevant_doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "relevant_chunk_id": 12, "relevant_text": "We use this approach to compare GraphRAG to vector RAG on two representative real-world text datasets.", "case_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization_020"}
{"query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"These datasets were gen- erated in a way that answers do not [MASK] to spans in support documents, t\"", "gold": "correspond", "relevant_doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "relevant_chunk_id": 14, "relevant_text": "These datasets were gen- erated in a way that answers do not correspond to spans in support documents, t", "case_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering_012"}
{"query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"While Toolformer (Schick et al., 2023) is [MASK] for calling APIs such as Wikipedia.\"", "gold": "pre-trained", "relevant_doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "relevant_chunk_id": 22, "relevant_text": "While Toolformer (Schick et al., 2023) is pre-trained for calling APIs such as Wikipedia.", "case_id": "2401.15884__corrective_retrieval_augmented_generation_030"}
{"query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"Type Input Output Definitions Retrieve x/x, y {yes, no, continue } Decides when to retrieve with R ISREL x, d {relevant , irrelevant } dprovides useful [MASK] to solve x.\"", "gold": "information", "relevant_doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "relevant_chunk_id": 30, "relevant_text": "Type Input Output Definitions Retrieve x/x, y {yes, no, continue } Decides when to retrieve with R ISREL x, d {relevant , irrelevant } dprovides useful information to solve x.", "case_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection_009"}
{"query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"der allows to scale to large number of contexts, as it only [MASK] self attention over one context at a time.\"", "gold": "performs", "relevant_doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "relevant_chunk_id": 23, "relevant_text": "der allows to scale to large number of contexts, as it only performs self attention over one context at a time.", "case_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering_006"}
{"query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"Average pooling is applied over the outputs of the last layer to obtain one vector [MASK] per query or document.\"", "gold": "representation", "relevant_doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "relevant_chunk_id": 19, "relevant_text": "Average pooling is applied over the outputs of the last layer to obtain one vector representation per query or document.", "case_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models_004"}
{"query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"Step 3: Critique outputs and select best [MASK] in a 16th-century novel Las Sergas de Esplandián.\"", "gold": "segmentorigins", "relevant_doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "relevant_chunk_id": 11, "relevant_text": "Step 3: Critique outputs and select best segmentorigins in a 16th-century novel Las Sergas de Esplandián.", "case_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection_020"}
{"query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"One simple way to [MASK] the retrieved documents as part of the input to the LM is to prepend xwith all kdocuments.\"", "gold": "incorporate", "relevant_doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "relevant_chunk_id": 25, "relevant_text": "One simple way to incorporate the retrieved documents as part of the input to the LM is to prepend xwith all kdocuments.", "case_id": "2301.12652__replug_retrieval_augmented_black_box_language_models_018"}
{"query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"During CPT, we input the first stokens x1:sinto the encoder and use its output to assist the decoder in [MASK] the next otokens xs+1:s+o.\"", "gold": "predicting", "relevant_doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "relevant_chunk_id": 27, "relevant_text": "During CPT, we input the first stokens x1:sinto the encoder and use its output to assist the decoder in predicting the next otokens xs+1:s+o.", "case_id": "2509.01092__refrag_rethinking_rag_based_decoding_003"}
{"query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"The ﬁrst loss that we consider is based on the attention scores of the language model, and is heavily [MASK] by Izacard & Grave (2021).\"", "gold": "inspired", "relevant_doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "relevant_chunk_id": 25, "relevant_text": "The ﬁrst loss that we consider is based on the attention scores of the language model, and is heavily inspired by Izacard & Grave (2021).", "case_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models_018"}
{"query": "From 2301.12652__replug_retrieval_augmented_black_box_language_models.txt, fill the exact missing word [MASK] in: \"ous NLP tasks, including language mod- eling (Min et al., 2022; Borgeaud et al., 2022; Khandelwal et al., 2020) and [MASK] question answering (Lewis et al., 2020; Izacard et al., 2022b; Hu et al., 2022).\"", "gold": "open-domain", "relevant_doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "relevant_chunk_id": 16, "relevant_text": "ous NLP tasks, including language mod- eling (Min et al., 2022; Borgeaud et al., 2022; Khandelwal et al., 2020) and open-domain question answering (Lewis et al., 2020; Izacard et al., 2022b; Hu et al., 2022).", "case_id": "2301.12652__replug_retrieval_augmented_black_box_language_models_011"}
{"query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"The Contriever uses a dual-encoder architecture, where the query and documents are embedded [MASK] by a transformer encoder (Huang et al., 2013; Karpukhin et al., 2020).\"", "gold": "independently", "relevant_doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "relevant_chunk_id": 19, "relevant_text": "The Contriever uses a dual-encoder architecture, where the query and documents are embedded independently by a transformer encoder (Huang et al., 2013; Karpukhin et al., 2020).", "case_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models_012"}
{"query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"Recently, multiple works show that retrieval systems entirely based on dense [MASK] and approximate nearest neighbors were competi- tive with traditional approaches.\"", "gold": "representation", "relevant_doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "relevant_chunk_id": 13, "relevant_text": "Recently, multiple works show that retrieval systems entirely based on dense representation and approximate nearest neighbors were competi- tive with traditional approaches.", "case_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering_002"}
{"query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"Both the retriever and the language model are based on [MASK] transformer networks, which we describe in more detail below.\"", "gold": "pre-trained", "relevant_doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "relevant_chunk_id": 18, "relevant_text": "Both the retriever and the language model are based on pre-trained transformer networks, which we describe in more detail below.", "case_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models_011"}
{"query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"Ms) have attracted increasing attention and exhibited impressive abili- ties to understand [MASK] and generate fluent language texts (Brown et al., 2020; Ouyang et al., 2022; Touvron et al., 2023a).\"", "gold": "instructions", "relevant_doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "relevant_chunk_id": 4, "relevant_text": "Ms) have attracted increasing attention and exhibited impressive abili- ties to understand instructions and generate fluent language texts (Brown et al., 2020; Ouyang et al., 2022; Touvron et al., 2023a).", "case_id": "2401.15884__corrective_retrieval_augmented_generation_026"}
{"query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"Experiments show that SELF- RAG(7B and 13B parameters) significantly outperforms state-of-the-art LLMs and [MASK] models on a diverse set of tasks.\"", "gold": "retrieval-augmented", "relevant_doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "relevant_chunk_id": 3, "relevant_text": "Experiments show that SELF- RAG(7B and 13B parameters) significantly outperforms state-of-the-art LLMs and retrieval-augmented models on a diverse set of tasks.", "case_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection_013"}
{"query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"GraphRAG leverages summaries over large sections of the data source as a form of ”[MASK]” (described in Cheng et al.\"", "gold": "self-memory", "relevant_doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "relevant_chunk_id": 16, "relevant_text": "GraphRAG leverages summaries over large sections of the data source as a form of ”self-memory” (described in Cheng et al.", "case_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization_017"}
{"query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"Training a [MASK] for QA task works similarly, but scoring will evaluate whether the LM will generate t\"", "gold": "compressor", "relevant_doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "relevant_chunk_id": 29, "relevant_text": "Training a compressor for QA task works similarly, but scoring will evaluate whether the LM will generate t", "case_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation_010"}
{"query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"This suggests that selecting the most relevant information for [MASK] tasks is still crucial.\"", "gold": "knowledge-intensive", "relevant_doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "relevant_chunk_id": 13, "relevant_text": "This suggests that selecting the most relevant information for knowledge-intensive tasks is still crucial.", "case_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval_005"}
{"query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"Large-scale unregulated training data collection, low proportion of high-quality sampling data, [MASK] of data allocation in the input space, and many other realistic factors could impact the LLMs and exacerbate the problems.\"", "gold": "imperfection", "relevant_doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "relevant_chunk_id": 17, "relevant_text": "Large-scale unregulated training data collection, low proportion of high-quality sampling data, imperfection of data allocation in the input space, and many other realistic factors could impact the LLMs and exacerbate the problems.", "case_id": "2401.15884__corrective_retrieval_augmented_generation_017"}
{"query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"Early US models include X, S and PRO-4X, with a choice of 6-speed manual or 5-speed automatic [MASK], a choice of [...] moved from Smyrna, Tennessee, to Nissan's facility in Canton, Mississippi.\"", "gold": "transmissions", "relevant_doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "relevant_chunk_id": 9, "relevant_text": "Early US models include X, S and PRO-4X, with a choice of 6-speed manual or 5-speed automatic transmissions, a choice of [...] moved from Smyrna, Tennessee, to Nissan's facility in Canton, Mississippi.", "case_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation_008"}
{"query": "From 2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt, fill the exact missing word [MASK] in: \"This allows the generator to choose content from several documents when [MASK] an answer.\"", "gold": "producing", "relevant_doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "relevant_chunk_id": 19, "relevant_text": "This allows the generator to choose content from several documents when producing an answer.", "case_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks_014"}
{"query": "From 2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt, fill the exact missing word [MASK] in: \"In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting [MASK] in generative modeling and retrieval for open domain question answering.\"", "gold": "developments", "relevant_doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "relevant_chunk_id": 7, "relevant_text": "In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting developments in generative modeling and retrieval for open domain question answering.", "case_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering_027"}
{"query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"further confuse LMs with irrelevant information, degrading model [MASK] (Mallen et al., 2022; Shi et al., 2023a).\"", "gold": "performances", "relevant_doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "relevant_chunk_id": 6, "relevant_text": "further confuse LMs with irrelevant information, degrading model performances (Mallen et al., 2022; Shi et al., 2023a).", "case_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation_023"}
{"query": "From 2509.01092__refrag_rethinking_rag_based_decoding.txt, fill the exact missing word [MASK] in: \"[MASK] inference latency for LLMs with extensive context is an active area of research, with approaches 1arXiv:2509.01092v2 [cs.CL] 12 Oct 2\"", "gold": "optimizing", "relevant_doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "relevant_chunk_id": 8, "relevant_text": "Optimizing inference latency for LLMs with extensive context is an active area of research, with approaches 1arXiv:2509.01092v2 [cs.CL] 12 Oct 2", "case_id": "2509.01092__refrag_rethinking_rag_based_decoding_007"}
{"query": "From 2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt, fill the exact missing word [MASK] in: \"Unfortunately, this approach does not scale with the number of documents, since the [MASK] in the encoder results in a quadratic complexity with respect to the number of documents.\"", "gold": "self-attention", "relevant_doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "relevant_chunk_id": 22, "relevant_text": "Unfortunately, this approach does not scale with the number of documents, since the self-attention in the encoder results in a quadratic complexity with respect to the number of documents.", "case_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models_030"}
{"query": "From 2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt, fill the exact missing word [MASK] in: \"We conclude with careful analyses of our approach that reveal both its strength and [MASK], thereby building foundation for\"", "gold": "weaknesses", "relevant_doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "relevant_chunk_id": 15, "relevant_text": "We conclude with careful analyses of our approach that reveal both its strength and weaknesses, thereby building foundation for", "case_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation_029"}
{"query": "From 2401.15884__corrective_retrieval_augmented_generation.txt, fill the exact missing word [MASK] in: \"A [MASK] retrieval evaluator is designed to assess the overall quality of retrieved documents for a query.\"", "gold": "lightweight", "relevant_doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "relevant_chunk_id": 10, "relevant_text": "A lightweight retrieval evaluator is designed to assess the overall quality of retrieved documents for a query.", "case_id": "2401.15884__corrective_retrieval_augmented_generation_008"}
{"query": "From 2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt, fill the exact missing word [MASK] in: \"F-RAGis a framework that enhances the quality and factuality of an LLM through retrieval and [MASK], without sacrificing LLM’s original creativity and versatility.\"", "gold": "self-reflection", "relevant_doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "relevant_chunk_id": 28, "relevant_text": "F-RAGis a framework that enhances the quality and factuality of an LLM through retrieval and self-reflection, without sacrificing LLM’s original creativity and versatility.", "case_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection_006"}
{"query": "From 2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt, fill the exact missing word [MASK] in: \"The overall probability [MASK] is a weighted combination P(x) =PK k=1πkN(x;µk,Σk), where πksignifies the mixture weight for the kthGaussian distribution.\"", "gold": "distribution", "relevant_doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "relevant_chunk_id": 28, "relevant_text": "The overall probability distribution is a weighted combination P(x) =PK k=1πkN(x;µk,Σk), where πksignifies the mixture weight for the kthGaussian distribution.", "case_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval_003"}
{"query": "From 2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt, fill the exact missing word [MASK] in: \"[MASK], GraphRAG recursively creates increasingly global summaries by using the LLM to create summaries spanni\"", "gold": "specifically", "relevant_doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "relevant_chunk_id": 21, "relevant_text": "Specifically, GraphRAG recursively creates increasingly global summaries by using the LLM to create summaries spanni", "case_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization_028"}
