{"query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \"4We note that in the literature the bidirectional Trans- Input/Output [BLANK] To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g.,‚ü®Question, Answer‚ü©) in one token sequence.\"?", "gold": "representations", "relevant_doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "relevant_chunk_id": 27, "relevant_text": "4We note that in the literature the bidirectional Trans- Input/Output Representations To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g.,‚ü®Question, Answer‚ü©) in one token sequence.", "case_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding_001", "slice": "natural_answerable_20docs"}
{"query": "From 1706.03762__attention_is_all_you_need.txt, what exact term fills [BLANK] in: \"h-to-French translation task, our model establishes a new single-model [BLANK] BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.\"?", "gold": "state-of-the-art", "relevant_doc_id": "1706.03762__attention_is_all_you_need.txt", "relevant_chunk_id": 3, "relevant_text": "h-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.", "case_id": "1706.03762__attention_is_all_you_need_018", "slice": "natural_answerable_20docs"}
{"query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"her the ob- tained responses are helpful for [BLANK] future tokens; this is used as a Ô¨Åltering criterion.\"?", "gold": "predicting", "relevant_doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "relevant_chunk_id": 17, "relevant_text": "her the ob- tained responses are helpful for predicting future tokens; this is used as a Ô¨Åltering criterion.", "case_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools_016", "slice": "natural_answerable_20docs"}
{"query": "From 2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt, what exact term fills [BLANK] in: \"ural networks have proven to be powerful language models, Ô¨Årst in the form of recurrent architectures (Graves, 2013; Jozefowicz et al., 2016; Mikolov et al., 2010) and more recently in the form of Transformers (Vaswani et al., 2017), that use attention to [BLANK] the past.\"?", "gold": "contextualise", "relevant_doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "relevant_chunk_id": 4, "relevant_text": "ural networks have proven to be powerful language models, Ô¨Årst in the form of recurrent architectures (Graves, 2013; Jozefowicz et al., 2016; Mikolov et al., 2010) and more recently in the form of Transformers (Vaswani et al., 2017), that use attention to contextualise the past.", "case_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens_018", "slice": "natural_answerable_20docs"}
{"query": "From 2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt, what exact term fills [BLANK] in: \"At a high level, our Retrieval Transformer ( R/e.sc/t.sc/r.sc/o.sc) model splits the input sequence into chunks and retrieves text similar to the previous chunk to improve the [BLANK] in the current chunk.\"?", "gold": "predictions", "relevant_doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "relevant_chunk_id": 7, "relevant_text": "At a high level, our Retrieval Transformer ( R/e.sc/t.sc/r.sc/o.sc) model splits the input sequence into chunks and retrieves text similar to the previous chunk to improve the predictions in the current chunk.", "case_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens_005", "slice": "natural_answerable_20docs"}
{"query": "From 2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt, what exact term fills [BLANK] in: \"Though simple for humans, [BLANK] reasoning is a task where language models often struggle (Hendrycks et al., 2021; Patel et al., 2021, inter alia ).\"?", "gold": "arithmetic", "relevant_doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "relevant_chunk_id": 19, "relevant_text": "Though simple for humans, arithmetic reasoning is a task where language models often struggle (Hendrycks et al., 2021; Patel et al., 2021, inter alia ).", "case_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models_011", "slice": "natural_answerable_20docs"}
{"query": "From 2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt, what exact term fills [BLANK] in: \"e end performance dips [BLANK] due to the huge amount of ‚Äúhard negatives‚Äù, which confuses the reader.\"?", "gold": "significantly", "relevant_doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "relevant_chunk_id": 19, "relevant_text": "e end performance dips significantly due to the huge amount of ‚Äúhard negatives‚Äù, which confuses the reader.", "case_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms_020", "slice": "natural_answerable_20docs"}
{"query": "From 2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt, what exact term fills [BLANK] in: \"Outputs from our 175B [BLANK] are preferred to 175B GPT-3 outputs 85 ¬±3% of the time, and preferred 71 ¬±4% of the time to few-shot 175B GPT-3.\"?", "gold": "instructgpt", "relevant_doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "relevant_chunk_id": 16, "relevant_text": "Outputs from our 175B InstructGPT are preferred to 175B GPT-3 outputs 85 ¬±3% of the time, and preferred 71 ¬±4% of the time to few-shot 175B GPT-3.", "case_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback_012", "slice": "natural_answerable_20docs"}
{"query": "From 2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt, what exact term fills [BLANK] in: \"B) General and Ô¨Çexible : Due to the Ô¨Çexible thought space and [BLANK] occurrence format, ReAct works for diverse tasks with distinct action spaces and reasoning needs, including but\"?", "gold": "thought-action", "relevant_doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "relevant_chunk_id": 33, "relevant_text": "B) General and Ô¨Çexible : Due to the Ô¨Çexible thought space and thought-action occurrence format, ReAct works for diverse tasks with distinct action spaces and reasoning needs, including but", "case_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models_001", "slice": "natural_answerable_20docs"}
{"query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"This is [BLANK]:2302.04761v1 [cs.CL] 9 Feb 2023 x1: i-1 = Pittsburgh is also known as xi: n = the Steel City x* = Pittsburgh is also known as [QA(What ‚Ä¶?\"?", "gold": "impor-arxiv", "relevant_doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "relevant_chunk_id": 8, "relevant_text": "This is impor-arXiv:2302.04761v1 [cs.CL] 9 Feb 2023 x1: i-1 = Pittsburgh is also known as xi: n = the Steel City x* = Pittsburgh is also known as [QA(What ‚Ä¶?", "case_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools_015", "slice": "natural_answerable_20docs"}
{"query": "From 2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt, what exact term fills [BLANK] in: \"In this paper, we mainly focus on the setup where a frozen large language model, PaLM-540B (Chowdhery et al., 2022)1, is prompted with few-shot in-context examples to generate both [BLANK]Ô¨Åc actions and free-form language thoughts for task solving (Figure 1 (1d), (2b)).\"?", "gold": "domain-speci", "relevant_doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "relevant_chunk_id": 30, "relevant_text": "In this paper, we mainly focus on the setup where a frozen large language model, PaLM-540B (Chowdhery et al., 2022)1, is prompted with few-shot in-context examples to generate both domain-speciÔ¨Åc actions and free-form language thoughts for task solving (Figure 1 (1d), (2b)).", "case_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models_011", "slice": "natural_answerable_20docs"}
{"query": "From 2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt, what exact term fills [BLANK] in: \"limited support of reasoning and acting behaviors), and perform initial Ô¨Ånetuning [BLANK] showing the potential of ReAct to improve with additional training data.\"?", "gold": "experiments", "relevant_doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "relevant_chunk_id": 25, "relevant_text": "limited support of reasoning and acting behaviors), and perform initial Ô¨Ånetuning experiments showing the potential of ReAct to improve with additional training data.", "case_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models_012", "slice": "natural_answerable_20docs"}
{"query": "From 2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt, what exact term fills [BLANK] in: \"to the phenomenon that all models are to some extent constrained by their underlying [BLANK] model (while Orca 2 training could be applied any base LLM, we report results on LLaMA-2 7B and 13B in this report).\"?", "gold": "pre-trained", "relevant_doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "relevant_chunk_id": 21, "relevant_text": "to the phenomenon that all models are to some extent constrained by their underlying pre-trained model (while Orca 2 training could be applied any base LLM, we report results on LLaMA-2 7B and 13B in this report).", "case_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason_019", "slice": "natural_answerable_20docs"}
{"query": "From 2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt, what exact term fills [BLANK] in: \"We analyze the [BLANK] of language models on two tasks that require identifying relevant information in t\"?", "gold": "performance", "relevant_doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "relevant_chunk_id": 0, "relevant_text": "We analyze the performance of language models on two tasks that require identifying relevant information in t", "case_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts_009", "slice": "natural_answerable_20docs"}
{"query": "From 2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt, what exact term fills [BLANK] in: \"answer questions about code, and sometimes follows [BLANK] in different languages, despite these instructions being very rare in the Ô¨Åne-tuning distribution.\"?", "gold": "instructions", "relevant_doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "relevant_chunk_id": 24, "relevant_text": "answer questions about code, and sometimes follows instructions in different languages, despite these instructions being very rare in the Ô¨Åne-tuning distribution.", "case_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback_003", "slice": "natural_answerable_20docs"}
{"query": "From 1706.03762__attention_is_all_you_need.txt, what exact term fills [BLANK] in: \"Recent work has achieved significant improvements in computational efficiency through [BLANK] tricks [ 21] and conditional computation [ 32], while also improving model performance in case of the latter.\"?", "gold": "factorization", "relevant_doc_id": "1706.03762__attention_is_all_you_need.txt", "relevant_chunk_id": 8, "relevant_text": "Recent work has achieved significant improvements in computational efficiency through factorization tricks [ 21] and conditional computation [ 32], while also improving model performance in case of the latter.", "case_id": "1706.03762__attention_is_all_you_need_015", "slice": "natural_answerable_20docs"}
{"query": "From 2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt, what exact term fills [BLANK] in: \"With a 2 trillion token database, our [BLANK] Transformer ( R/e.sc/t.sc/r.sc/o.sc) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25 \u0002fewer parameters.\"?", "gold": "retrieval-enhanced", "relevant_doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "relevant_chunk_id": 1, "relevant_text": "With a 2 trillion token database, our Retrieval-Enhanced Transformer ( R/e.sc/t.sc/r.sc/o.sc) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25 \u0002fewer parameters.", "case_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens_013", "slice": "natural_answerable_20docs"}
{"query": "From 2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"fluency) and then scores passages based on the average probability of the generated tokens, according to a given [BLANK] LM.\"?", "gold": "autoregressive", "relevant_doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "relevant_chunk_id": 15, "relevant_text": "fluency) and then scores passages based on the average probability of the generated tokens, according to a given autoregressive LM.", "case_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation_016", "slice": "natural_answerable_20docs"}
{"query": "From 2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt, what exact term fills [BLANK] in: \"We analyze the IO complexity [ 1] [BLANK] , proving that it requires ùëÇ¬πùëÅ2ùëë2ùëÄ\u00001¬∫HBM accesses where ùëëis the head dimension and ùëÄis the size of SRAM, as compared to Œ©¬πùëÅùëë¬∏ùëÅ2¬∫of standard attention.\"?", "gold": "offlashattention", "relevant_doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "relevant_chunk_id": 14, "relevant_text": "We analyze the IO complexity [ 1] ofFlashAttention , proving that it requires ùëÇ¬πùëÅ2ùëë2ùëÄ\u00001¬∫HBM accesses where ùëëis the head dimension and ùëÄis the size of SRAM, as compared to Œ©¬πùëÅùëë¬∏ùëÅ2¬∫of standard attention.", "case_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness_012", "slice": "natural_answerable_20docs"}
{"query": "From 2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"First, [BLANK] refers to the idea that the an- swer should be grounded in the given context.\"?", "gold": "faithfulness", "relevant_doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "relevant_chunk_id": 20, "relevant_text": "First, Faithfulness refers to the idea that the an- swer should be grounded in the given context.", "case_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation_008", "slice": "natural_answerable_20docs"}
{"query": "From 2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt, what exact term fills [BLANK] in: \"Since John did not see the ball in the [BLANK] after he left, it must have still been in the box.\"?", "gold": "basket", "relevant_doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "relevant_chunk_id": 9, "relevant_text": "Since John did not see the ball in the basket after he left, it must have still been in the box.", "case_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason_005", "slice": "natural_answerable_20docs"}
{"query": "From 2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt, what exact term fills [BLANK] in: \"t if demonstrations of [BLANK] reasoning are provided in the exemplars for few-shot prompting.\"?", "gold": "chain-of-thought", "relevant_doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "relevant_chunk_id": 15, "relevant_text": "t if demonstrations of chain-of-thought reasoning are provided in the exemplars for few-shot prompting.", "case_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models_012", "slice": "natural_answerable_20docs"}
{"query": "From 2305.06983__active_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"The first method prompts the LM to generate retrieval queries when necessary while generating the an- swer using [BLANK] instructions, de- noted as FLARE instruct .\"?", "gold": "retrieval-encouraging", "relevant_doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "relevant_chunk_id": 25, "relevant_text": "The first method prompts the LM to generate retrieval queries when necessary while generating the an- swer using retrieval-encouraging instructions, de- noted as FLARE instruct .", "case_id": "2305.06983__active_retrieval_augmented_generation_008", "slice": "natural_answerable_20docs"}
{"query": "From 2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natu- ral language layer between a user and textual databases, reducing the risk of [BLANK].\"?", "gold": "hallucinations", "relevant_doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "relevant_chunk_id": 1, "relevant_text": "RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natu- ral language layer between a user and textual databases, reducing the risk of hallucinations.", "case_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation_009", "slice": "natural_answerable_20docs"}
{"query": "From 1706.03762__attention_is_all_you_need.txt, what exact term fills [BLANK] in: \"Aligning the positions to steps in [BLANK] time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht‚àí1and the input for position t.\"?", "gold": "computation", "relevant_doc_id": "1706.03762__attention_is_all_you_need.txt", "relevant_chunk_id": 7, "relevant_text": "Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht‚àí1and the input for position t.", "case_id": "1706.03762__attention_is_all_you_need_005", "slice": "natural_answerable_20docs"}
{"query": "From 2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"Other [BLANK] rely on linking the generated responses to facts from an external knowledge base (Min et al., 2023), but this is not always possible.\"?", "gold": "approaches", "relevant_doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "relevant_chunk_id": 11, "relevant_text": "Other approaches rely on linking the generated responses to facts from an external knowledge base (Min et al., 2023), but this is not always possible.", "case_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation_017", "slice": "natural_answerable_20docs"}
{"query": "From 2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt, what exact term fills [BLANK] in: \"In this work, we show that retrieval can be practically implemented us- ingdense [BLANK] alone, where em- beddings are learned from a small number of questions and passages by a simple dual- encoder framework.\"?", "gold": "representations", "relevant_doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "relevant_chunk_id": 1, "relevant_text": "In this work, we show that retrieval can be practically implemented us- ingdense representations alone, where em- beddings are learned from a small number of questions and passages by a simple dual- encoder framework.", "case_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering_013", "slice": "natural_answerable_20docs"}
{"query": "From 2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt, what exact term fills [BLANK] in: \"For comparison with other Phi series models, phi-3.5-MoE uses the same tokenizer as phi-3-medium andphi-3-mini with [BLANK] size of 32064.\"?", "gold": "vocabulary", "relevant_doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "relevant_chunk_id": 16, "relevant_text": "For comparison with other Phi series models, phi-3.5-MoE uses the same tokenizer as phi-3-medium andphi-3-mini with vocabulary size of 32064.", "case_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone_016", "slice": "natural_answerable_20docs"}
{"query": "From 2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt, what exact term fills [BLANK] in: \"3 E XPERIMENTS We conducted a series of experiments to compare the proposed [BLANK] method with existing approaches on a range of reasoning benchmarks.\"?", "gold": "self-consistency", "relevant_doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "relevant_chunk_id": 31, "relevant_text": "3 E XPERIMENTS We conducted a series of experiments to compare the proposed self-consistency method with existing approaches on a range of reasoning benchmarks.", "case_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models_014", "slice": "natural_answerable_20docs"}
{"query": "From 2305.06983__active_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"As shown in Figure 2, when the LM [BLANK] ‚Äú[Search(query)]‚Äù (shown in gray italic ), we stop the generation and use the\"?", "gold": "generates", "relevant_doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "relevant_chunk_id": 30, "relevant_text": "As shown in Figure 2, when the LM generates ‚Äú[Search(query)]‚Äù (shown in gray italic ), we stop the generation and use the", "case_id": "2305.06983__active_retrieval_augmented_generation_003", "slice": "natural_answerable_20docs"}
{"query": "From 2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt, what exact term fills [BLANK] in: \"(2021), we want language models to be helpful (they should help the user solve their task), honest (they shouldn‚Äôt fabricate [BLANK] or mislead the user), and harmless (they s\"?", "gold": "information", "relevant_doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "relevant_chunk_id": 8, "relevant_text": "(2021), we want language models to be helpful (they should help the user solve their task), honest (they shouldn‚Äôt fabricate information or mislead the user), and harmless (they s", "case_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback_015", "slice": "natural_answerable_20docs"}
{"query": "From 1706.03762__attention_is_all_you_need.txt, what exact term fills [BLANK] in: \"In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and [BLANK] for ByteNet.\"?", "gold": "logarithmically", "relevant_doc_id": "1706.03762__attention_is_all_you_need.txt", "relevant_chunk_id": 11, "relevant_text": "In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet.", "case_id": "1706.03762__attention_is_all_you_need_017", "slice": "natural_answerable_20docs"}
{"query": "From 2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt, what exact term fills [BLANK] in: \"This deÔ¨Ånes the following [BLANK] sequence log-likelihood: ùêø¬πùëãjùúÉ¬îD¬∫,ùëô‚àëÔ∏Å ùë¢=1ùëö‚àëÔ∏Å ùëñ=1\u0012ùúÉ\u0000 ùë•¬πùë¢\u00001¬∫ùëö¬∏ùëñj¬πùë•ùëó¬∫ùëó¬ù¬πùë¢\u00001¬∫ùëö¬∏ùëñ¬î¬πR/e.sc/t.scD¬πùê∂ùë¢0¬∫¬∫ùë¢0¬ùùë¢\u0001 ¬ì (1) We set R/e.sc/t.sc¬πùê∂1¬∫=;, namely the likelihood of tokens from the Ô¨Årst chunk does not depend on any retrieval data.\"?", "gold": "retrieval-enhanced", "relevant_doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "relevant_chunk_id": 23, "relevant_text": "This deÔ¨Ånes the following retrieval-enhanced sequence log-likelihood: ùêø¬πùëãjùúÉ¬îD¬∫,ùëô‚àëÔ∏Å ùë¢=1ùëö‚àëÔ∏Å ùëñ=1\u0012ùúÉ\u0000 ùë•¬πùë¢\u00001¬∫ùëö¬∏ùëñj¬πùë•ùëó¬∫ùëó¬ù¬πùë¢\u00001¬∫ùëö¬∏ùëñ¬î¬πR/e.sc/t.scD¬πùê∂ùë¢0¬∫¬∫ùë¢0¬ùùë¢\u0001 ¬ì (1) We set R/e.sc/t.sc¬πùê∂1¬∫=;, namely the likelihood of tokens from the Ô¨Årst chunk does not depend on any retrieval data.", "case_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens_003", "slice": "natural_answerable_20docs"}
{"query": "From 2302.13971__llama_open_and_efficient_foundation_language_models.txt, what exact term fills [BLANK] in: \"This leads to the following mixture of data and the per- centage they represent in the training set: English [BLANK] [67%].\"?", "gold": "commoncrawl", "relevant_doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "relevant_chunk_id": 10, "relevant_text": "This leads to the following mixture of data and the per- centage they represent in the training set: English CommonCrawl [67%].", "case_id": "2302.13971__llama_open_and_efficient_foundation_language_models_017", "slice": "natural_answerable_20docs"}
{"query": "From 2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt, what exact term fills [BLANK] in: \"It [BLANK] MoE layer as its feedforward models, employing the top2 routing among 16 expert networks.\"?", "gold": "incorporates", "relevant_doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "relevant_chunk_id": 14, "relevant_text": "It incorporates MoE layer as its feedforward models, employing the top2 routing among 16 expert networks.", "case_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone_012", "slice": "natural_answerable_20docs"}
{"query": "From 2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt, what exact term fills [BLANK] in: \"In this paper, we explore the use of LLMs to generate both reasoning traces and task-speciÔ¨Åc actions in an [BLANK] manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, whi\"?", "gold": "interleaved", "relevant_doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "relevant_chunk_id": 1, "relevant_text": "In this paper, we explore the use of LLMs to generate both reasoning traces and task-speciÔ¨Åc actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, whi", "case_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models_013", "slice": "natural_answerable_20docs"}
{"query": "From 2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt, what exact term fills [BLANK] in: \"Following the experimental settings of Position [BLANK] (Chen et al., 2023), we fine-tune models with proper position embeddings.\"?", "gold": "interpolation", "relevant_doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "relevant_chunk_id": 19, "relevant_text": "Following the experimental settings of Position Interpolation (Chen et al., 2023), we fine-tune models with proper position embeddings.", "case_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models_011", "slice": "natural_answerable_20docs"}
{"query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"is to give them the abil- ity to use external tools such as search engines, [BLANK], or calendars.\"?", "gold": "calculators", "relevant_doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "relevant_chunk_id": 7, "relevant_text": "is to give them the abil- ity to use external tools such as search engines, calculators, or calendars.", "case_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools_007", "slice": "natural_answerable_20docs"}
{"query": "From 2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt, what exact term fills [BLANK] in: \"We are not implying that we actually found the [BLANK] ‚Äúoptimal‚Äù data mixture for a given scale.\"?", "gold": "provably", "relevant_doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "relevant_chunk_id": 23, "relevant_text": "We are not implying that we actually found the provably ‚Äúoptimal‚Äù data mixture for a given scale.", "case_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone_001", "slice": "natural_answerable_20docs"}
{"query": "From 2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt, what exact term fills [BLANK] in: \"D) Human aligned and controllable :ReAct promises an [BLANK] sequential decision making and reasoning process where humans can easily inspect reasoning and factual correctness.\"?", "gold": "interpretable", "relevant_doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "relevant_chunk_id": 35, "relevant_text": "D) Human aligned and controllable :ReAct promises an interpretable sequential decision making and reasoning process where humans can easily inspect reasoning and factual correctness.", "case_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models_003", "slice": "natural_answerable_20docs"}
{"query": "From 1706.03762__attention_is_all_you_need.txt, what exact term fills [BLANK] in: \"We employ a residual connection [ 11] around each of the two sub-layers, followed by layer [BLANK] [ 1].\"?", "gold": "normalization", "relevant_doc_id": "1706.03762__attention_is_all_you_need.txt", "relevant_chunk_id": 17, "relevant_text": "We employ a residual connection [ 11] around each of the two sub-layers, followed by layer normalization [ 1].", "case_id": "1706.03762__attention_is_all_you_need_002", "slice": "natural_answerable_20docs"}
{"query": "From 2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt, what exact term fills [BLANK] in: \"hat train language models in either ‚Äúcompute optimal regime‚Äù [HBM+22] or ‚Äú[BLANK] regime‚Äù, we mainly focus on the quality of data for a given scale .3 We try to calibrate the training data to be closer to the ‚Äúdata optimal‚Äù regime for small models.\"?", "gold": "over-train", "relevant_doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "relevant_chunk_id": 20, "relevant_text": "hat train language models in either ‚Äúcompute optimal regime‚Äù [HBM+22] or ‚Äúover-train regime‚Äù, we mainly focus on the quality of data for a given scale .3 We try to calibrate the training data to be closer to the ‚Äúdata optimal‚Äù regime for small models.", "case_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone_019", "slice": "natural_answerable_20docs"}
{"query": "From 2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"Answering a question then [BLANK] involves retrieving relevant pas- sages from a corpus and feeding these passages, along with the original question, to the LM.\"?", "gold": "essentially", "relevant_doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "relevant_chunk_id": 5, "relevant_text": "Answering a question then essentially involves retrieving relevant pas- sages from a corpus and feeding these passages, along with the original question, to the LM.", "case_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation_010", "slice": "natural_answerable_20docs"}
{"query": "From 2005.14165__language_models_are_few_shot_learners.txt, what exact term fills [BLANK] in: \"Since in-context learning involves absorbing many skills and tasks within the [BLANK] of the model, it is plaus\"?", "gold": "parameters", "relevant_doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "relevant_chunk_id": 24, "relevant_text": "Since in-context learning involves absorbing many skills and tasks within the parameters of the model, it is plaus", "case_id": "2005.14165__language_models_are_few_shot_learners_011", "slice": "natural_answerable_20docs"}
{"query": "From 2005.14165__language_models_are_few_shot_learners.txt, what exact term fills [BLANK] in: \"[BLANK] clearly requires substantial improvement in order to be viable as a practical method of solving language tasks.\"?", "gold": "meta-learning", "relevant_doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "relevant_chunk_id": 23, "relevant_text": "Meta-learning clearly requires substantial improvement in order to be viable as a practical method of solving language tasks.", "case_id": "2005.14165__language_models_are_few_shot_learners_002", "slice": "natural_answerable_20docs"}
{"query": "From 2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt, what exact term fills [BLANK] in: \"[BLANK] extends Llama2 7B from 4k context to 100k, or Llama2 70B to 32k on a single 8 √óA100 machine.\"?", "gold": "longlora", "relevant_doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "relevant_chunk_id": 3, "relevant_text": "LongLoRA extends Llama2 7B from 4k context to 100k, or Llama2 70B to 32k on a single 8 √óA100 machine.", "case_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models_018", "slice": "natural_answerable_20docs"}
{"query": "From 2302.13971__llama_open_and_efficient_foundation_language_models.txt, what exact term fills [BLANK] in: \"2.2 [BLANK] Following recent work on large language models, our network is based on the transformer architec- ture (Vaswani et al., 2017).\"?", "gold": "architecture", "relevant_doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "relevant_chunk_id": 19, "relevant_text": "2.2 Architecture Following recent work on large language models, our network is based on the transformer architec- ture (Vaswani et al., 2017).", "case_id": "2302.13971__llama_open_and_efficient_foundation_language_models_011", "slice": "natural_answerable_20docs"}
{"query": "From 2302.13971__llama_open_and_efficient_foundation_language_models.txt, what exact term fills [BLANK] in: \"We kept the data from the 28 largest [BLANK], re- moved the HTML tags from text and sorted the answers by score (from highest to lowest).\"?", "gold": "websites", "relevant_doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "relevant_chunk_id": 17, "relevant_text": "We kept the data from the 28 largest websites, re- moved the HTML tags from text and sorted the answers by score (from highest to lowest).", "case_id": "2302.13971__llama_open_and_efficient_foundation_language_models_002", "slice": "natural_answerable_20docs"}
{"query": "From 2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt, what exact term fills [BLANK] in: \"In more detail, assume the [BLANK] answers aiare from a Ô¨Åxed answer set, ai‚ààA, where i= 1, .\"?", "gold": "generated", "relevant_doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "relevant_chunk_id": 21, "relevant_text": "In more detail, assume the generated answers aiare from a Ô¨Åxed answer set, ai‚ààA, where i= 1, .", "case_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models_006", "slice": "natural_answerable_20docs"}
{"query": "From 2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt, what exact term fills [BLANK] in: \"This procedure aligns the behavior of GPT-3 to the stated preferences of a speciÔ¨Åc group of people (mostly our labelers and [BLANK]), rather than any broader notion of ‚Äúhuman values‚Äù; we discuss this further in Section 5.2.\"?", "gold": "researchers", "relevant_doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "relevant_chunk_id": 12, "relevant_text": "This procedure aligns the behavior of GPT-3 to the stated preferences of a speciÔ¨Åc group of people (mostly our labelers and researchers), rather than any broader notion of ‚Äúhuman values‚Äù; we discuss this further in Section 5.2.", "case_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback_017", "slice": "natural_answerable_20docs"}
{"query": "From 2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt, what exact term fills [BLANK] in: \"It is designed to equip readers and professionals with a detailed and structured [BLANK] of both large models a\"?", "gold": "understanding", "relevant_doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "relevant_chunk_id": 10, "relevant_text": "It is designed to equip readers and professionals with a detailed and structured understanding of both large models a", "case_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey_017", "slice": "natural_answerable_20docs"}
{"query": "From 2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt, what exact term fills [BLANK] in: \"We also show in Section 3 additional beneÔ¨Åts when Ô¨Ånetuning is enabled, and in Section 4 how ReAct [BLANK] is robust to prompt selections.\"?", "gold": "performance", "relevant_doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "relevant_chunk_id": 34, "relevant_text": "We also show in Section 3 additional beneÔ¨Åts when Ô¨Ånetuning is enabled, and in Section 4 how ReAct performance is robust to prompt selections.", "case_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models_014", "slice": "natural_answerable_20docs"}
{"query": "From 2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt, what exact term fills [BLANK] in: \"e than our SFT baseline, and labelers signiÔ¨Åcantly prefer [BLANK] to these models (InstructGPT has a 73.4 ¬±2%winrate vs.\"?", "gold": "instructgpt", "relevant_doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "relevant_chunk_id": 23, "relevant_text": "e than our SFT baseline, and labelers signiÔ¨Åcantly prefer InstructGPT to these models (InstructGPT has a 73.4 ¬±2%winrate vs.", "case_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback_001", "slice": "natural_answerable_20docs"}
{"query": "From 2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt, what exact term fills [BLANK] in: \"[BLANK] and closed-domain QA), InstructGPT models make up information not present in the input about half as often as GPT-3 (a 21% vs.\"?", "gold": "summarization", "relevant_doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "relevant_chunk_id": 17, "relevant_text": "summarization and closed-domain QA), InstructGPT models make up information not present in the input about half as often as GPT-3 (a 21% vs.", "case_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback_016", "slice": "natural_answerable_20docs"}
{"query": "From 2302.13971__llama_open_and_efficient_foundation_language_models.txt, what exact term fills [BLANK] in: \"(2022) is to determine how to best scale the dataset and model sizes for a [BLANK] training compute budget.\"?", "gold": "particular", "relevant_doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "relevant_chunk_id": 3, "relevant_text": "(2022) is to determine how to best scale the dataset and model sizes for a particular training compute budget.", "case_id": "2302.13971__llama_open_and_efficient_foundation_language_models_018", "slice": "natural_answerable_20docs"}
{"query": "From 2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt, what exact term fills [BLANK] in: \"The imbalanced ‚Äúheavy‚Äù retriever and ‚Äúlight‚Äù reader design can lead to [BLANK] performance.\"?", "gold": "sub-optimal", "relevant_doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "relevant_chunk_id": 1, "relevant_text": "The imbalanced ‚Äúheavy‚Äù retriever and ‚Äúlight‚Äù reader design can lead to sub-optimal performance.", "case_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms_013", "slice": "natural_answerable_20docs"}
{"query": "From 2305.06983__active_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"If so (step 2 and 3), the system retrieves relevant documents and [BLANK] the sentence.\"?", "gold": "regenerates", "relevant_doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "relevant_chunk_id": 11, "relevant_text": "If so (step 2 and 3), the system retrieves relevant documents and regenerates the sentence.", "case_id": "2305.06983__active_retrieval_augmented_generation_005", "slice": "natural_answerable_20docs"}
{"query": "From 2005.14165__language_models_are_few_shot_learners.txt, what exact term fills [BLANK] in: \"To avoid this confusion, we use the term ‚Äúmeta-learning‚Äù to capture the inner-loop / outer-loop structure of the general method, and the term ‚Äúin [BLANK]‚Äù to refer to the inner loop of meta-learning.\"?", "gold": "context-learning", "relevant_doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "relevant_chunk_id": 26, "relevant_text": "To avoid this confusion, we use the term ‚Äúmeta-learning‚Äù to capture the inner-loop / outer-loop structure of the general method, and the term ‚Äúin context-learning‚Äù to refer to the inner loop of meta-learning.", "case_id": "2005.14165__language_models_are_few_shot_learners_019", "slice": "natural_answerable_20docs"}
{"query": "From 2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt, what exact term fills [BLANK] in: \"3 Dense Passage Retriever (DPR) We focus our research in this work on improv- ing the retrieval component in [BLANK] QA.\"?", "gold": "open-domain", "relevant_doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "relevant_chunk_id": 17, "relevant_text": "3 Dense Passage Retriever (DPR) We focus our research in this work on improv- ing the retrieval component in open-domain QA.", "case_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering_006", "slice": "natural_answerable_20docs"}
{"query": "From 2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt, what exact term fills [BLANK] in: \"For example, large language models can generate outputs that are [BLANK], toxic, or simply not helpful to the user.\"?", "gold": "untruthful", "relevant_doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "relevant_chunk_id": 1, "relevant_text": "For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user.", "case_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback_013", "slice": "natural_answerable_20docs"}
{"query": "From 2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt, what exact term fills [BLANK] in: \"e models use their input context, we release our code and evaluation data.1 2 [BLANK] Question Answering Our goal is to better understand how language mod- els use their input context.\"?", "gold": "multi-document", "relevant_doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "relevant_chunk_id": 22, "relevant_text": "e models use their input context, we release our code and evaluation data.1 2 Multi-Document Question Answering Our goal is to better understand how language mod- els use their input context.", "case_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts_016", "slice": "natural_answerable_20docs"}
{"query": "From 2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt, what exact term fills [BLANK] in: \"Naive RAG The Naive RAG research paradigm represents the earli- est [BLANK], which gained prominence shortly after the 3 Fig.\"?", "gold": "methodology", "relevant_doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "relevant_chunk_id": 18, "relevant_text": "Naive RAG The Naive RAG research paradigm represents the earli- est methodology, which gained prominence shortly after the 3 Fig.", "case_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey_019", "slice": "natural_answerable_20docs"}
{"query": "From 2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt, what exact term fills [BLANK] in: \"We took a closer look at the model‚Äôs output [BLANK] and found this is because for each (ri,ai), the normaliz\"?", "gold": "probabilities", "relevant_doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "relevant_chunk_id": 25, "relevant_text": "We took a closer look at the model‚Äôs output probabilities and found this is because for each (ri,ai), the normaliz", "case_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models_011", "slice": "natural_answerable_20docs"}
{"query": "From 2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"3 Evaluation [BLANK] We consider a standard RAG setting, where given a question q, the system first retrieves some context c(q)and then uses the retrieved context to generate an answer as(q).\"?", "gold": "strategies", "relevant_doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "relevant_chunk_id": 19, "relevant_text": "3 Evaluation Strategies We consider a standard RAG setting, where given a question q, the system first retrieves some context c(q)and then uses the retrieved context to generate an answer as(q).", "case_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation_019", "slice": "natural_answerable_20docs"}
{"query": "From 2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt, what exact term fills [BLANK] in: \"(2020), in which a language model is given in-context exemplars of input‚Äìoutput pairs before outputting a [BLANK] for a test-time example.\"?", "gold": "prediction", "relevant_doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "relevant_chunk_id": 22, "relevant_text": "(2020), in which a language model is given in-context exemplars of input‚Äìoutput pairs before outputting a prediction for a test-time example.", "case_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models_008", "slice": "natural_answerable_20docs"}
{"query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"Therefore, we propose [BLANK] , a model that learns to use tools in a novel way, which fulÔ¨Ålls the following desiderata: ‚Ä¢T\"?", "gold": "toolformer", "relevant_doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "relevant_chunk_id": 7, "relevant_text": "Therefore, we propose Toolformer , a model that learns to use tools in a novel way, which fulÔ¨Ålls the following desiderata: ‚Ä¢T", "case_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools_005", "slice": "natural_answerable_20docs"}
{"query": "From 2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt, what exact term fills [BLANK] in: \"Learning a policy is challenging when the mapping ct‚Ü¶‚Üíatis highly implicit and requires extensive [BLANK].\"?", "gold": "computation", "relevant_doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "relevant_chunk_id": 27, "relevant_text": "Learning a policy is challenging when the mapping ct‚Ü¶‚Üíatis highly implicit and requires extensive computation.", "case_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models_016", "slice": "natural_answerable_20docs"}
{"query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"In this paper, we show that LMs can teach [BLANK] to use external tools via simple APIs and achieve the best of both worlds.\"?", "gold": "themselves", "relevant_doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "relevant_chunk_id": 1, "relevant_text": "In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds.", "case_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools_013", "slice": "natural_answerable_20docs"}
{"query": "From 2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt, what exact term fills [BLANK] in: \"Write a [BLANK] answer for the given question using only the provided search results (some of which might be irrelevant).\"?", "gold": "high-quality", "relevant_doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "relevant_chunk_id": 29, "relevant_text": "Write a high-quality answer for the given question using only the provided search results (some of which might be irrelevant).", "case_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts_014", "slice": "natural_answerable_20docs"}
{"query": "From 2302.13971__llama_open_and_efficient_foundation_language_models.txt, what exact term fills [BLANK] in: \"There exist some exceptions, notably OPT (Zhang et al., 2022), GPT-NeoX (Black et al., 2022), BLOOM (Scao et al., 2022) and GLM (Zeng et al., 2022), but none that are [BLANK] with PaLM-62B or Chinchilla.\"?", "gold": "competitive", "relevant_doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "relevant_chunk_id": 7, "relevant_text": "There exist some exceptions, notably OPT (Zhang et al., 2022), GPT-NeoX (Black et al., 2022), BLOOM (Scao et al., 2022) and GLM (Zeng et al., 2022), but none that are competitive with PaLM-62B or Chinchilla.", "case_id": "2302.13971__llama_open_and_efficient_foundation_language_models_015", "slice": "natural_answerable_20docs"}
{"query": "From 2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt, what exact term fills [BLANK] in: \"[BLANK] scales Transformers to longer sequences, which improves their quality and enables new capabilities.\"?", "gold": "flashattention", "relevant_doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "relevant_chunk_id": 18, "relevant_text": "FlashAttention scales Transformers to longer sequences, which improves their quality and enables new capabilities.", "case_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness_011", "slice": "natural_answerable_20docs"}
{"query": "From 2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt, what exact term fills [BLANK] in: \"(2023), we use accuracy as our primary evaluation metric, judging whether any of the correct answers (as taken from the [BLANK] annotations) appear in the predicted output.\"?", "gold": "naturalquestions", "relevant_doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "relevant_chunk_id": 27, "relevant_text": "(2023), we use accuracy as our primary evaluation metric, judging whether any of the correct answers (as taken from the NaturalQuestions annotations) appear in the predicted output.", "case_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts_020", "slice": "natural_answerable_20docs"}
{"query": "From 2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt, what exact term fills [BLANK] in: \"Despite RAG method are [BLANK] and surpass the performance of the native LLM, they also exhibit several limitations.\"?", "gold": "cost-effective", "relevant_doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "relevant_chunk_id": 18, "relevant_text": "Despite RAG method are cost-effective and surpass the performance of the native LLM, they also exhibit several limitations.", "case_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey_011", "slice": "natural_answerable_20docs"}
{"query": "From 2302.13971__llama_open_and_efficient_foundation_language_models.txt, what exact term fills [BLANK] in: \"At the higher-end of the scale, our 65B-parameter model is also [BLANK] with the best large lan- guage models such as Chinchilla or PaLM-540B.\"?", "gold": "competitive", "relevant_doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "relevant_chunk_id": 6, "relevant_text": "At the higher-end of the scale, our 65B-parameter model is also competitive with the best large lan- guage models such as Chinchilla or PaLM-540B.", "case_id": "2302.13971__llama_open_and_efficient_foundation_language_models_005", "slice": "natural_answerable_20docs"}
{"query": "From 2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt, what exact term fills [BLANK] in: \"seminal work to over hundred billion parameters (Brown et al., 2020; Radford et al., 2019) in the last two years which has led to models that do very well on a wide array of tasks in a zero or few-shot [BLANK].\"?", "gold": "formulation", "relevant_doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "relevant_chunk_id": 5, "relevant_text": "seminal work to over hundred billion parameters (Brown et al., 2020; Radford et al., 2019) in the last two years which has led to models that do very well on a wide array of tasks in a zero or few-shot formulation.", "case_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens_010", "slice": "natural_answerable_20docs"}
{"query": "From 2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt, what exact term fills [BLANK] in: \"36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2201.11903v6 [cs.CL] 10 Jan 2023 1 [BLANK] Math\"?", "gold": "introduction", "relevant_doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "relevant_chunk_id": 4, "relevant_text": "36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2201.11903v6 [cs.CL] 10 Jan 2023 1 Introduction Math", "case_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models_018", "slice": "natural_answerable_20docs"}
{"query": "From 1706.03762__attention_is_all_you_need.txt, what exact term fills [BLANK] in: \"In practice, we compute the attention function on a set of queries [BLANK], packed together into a matrix Q.\"?", "gold": "simultaneously", "relevant_doc_id": "1706.03762__attention_is_all_you_need.txt", "relevant_chunk_id": 21, "relevant_text": "In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q.", "case_id": "1706.03762__attention_is_all_you_need_001", "slice": "natural_answerable_20docs"}
{"query": "From 2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt, what exact term fills [BLANK] in: \"cusing on enhancing language models by [BLANK] additional knowledge through Pre- Training Models (PTM).\"?", "gold": "incorporating", "relevant_doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "relevant_chunk_id": 7, "relevant_text": "cusing on enhancing language models by incorporating additional knowledge through Pre- Training Models (PTM).", "case_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey_005", "slice": "natural_answerable_20docs"}
{"query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"They also, [BLANK], struggle with basic functionality, such as arithmetic or fac- tual lookup, where much simpler and smaller models\"?", "gold": "paradoxically", "relevant_doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "relevant_chunk_id": 0, "relevant_text": "They also, paradoxically, struggle with basic functionality, such as arithmetic or fac- tual lookup, where much simpler and smaller models", "case_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools_009", "slice": "natural_answerable_20docs"}
{"query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \"The Ô¨Ånal hidden state corresponding to this token is used as the ag- gregate sequence [BLANK] for classiÔ¨Åcation tasks.\"?", "gold": "representation", "relevant_doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "relevant_chunk_id": 28, "relevant_text": "The Ô¨Ånal hidden state corresponding to this token is used as the ag- gregate sequence representation for classiÔ¨Åcation tasks.", "case_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding_014", "slice": "natural_answerable_20docs"}
{"query": "From 2005.14165__language_models_are_few_shot_learners.txt, what exact term fills [BLANK] in: \"There exists a very wide range of possible useful language tasks, [BLANK] anything from correcting grammar, to generating examples of an abstract concept, to critiquing a short story.\"?", "gold": "encompassing", "relevant_doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "relevant_chunk_id": 14, "relevant_text": "There exists a very wide range of possible useful language tasks, encompassing anything from correcting grammar, to generating examples of an abstract concept, to critiquing a short story.", "case_id": "2005.14165__language_models_are_few_shot_learners_015", "slice": "natural_answerable_20docs"}
{"query": "From 2305.06983__active_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"t al., 2017), where the informa- tion needs are clear in the user‚Äôs input, and it is [BLANK] to retrieve relevant knowledge once solely based on the input .\"?", "gold": "sufficient", "relevant_doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "relevant_chunk_id": 7, "relevant_text": "t al., 2017), where the informa- tion needs are clear in the user‚Äôs input, and it is sufficient to retrieve relevant knowledge once solely based on the input .", "case_id": "2305.06983__active_retrieval_augmented_generation_010", "slice": "natural_answerable_20docs"}
{"query": "From 2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt, what exact term fills [BLANK] in: \"For each of the queries, we need a document that contains the answer and k‚àí1[BLANK] documents that do not contain the answer.\"?", "gold": "distractor", "relevant_doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "relevant_chunk_id": 25, "relevant_text": "For each of the queries, we need a document that contains the answer and k‚àí1distractor documents that do not contain the answer.", "case_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts_019", "slice": "natural_answerable_20docs"}
{"query": "From 2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt, what exact term fills [BLANK] in: \"We [BLANK] run experiments on subset of unam- biguous questions, finding similar results and conclusions; see Appendix A.\"?", "gold": "additionally", "relevant_doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "relevant_chunk_id": 28, "relevant_text": "We additionally run experiments on subset of unam- biguous questions, finding similar results and conclusions; see Appendix A.", "case_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts_001", "slice": "natural_answerable_20docs"}
{"query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \"Second, we add a learned embed- ding to every token [BLANK] whether it belongs to sentence Aor sentence B.\"?", "gold": "indicating", "relevant_doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "relevant_chunk_id": 29, "relevant_text": "Second, we add a learned embed- ding to every token indicating whether it belongs to sentence Aor sentence B.", "case_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding_003", "slice": "natural_answerable_20docs"}
{"query": "From 2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"n this way, although it comes with the [BLANK] of being sensitive to the design of the prompt.\"?", "gold": "limitation", "relevant_doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "relevant_chunk_id": 17, "relevant_text": "n this way, although it comes with the limitation of being sensitive to the design of the prompt.", "case_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation_002", "slice": "natural_answerable_20docs"}
{"query": "From 2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"For models that do not provide access to token [BLANK], such as ChatGPT and GPT-4, differ- ent methods are needed.\"?", "gold": "probabilities", "relevant_doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "relevant_chunk_id": 14, "relevant_text": "For models that do not provide access to token probabilities, such as ChatGPT and GPT-4, differ- ent methods are needed.", "case_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation_012", "slice": "natural_answerable_20docs"}
{"query": "From 2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt, what exact term fills [BLANK] in: \"These results could be comparable to the strongest fully trained RAG models like Atlas([BLANK].,2022)andMDR(Xiongetal.,2020b).\"?", "gold": "izacardetal", "relevant_doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "relevant_chunk_id": 17, "relevant_text": "These results could be comparable to the strongest fully trained RAG models like Atlas(Izacardetal.,2022)andMDR(Xiongetal.,2020b).", "case_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms_011", "slice": "natural_answerable_20docs"}
{"query": "From 2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt, what exact term fills [BLANK] in: \"(1) Although more expressive model forms for measur- ing the similarity between a question and a passage do exist, such as networks consisting of multiple layers of cross attentions, the similarity function needs to be [BLANK] so that the represen- tation\"?", "gold": "decomposable", "relevant_doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "relevant_chunk_id": 20, "relevant_text": "(1) Although more expressive model forms for measur- ing the similarity between a question and a passage do exist, such as networks consisting of multiple layers of cross attentions, the similarity function needs to be decomposable so that the represen- tation", "case_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering_008", "slice": "natural_answerable_20docs"}
{"query": "From 2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt, what exact term fills [BLANK] in: \"By meticulously curating and optimizing the training dataset, researchers can [BLANK] reduce the model‚Äôs size without compromising its performance.\"?", "gold": "significantly", "relevant_doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "relevant_chunk_id": 8, "relevant_text": "By meticulously curating and optimizing the training dataset, researchers can significantly reduce the model‚Äôs size without compromising its performance.", "case_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone_015", "slice": "natural_answerable_20docs"}
{"query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"You can call the API by writing \"[QA([BLANK])]\" where \"question\" is the question you want to ask.\"?", "gold": "question", "relevant_doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "relevant_chunk_id": 18, "relevant_text": "You can call the API by writing \"[QA(question)]\" where \"question\" is the question you want to ask.", "case_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools_002", "slice": "natural_answerable_20docs"}
{"query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \"In addi- tion to the masked language model, we also use a ‚Äúnext sentence prediction‚Äù task that jointly pre- trains text-pair [BLANK].\"?", "gold": "representations", "relevant_doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "relevant_chunk_id": 9, "relevant_text": "In addi- tion to the masked language model, we also use a ‚Äúnext sentence prediction‚Äù task that jointly pre- trains text-pair representations.", "case_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding_005", "slice": "natural_answerable_20docs"}
{"query": "From 2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt, what exact term fills [BLANK] in: \"2 1 Introduction Large Language Models (LLMs) are enabling more natural and [BLANK] interactions between humans and machines, enhancing user experience in existing appli\"?", "gold": "sophisticated", "relevant_doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "relevant_chunk_id": 11, "relevant_text": "2 1 Introduction Large Language Models (LLMs) are enabling more natural and sophisticated interactions between humans and machines, enhancing user experience in existing appli", "case_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason_004", "slice": "natural_answerable_20docs"}
{"query": "From 2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt, what exact term fills [BLANK] in: \"Our training dataset is a scaled-up version of the one used for phi-2 , composed of heavily filtered publicly available web data and [BLANK] data.\"?", "gold": "synthetic", "relevant_doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "relevant_chunk_id": 1, "relevant_text": "Our training dataset is a scaled-up version of the one used for phi-2 , composed of heavily filtered publicly available web data and synthetic data.", "case_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone_009", "slice": "natural_answerable_20docs"}
{"query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \"(2018), which uses unidirec- tional language models for pre-training, BERT uses masked language models to enable pre- trained deep bidirectional [BLANK].\"?", "gold": "representations", "relevant_doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "relevant_chunk_id": 10, "relevant_text": "(2018), which uses unidirec- tional language models for pre-training, BERT uses masked language models to enable pre- trained deep bidirectional representations.", "case_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding_015", "slice": "natural_answerable_20docs"}
{"query": "From 2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt, what exact term fills [BLANK] in: \"So the answer is no.Q: The concert was [BLANK] to be on 06/01/1943, but was delayed by one day to today.\"?", "gold": "scheduled", "relevant_doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "relevant_chunk_id": 25, "relevant_text": "So the answer is no.Q: The concert was scheduled to be on 06/01/1943, but was delayed by one day to today.", "case_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models_003", "slice": "natural_answerable_20docs"}
{"query": "From 2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt, what exact term fills [BLANK] in: \"We perform ablation studies in [BLANK] 3.5 to prove why longer retrieval units are necessary.\"?", "gold": "subsection", "relevant_doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "relevant_chunk_id": 18, "relevant_text": "We perform ablation studies in subsection 3.5 to prove why longer retrieval units are necessary.", "case_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms_008", "slice": "natural_answerable_20docs"}
{"query": "From 2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt, what exact term fills [BLANK] in: \"User: Okay now more serious answer, and note that this was achieved solely by [BLANK] the training data.\"?", "gold": "changing", "relevant_doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "relevant_chunk_id": 7, "relevant_text": "User: Okay now more serious answer, and note that this was achieved solely by changing the training data.", "case_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone_005", "slice": "natural_answerable_20docs"}
{"query": "From 2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt, what exact term fills [BLANK] in: \"[BLANK] Prompting Elicits Reasoning in Large Language Models Jason Wei Xuezhi Wang Dale Schuurmans Maarten Bosma Brian Ichter Fei Xia Ed H.\"?", "gold": "chain-of-thought", "relevant_doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "relevant_chunk_id": 0, "relevant_text": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models Jason Wei Xuezhi Wang Dale Schuurmans Maarten Bosma Brian Ichter Fei Xia Ed H.", "case_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models_009", "slice": "natural_answerable_20docs"}
{"query": "From 2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt, what exact term fills [BLANK] in: \"To accommodate the context [BLANK] of language models, text is segmented into smaller, digestible chunks.\"?", "gold": "limitations", "relevant_doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "relevant_chunk_id": 20, "relevant_text": "To accommodate the context limitations of language models, text is segmented into smaller, digestible chunks.", "case_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey_020", "slice": "natural_answerable_20docs"}
{"query": "From 2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt, what exact term fills [BLANK] in: \"Second, large language models offer the exciting prospect of [BLANK] few-shot learning via prompting .\"?", "gold": "in-context", "relevant_doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "relevant_chunk_id": 8, "relevant_text": "Second, large language models offer the exciting prospect of in-context few-shot learning via prompting .", "case_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models_005", "slice": "natural_answerable_20docs"}
{"query": "From 2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt, what exact term fills [BLANK] in: \"on two tasks that require identifying relevant information in their in- put contexts: [BLANK] question an- swering and key-value retrieval.\"?", "gold": "multi-document", "relevant_doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "relevant_chunk_id": 1, "relevant_text": "on two tasks that require identifying relevant information in their in- put contexts: multi-document question an- swering and key-value retrieval.", "case_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts_013", "slice": "natural_answerable_20docs"}
{"query": "From 2005.14165__language_models_are_few_shot_learners.txt, what exact term fills [BLANK] in: \"g‚Äù, using the text input of a pretrained language model as a form of task speciÔ¨Åcation: the model is conditioned on a natural language instruction and/or a few [BLANK] of the task and is then expected to complete further instances of the task simply by predicting what comes next.\"?", "gold": "demonstrations", "relevant_doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "relevant_chunk_id": 22, "relevant_text": "g‚Äù, using the text input of a pretrained language model as a form of task speciÔ¨Åcation: the model is conditioned on a natural language instruction and/or a few demonstrations of the task and is then expected to complete further instances of the task simply by predicting what comes next.", "case_id": "2005.14165__language_models_are_few_shot_learners_016", "slice": "natural_answerable_20docs"}
{"query": "From 2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt, what exact term fills [BLANK] in: \"For example, synonyms or [BLANK] that consist of completely different tokens may still be mapped to vectors close to each other.\"?", "gold": "paraphrases", "relevant_doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "relevant_chunk_id": 5, "relevant_text": "For example, synonyms or paraphrases that consist of completely different tokens may still be mapped to vectors close to each other.", "case_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering_018", "slice": "natural_answerable_20docs"}
{"query": "From 2305.06983__active_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"It is a generic framework that actively decides when and what to retrieve through the generation process,resulting in the [BLANK] of retrieval and genera- tion.\"?", "gold": "interleaving", "relevant_doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "relevant_chunk_id": 22, "relevant_text": "It is a generic framework that actively decides when and what to retrieve through the generation process,resulting in the interleaving of retrieval and genera- tion.", "case_id": "2305.06983__active_retrieval_augmented_generation_006", "slice": "natural_answerable_20docs"}
{"query": "From 2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt, what exact term fills [BLANK] in: \"They often rely on small number of tasks or on using other models for auto-evaluation by asking [BLANK]‚Äúgivenresponsesfromsystem1 (reference)andsystem2(target), whichoneisbetter?‚Äù.\"?", "gold": "themtocomparetheoutputsoftwosystemswithapromptlike", "relevant_doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "relevant_chunk_id": 18, "relevant_text": "They often rely on small number of tasks or on using other models for auto-evaluation by asking themtocomparetheoutputsoftwosystemswithapromptlike‚Äúgivenresponsesfromsystem1 (reference)andsystem2(target), whichoneisbetter?‚Äù.", "case_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason_006", "slice": "natural_answerable_20docs"}
{"query": "From 2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt, what exact term fills [BLANK] in: \"We open-source [BLANK] to make it easier to build on this primitive.1 We empirically va\"?", "gold": "flashattention", "relevant_doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "relevant_chunk_id": 16, "relevant_text": "We open-source FlashAttention to make it easier to build on this primitive.1 We empirically va", "case_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness_002", "slice": "natural_answerable_20docs"}
{"query": "From 2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt, what exact term fills [BLANK] in: \"Such a design forces the [BLANK] to search over a large corpus to find the ‚Äúneedle‚Äù unit.\"?", "gold": "retriever", "relevant_doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "relevant_chunk_id": 0, "relevant_text": "Such a design forces the retriever to search over a large corpus to find the ‚Äúneedle‚Äù unit.", "case_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms_009", "slice": "natural_answerable_20docs"}
{"query": "From 2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt, what exact term fills [BLANK] in: \"Dense retrieval methods have thus never be shown to [BLANK] TF-IDF/BM25 for open- domain QA before ORQA (Lee et al., 2019), which\"?", "gold": "outperform", "relevant_doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "relevant_chunk_id": 7, "relevant_text": "Dense retrieval methods have thus never be shown to outperform TF-IDF/BM25 for open- domain QA before ORQA (Lee et al., 2019), which", "case_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering_007", "slice": "natural_answerable_20docs"}
{"query": "From 2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt, what exact term fills [BLANK] in: \"Our results indicate that prompting language models with longer input contexts is a trade-off‚Äî providing the language model with more informa- tion may help it perform the downstream task, but it also increases the amount of content that the model must reason over, [BLANK] decreasing ac- curacy.\"?", "gold": "potentially", "relevant_doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "relevant_chunk_id": 18, "relevant_text": "Our results indicate that prompting language models with longer input contexts is a trade-off‚Äî providing the language model with more informa- tion may help it perform the downstream task, but it also increases the amount of content that the model must reason over, potentially decreasing ac- curacy.", "case_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts_012", "slice": "natural_answerable_20docs"}
{"query": "From 2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt, what exact term fills [BLANK] in: \"For example, training on the context length of 8192 needs 16 √ócomputational costs in [BLANK] layers as that of 2048.\"?", "gold": "self-attention", "relevant_doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "relevant_chunk_id": 1, "relevant_text": "For example, training on the context length of 8192 needs 16 √ócomputational costs in self-attention layers as that of 2048.", "case_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models_013", "slice": "natural_answerable_20docs"}
{"query": "From 2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt, what exact term fills [BLANK] in: \"[BLANK] During inference time, we apply the passage encoder EPto all the passages and index them using FAISS (Johnson et al., 2017) ofÔ¨Çine.\"?", "gold": "inference", "relevant_doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "relevant_chunk_id": 23, "relevant_text": "Inference During inference time, we apply the passage encoder EPto all the passages and index them using FAISS (Johnson et al., 2017) ofÔ¨Çine.", "case_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering_003", "slice": "natural_answerable_20docs"}
{"query": "From 2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt, what exact term fills [BLANK] in: \"ets are composed of [BLANK] short (averaging less than 1K tokens) but vast Wikipedia documents.\"?", "gold": "relatively", "relevant_doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "relevant_chunk_id": 16, "relevant_text": "ets are composed of relatively short (averaging less than 1K tokens) but vast Wikipedia documents.", "case_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms_002", "slice": "natural_answerable_20docs"}
{"query": "From 2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt, what exact term fills [BLANK] in: \"In additional experiments, we show [BLANK] can robustly boost performance on NLP tasks where adding a chain-of-thought might hurt performance compared to standard prompting (Ye & Durrett, 2022).\"?", "gold": "self-consistency", "relevant_doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "relevant_chunk_id": 15, "relevant_text": "In additional experiments, we show self-consistency can robustly boost performance on NLP tasks where adding a chain-of-thought might hurt performance compared to standard prompting (Ye & Durrett, 2022).", "case_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models_017", "slice": "natural_answerable_20docs"}
{"query": "From 2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt, what exact term fills [BLANK] in: \"OurmethodÔ¨Årst [BLANK] a key-value database, where values store raw chunks of text tokens and keys\"?", "gold": "constructs", "relevant_doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "relevant_chunk_id": 14, "relevant_text": "OurmethodÔ¨Årst constructs a key-value database, where values store raw chunks of text tokens and keys", "case_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens_012", "slice": "natural_answerable_20docs"}
{"query": "From 2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt, what exact term fills [BLANK] in: \"a new [BLANK] to evaluate language models when an evaluation set is partially present in the training set.\"?", "gold": "methodology", "relevant_doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "relevant_chunk_id": 18, "relevant_text": "a new methodology to evaluate language models when an evaluation set is partially present in the training set.", "case_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens_006", "slice": "natural_answerable_20docs"}
{"query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"As [BLANK] in Figure 1, through this simple approach, LMs can learn to control a va- riety of tools, and to choose for themselves which tool to use when and how.\"?", "gold": "illustrated", "relevant_doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "relevant_chunk_id": 12, "relevant_text": "As illustrated in Figure 1, through this simple approach, LMs can learn to control a va- riety of tools, and to choose for themselves which tool to use when and how.", "case_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools_017", "slice": "natural_answerable_20docs"}
{"query": "From 1706.03762__attention_is_all_you_need.txt, what exact term fills [BLANK] in: \"Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in [BLANK] or scholarly works.\"?", "gold": "journalistic", "relevant_doc_id": "1706.03762__attention_is_all_you_need.txt", "relevant_chunk_id": 0, "relevant_text": "Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works.", "case_id": "1706.03762__attention_is_all_you_need_009", "slice": "natural_answerable_20docs"}
{"query": "From 2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt, what exact term fills [BLANK] in: \"3.1 Experimental Setup We explore [BLANK] prompting for various language models on multiple benchmarks.\"?", "gold": "chain-of-thought", "relevant_doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "relevant_chunk_id": 20, "relevant_text": "3.1 Experimental Setup We explore chain-of-thought prompting for various language models on multiple benchmarks.", "case_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models_019", "slice": "natural_answerable_20docs"}
{"query": "From 2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"In practice, RAG systems are often evaluated in terms of the language [BLANK] task itself, i.e.\"?", "gold": "modelling", "relevant_doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "relevant_chunk_id": 7, "relevant_text": "In practice, RAG systems are often evaluated in terms of the language modelling task itself, i.e.", "case_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation_007", "slice": "natural_answerable_20docs"}
{"query": "From 2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt, what exact term fills [BLANK] in: \"LongRAG [BLANK] each entire document as a single unit rather than chunking them into smaller units.\"?", "gold": "processes", "relevant_doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "relevant_chunk_id": 17, "relevant_text": "LongRAG processes each entire document as a single unit rather than chunking them into smaller units.", "case_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms_019", "slice": "natural_answerable_20docs"}
{"query": "From 2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt, what exact term fills [BLANK] in: \"By leveraging the now standard BERT pretrained model (Devlin et al., 2019) and a [BLANK] architecture (Bromley et al., 1994), we focus on developing the right training scheme using a relatively small number of question and passage pairs.\"?", "gold": "dual-encoder", "relevant_doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "relevant_chunk_id": 10, "relevant_text": "By leveraging the now standard BERT pretrained model (Devlin et al., 2019) and a dual-encoder architecture (Bromley et al., 1994), we focus on developing the right training scheme using a relatively small number of question and passage pairs.", "case_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering_004", "slice": "natural_answerable_20docs"}
{"query": "From 2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt, what exact term fills [BLANK] in: \"These datasets consist of a variety of NLP tasks, combined with natural language [BLANK] for each task.\"?", "gold": "instructions", "relevant_doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "relevant_chunk_id": 22, "relevant_text": "These datasets consist of a variety of NLP tasks, combined with natural language instructions for each task.", "case_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback_020", "slice": "natural_answerable_20docs"}
{"query": "From 1706.03762__attention_is_all_you_need.txt, what exact term fills [BLANK] in: \"Additive attention computes the [BLANK] function using a feed-forward network with a single hidden layer.\"?", "gold": "compatibility", "relevant_doc_id": "1706.03762__attention_is_all_you_need.txt", "relevant_chunk_id": 22, "relevant_text": "Additive attention computes the compatibility function using a feed-forward network with a single hidden layer.", "case_id": "1706.03762__attention_is_all_you_need_003", "slice": "natural_answerable_20docs"}
{"query": "From 2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt, what exact term fills [BLANK] in: \"The reason behind this is that short attention resembles the attention scheme in the [BLANK] stage of LLMs.\"?", "gold": "pre-training", "relevant_doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "relevant_chunk_id": 15, "relevant_text": "The reason behind this is that short attention resembles the attention scheme in the pre-training stage of LLMs.", "case_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models_016", "slice": "natural_answerable_20docs"}
{"query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \"Unlike left-to- right language model pre-training, the MLM ob- jective enables the [BLANK] to fuse the l\"?", "gold": "representation", "relevant_doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "relevant_chunk_id": 8, "relevant_text": "Unlike left-to- right language model pre-training, the MLM ob- jective enables the representation to fuse the l", "case_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding_007", "slice": "natural_answerable_20docs"}
{"query": "From 2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt, what exact term fills [BLANK] in: \"A prompting only approach is important because it does not require a large training dataset and because a single model checkpoint can perform many tasks without loss of [BLANK].\"?", "gold": "generality", "relevant_doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "relevant_chunk_id": 12, "relevant_text": "A prompting only approach is important because it does not require a large training dataset and because a single model checkpoint can perform many tasks without loss of generality.", "case_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models_017", "slice": "natural_answerable_20docs"}
{"query": "From 2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt, what exact term fills [BLANK] in: \"The idea of ReAct is simple: we augment the agent‚Äôs action space to ÀÜA=A‚à™L , [BLANK] the space of language.\"?", "gold": "wherelis", "relevant_doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "relevant_chunk_id": 28, "relevant_text": "The idea of ReAct is simple: we augment the agent‚Äôs action space to ÀÜA=A‚à™L , whereLis the space of language.", "case_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models_002", "slice": "natural_answerable_20docs"}
{"query": "From 2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt, what exact term fills [BLANK] in: \"[BLANK] Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases.\"?", "gold": "retrieval-augmented", "relevant_doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "relevant_chunk_id": 1, "relevant_text": "Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases.", "case_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey_013", "slice": "natural_answerable_20docs"}
{"query": "From 2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt, what exact term fills [BLANK] in: \"The system [BLANK] and retrieves the top K chunks that demonstrate the greatest similarity to the query.\"?", "gold": "prioritizes", "relevant_doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "relevant_chunk_id": 22, "relevant_text": "The system prioritizes and retrieves the top K chunks that demonstrate the greatest similarity to the query.", "case_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey_014", "slice": "natural_answerable_20docs"}
{"query": "From 2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"An- other approach directly asks ChatGPT to evaluate a [BLANK] aspect of the given answer by provid- ing a score between 0 and 100, or by providing a rating on a 5-star scale (Wang et al., 2023a).\"?", "gold": "particular", "relevant_doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "relevant_chunk_id": 16, "relevant_text": "An- other approach directly asks ChatGPT to evaluate a particular aspect of the given answer by provid- ing a score between 0 and 100, or by providing a rating on a 5-star scale (Wang et al., 2023a).", "case_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation_006", "slice": "natural_answerable_20docs"}
{"query": "From 2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt, what exact term fills [BLANK] in: \"[BLANK] of phi-3 went through two stages, including supervised finetuning (SFT) and direct preference optimization (DPO).\"?", "gold": "post-training", "relevant_doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "relevant_chunk_id": 24, "relevant_text": "Post-training of phi-3 went through two stages, including supervised finetuning (SFT) and direct preference optimization (DPO).", "case_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone_014", "slice": "natural_answerable_20docs"}
{"query": "From 2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt, what exact term fills [BLANK] in: \"On the other hand, recent work has explored the use of pre-trained language models for planning and acting in interactive [BLANK] (Ahn et al., 2022; Nakano et al., 2021; Yao et al., 2020; Huang et al., 2022a), with a focus on predicting actions via language priors.\"?", "gold": "environments", "relevant_doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "relevant_chunk_id": 17, "relevant_text": "On the other hand, recent work has explored the use of pre-trained language models for planning and acting in interactive environments (Ahn et al., 2022; Nakano et al., 2021; Yao et al., 2020; Huang et al., 2022a), with a focus on predicting actions via language priors.", "case_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models_005", "slice": "natural_answerable_20docs"}
{"query": "From 2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"2 Related Work Estimating faithfulness using LLMs The prob- lem of detecting [BLANK] in LLM generated responses has been extensively studied (Ji et al., 2023).\"?", "gold": "hallucinations", "relevant_doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "relevant_chunk_id": 10, "relevant_text": "2 Related Work Estimating faithfulness using LLMs The prob- lem of detecting hallucinations in LLM generated responses has been extensively studied (Ji et al., 2023).", "case_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation_004", "slice": "natural_answerable_20docs"}
{"query": "From 2305.06983__active_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"3.1 FLARE with Retrieval Instructions Inspired by Toolformer (Schick et al., 2023), a [BLANK] way of expressing information needs for retrieval is to generate ‚Äú[Se\"?", "gold": "straightforward", "relevant_doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "relevant_chunk_id": 26, "relevant_text": "3.1 FLARE with Retrieval Instructions Inspired by Toolformer (Schick et al., 2023), a straightforward way of expressing information needs for retrieval is to generate ‚Äú[Se", "case_id": "2305.06983__active_retrieval_augmented_generation_020", "slice": "natural_answerable_20docs"}
{"query": "From 2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt, what exact term fills [BLANK] in: \"relevant [BLANK] from a search engine, database query results, etc; Petroni et al., 2020; Ram et al., 2023; Shi et al., 2023; Mallen et al., 2023; Schick et al., 2023, inter alia ).\"?", "gold": "documents", "relevant_doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "relevant_chunk_id": 6, "relevant_text": "relevant documents from a search engine, database query results, etc; Petroni et al., 2020; Ram et al., 2023; Shi et al., 2023; Mallen et al., 2023; Schick et al., 2023, inter alia ).", "case_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts_010", "slice": "natural_answerable_20docs"}
{"query": "From 2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt, what exact term fills [BLANK] in: \"[BLANK]:2004.04906v3 [cs.CL] 30 Sep 2020 QA datasets, it also suffers from two weaknesses.\"?", "gold": "open-domainarxiv", "relevant_doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "relevant_chunk_id": 9, "relevant_text": "open-domainarXiv:2004.04906v3 [cs.CL] 30 Sep 2020 QA datasets, it also suffers from two weaknesses.", "case_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering_015", "slice": "natural_answerable_20docs"}
{"query": "From 2005.14165__language_models_are_few_shot_learners.txt, what exact term fills [BLANK] in: \"This last paradigm has led to substantial progress on many challenging NLP tasks such as reading [BLANK], question answering, textual entailment, and many others, and has continued to advance based on new architectures and algorithms [ RSR+19,LOG+19,YDY+19,LCG+19].\"?", "gold": "comprehension", "relevant_doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "relevant_chunk_id": 12, "relevant_text": "This last paradigm has led to substantial progress on many challenging NLP tasks such as reading comprehension, question answering, textual entailment, and many others, and has continued to advance based on new architectures and algorithms [ RSR+19,LOG+19,YDY+19,LCG+19].", "case_id": "2005.14165__language_models_are_few_shot_learners_007", "slice": "natural_answerable_20docs"}
{"query": "From 2305.06983__active_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"Therefore, we propose anticipating the future by generating a temporary next sentence, using it as a query to retrieve rel- evant documents, and then [BLANK] the next sentence conditioning on the retrieved documents.\"?", "gold": "regenerating", "relevant_doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "relevant_chunk_id": 16, "relevant_text": "Therefore, we propose anticipating the future by generating a temporary next sentence, using it as a query to retrieve rel- evant documents, and then regenerating the next sentence conditioning on the retrieved documents.", "case_id": "2305.06983__active_retrieval_augmented_generation_017", "slice": "natural_answerable_20docs"}
{"query": "From 2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt, what exact term fills [BLANK] in: \"In our experiments, we adopt [BLANK] retrievers like BGE (Xiao et al., 2023) and readers like Gemini- 1.5-Pro (Reid et al., 2024) or GPT-4o (OpenAI, 2024) without any further tuning.\"?", "gold": "off-the-shelf", "relevant_doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "relevant_chunk_id": 15, "relevant_text": "In our experiments, we adopt off-the-shelf retrievers like BGE (Xiao et al., 2023) and readers like Gemini- 1.5-Pro (Reid et al., 2024) or GPT-4o (OpenAI, 2024) without any further tuning.", "case_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms_006", "slice": "natural_answerable_20docs"}
{"query": "From 2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt, what exact term fills [BLANK] in: \"We use a chunked [BLANK] module to incorporate the retrieved text (¬ß2.4), with time complexity linear in the amount of retrieved data.\"?", "gold": "cross-attention", "relevant_doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "relevant_chunk_id": 11, "relevant_text": "We use a chunked cross-attention module to incorporate the retrieved text (¬ß2.4), with time complexity linear in the amount of retrieved data.", "case_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens_004", "slice": "natural_answerable_20docs"}
{"query": "From 2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt, what exact term fills [BLANK] in: \"Generation (RAG) enhances LLMs by retrieving relevant document chunks from external knowledge base through semantic [BLANK] calcu- lation.\"?", "gold": "similarity", "relevant_doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "relevant_chunk_id": 5, "relevant_text": "Generation (RAG) enhances LLMs by retrieving relevant document chunks from external knowledge base through semantic similarity calcu- lation.", "case_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey_010", "slice": "natural_answerable_20docs"}
{"query": "From 2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt, what exact term fills [BLANK] in: \"HU\u0003RQ\u0003D\u0003GUDZHU\u0011 \u0015 \u0003$OI:RUOG \u0014 \u0003+RWVSRW\u00034$ Figure 1: (1) Comparison of 4 prompting methods, (a) Standard , (b) [BLANK] ( CoT, Reason Only), (c) Act-only, and (d) ReAct (Reason+Act), solving a HotpotQA (Yang et al., 2018) question.\"?", "gold": "chain-of-thought", "relevant_doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "relevant_chunk_id": 15, "relevant_text": "HU\u0003RQ\u0003D\u0003GUDZHU\u0011 \u0015 \u0003$OI:RUOG \u0014 \u0003+RWVSRW\u00034$ Figure 1: (1) Comparison of 4 prompting methods, (a) Standard , (b) Chain-of-thought ( CoT, Reason Only), (c) Act-only, and (d) ReAct (Reason+Act), solving a HotpotQA (Yang et al., 2018) question.", "case_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models_010", "slice": "natural_answerable_20docs"}
{"query": "From 2005.14165__language_models_are_few_shot_learners.txt, what exact term fills [BLANK] in: \"These terms are intended to remain agnostic on the question of whether the model learns new tasks from scratch at [BLANK]\"?", "gold": "inference", "relevant_doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "relevant_chunk_id": 26, "relevant_text": "These terms are intended to remain agnostic on the question of whether the model learns new tasks from scratch at inference", "case_id": "2005.14165__language_models_are_few_shot_learners_008", "slice": "natural_answerable_20docs"}
{"query": "From 2005.14165__language_models_are_few_shot_learners.txt, what exact term fills [BLANK] in: \"Ziegler Jeffrey Wu Clemens Winter [BLANK] Hesse Mark Chen Eric Sigler Mateusz Litwin Scott Gray Benjamin Chess Jack Clark Christopher Berner Sam McCandlish Alec Radford Ilya Sutskever Dario Amodei OpenAI Abstract Recent wo\"?", "gold": "christopher", "relevant_doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "relevant_chunk_id": 0, "relevant_text": "Ziegler Jeffrey Wu Clemens Winter Christopher Hesse Mark Chen Eric Sigler Mateusz Litwin Scott Gray Benjamin Chess Jack Clark Christopher Berner Sam McCandlish Alec Radford Ilya Sutskever Dario Amodei OpenAI Abstract Recent wo", "case_id": "2005.14165__language_models_are_few_shot_learners_013", "slice": "natural_answerable_20docs"}
{"query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \", 2018), BERT is designed to pre- train deep bidirectional [BLANK] from unlabeled text by jointly conditioning on both left and right context in all layers.\"?", "gold": "representations", "relevant_doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "relevant_chunk_id": 1, "relevant_text": ", 2018), BERT is designed to pre- train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.", "case_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding_013", "slice": "natural_answerable_20docs"}
{"query": "From 2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt, what exact term fills [BLANK] in: \"Token likelihoods are provided by a model, [BLANK] by ùúÉ, that takes as input both previous tokens and their retrieved neighbours.\"?", "gold": "parameterized", "relevant_doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "relevant_chunk_id": 23, "relevant_text": "Token likelihoods are provided by a model, parameterized by ùúÉ, that takes as input both previous tokens and their retrieved neighbours.", "case_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens_014", "slice": "natural_answerable_20docs"}
{"query": "From 2305.06983__active_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"Augmenting LMs with retrieval components that look up relevant in- formation from external knowledge resources is a promising direction to address [BLANK] (Khan- delwal et al., 2020; Izacard et al., 2022).\"?", "gold": "hallucination", "relevant_doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "relevant_chunk_id": 5, "relevant_text": "Augmenting LMs with retrieval components that look up relevant in- formation from external knowledge resources is a promising direction to address hallucination (Khan- delwal et al., 2020; Izacard et al., 2022).", "case_id": "2305.06983__active_retrieval_augmented_generation_018", "slice": "natural_answerable_20docs"}
{"query": "From 2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt, what exact term fills [BLANK] in: \"1arXiv:2203.11171v4 [cs.CL] 7 Mar 2023 Published as a [BLANK] paper at ICLR 2023 Language model Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\"?", "gold": "conference", "relevant_doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "relevant_chunk_id": 8, "relevant_text": "1arXiv:2203.11171v4 [cs.CL] 7 Mar 2023 Published as a conference paper at ICLR 2023 Language model Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?", "case_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models_007", "slice": "natural_answerable_20docs"}
{"query": "From 1706.03762__attention_is_all_you_need.txt, what exact term fills [BLANK] in: \"Attention Is All You Need Ashish Vaswani‚àó Google Brain avaswani@google.comNoam Shazeer‚àó Google Brain noam@google.comNiki Parmar‚àó Google Research nikip@google.comJakob [BLANK]‚àó Google Research usz@google.com Llion Jones‚àó Google Research llion@google.comAidan N.\"?", "gold": "uszkoreit", "relevant_doc_id": "1706.03762__attention_is_all_you_need.txt", "relevant_chunk_id": 0, "relevant_text": "Attention Is All You Need Ashish Vaswani‚àó Google Brain avaswani@google.comNoam Shazeer‚àó Google Brain noam@google.comNiki Parmar‚àó Google Research nikip@google.comJakob Uszkoreit‚àó Google Research usz@google.com Llion Jones‚àó Google Research llion@google.comAidan N.", "case_id": "1706.03762__attention_is_all_you_need_013", "slice": "natural_answerable_20docs"}
{"query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"o different [BLANK] of this loss: L+ i=Li(e(ci,ri)) L‚àí i= min (Li(Œµ),Li(e(ci,Œµ))) whereŒµdenotes an empty sequence.\"?", "gold": "instantiations", "relevant_doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "relevant_chunk_id": 24, "relevant_text": "o different instantiations of this loss: L+ i=Li(e(ci,ri)) L‚àí i= min (Li(Œµ),Li(e(ci,Œµ))) whereŒµdenotes an empty sequence.", "case_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools_014", "slice": "natural_answerable_20docs"}
{"query": "From 2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt, what exact term fills [BLANK] in: \"RAG research shifted towards providing better information for LLMs to answer more com- plex and [BLANK] tasks during the inference stage, leading to\"?", "gold": "knowledge-intensive", "relevant_doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "relevant_chunk_id": 7, "relevant_text": "RAG research shifted towards providing better information for LLMs to answer more com- plex and knowledge-intensive tasks during the inference stage, leading to", "case_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey_015", "slice": "natural_answerable_20docs"}
{"query": "From 2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt, what exact term fills [BLANK] in: \"ap by mapping out the RAG process and charting its evolution and anticipated future paths, with a focus on the [BLANK] of RAG within LLMs.\"?", "gold": "integration", "relevant_doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "relevant_chunk_id": 9, "relevant_text": "ap by mapping out the RAG process and charting its evolution and anticipated future paths, with a focus on the integration of RAG within LLMs.", "case_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey_004", "slice": "natural_answerable_20docs"}
{"query": "From 2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt, what exact term fills [BLANK] in: \"lock-sparse FlashAttention , a sparse attention algorithm that is 2-4 \u0002faster than [BLANK] , scaling up to sequence length of 64k.\"?", "gold": "evenflashattention", "relevant_doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "relevant_chunk_id": 16, "relevant_text": "lock-sparse FlashAttention , a sparse attention algorithm that is 2-4 \u0002faster than evenFlashAttention , scaling up to sequence length of 64k.", "case_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness_016", "slice": "natural_answerable_20docs"}
{"query": "From 2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt, what exact term fills [BLANK] in: \"The dataset consists of text documents from multiple sources and multiple languages [BLANK] over 5 trillion tokens (detailed\"?", "gold": "totalling", "relevant_doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "relevant_chunk_id": 18, "relevant_text": "The dataset consists of text documents from multiple sources and multiple languages totalling over 5 trillion tokens (detailed", "case_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens_002", "slice": "natural_answerable_20docs"}
{"query": "From 2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt, what exact term fills [BLANK] in: \"dels (LLMs) showcase impres- sive capabilities but encounter challenges like hallucination, outdated knowledge, and [BLANK], untraceable reasoning processes.\"?", "gold": "non-transparent", "relevant_doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "relevant_chunk_id": 1, "relevant_text": "dels (LLMs) showcase impres- sive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes.", "case_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey_009", "slice": "natural_answerable_20docs"}
{"query": "From 2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt, what exact term fills [BLANK] in: \"We note that instruction tuning, while very beneficial for teaching the model how to solve a task, does not [BLANK] teach the model new knowledge.\"?", "gold": "necessarily", "relevant_doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "relevant_chunk_id": 24, "relevant_text": "We note that instruction tuning, while very beneficial for teaching the model how to solve a task, does not necessarily teach the model new knowledge.", "case_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason_014", "slice": "natural_answerable_20docs"}
{"query": "From 1706.03762__attention_is_all_you_need.txt, what exact term fills [BLANK] in: \"Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position [BLANK] and became the other person involved in nearly every detail.\"?", "gold": "representation", "relevant_doc_id": "1706.03762__attention_is_all_you_need.txt", "relevant_chunk_id": 4, "relevant_text": "Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail.", "case_id": "1706.03762__attention_is_all_you_need_010", "slice": "natural_answerable_20docs"}
{"query": "From 2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt, what exact term fills [BLANK] in: \"[BLANK] : Fast and Memory-EÔ¨Écient Exact Attention with IO-Awareness Tri Daoy, Daniel Y.\"?", "gold": "flashattention", "relevant_doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "relevant_chunk_id": 0, "relevant_text": "FlashAttention : Fast and Memory-EÔ¨Écient Exact Attention with IO-Awareness Tri Daoy, Daniel Y.", "case_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness_009", "slice": "natural_answerable_20docs"}
{"query": "From 2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt, what exact term fills [BLANK] in: \"We find that changing the position of relevant information in the input context can [BLANK] affect model performance, indicating that current language models do not robustly access and use i\"?", "gold": "substantially", "relevant_doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "relevant_chunk_id": 11, "relevant_text": "We find that changing the position of relevant information in the input context can substantially affect model performance, indicating that current language models do not robustly access and use i", "case_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts_015", "slice": "natural_answerable_20docs"}
{"query": "From 2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt, what exact term fills [BLANK] in: \"In Orca 2, we continue exploring how improved training signals can enhance smaller LMs‚Äô [BLANK] abilities.\"?", "gold": "reasoning", "relevant_doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "relevant_chunk_id": 1, "relevant_text": "In Orca 2, we continue exploring how improved training signals can enhance smaller LMs‚Äô reasoning abilities.", "case_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason_009", "slice": "natural_answerable_20docs"}
{"query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \"ELMo and its predecessor (Peters et al., 2017, 2018a) generalize [BLANK] word embedding re- search along a diffe\"?", "gold": "traditional", "relevant_doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "relevant_chunk_id": 14, "relevant_text": "ELMo and its predecessor (Peters et al., 2017, 2018a) generalize traditional word embedding re- search along a diffe", "case_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding_017", "slice": "natural_answerable_20docs"}
{"query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \"The same [BLANK] model parameters are used to initialize models for different down-stream tasks.\"?", "gold": "pre-trained", "relevant_doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "relevant_chunk_id": 20, "relevant_text": "The same pre-trained model parameters are used to initialize models for different down-stream tasks.", "case_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding_016", "slice": "natural_answerable_20docs"}
{"query": "From 2302.13971__llama_open_and_efficient_foundation_language_models.txt, what exact term fills [BLANK] in: \"First, we use an efÔ¨Åcient [BLANK] of the causal multi-head attention to reduce memory usage and runtime.\"?", "gold": "implementation", "relevant_doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "relevant_chunk_id": 24, "relevant_text": "First, we use an efÔ¨Åcient implementation of the causal multi-head attention to reduce memory usage and runtime.", "case_id": "2302.13971__llama_open_and_efficient_foundation_language_models_014", "slice": "natural_answerable_20docs"}
{"query": "From 2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"In our [BLANK] and experiments, all prompts are evaluated using the gpt-3.5-turbo-16k model, which is available through the OpenAI API2.\"?", "gold": "implementation", "relevant_doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "relevant_chunk_id": 23, "relevant_text": "In our implementation and experiments, all prompts are evaluated using the gpt-3.5-turbo-16k model, which is available through the OpenAI API2.", "case_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation_003", "slice": "natural_answerable_20docs"}
{"query": "From 2005.14165__language_models_are_few_shot_learners.txt, what exact term fills [BLANK] in: \"Figure 1.2 [BLANK] the conditions we study, and shows few-shot learning of a simple task requiring the model to remove extraneous symbols from a word.\"?", "gold": "illustrates", "relevant_doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "relevant_chunk_id": 30, "relevant_text": "Figure 1.2 illustrates the conditions we study, and shows few-shot learning of a simple task requiring the model to remove extraneous symbols from a word.", "case_id": "2005.14165__language_models_are_few_shot_learners_014", "slice": "natural_answerable_20docs"}
{"query": "From 2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt, what exact term fills [BLANK] in: \"For example, the agent shown in Figure 1(1c) is unable to generate the correct Ô¨Ånal action (Act 4) to Ô¨Ånish the QA task as it requires complex reasoning over the [BLANK] context (Question, Act 1-3, Obs 1-3).\"?", "gold": "trajectory", "relevant_doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "relevant_chunk_id": 27, "relevant_text": "For example, the agent shown in Figure 1(1c) is unable to generate the correct Ô¨Ånal action (Act 4) to Ô¨Ånish the QA task as it requires complex reasoning over the trajectory context (Question, Act 1-3, Obs 1-3).", "case_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models_006", "slice": "natural_answerable_20docs"}
{"query": "From 1706.03762__attention_is_all_you_need.txt, what exact term fills [BLANK] in: \"In the following sections, we will describe the Transformer, motivate [BLANK] and discuss its advantages over models such as [17, 18] and [9].\"?", "gold": "self-attention", "relevant_doc_id": "1706.03762__attention_is_all_you_need.txt", "relevant_chunk_id": 14, "relevant_text": "In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9].", "case_id": "1706.03762__attention_is_all_you_need_012", "slice": "natural_answerable_20docs"}
{"query": "From 2305.06983__active_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"Augmenting LMs by [BLANK] informa- tion from external knowledge resources is one promising solution.\"?", "gold": "retrieving", "relevant_doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "relevant_chunk_id": 1, "relevant_text": "Augmenting LMs by retrieving informa- tion from external knowledge resources is one promising solution.", "case_id": "2305.06983__active_retrieval_augmented_generation_009", "slice": "natural_answerable_20docs"}
{"query": "From 2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt, what exact term fills [BLANK] in: \"Race tracks, desert, apartments, and [BLANK] don't have a lot of people, but populated areas do.\"?", "gold": "roadblocks", "relevant_doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "relevant_chunk_id": 25, "relevant_text": "Race tracks, desert, apartments, and roadblocks don't have a lot of people, but populated areas do.", "case_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models_014", "slice": "natural_answerable_20docs"}
{"query": "From 2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt, what exact term fills [BLANK] in: \"Inner product search has been widely used and studied, as well as its connection to cosine [BLANK] and L2 distance (Mussmann and Ermon, 2016; Ram\"?", "gold": "similarity", "relevant_doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "relevant_chunk_id": 21, "relevant_text": "Inner product search has been widely used and studied, as well as its connection to cosine similarity and L2 distance (Mussmann and Ermon, 2016; Ram", "case_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering_001", "slice": "natural_answerable_20docs"}
{"query": "From 2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt, what exact term fills [BLANK] in: \"As compute has gotten faster relative to memory speed [ 61,62,63], operations are [BLANK] bottlenecked by me\"?", "gold": "increasingly", "relevant_doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "relevant_chunk_id": 22, "relevant_text": "As compute has gotten faster relative to memory speed [ 61,62,63], operations are increasingly bottlenecked by me", "case_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness_014", "slice": "natural_answerable_20docs"}
{"query": "From 2005.14165__language_models_are_few_shot_learners.txt, what exact term fills [BLANK] in: \"For all tasks, GPT-3 is applied without any gradient updates or Ô¨Åne-tuning, with tasks and few-shot [BLANK] speciÔ¨Åed purely via text interaction with the model.\"?", "gold": "demonstrations", "relevant_doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "relevant_chunk_id": 3, "relevant_text": "For all tasks, GPT-3 is applied without any gradient updates or Ô¨Åne-tuning, with tasks and few-shot demonstrations speciÔ¨Åed purely via text interaction with the model.", "case_id": "2005.14165__language_models_are_few_shot_learners_018", "slice": "natural_answerable_20docs"}
{"query": "From 2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt, what exact term fills [BLANK] in: \"4Exceptions include (Seo et al., 2019) and (Roberts et al., 2020), which retrieves andgenerates the answers, [BLANK].\"?", "gold": "respectively", "relevant_doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "relevant_chunk_id": 19, "relevant_text": "4Exceptions include (Seo et al., 2019) and (Roberts et al., 2020), which retrieves andgenerates the answers, respectively.", "case_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering_011", "slice": "natural_answerable_20docs"}
{"query": "From 2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt, what exact term fills [BLANK] in: \"‚Äù A [BLANK] system would have difÔ¨Åculty retrieving such a context, while a dense retrieval system would be able to better match ‚Äúbad guy‚Äù with ‚Äúvillain‚Äù and fetch the cor- rect context.\"?", "gold": "term-based", "relevant_doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "relevant_chunk_id": 6, "relevant_text": "‚Äù A term-based system would have difÔ¨Åculty retrieving such a context, while a dense retrieval system would be able to better match ‚Äúbad guy‚Äù with ‚Äúvillain‚Äù and fetch the cor- rect context.", "case_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering_010", "slice": "natural_answerable_20docs"}
{"query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"We Ô¨Årst sample up to kcandidate posi- tions for doing API calls by computing, for each i‚àà{1,...,n}, the probability pi=pM(<API>|P(x),x1:i‚àí1) [BLANK] to starting an API call at position i.\"?", "gold": "thatmassigns", "relevant_doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "relevant_chunk_id": 21, "relevant_text": "We Ô¨Årst sample up to kcandidate posi- tions for doing API calls by computing, for each i‚àà{1,...,n}, the probability pi=pM(<API>|P(x),x1:i‚àí1) thatMassigns to starting an API call at position i.", "case_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools_008", "slice": "natural_answerable_20docs"}
{"query": "From 2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt, what exact term fills [BLANK] in: \"LongLoRA can [BLANK] Llama2 7B up to 100k context, or a 70B model up to 32k, on a single 8√óA100 machine.\"?", "gold": "fine-tune", "relevant_doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "relevant_chunk_id": 20, "relevant_text": "LongLoRA can fine-tune Llama2 7B up to 100k context, or a 70B model up to 32k, on a single 8√óA100 machine.", "case_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models_019", "slice": "natural_answerable_20docs"}
{"query": "From 2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt, what exact term fills [BLANK] in: \"On modern 1arXiv:2205.14135v2 [cs.LG] 23 Jun 2022 [BLANK] Hierarchy with Bandwidth & Memory SizeAttention on GPT-2 Flas\"?", "gold": "flashattentionmemory", "relevant_doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "relevant_chunk_id": 7, "relevant_text": "On modern 1arXiv:2205.14135v2 [cs.LG] 23 Jun 2022 FlashAttentionMemory Hierarchy with Bandwidth & Memory SizeAttention on GPT-2 Flas", "case_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness_015", "slice": "natural_answerable_20docs"}
{"query": "From 2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt, what exact term fills [BLANK] in: \"We design various types of questions for [BLANK] papers, science fiction, and other books.\"?", "gold": "technical", "relevant_doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "relevant_chunk_id": 21, "relevant_text": "We design various types of questions for technical papers, science fiction, and other books.", "case_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models_008", "slice": "natural_answerable_20docs"}
{"query": "From 2305.06983__active_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"2.1 Notations and [BLANK] Given a user input xand a document corpus D= {di}|D| i=1(such as all Wikipedia articles), the goal of retrieval augmented LMs is to generate the answer y= [s1,s2, ...,s\"?", "gold": "definitions", "relevant_doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "relevant_chunk_id": 19, "relevant_text": "2.1 Notations and Definitions Given a user input xand a document corpus D= {di}|D| i=1(such as all Wikipedia articles), the goal of retrieval augmented LMs is to generate the answer y= [s1,s2, ...,s", "case_id": "2305.06983__active_retrieval_augmented_generation_012", "slice": "natural_answerable_20docs"}
{"query": "From 2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt, what exact term fills [BLANK] in: \"the input question to a [BLANK] vector, and retrieves kpassages of which vectors are the closest to the question vector.\"?", "gold": "d-dimensional", "relevant_doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "relevant_chunk_id": 20, "relevant_text": "the input question to a d-dimensional vector, and retrieves kpassages of which vectors are the closest to the question vector.", "case_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering_019", "slice": "natural_answerable_20docs"}
{"query": "From 2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt, what exact term fills [BLANK] in: \"rieval accuracy , which is the fraction of ques- tions for [BLANK] a span that answers the question.\"?", "gold": "whichcfcontains", "relevant_doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "relevant_chunk_id": 17, "relevant_text": "rieval accuracy , which is the fraction of ques- tions for whichCFcontains a span that answers the question.", "case_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering_016", "slice": "natural_answerable_20docs"}
{"query": "From 2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt, what exact term fills [BLANK] in: \"Published as a conference paper at ICLR 2023 [BLANK] IMPROVES CHAIN OF THOUGHT REASONING IN LANGUAGE MODELS Xuezhi Wang‚Ä†‚Ä°Jason Wei‚Ä†Dale Schuurmans‚Ä†Quoc Le‚Ä†Ed H.\"?", "gold": "self-consistency", "relevant_doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "relevant_chunk_id": 0, "relevant_text": "Published as a conference paper at ICLR 2023 SELF-CONSISTENCY IMPROVES CHAIN OF THOUGHT REASONING IN LANGUAGE MODELS Xuezhi Wang‚Ä†‚Ä°Jason Wei‚Ä†Dale Schuurmans‚Ä†Quoc Le‚Ä†Ed H.", "case_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models_009", "slice": "natural_answerable_20docs"}
{"query": "From 2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt, what exact term fills [BLANK] in: \"For [BLANK] reasoning, we parse the Ô¨Årst numerical part as the Ô¨Ånal answer after the model generates ‚ÄúThe answer is ‚Äù.\"?", "gold": "arithmetic", "relevant_doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "relevant_chunk_id": 28, "relevant_text": "For arithmetic reasoning, we parse the Ô¨Årst numerical part as the Ô¨Ånal answer after the model generates ‚ÄúThe answer is ‚Äù.", "case_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models_020", "slice": "natural_answerable_20docs"}
{"query": "From 2005.14165__language_models_are_few_shot_learners.txt, what exact term fills [BLANK] in: \"SpeciÔ¨Åcally, we evaluate GPT-3 on over two dozen NLP datasets, as well as several novel tasks designed to test rapid [BLANK]\"?", "gold": "adaptatio", "relevant_doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "relevant_chunk_id": 28, "relevant_text": "SpeciÔ¨Åcally, we evaluate GPT-3 on over two dozen NLP datasets, as well as several novel tasks designed to test rapid adaptatio", "case_id": "2005.14165__language_models_are_few_shot_learners_001", "slice": "natural_answerable_20docs"}
{"query": "From 2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt, what exact term fills [BLANK] in: \"However, this ‚Äú[BLANK]‚Äù reasoning is a static black box, in that the model uses its own internal representations to generate thoughts and is not grounded in the external world, which limits its ability to reason reactively or update its knowledge.\"?", "gold": "chain-of-thought", "relevant_doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "relevant_chunk_id": 16, "relevant_text": "However, this ‚Äúchain-of-thought‚Äù reasoning is a static black box, in that the model uses its own internal representations to generate thoughts and is not grounded in the external world, which limits its ability to reason reactively or update its knowledge.", "case_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models_007", "slice": "natural_answerable_20docs"}
{"query": "From 2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt, what exact term fills [BLANK] in: \"remove all training documents with high similarity (0.8 or higher) to a [BLANK] or test set document.\"?", "gold": "validation", "relevant_doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "relevant_chunk_id": 21, "relevant_text": "remove all training documents with high similarity (0.8 or higher) to a validation or test set document.", "case_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens_008", "slice": "natural_answerable_20docs"}
{"query": "From 2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt, what exact term fills [BLANK] in: \"This step is crucial for enabling efficient similarity searches in the [BLANK] retrieval phase.\"?", "gold": "subsequent", "relevant_doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "relevant_chunk_id": 21, "relevant_text": "This step is crucial for enabling efficient similarity searches in the subsequent retrieval phase.", "case_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey_001", "slice": "natural_answerable_20docs"}
{"query": "From 2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt, what exact term fills [BLANK] in: \"One limitation of these works is that these [BLANK] have a large gap to full attention, making it infeasible to\"?", "gold": "compressions", "relevant_doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "relevant_chunk_id": 23, "relevant_text": "One limitation of these works is that these compressions have a large gap to full attention, making it infeasible to", "case_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models_003", "slice": "natural_answerable_20docs"}
{"query": "From 2302.13971__llama_open_and_efficient_foundation_language_models.txt, what exact term fills [BLANK] in: \"We use 2,000warmup 0 200 400 600 800 1000 1200 1400 Billion of tokens1.51.61.71.81.92.02.12.2Training [BLANK] 7B LLaMA 13B LLaMA 33B LLaMA 65BFigure 1: Training loss over train tokens for the 7B, 13B, 33B, and 65 models.\"?", "gold": "lossllama", "relevant_doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "relevant_chunk_id": 23, "relevant_text": "We use 2,000warmup 0 200 400 600 800 1000 1200 1400 Billion of tokens1.51.61.71.81.92.02.12.2Training lossLLaMA 7B LLaMA 13B LLaMA 33B LLaMA 65BFigure 1: Training loss over train tokens for the 7B, 13B, 33B, and 65 models.", "case_id": "2302.13971__llama_open_and_efficient_foundation_language_models_001", "slice": "natural_answerable_20docs"}
{"query": "From 2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt, what exact term fills [BLANK] in: \"First, a language model is prompted with a set of manually written [BLANK] exemplars (Wei et al., 2022).\"?", "gold": "chain-of-thought", "relevant_doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "relevant_chunk_id": 18, "relevant_text": "First, a language model is prompted with a set of manually written chain-of-thought exemplars (Wei et al., 2022).", "case_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models_012", "slice": "natural_answerable_20docs"}
{"query": "From 2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"First, LLMs are not able to answer [BLANK] about events that have happened after they were trained.\"?", "gold": "questions", "relevant_doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "relevant_chunk_id": 4, "relevant_text": "First, LLMs are not able to answer questions about events that have happened after they were trained.", "case_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation_018", "slice": "natural_answerable_20docs"}
{"query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \"rs, and all of the param- eters are Ô¨Åne-tuned using labeled data from the [BLANK] tasks.\"?", "gold": "downstream", "relevant_doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "relevant_chunk_id": 23, "relevant_text": "rs, and all of the param- eters are Ô¨Åne-tuned using labeled data from the downstream tasks.", "case_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding_019", "slice": "natural_answerable_20docs"}
{"query": "From 2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt, what exact term fills [BLANK] in: \"nput context and the position of the relevant information and measure changes in task [BLANK].\"?", "gold": "performance", "relevant_doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "relevant_chunk_id": 23, "relevant_text": "nput context and the position of the relevant information and measure changes in task performance.", "case_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts_002", "slice": "natural_answerable_20docs"}
{"query": "From 2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt, what exact term fills [BLANK] in: \"Although some models perform the [BLANK] key-value retrieval task perfectly, other models struggle to simply retrieve matching tokens that occur in the middle of their input context and continue to\"?", "gold": "synthetic", "relevant_doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "relevant_chunk_id": 15, "relevant_text": "Although some models perform the synthetic key-value retrieval task perfectly, other models struggle to simply retrieve matching tokens that occur in the middle of their input context and continue to", "case_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts_017", "slice": "natural_answerable_20docs"}
{"query": "From 2305.06983__active_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"[BLANK]$%$%% Figure 1: An illustration of forward-looking active retrieval augmented generation (FLARE).\"?", "gold": "eddocumentslmgeneration", "relevant_doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "relevant_chunk_id": 11, "relevant_text": "eddocumentsLMGeneration$%$%% Figure 1: An illustration of forward-looking active retrieval augmented generation (FLARE).", "case_id": "2305.06983__active_retrieval_augmented_generation_007", "slice": "natural_answerable_20docs"}
{"query": "From 2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt, what exact term fills [BLANK] in: \"Finally, we aggregate the answers by [BLANK] out the sampled reasoning paths and choosing the answer that is the most consistent among the generated answers.\"?", "gold": "marginalizing", "relevant_doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "relevant_chunk_id": 21, "relevant_text": "Finally, we aggregate the answers by marginalizing out the sampled reasoning paths and choosing the answer that is the most consistent among the generated answers.", "case_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models_016", "slice": "natural_answerable_20docs"}
{"query": "From 2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt, what exact term fills [BLANK] in: \"ng low-rank matrices, which are generally efficient and reduce the number of trainable [BLANK].\"?", "gold": "parameters", "relevant_doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "relevant_chunk_id": 11, "relevant_text": "ng low-rank matrices, which are generally efficient and reduce the number of trainable parameters.", "case_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models_017", "slice": "natural_answerable_20docs"}
{"query": "From 2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"To address these issues, in this paper we present Ragas1, a framework for the automated [BLANK] 1Rag\"?", "gold": "assessment", "relevant_doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "relevant_chunk_id": 8, "relevant_text": "To address these issues, in this paper we present Ragas1, a framework for the automated assessment 1Rag", "case_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation_015", "slice": "natural_answerable_20docs"}
{"query": "From 2005.14165__language_models_are_few_shot_learners.txt, what exact term fills [BLANK] in: \"The steeper ‚Äúin-context learning curves‚Äù for large models demonstrate improved ability to learn a task from contextual [BLANK].\"?", "gold": "information", "relevant_doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "relevant_chunk_id": 19, "relevant_text": "The steeper ‚Äúin-context learning curves‚Äù for large models demonstrate improved ability to learn a task from contextual information.", "case_id": "2005.14165__language_models_are_few_shot_learners_012", "slice": "natural_answerable_20docs"}
{"query": "From 2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt, what exact term fills [BLANK] in: \"Orca 2 [BLANK] surpasses models of similar size and attains performance levels similar or better to those of models 5-10x larger, as assessed on complex tasks that test advanced reasoning abilities in zero-shot settings.\"?", "gold": "significantly", "relevant_doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "relevant_chunk_id": 3, "relevant_text": "Orca 2 significantly surpasses models of similar size and attains performance levels similar or better to those of models 5-10x larger, as assessed on complex tasks that test advanced reasoning abilities in zero-shot settings.", "case_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason_018", "slice": "natural_answerable_20docs"}
{"query": "From 2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt, what exact term fills [BLANK] in: \"Instruction tuning has been shown to improve the model‚Äôs ability to follow [BLANK] on both seen and unseen tasks [ 47], improve the overall quality of the generations [ 7] and give models enhanced zero-shot and reasoning ab\"?", "gold": "instructions", "relevant_doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "relevant_chunk_id": 22, "relevant_text": "Instruction tuning has been shown to improve the model‚Äôs ability to follow instructions on both seen and unseen tasks [ 47], improve the overall quality of the generations [ 7] and give models enhanced zero-shot and reasoning ab", "case_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason_020", "slice": "natural_answerable_20docs"}
{"query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"Input: x Output: Figure 3: An [BLANK] prompt P(x)used to generate API calls for the question answering tool.\"?", "gold": "exemplary", "relevant_doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "relevant_chunk_id": 19, "relevant_text": "Input: x Output: Figure 3: An exemplary prompt P(x)used to generate API calls for the question answering tool.", "case_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools_011", "slice": "natural_answerable_20docs"}
{"query": "From 2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt, what exact term fills [BLANK] in: \"3 Published as a conference paper at ICLR 2023 sampling, as commonly used for [BLANK] text generation (Radford et al., 2019; Brown et al., 2020; Thoppilan et al., 2022), to achieve this goal.\"?", "gold": "open-ended", "relevant_doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "relevant_chunk_id": 30, "relevant_text": "3 Published as a conference paper at ICLR 2023 sampling, as commonly used for open-ended text generation (Radford et al., 2019; Brown et al., 2020; Thoppilan et al., 2022), to achieve this goal.", "case_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models_001", "slice": "natural_answerable_20docs"}
{"query": "From 2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt, what exact term fills [BLANK] in: \"To measure toxicity, we use the [BLANK] dataset (Gehman et al., 2020) and conduct both automatic and human evaluations.\"?", "gold": "realtoxicityprompts", "relevant_doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "relevant_chunk_id": 18, "relevant_text": "To measure toxicity, we use the RealToxicityPrompts dataset (Gehman et al., 2020) and conduct both automatic and human evaluations.", "case_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback_006", "slice": "natural_answerable_20docs"}
{"query": "From 2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt, what exact term fills [BLANK] in: \"1 [BLANK] Transformer models [ 82] have emerged as the most widely used architecture in applications such as natural language processing and image classiÔ¨Åcation.\"?", "gold": "introduction", "relevant_doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "relevant_chunk_id": 4, "relevant_text": "1 Introduction Transformer models [ 82] have emerged as the most widely used architecture in applications such as natural language processing and image classiÔ¨Åcation.", "case_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness_010", "slice": "natural_answerable_20docs"}
{"query": "From 2005.14165__language_models_are_few_shot_learners.txt, what exact term fills [BLANK] in: \"Third, humans do not require large [BLANK] datasets to learn most language tasks ‚Äì a brief directive in natural language (e.g.\"?", "gold": "supervised", "relevant_doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "relevant_chunk_id": 16, "relevant_text": "Third, humans do not require large supervised datasets to learn most language tasks ‚Äì a brief directive in natural language (e.g.", "case_id": "2005.14165__language_models_are_few_shot_learners_017", "slice": "natural_answerable_20docs"}
{"query": "From 2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt, what exact term fills [BLANK] in: \"Each kernel loads inputs from HBM to [BLANK] and SRAM, computes, then writes outputs to HBM.\"?", "gold": "registers", "relevant_doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "relevant_chunk_id": 23, "relevant_text": "Each kernel loads inputs from HBM to registers and SRAM, computes, then writes outputs to HBM.", "case_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness_003", "slice": "natural_answerable_20docs"}
{"query": "From 2005.14165__language_models_are_few_shot_learners.txt, what exact term fills [BLANK] in: \"Model [BLANK] improves with the addition of a natural language task description, and with the number of examples in the model‚Äôs context, K.\"?", "gold": "performance", "relevant_doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "relevant_chunk_id": 30, "relevant_text": "Model performance improves with the addition of a natural language task description, and with the number of examples in the model‚Äôs context, K.", "case_id": "2005.14165__language_models_are_few_shot_learners_003", "slice": "natural_answerable_20docs"}
{"query": "From 2005.14165__language_models_are_few_shot_learners.txt, what exact term fills [BLANK] in: \"ostic, there is still a need for task-speciÔ¨Åc datasets and task-speciÔ¨Åc Ô¨Åne-tuning: to achieve strong [BLANK] on a desired task typically requires Ô¨Åne-tuning on a dataset of thousands to hundreds of thousands of examples speciÔ¨Åc to that task.\"?", "gold": "performance", "relevant_doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "relevant_chunk_id": 13, "relevant_text": "ostic, there is still a need for task-speciÔ¨Åc datasets and task-speciÔ¨Åc Ô¨Åne-tuning: to achieve strong performance on a desired task typically requires Ô¨Åne-tuning on a dataset of thousands to hundreds of thousands of examples speciÔ¨Åc to that task.", "case_id": "2005.14165__language_models_are_few_shot_learners_005", "slice": "natural_answerable_20docs"}
{"query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \"Dur- ing [BLANK], the model is trained on unlabeled data over different pre-training tasks.\"?", "gold": "pre-training", "relevant_doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "relevant_chunk_id": 22, "relevant_text": "Dur- ing pre-training, the model is trained on unlabeled data over different pre-training tasks.", "case_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding_011", "slice": "natural_answerable_20docs"}
{"query": "From 2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt, what exact term fills [BLANK] in: \"We Ô¨Ånd that [BLANK] robustly improves reasoning accuracy for every language model considered, spanning a wide range of model scales.\"?", "gold": "self-consistency", "relevant_doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "relevant_chunk_id": 31, "relevant_text": "We Ô¨Ånd that self-consistency robustly improves reasoning accuracy for every language model considered, spanning a wide range of model scales.", "case_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models_003", "slice": "natural_answerable_20docs"}
{"query": "From 2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt, what exact term fills [BLANK] in: \"LongRAG processes each [BLANK] document as a single (long) unit rather than chunking them into smaller units.\"?", "gold": "individual", "relevant_doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "relevant_chunk_id": 4, "relevant_text": "LongRAG processes each individual document as a single (long) unit rather than chunking them into smaller units.", "case_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms_010", "slice": "natural_answerable_20docs"}
{"query": "From 2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt, what exact term fills [BLANK] in: \"s an example, the result of a game in premier league in a particular day might be good training data for frontier models, but we need to remove such [BLANK] to leave more model capacity for ‚Äúreasoning‚Äù for the mini size models.\"?", "gold": "information", "relevant_doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "relevant_chunk_id": 21, "relevant_text": "s an example, the result of a game in premier league in a particular day might be good training data for frontier models, but we need to remove such information to leave more model capacity for ‚Äúreasoning‚Äù for the mini size models.", "case_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone_008", "slice": "natural_answerable_20docs"}
{"query": "From 2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt, what exact term fills [BLANK] in: \"the next token on a webpage from the internet‚Äîis different from the objective ‚Äúfollow the user‚Äôs [BLANK] helpfully and safely‚Äù (Radford et al., 2019; Brown et al., 2020; Fedus et al., 2021; Rae et al., 2021; Thoppilan et al., 2022).\"?", "gold": "instructions", "relevant_doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "relevant_chunk_id": 7, "relevant_text": "the next token on a webpage from the internet‚Äîis different from the objective ‚Äúfollow the user‚Äôs instructions helpfully and safely‚Äù (Radford et al., 2019; Brown et al., 2020; Fedus et al., 2021; Rae et al., 2021; Thoppilan et al., 2022).", "case_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback_007", "slice": "natural_answerable_20docs"}
{"query": "From 2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt, what exact term fills [BLANK] in: \"Remarkably, this has been successful for a range of simple [BLANK] tasks (Brown et al., 2020).\"?", "gold": "question-answering", "relevant_doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "relevant_chunk_id": 9, "relevant_text": "Remarkably, this has been successful for a range of simple question-answering tasks (Brown et al., 2020).", "case_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models_015", "slice": "natural_answerable_20docs"}
{"query": "From 2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt, what exact term fills [BLANK] in: \"On all four language models, [BLANK] improves over chain-of-thought prompting by a striking margin across all tasks.\"?", "gold": "self-consistency", "relevant_doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "relevant_chunk_id": 14, "relevant_text": "On all four language models, self-consistency improves over chain-of-thought prompting by a striking margin across all tasks.", "case_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models_004", "slice": "natural_answerable_20docs"}
{"query": "From 2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt, what exact term fills [BLANK] in: \"For the tasks where reasoning is of primary importance (Figure 1(1)), we alternate the generation of thoughts and actions so that the task-solving trajectory consists of multiple [BLANK] steps.\"?", "gold": "thought-action-observation", "relevant_doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "relevant_chunk_id": 31, "relevant_text": "For the tasks where reasoning is of primary importance (Figure 1(1)), we alternate the generation of thoughts and actions so that the task-solving trajectory consists of multiple thought-action-observation steps.", "case_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models_019", "slice": "natural_answerable_20docs"}
{"query": "From 2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt, what exact term fills [BLANK] in: \"rformance across tasks in language [BLANK] and interactive decision making, their abilities for reasoning (e.g.\"?", "gold": "understanding", "relevant_doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "relevant_chunk_id": 1, "relevant_text": "rformance across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g.", "case_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models_009", "slice": "natural_answerable_20docs"}
{"query": "From 2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt, what exact term fills [BLANK] in: \"Even though [BLANK] still makes simple mistakes, our results show that Ô¨Åne-tuning with human feedback is a promising direction for aligning language models with human intent.\"?", "gold": "instructgpt", "relevant_doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "relevant_chunk_id": 3, "relevant_text": "Even though InstructGPT still makes simple mistakes, our results show that Ô¨Åne-tuning with human feedback is a promising direction for aligning language models with human intent.", "case_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback_018", "slice": "natural_answerable_20docs"}
{"query": "From 2305.06983__active_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"We discard [BLANK] retrieved documents ‚à™t‚Ä≤<tDqt‚Ä≤and only use the retrieved documents from the current step to condition the next generation to prevent reaching the input length limit of LMs.\"?", "gold": "previously", "relevant_doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "relevant_chunk_id": 24, "relevant_text": "We discard previously retrieved documents ‚à™t‚Ä≤<tDqt‚Ä≤and only use the retrieved documents from the current step to condition the next generation to prevent reaching the input length limit of LMs.", "case_id": "2305.06983__active_retrieval_augmented_generation_011", "slice": "natural_answerable_20docs"}
{"query": "From 2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt, what exact term fills [BLANK] in: \"Some of these approaches are [BLANK] (Karpukhin et al., 2020; Izacard et al., 2022; Gu\"?", "gold": "retrieval-based", "relevant_doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "relevant_chunk_id": 21, "relevant_text": "Some of these approaches are retrieval-based (Karpukhin et al., 2020; Izacard et al., 2022; Gu", "case_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models_020", "slice": "natural_answerable_20docs"}
{"query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \"[CLS] is a special symbol added in front of every input example, and [SEP] is a special [BLANK] token (e.g.\"?", "gold": "separator", "relevant_doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "relevant_chunk_id": 20, "relevant_text": "[CLS] is a special symbol added in front of every input example, and [SEP] is a special separator token (e.g.", "case_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding_006", "slice": "natural_answerable_20docs"}
{"query": "From 2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt, what exact term fills [BLANK] in: \"where [BLANK] sparse vector space models, such as TF-IDF or BM25, are the de facto method.\"?", "gold": "traditional", "relevant_doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "relevant_chunk_id": 1, "relevant_text": "where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method.", "case_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering_009", "slice": "natural_answerable_20docs"}
{"query": "From 2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt, what exact term fills [BLANK] in: \"The Naive RAG follows a traditional process that includes indexing, retrieval, and generation, which is also characterized as a ‚Äú[BLANK]‚Äù framework [7].\"?", "gold": "retrieve-read", "relevant_doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "relevant_chunk_id": 20, "relevant_text": "The Naive RAG follows a traditional process that includes indexing, retrieval, and generation, which is also characterized as a ‚ÄúRetrieve-Read‚Äù framework [7].", "case_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey_008", "slice": "natural_answerable_20docs"}
{"query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \"Model Architecture BERT‚Äôs model architec- ture is a multi-layer bidirectional Transformer en- coder based on the original [BLANK] de- scribed in Vaswani et al.\"?", "gold": "implementation", "relevant_doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "relevant_chunk_id": 24, "relevant_text": "Model Architecture BERT‚Äôs model architec- ture is a multi-layer bidirectional Transformer en- coder based on the original implementation de- scribed in Vaswani et al.", "case_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding_020", "slice": "natural_answerable_20docs"}
{"query": "From 2005.14165__language_models_are_few_shot_learners.txt, what exact term fills [BLANK] in: \"ew-shot performance increases more rapidly, [BLANK] that larger models are more proÔ¨Åcient at in-context learning.\"?", "gold": "demonstrating", "relevant_doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "relevant_chunk_id": 28, "relevant_text": "ew-shot performance increases more rapidly, demonstrating that larger models are more proÔ¨Åcient at in-context learning.", "case_id": "2005.14165__language_models_are_few_shot_learners_020", "slice": "natural_answerable_20docs"}
{"query": "From 2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt, what exact term fills [BLANK] in: \"Causality is maintained as [BLANK] of the Ô¨Årst chunk only aÔ¨Äect the last token of the Ô¨Årst chunk and tokens from the second chunk.\"?", "gold": "neighbours", "relevant_doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "relevant_chunk_id": 18, "relevant_text": "Causality is maintained as neighbours of the Ô¨Årst chunk only aÔ¨Äect the last token of the Ô¨Årst chunk and tokens from the second chunk.", "case_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens_016", "slice": "natural_answerable_20docs"}
{"query": "From 2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt, what exact term fills [BLANK] in: \"Document [1](Title: Asian Americans in science and [BLANK]) Prize in physics for discovery of the subatomic particle J/œà.\"?", "gold": "technology", "relevant_doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "relevant_chunk_id": 29, "relevant_text": "Document [1](Title: Asian Americans in science and technology) Prize in physics for discovery of the subatomic particle J/œà.", "case_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts_003", "slice": "natural_answerable_20docs"}
{"query": "From 2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt, what exact term fills [BLANK] in: \"If language models can robustly use information within long input con- texts, then their [BLANK] should be minimally affected by the position of the relevant information in the input context.\"?", "gold": "performance", "relevant_doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "relevant_chunk_id": 9, "relevant_text": "If language models can robustly use information within long input con- texts, then their performance should be minimally affected by the position of the relevant information in the input context.", "case_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts_005", "slice": "natural_answerable_20docs"}
{"query": "From 2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt, what exact term fills [BLANK] in: \"Sample a diverse set of [BLANK] paths She eats 3 for breakfast, so she has 16 - 3 = 13 left.\"?", "gold": "reasoning", "relevant_doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "relevant_chunk_id": 9, "relevant_text": "Sample a diverse set of reasoning paths She eats 3 for breakfast, so she has 16 - 3 = 13 left.", "case_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models_005", "slice": "natural_answerable_20docs"}
{"query": "From 2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt, what exact term fills [BLANK] in: \"operates on long retrieval units, with only a few ([BLANK] fewer than 10) being fed into the reader.\"?", "gold": "typically", "relevant_doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "relevant_chunk_id": 21, "relevant_text": "operates on long retrieval units, with only a few (typically fewer than 10) being fed into the reader.", "case_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms_014", "slice": "natural_answerable_20docs"}
{"query": "From 2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt, what exact term fills [BLANK] in: \"RAG bridges this information gap by sourcing and [BLANK] knowledge from external databases.\"?", "gold": "incorporating", "relevant_doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "relevant_chunk_id": 17, "relevant_text": "RAG bridges this information gap by sourcing and incorporating knowledge from external databases.", "case_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey_002", "slice": "natural_answerable_20docs"}
{"query": "From 2302.13971__llama_open_and_efficient_foundation_language_models.txt, what exact term fills [BLANK] in: \"Correspondence: {htouvron, thibautlav,gizacard,egrave,glample}@meta.com 1https://github.com/facebookresearch/[BLANK], a smaller one trained longer will ultimately be cheaper at inference.\"?", "gold": "llamaperformance", "relevant_doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "relevant_chunk_id": 4, "relevant_text": "Correspondence: {htouvron, thibautlav,gizacard,egrave,glample}@meta.com 1https://github.com/facebookresearch/llamaperformance, a smaller one trained longer will ultimately be cheaper at inference.", "case_id": "2302.13971__llama_open_and_efficient_foundation_language_models_010", "slice": "natural_answerable_20docs"}
{"query": "From 2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt, what exact term fills [BLANK] in: \"Attention only computes in each group in ours while the [BLANK] flows between groups via shifting.\"?", "gold": "information", "relevant_doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "relevant_chunk_id": 18, "relevant_text": "Attention only computes in each group in ours while the information flows between groups via shifting.", "case_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models_002", "slice": "natural_answerable_20docs"}
{"query": "From 2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt, what exact term fills [BLANK] in: \"One main reason is that they focus on FLOP reduction (which may not correlate with [BLANK] speed) and tend to ignore overheads from memory\"?", "gold": "wall-clock", "relevant_doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "relevant_chunk_id": 6, "relevant_text": "One main reason is that they focus on FLOP reduction (which may not correlate with wall-clock speed) and tend to ignore overheads from memory", "case_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness_005", "slice": "natural_answerable_20docs"}
{"query": "From 2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt, what exact term fills [BLANK] in: \"To this end, we analyze model performance on [BLANK] question answering, which requires models to find relevant information within an input context and use it to answer the question.\"?", "gold": "multi-document", "relevant_doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "relevant_chunk_id": 22, "relevant_text": "To this end, we analyze model performance on multi-document question answering, which requires models to find relevant information within an input context and use it to answer the question.", "case_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts_006", "slice": "natural_answerable_20docs"}
{"query": "From 2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt, what exact term fills [BLANK] in: \"Rather than naively imitating powerful LLMs, we treat them as a [BLANK] of behaviors from which we carefully select those best suited for the task at hand.\"?", "gold": "reservoir", "relevant_doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "relevant_chunk_id": 18, "relevant_text": "Rather than naively imitating powerful LLMs, we treat them as a reservoir of behaviors from which we carefully select those best suited for the task at hand.", "case_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason_016", "slice": "natural_answerable_20docs"}
{"query": "From 2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt, what exact term fills [BLANK] in: \"Typically, training LLMs with long context sizes is [BLANK] expensive, requiring extensive training hours and GPU re\"?", "gold": "computationally", "relevant_doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "relevant_chunk_id": 0, "relevant_text": "Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU re", "case_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models_009", "slice": "natural_answerable_20docs"}
{"query": "From 2302.13971__llama_open_and_efficient_foundation_language_models.txt, what exact term fills [BLANK] in: \"This imple- [BLANK], available in the xformers library,2is inspired by Rabe and Staats (2021) and uses the backward from Dao et al.\"?", "gold": "mentation", "relevant_doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "relevant_chunk_id": 24, "relevant_text": "This imple- mentation, available in the xformers library,2is inspired by Rabe and Staats (2021) and uses the backward from Dao et al.", "case_id": "2302.13971__llama_open_and_efficient_foundation_language_models_003", "slice": "natural_answerable_20docs"}
{"query": "From 2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt, what exact term fills [BLANK] in: \"1 left) [BLANK] multiple forms of memory of diÔ¨Äerent sizes and speeds, with smaller memor\"?", "gold": "comprises", "relevant_doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "relevant_chunk_id": 21, "relevant_text": "1 left) comprises multiple forms of memory of diÔ¨Äerent sizes and speeds, with smaller memor", "case_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness_001", "slice": "natural_answerable_20docs"}
{"query": "From 2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt, what exact term fills [BLANK] in: \"However, these models often express unintended behaviors such as making up facts, generating biased or toxic text, or simply not following user [BLANK] (Bender et al., 2021; Bommasani et al., 2021; Kenton et al., 2021; Weidinger et al., 2021; Tamkin et al., 2021; Gehman et al., 2020).\"?", "gold": "instructions", "relevant_doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "relevant_chunk_id": 4, "relevant_text": "However, these models often express unintended behaviors such as making up facts, generating biased or toxic text, or simply not following user instructions (Bender et al., 2021; Bommasani et al., 2021; Kenton et al., 2021; Weidinger et al., 2021; Tamkin et al., 2021; Gehman et al., 2020).", "case_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback_010", "slice": "natural_answerable_20docs"}
{"query": "From 2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt, what exact term fills [BLANK] in: \"Our work is [BLANK] to these works, as our attention mechanism is unmodified during inference.\"?", "gold": "complementary", "relevant_doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "relevant_chunk_id": 22, "relevant_text": "Our work is complementary to these works, as our attention mechanism is unmodified during inference.", "case_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models_001", "slice": "natural_answerable_20docs"}
{"query": "From 2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt, what exact term fills [BLANK] in: \"We prove that block-sparse [BLANK] has better IO complexity than FlashAttention by a factor proportional to the sparsity ratio.\"?", "gold": "flashattention", "relevant_doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "relevant_chunk_id": 16, "relevant_text": "We prove that block-sparse FlashAttention has better IO complexity than FlashAttention by a factor proportional to the sparsity ratio.", "case_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness_006", "slice": "natural_answerable_20docs"}
{"query": "From 1706.03762__attention_is_all_you_need.txt, what exact term fills [BLANK] in: \"ransformer follows this overall architecture using stacked [BLANK] and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.\"?", "gold": "self-attention", "relevant_doc_id": "1706.03762__attention_is_all_you_need.txt", "relevant_chunk_id": 16, "relevant_text": "ransformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.", "case_id": "1706.03762__attention_is_all_you_need_016", "slice": "natural_answerable_20docs"}
{"query": "From 2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt, what exact term fills [BLANK] in: \"We study this question with a [BLANK] key- value retrieval task, which is designed to be a mini- mal testbed for the basic ability to retrieve matching tokens from the input context.\"?", "gold": "synthetic", "relevant_doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "relevant_chunk_id": 14, "relevant_text": "We study this question with a synthetic key- value retrieval task, which is designed to be a mini- mal testbed for the basic ability to retrieve matching tokens from the input context.", "case_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts_004", "slice": "natural_answerable_20docs"}
{"query": "From 2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt, what exact term fills [BLANK] in: \"Prior work has given models the ability to generate natural language inter- mediate steps by training from scratch (Ling et al., 2017) or Ô¨Ånetuning a [BLANK] model (Cobbe et al., 2021), in ad\"?", "gold": "pretrained", "relevant_doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "relevant_chunk_id": 7, "relevant_text": "Prior work has given models the ability to generate natural language inter- mediate steps by training from scratch (Ling et al., 2017) or Ô¨Ånetuning a pretrained model (Cobbe et al., 2021), in ad", "case_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models_007", "slice": "natural_answerable_20docs"}
{"query": "From 1706.03762__attention_is_all_you_need.txt, what exact term fills [BLANK] in: \"Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer [BLANK].\"?", "gold": "normalization", "relevant_doc_id": "1706.03762__attention_is_all_you_need.txt", "relevant_chunk_id": 18, "relevant_text": "Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization.", "case_id": "1706.03762__attention_is_all_you_need_011", "slice": "natural_answerable_20docs"}
{"query": "From 2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt, what exact term fills [BLANK] in: \"and delve into their synergies, elucidating how these com- ponents [BLANK] collaborate to form a cohesive and effective RAG framework.\"?", "gold": "intricately", "relevant_doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "relevant_chunk_id": 14, "relevant_text": "and delve into their synergies, elucidating how these com- ponents intricately collaborate to form a cohesive and effective RAG framework.", "case_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey_012", "slice": "natural_answerable_20docs"}
{"query": "From 2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"Moreover, this evaluation strategy relies on the LM [BLANK], which are not accessible for some closed models (e.g.\"?", "gold": "probabilities", "relevant_doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "relevant_chunk_id": 8, "relevant_text": "Moreover, this evaluation strategy relies on the LM probabilities, which are not accessible for some closed models (e.g.", "case_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation_005", "slice": "natural_answerable_20docs"}
{"query": "From 2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt, what exact term fills [BLANK] in: \"In Table 1, we show the test accuracy over a set of reasoning tasks by using different answer [BLANK] strategies.\"?", "gold": "aggregation", "relevant_doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "relevant_chunk_id": 23, "relevant_text": "In Table 1, we show the test accuracy over a set of reasoning tasks by using different answer aggregation strategies.", "case_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models_002", "slice": "natural_answerable_20docs"}
{"query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \"r [BLANK] such models (Howard and Ruder, 2018; Radford et al., 2018; Dai and Le, 2015).\"?", "gold": "pre-training", "relevant_doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "relevant_chunk_id": 21, "relevant_text": "r pre-training such models (Howard and Ruder, 2018; Radford et al., 2018; Dai and Le, 2015).", "case_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding_002", "slice": "natural_answerable_20docs"}
{"query": "From 2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt, what exact term fills [BLANK] in: \"Most decomposable similarity functions are some [BLANK] of Euclidean distance (L2).\"?", "gold": "transformations", "relevant_doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "relevant_chunk_id": 21, "relevant_text": "Most decomposable similarity functions are some transformations of Euclidean distance (L2).", "case_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering_020", "slice": "natural_answerable_20docs"}
{"query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"The former is the weighted loss over all tokens xi,...,x nif the API call and its result are given to Mas a preÔ¨Åx;3 the latter is the minimum of the losses obtained from (i) doing no API call at all and (ii) doing an API call, but not [BLANK] the response.\"?", "gold": "providing", "relevant_doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "relevant_chunk_id": 24, "relevant_text": "The former is the weighted loss over all tokens xi,...,x nif the API call and its result are given to Mas a preÔ¨Åx;3 the latter is the minimum of the losses obtained from (i) doing no API call at all and (ii) doing an API call, but not providing the response.", "case_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools_003", "slice": "natural_answerable_20docs"}
{"query": "From 2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt, what exact term fills [BLANK] in: \"1 we also report the results by taking a ‚Äúweighted average‚Äù, i.e., each agets a score of its weighted sum divided by‚àëm i=11(ai=a), which results in a much worse [BLANK].\"?", "gold": "performance", "relevant_doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "relevant_chunk_id": 27, "relevant_text": "1 we also report the results by taking a ‚Äúweighted average‚Äù, i.e., each agets a score of its weighted sum divided by‚àëm i=11(ai=a), which results in a much worse performance.", "case_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models_019", "slice": "natural_answerable_20docs"}
{"query": "From 2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt, what exact term fills [BLANK] in: \"This can result in the loss of contextual information, which may ultimately harm overall [BLANK].\"?", "gold": "performance", "relevant_doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "relevant_chunk_id": 10, "relevant_text": "This can result in the loss of contextual information, which may ultimately harm overall performance.", "case_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms_017", "slice": "natural_answerable_20docs"}
{"query": "From 2302.13971__llama_open_and_efficient_foundation_language_models.txt, what exact term fills [BLANK] in: \"ow that it is possible to train [BLANK] models using publicly avail- able datasets exclusively, without resorting to proprietary and inaccessible datasets.\"?", "gold": "state-of-the-art", "relevant_doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "relevant_chunk_id": 1, "relevant_text": "ow that it is possible to train state-of-the-art models using publicly avail- able datasets exclusively, without resorting to proprietary and inaccessible datasets.", "case_id": "2302.13971__llama_open_and_efficient_foundation_language_models_013", "slice": "natural_answerable_20docs"}
{"query": "From 2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt, what exact term fills [BLANK] in: \"1.First, chain of thought, in principle, allows models to decompose multi-step problems into [BLANK] steps, which means that additional computation can be allocated to problems that require more reason\"?", "gold": "intermediate", "relevant_doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "relevant_chunk_id": 16, "relevant_text": "1.First, chain of thought, in principle, allows models to decompose multi-step problems into intermediate steps, which means that additional computation can be allocated to problems that require more reason", "case_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models_016", "slice": "natural_answerable_20docs"}
{"query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \"We argue that current techniques restrict the power of the pre-trained [BLANK], espe- cially for the Ô¨Åne-tuning approaches.\"?", "gold": "representations", "relevant_doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "relevant_chunk_id": 6, "relevant_text": "We argue that current techniques restrict the power of the pre-trained representations, espe- cially for the Ô¨Åne-tuning approaches.", "case_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding_018", "slice": "natural_answerable_20docs"}
{"query": "From 2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt, what exact term fills [BLANK] in: \"All our code, models, dataset, and demo are available at github.com/[BLANK]/LongLoRA.\"?", "gold": "dvlab-research", "relevant_doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "relevant_chunk_id": 4, "relevant_text": "All our code, models, dataset, and demo are available at github.com/dvlab-research/LongLoRA.", "case_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models_010", "slice": "natural_answerable_20docs"}
{"query": "From 2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt, what exact term fills [BLANK] in: \"We follow the sequence of works initiated in ‚ÄúTextbooks Are All You Need‚Äù [GZA+23], which utilize high quality training data to improve the [BLANK] of small language mod\"?", "gold": "performance", "relevant_doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "relevant_chunk_id": 17, "relevant_text": "We follow the sequence of works initiated in ‚ÄúTextbooks Are All You Need‚Äù [GZA+23], which utilize high quality training data to improve the performance of small language mod", "case_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone_002", "slice": "natural_answerable_20docs"}
{"query": "From 2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt, what exact term fills [BLANK] in: \"As most of the datasets only have an [BLANK] split, we manually composed a set of eight few-shot exemplars with chains of thought for prompting‚ÄîFigure 1 (right) shows one chain of thought exemplar, and the full set of exemplars is given in Appendix Table 20.\"?", "gold": "evaluation", "relevant_doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "relevant_chunk_id": 23, "relevant_text": "As most of the datasets only have an evaluation split, we manually composed a set of eight few-shot exemplars with chains of thought for prompting‚ÄîFigure 1 (right) shows one chain of thought exemplar, and the full set of exemplars is given in Appendix Table 20.", "case_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models_020", "slice": "natural_answerable_20docs"}
{"query": "From 2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt, what exact term fills [BLANK] in: \"Like Orca 1, we utilize more capable LLMs to [BLANK] various reasoning strategies across various tasks.\"?", "gold": "demonstrate", "relevant_doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "relevant_chunk_id": 16, "relevant_text": "Like Orca 1, we utilize more capable LLMs to demonstrate various reasoning strategies across various tasks.", "case_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason_012", "slice": "natural_answerable_20docs"}
{"query": "From 2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt, what exact term fills [BLANK] in: \"These computation resources are typically [BLANK] for common researchers, which naturally leads us to question: can we extend the context window of LLMs efficiently?\"?", "gold": "unaffordable", "relevant_doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "relevant_chunk_id": 10, "relevant_text": "These computation resources are typically unaffordable for common researchers, which naturally leads us to question: can we extend the context window of LLMs efficiently?", "case_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models_004", "slice": "natural_answerable_20docs"}
{"query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"An example of such a prompt for a [BLANK] an- swering tool is shown in Figure 3; all prompts used are shown in Appendix A.2.\"?", "gold": "question", "relevant_doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "relevant_chunk_id": 20, "relevant_text": "An example of such a prompt for a question an- swering tool is shown in Figure 3; all prompts used are shown in Appendix A.2.", "case_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools_019", "slice": "natural_answerable_20docs"}
{"query": "From 2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt, what exact term fills [BLANK] in: \"The model is already [BLANK], and the chat template is as follows: <|user|> /n Question <|end|> /n <|assistant|> Thephi-3-small model (7B parameters) leverages the tiktoken tokenizer (for better multilingual tokenization) with a vocabulary size of 1003522and has default context length 8192.\"?", "gold": "chat-finetuned", "relevant_doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "relevant_chunk_id": 10, "relevant_text": "The model is already chat-finetuned, and the chat template is as follows: <|user|> /n Question <|end|> /n <|assistant|> Thephi-3-small model (7B parameters) leverages the tiktoken tokenizer (for better multilingual tokenization) with a vocabulary size of 1003522and has default context length 8192.", "case_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone_004", "slice": "natural_answerable_20docs"}
{"query": "From 2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt, what exact term fills [BLANK] in: \"For example our previous model trained on this data recipe, phi-2 (2.7B parameters), matched the [BLANK] of models 25 times large\"?", "gold": "performance", "relevant_doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "relevant_chunk_id": 5, "relevant_text": "For example our previous model trained on this data recipe, phi-2 (2.7B parameters), matched the performance of models 25 times large", "case_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone_010", "slice": "natural_answerable_20docs"}
{"query": "From 2302.13971__llama_open_and_efficient_foundation_language_models.txt, what exact term fills [BLANK] in: \"2.3 Optimizer Our models are trained using the AdamW opti- mizer (Loshchilov and Hutter, 2017), with the fol- lowing [BLANK]: Œ≤1= 0.9,Œ≤2= 0.95.\"?", "gold": "hyper-parameters", "relevant_doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "relevant_chunk_id": 22, "relevant_text": "2.3 Optimizer Our models are trained using the AdamW opti- mizer (Loshchilov and Hutter, 2017), with the fol- lowing hyper-parameters: Œ≤1= 0.9,Œ≤2= 0.95.", "case_id": "2302.13971__llama_open_and_efficient_foundation_language_models_020", "slice": "natural_answerable_20docs"}
{"query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"We represent each API call as a tuple c= (ac,ic) whereacis the name of the API and icis the cor- [BLANK] input.\"?", "gold": "responding", "relevant_doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "relevant_chunk_id": 15, "relevant_text": "We represent each API call as a tuple c= (ac,ic) whereacis the name of the API and icis the cor- responding input.", "case_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools_012", "slice": "natural_answerable_20docs"}
{"query": "From 2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt, what exact term fills [BLANK] in: \"3.Third, [BLANK] reasoning can be used for tasks such as math word problems, commonsense reasonin\"?", "gold": "chain-of-thought", "relevant_doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "relevant_chunk_id": 17, "relevant_text": "3.Third, chain-of-thought reasoning can be used for tasks such as math word problems, commonsense reasonin", "case_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models_006", "slice": "natural_answerable_20docs"}
{"query": "From 2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt, what exact term fills [BLANK] in: \"These models perform downstream tasks primarily via prompting: all relevant task [BLANK] and data to process is formatted as a textual input context, and the model returns a generated text completion.\"?", "gold": "specification", "relevant_doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "relevant_chunk_id": 3, "relevant_text": "These models perform downstream tasks primarily via prompting: all relevant task specification and data to process is formatted as a textual input context, and the model returns a generated text completion.", "case_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts_018", "slice": "natural_answerable_20docs"}
{"query": "From 2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt, what exact term fills [BLANK] in: \"Thanks to its small size, phi- 3-mini can be [BLANK] to 4-bits so that it only occupies ‚âà1.8GB of memory.\"?", "gold": "quantized", "relevant_doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "relevant_chunk_id": 17, "relevant_text": "Thanks to its small size, phi- 3-mini can be quantized to 4-bits so that it only occupies ‚âà1.8GB of memory.", "case_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone_006", "slice": "natural_answerable_20docs"}
{"query": "From 2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt, what exact term fills [BLANK] in: \"nstruction Tuning [BLANK] tuning [ 46,38,62,61] has emerged as a crucial step in training language models.\"?", "gold": "instruction", "relevant_doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "relevant_chunk_id": 22, "relevant_text": "nstruction Tuning Instruction tuning [ 46,38,62,61] has emerged as a crucial step in training language models.", "case_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason_008", "slice": "natural_answerable_20docs"}
{"query": "From 2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt, what exact term fills [BLANK] in: \"ughts only need to 1We show some GPT-3 (Brown et al., 2020) results in Appendix A.1, which [BLANK] PaLM-540B.\"?", "gold": "outperforms", "relevant_doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "relevant_chunk_id": 32, "relevant_text": "ughts only need to 1We show some GPT-3 (Brown et al., 2020) results in Appendix A.1, which outperforms PaLM-540B.", "case_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models_008", "slice": "natural_answerable_20docs"}
{"query": "From 1706.03762__attention_is_all_you_need.txt, what exact term fills [BLANK] in: \"The Transformer allows for significantly more [BLANK] and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\"?", "gold": "parallelization", "relevant_doc_id": "1706.03762__attention_is_all_you_need.txt", "relevant_chunk_id": 10, "relevant_text": "The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.", "case_id": "1706.03762__attention_is_all_you_need_004", "slice": "natural_answerable_20docs"}
{"query": "From 2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt, what exact term fills [BLANK] in: \"The [BLANK] says, ‚ÄúI‚Äôm sorry, but we don‚Äôt serve alcohol here.‚Äù The man replies, ‚ÄúOh, I didn‚Äôt realize this place was a church!‚Äù What does the man mean by his response?\"?", "gold": "bartender", "relevant_doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "relevant_chunk_id": 8, "relevant_text": "The bartender says, ‚ÄúI‚Äôm sorry, but we don‚Äôt serve alcohol here.‚Äù The man replies, ‚ÄúOh, I didn‚Äôt realize this place was a church!‚Äù What does the man mean by his response?", "case_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason_007", "slice": "natural_answerable_20docs"}
{"query": "From 2305.06983__active_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"These attempts include methods that passively use the past context to retrieve additional [BLANK] at a fixed interval (Khandelwal et\"?", "gold": "information", "relevant_doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "relevant_chunk_id": 12, "relevant_text": "These attempts include methods that passively use the past context to retrieve additional information at a fixed interval (Khandelwal et", "case_id": "2305.06983__active_retrieval_augmented_generation_015", "slice": "natural_answerable_20docs"}
{"query": "From 2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt, what exact term fills [BLANK] in: \"The question encoder and the reader model are then Ô¨Åne- tuned using pairs of [BLANK] and answers jointly.\"?", "gold": "questions", "relevant_doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "relevant_chunk_id": 8, "relevant_text": "The question encoder and the reader model are then Ô¨Åne- tuned using pairs of questions and answers jointly.", "case_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering_005", "slice": "natural_answerable_20docs"}
{"query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \"More recently, sentence or document encoders which produce contextual token [BLANK] have been pre-trained from unlabeled text and Ô¨Åne-tuned for a supervised downstream task (Dai and Le, 2015; Howard and R\"?", "gold": "representations", "relevant_doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "relevant_chunk_id": 17, "relevant_text": "More recently, sentence or document encoders which produce contextual token representations have been pre-trained from unlabeled text and Ô¨Åne-tuned for a supervised downstream task (Dai and Le, 2015; Howard and R", "case_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding_012", "slice": "natural_answerable_20docs"}
{"query": "From 1706.03762__attention_is_all_you_need.txt, what exact term fills [BLANK] in: \"Numerous efforts have since continued to push the boundaries of recurrent language models and [BLANK] architectures [38, 24, 15].\"?", "gold": "encoder-decoder", "relevant_doc_id": "1706.03762__attention_is_all_you_need.txt", "relevant_chunk_id": 7, "relevant_text": "Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15].", "case_id": "1706.03762__attention_is_all_you_need_007", "slice": "natural_answerable_20docs"}
{"query": "From 2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"d to ensure that the retrieved context can act as a [BLANK] for the generated answer.\"?", "gold": "justification", "relevant_doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "relevant_chunk_id": 21, "relevant_text": "d to ensure that the retrieved context can act as a justification for the generated answer.", "case_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation_020", "slice": "natural_answerable_20docs"}
{"query": "From 2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt, what exact term fills [BLANK] in: \"Our largest model obtains [BLANK] results on a range of downstream evaluation datasets including Wikitext103 (Merity et al., 2017) and the Pile (Gao et al., 2020) (¬ß4).\"?", "gold": "state-of-the-art", "relevant_doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "relevant_chunk_id": 12, "relevant_text": "Our largest model obtains state-of-the-art results on a range of downstream evaluation datasets including Wikitext103 (Merity et al., 2017) and the Pile (Gao et al., 2020) (¬ß4).", "case_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens_017", "slice": "natural_answerable_20docs"}
{"query": "From 2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt, what exact term fills [BLANK] in: \"Section III focuses on [BLANK] methods in retrieval,including indexing, query and embedding optimization.\"?", "gold": "optimization", "relevant_doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "relevant_chunk_id": 15, "relevant_text": "Section III focuses on optimization methods in retrieval,including indexing, query and embedding optimization.", "case_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey_016", "slice": "natural_answerable_20docs"}
{"query": "From 2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt, what exact term fills [BLANK] in: \"We observe that some benchmarks improve much less from 7B to 14B than they do from 3.8B to 7B, perhaps indicating that our data mixture needs further work to be in the ‚Äúdata optimal regime‚Äù for 14B [BLANK] model.\"?", "gold": "parameters", "relevant_doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "relevant_chunk_id": 22, "relevant_text": "We observe that some benchmarks improve much less from 7B to 14B than they do from 3.8B to 7B, perhaps indicating that our data mixture needs further work to be in the ‚Äúdata optimal regime‚Äù for 14B parameters model.", "case_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone_020", "slice": "natural_answerable_20docs"}
{"query": "From 1706.03762__attention_is_all_you_need.txt, what exact term fills [BLANK] in: \"[BLANK] attention is identical to our algorithm, except for the scaling factor of1‚àödk.\"?", "gold": "dot-product", "relevant_doc_id": "1706.03762__attention_is_all_you_need.txt", "relevant_chunk_id": 22, "relevant_text": "Dot-product attention is identical to our algorithm, except for the scaling factor of1‚àödk.", "case_id": "1706.03762__attention_is_all_you_need_014", "slice": "natural_answerable_20docs"}
{"query": "From 2005.14165__language_models_are_few_shot_learners.txt, what exact term fills [BLANK] in: \"Finally, we Ô¨Ånd that GPT-3 can generate samples of news articles which human evaluators have difÔ¨Åculty [BLANK] from articles written by humans.\"?", "gold": "distinguishing", "relevant_doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "relevant_chunk_id": 4, "relevant_text": "Finally, we Ô¨Ånd that GPT-3 can generate samples of news articles which human evaluators have difÔ¨Åculty distinguishing from articles written by humans.", "case_id": "2005.14165__language_models_are_few_shot_learners_010", "slice": "natural_answerable_20docs"}
{"query": "From 2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt, what exact term fills [BLANK] in: \"However, the pre-defined size limits LLMs in many [BLANK], like summarizing long documents or answering long questions.\"?", "gold": "applications", "relevant_doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "relevant_chunk_id": 8, "relevant_text": "However, the pre-defined size limits LLMs in many applications, like summarizing long documents or answering long questions.", "case_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models_015", "slice": "natural_answerable_20docs"}
{"query": "From 2302.13971__llama_open_and_efficient_foundation_language_models.txt, what exact term fills [BLANK] in: \"We re- place the ReLU [BLANK] by the SwiGLU ac- tivation function, introduced by Shazeer (2020) to improve the performance.\"?", "gold": "non-linearity", "relevant_doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "relevant_chunk_id": 21, "relevant_text": "We re- place the ReLU non-linearity by the SwiGLU ac- tivation function, introduced by Shazeer (2020) to improve the performance.", "case_id": "2302.13971__llama_open_and_efficient_foundation_language_models_008", "slice": "natural_answerable_20docs"}
{"query": "From 2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt, what exact term fills [BLANK] in: \"Therefore, we believe the modern RAG systems should re-consider the granularity of their retrieval units to exploit the advantages of the current [BLANK] LLMs.\"?", "gold": "long-context", "relevant_doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "relevant_chunk_id": 20, "relevant_text": "Therefore, we believe the modern RAG systems should re-consider the granularity of their retrieval units to exploit the advantages of the current long-context LLMs.", "case_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms_001", "slice": "natural_answerable_20docs"}
{"query": "From 2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt, what exact term fills [BLANK] in: \"Raez, Erich Elsenzand Laurent Sifrey,z All authors from DeepMind,yEqual contributions,zEqual senior authorship We enhance [BLANK] language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens.\"?", "gold": "auto-regressive", "relevant_doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "relevant_chunk_id": 1, "relevant_text": "Raez, Erich Elsenzand Laurent Sifrey,z All authors from DeepMind,yEqual contributions,zEqual senior authorship We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens.", "case_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens_009", "slice": "natural_answerable_20docs"}
{"query": "From 2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"For instance, BERTScore (Zhang et al., 2020) and MoverScore (Zhao et al., 2019) use contex- tualised embeddings, produced by a [BLANK] BERT model, to compare the similarity between the generated answer and the reference answers.\"?", "gold": "pre-trained", "relevant_doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "relevant_chunk_id": 18, "relevant_text": "For instance, BERTScore (Zhang et al., 2020) and MoverScore (Zhao et al., 2019) use contex- tualised embeddings, produced by a pre-trained BERT model, to compare the similarity between the generated answer and the reference answers.", "case_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation_011", "slice": "natural_answerable_20docs"}
{"query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \"The [BLANK] example in Figure 1 will serve as a running example for this section.\"?", "gold": "question-answering", "relevant_doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "relevant_chunk_id": 23, "relevant_text": "The question-answering example in Figure 1 will serve as a running example for this section.", "case_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding_008", "slice": "natural_answerable_20docs"}
{"query": "From 2302.13971__llama_open_and_efficient_foundation_language_models.txt, what exact term fills [BLANK] in: \"and [BLANK] deÔ¨Ånitions and macros written by users to increase consistency across papers.\"?", "gold": "inline-expanded", "relevant_doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "relevant_chunk_id": 17, "relevant_text": "and inline-expanded deÔ¨Ånitions and macros written by users to increase consistency across papers.", "case_id": "2302.13971__llama_open_and_efficient_foundation_language_models_006", "slice": "natural_answerable_20docs"}
{"query": "From 2005.14165__language_models_are_few_shot_learners.txt, what exact term fills [BLANK] in: \"Brown‚àóBenjamin Mann‚àóNick Ryder‚àóMelanie Subbiah‚àó Jared Kaplan‚Ä†Prafulla Dhariwal Arvind Neelakantan Pranav Shyam Girish Sastry Amanda Askell Sandhini Agarwal Ariel [BLANK] Gretchen Krueger Tom Henighan Rewon Child Aditya Ramesh Daniel M.\"?", "gold": "herbert-voss", "relevant_doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "relevant_chunk_id": 0, "relevant_text": "Brown‚àóBenjamin Mann‚àóNick Ryder‚àóMelanie Subbiah‚àó Jared Kaplan‚Ä†Prafulla Dhariwal Arvind Neelakantan Pranav Shyam Girish Sastry Amanda Askell Sandhini Agarwal Ariel Herbert-Voss Gretchen Krueger Tom Henighan Rewon Child Aditya Ramesh Daniel M.", "case_id": "2005.14165__language_models_are_few_shot_learners_009", "slice": "natural_answerable_20docs"}
{"query": "From 2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt, what exact term fills [BLANK] in: \"Pre-training is performed in two disjoint and sequential phases; phase-1 comprises mostly of web sources aimed at teaching the model general knowledge and language [BLANK].\"?", "gold": "understanding", "relevant_doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "relevant_chunk_id": 19, "relevant_text": "Pre-training is performed in two disjoint and sequential phases; phase-1 comprises mostly of web sources aimed at teaching the model general knowledge and language understanding.", "case_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone_011", "slice": "natural_answerable_20docs"}
{"query": "From 2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt, what exact term fills [BLANK] in: \"Other efficient [BLANK], e.g., dilated or sparse attention, have a large gap to the standard style and do not work well like ours, as in Table 6.\"?", "gold": "attentions", "relevant_doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "relevant_chunk_id": 15, "relevant_text": "Other efficient attentions, e.g., dilated or sparse attention, have a large gap to the standard style and do not work well like ours, as in Table 6.", "case_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models_006", "slice": "natural_answerable_20docs"}
{"query": "From 2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt, what exact term fills [BLANK] in: \"Toourknowledge,ourworkistheÔ¨Å[BLANK]Ô¨Åts of scaling the retrieval database to trillions of tokens for large parametric language models.\"?", "gold": "rsttoshowthebene", "relevant_doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "relevant_chunk_id": 8, "relevant_text": "Toourknowledge,ourworkistheÔ¨ÅrsttoshowthebeneÔ¨Åts of scaling the retrieval database to trillions of tokens for large parametric language models.", "case_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens_015", "slice": "natural_answerable_20docs"}
{"query": "From 2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt, what exact term fills [BLANK] in: \"1arXiv:2309.12307v3 [cs.CL] 8 Mar 2024 Published as a conference paper at ICLR 2024 !Trainable ‚ùÑ[BLANK](-headSelf-A1en(onFeed ForwardNorminput++Lora !\"?", "gold": "frozennormpostmul", "relevant_doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "relevant_chunk_id": 6, "relevant_text": "1arXiv:2309.12307v3 [cs.CL] 8 Mar 2024 Published as a conference paper at ICLR 2024 !Trainable ‚ùÑFrozenNormpostMul(-headSelf-A1en(onFeed ForwardNorminput++Lora !", "case_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models_007", "slice": "natural_answerable_20docs"}
{"query": "From 2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt, what exact term fills [BLANK] in: \"Sec- tion VII mainly discusses the challenges that RAG [BLANK] and its future development directions.\"?", "gold": "currentlyfaces", "relevant_doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "relevant_chunk_id": 16, "relevant_text": "Sec- tion VII mainly discusses the challenges that RAG currentlyfaces and its future development directions.", "case_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey_006", "slice": "natural_answerable_20docs"}
{"query": "From 2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt, what exact term fills [BLANK] in: \"[BLANK] is far simpler than prior approaches that either train an additional veriÔ¨Åer (Cobbe et al., 2021) or train a re-ranker given additional human annotations to improve generation quality (Thoppilan et al., 2022).\"?", "gold": "self-consistency", "relevant_doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "relevant_chunk_id": 12, "relevant_text": "Self-consistency is far simpler than prior approaches that either train an additional veriÔ¨Åer (Cobbe et al., 2021) or train a re-ranker given additional human annotations to improve generation quality (Thoppilan et al., 2022).", "case_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models_015", "slice": "natural_answerable_20docs"}
{"query": "From 2305.06983__active_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"2.3 Active Retrieval Augmented Generation To aid [BLANK] generation with retrieval, we pro- pose active retrieval augmented generation.\"?", "gold": "long-form", "relevant_doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "relevant_chunk_id": 22, "relevant_text": "2.3 Active Retrieval Augmented Generation To aid long-form generation with retrieval, we pro- pose active retrieval augmented generation.", "case_id": "2305.06983__active_retrieval_augmented_generation_016", "slice": "natural_answerable_20docs"}
{"query": "From 2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt, what exact term fills [BLANK] in: \"Training language models to follow [BLANK] with human feedback Long Ouyang‚àóJeff Wu‚àóXu Jiang‚àóDiogo Almeida‚àóCarroll L.\"?", "gold": "instructions", "relevant_doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "relevant_chunk_id": 0, "relevant_text": "Training language models to follow instructions with human feedback Long Ouyang‚àóJeff Wu‚àóXu Jiang‚àóDiogo Almeida‚àóCarroll L.", "case_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback_009", "slice": "natural_answerable_20docs"}
{"query": "From 2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt, what exact term fills [BLANK] in: \"SpeciÔ¨Åcally, we suggest retrieval from a large text database as a [BLANK] path to scaling language models.\"?", "gold": "complementary", "relevant_doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "relevant_chunk_id": 6, "relevant_text": "SpeciÔ¨Åcally, we suggest retrieval from a large text database as a complementary path to scaling language models.", "case_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens_007", "slice": "natural_answerable_20docs"}
{"query": "From 2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt, what exact term fills [BLANK] in: \"Reasoning tasks typically have Ô¨Åxed answers, which is why [BLANK] have generally considered greedy decoding approaches (Radford et al., 2019; Wei et al., 2022; Chowdhery et al., 2022).\"?", "gold": "researchers", "relevant_doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "relevant_chunk_id": 27, "relevant_text": "Reasoning tasks typically have Ô¨Åxed answers, which is why researchers have generally considered greedy decoding approaches (Radford et al., 2019; Wei et al., 2022; Chowdhery et al., 2022).", "case_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models_008", "slice": "natural_answerable_20docs"}
{"query": "From 2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt, what exact term fills [BLANK] in: \"BERT (Devlin et al., 2019) networks (base, un- cased) and take the [BLANK] at the [CLS] token as the output, so d= 768 .\"?", "gold": "representation", "relevant_doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "relevant_chunk_id": 23, "relevant_text": "BERT (Devlin et al., 2019) networks (base, un- cased) and take the representation at the [CLS] token as the output, so d= 768 .", "case_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering_014", "slice": "natural_answerable_20docs"}
{"query": "From 2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt, what exact term fills [BLANK] in: \"GPUs, compute speed has out-paced memory speed [ 61,62,63], and most operations in [BLANK] are bottlenecked by memory accesses [ 43].\"?", "gold": "transformers", "relevant_doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "relevant_chunk_id": 10, "relevant_text": "GPUs, compute speed has out-paced memory speed [ 61,62,63], and most operations in Transformers are bottlenecked by memory accesses [ 43].", "case_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness_004", "slice": "natural_answerable_20docs"}
{"query": "From 2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt, what exact term fills [BLANK] in: \"As shown in Figure 2, we split context length into several groups and conduct attention in each group [BLANK].\"?", "gold": "individually", "relevant_doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "relevant_chunk_id": 13, "relevant_text": "As shown in Figure 2, we split context length into several groups and conduct attention in each group individually.", "case_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models_012", "slice": "natural_answerable_20docs"}
{"query": "From 2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt, what exact term fills [BLANK] in: \"Since decision making and reasoning [BLANK] are integrated into a large language model, ReAct enjoys several unique features: A) Intuitive and easy to design : Designing ReAct promp\"?", "gold": "capabilities", "relevant_doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "relevant_chunk_id": 32, "relevant_text": "Since decision making and reasoning capabilities are integrated into a large language model, ReAct enjoys several unique features: A) Intuitive and easy to design : Designing ReAct promp", "case_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models_020", "slice": "natural_answerable_20docs"}
{"query": "From 2305.06983__active_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"n when they do not have the necessary knowledge to avoid unnecessary or [BLANK] retrieval, and (2) the retrieval queries should reflect the intents of future generations.\"?", "gold": "inappropriate", "relevant_doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "relevant_chunk_id": 25, "relevant_text": "n when they do not have the necessary knowledge to avoid unnecessary or inappropriate retrieval, and (2) the retrieval queries should reflect the intents of future generations.", "case_id": "2305.06983__active_retrieval_augmented_generation_019", "slice": "natural_answerable_20docs"}
{"query": "From 2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt, what exact term fills [BLANK] in: \"Note that Mcan be very large (e.g., 21 million passages in our [BLANK], de- scribed in Section 4.1) and kis usually small, such as20‚Äì100.\"?", "gold": "experiments", "relevant_doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "relevant_chunk_id": 18, "relevant_text": "Note that Mcan be very large (e.g., 21 million passages in our experiments, de- scribed in Section 4.1) and kis usually small, such as20‚Äì100.", "case_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering_002", "slice": "natural_answerable_20docs"}
{"query": "From 1706.03762__attention_is_all_you_need.txt, what exact term fills [BLANK] in: \"The output is computed as a weighted sum 3 Scaled [BLANK] Attention Multi-Head Attention Figure 2: (left) Scaled Dot-Pr\"?", "gold": "dot-product", "relevant_doc_id": "1706.03762__attention_is_all_you_need.txt", "relevant_chunk_id": 19, "relevant_text": "The output is computed as a weighted sum 3 Scaled Dot-Product Attention Multi-Head Attention Figure 2: (left) Scaled Dot-Pr", "case_id": "1706.03762__attention_is_all_you_need_008", "slice": "natural_answerable_20docs"}
{"query": "From 2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt, what exact term fills [BLANK] in: \"The existing RAG framework tends to use short retrieval units, such as 100-word passages in popular open- domain [BLANK] tasks (Chen et al., 2017; Lewis et al., 2020; Karpukhin et al., 2020).\"?", "gold": "question-answering", "relevant_doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "relevant_chunk_id": 6, "relevant_text": "The existing RAG framework tends to use short retrieval units, such as 100-word passages in popular open- domain question-answering tasks (Chen et al., 2017; Lewis et al., 2020; Karpukhin et al., 2020).", "case_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms_005", "slice": "natural_answerable_20docs"}
{"query": "From 2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"Sec- ond,Answer [BLANK] refers to the idea that the generated answer should address the actual ques- tion that was provided.\"?", "gold": "relevance", "relevant_doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "relevant_chunk_id": 21, "relevant_text": "Sec- ond,Answer Relevance refers to the idea that the generated answer should address the actual ques- tion that was provided.", "case_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation_001", "slice": "natural_answerable_20docs"}
{"query": "From 2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt, what exact term fills [BLANK] in: \"The posed query and selected documents are [BLANK] into a coherent prompt to which a large language model is tasked with formulating a response.\"?", "gold": "synthesized", "relevant_doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "relevant_chunk_id": 22, "relevant_text": "The posed query and selected documents are synthesized into a coherent prompt to which a large language model is tasked with formulating a response.", "case_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey_003", "slice": "natural_answerable_20docs"}
{"query": "From 2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt, what exact term fills [BLANK] in: \"42.9% in Top-5 accuracy), but also results in a [BLANK] improvement on the end-to-end QA accuracy compared to ORQA (41.5% vs.\"?", "gold": "substantial", "relevant_doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "relevant_chunk_id": 11, "relevant_text": "42.9% in Top-5 accuracy), but also results in a substantial improvement on the end-to-end QA accuracy compared to ORQA (41.5% vs.", "case_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering_017", "slice": "natural_answerable_20docs"}
{"query": "From 2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt, what exact term fills [BLANK] in: \"However, common Python interfaces to deep learning such as PyTorch and TensorÔ¨Çow do not allow Ô¨Å[BLANK] control of memory access.\"?", "gold": "ne-grained", "relevant_doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "relevant_chunk_id": 11, "relevant_text": "However, common Python interfaces to deep learning such as PyTorch and TensorÔ¨Çow do not allow Ô¨Åne-grained control of memory access.", "case_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness_017", "slice": "natural_answerable_20docs"}
{"query": "From 2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt, what exact term fills [BLANK] in: \"In this work, we provide a [BLANK] evaluation comparing Orca 2 to several other models.\"?", "gold": "comprehensive", "relevant_doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "relevant_chunk_id": 19, "relevant_text": "In this work, we provide a comprehensive evaluation comparing Orca 2 to several other models.", "case_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason_002", "slice": "natural_answerable_20docs"}
{"query": "From 2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt, what exact term fills [BLANK] in: \"In addition to training LoRA weights in linear layers, LongLoRA further makes embedding and [BLANK] layers trainable.\"?", "gold": "normalization", "relevant_doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "relevant_chunk_id": 8, "relevant_text": "In addition to training LoRA weights in linear layers, LongLoRA further makes embedding and normalization layers trainable.", "case_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models_005", "slice": "natural_answerable_20docs"}
{"query": "From 2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt, what exact term fills [BLANK] in: \"Next, we collect a dataset of [BLANK] comparisons between outputs from our models on a larger set of API prompts.\"?", "gold": "human-labeled", "relevant_doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "relevant_chunk_id": 11, "relevant_text": "Next, we collect a dataset of human-labeled comparisons between outputs from our models on a larger set of API prompts.", "case_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback_004", "slice": "natural_answerable_20docs"}
{"query": "From 2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt, what exact term fills [BLANK] in: \"Wespliteach ùëõ-[BLANK] ùëã=¬πùë•1¬î¬ì¬ì¬ì¬îùë•ùëõ¬∫intoasequenceof ùëôchunks¬πùê∂1¬î¬ì¬ì¬ì¬îùê∂ùëô¬∫ of sizeùëö=ùëõ ùëô, i.e.ùê∂1,¬πùë•1¬î¬ì¬ì¬ì¬îùë•ùëö¬∫¬î ¬ì¬ì¬ì¬î ùê∂ùëô,¬πùë•ùëõ\u0000ùëö¬∏1¬î¬ì¬ì¬ì¬îùë•ùëõ¬∫2ùïçùëö.\"?", "gold": "token-longexample", "relevant_doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "relevant_chunk_id": 22, "relevant_text": "Wespliteach ùëõ-token-longexample ùëã=¬πùë•1¬î¬ì¬ì¬ì¬îùë•ùëõ¬∫intoasequenceof ùëôchunks¬πùê∂1¬î¬ì¬ì¬ì¬îùê∂ùëô¬∫ of sizeùëö=ùëõ ùëô, i.e.ùê∂1,¬πùë•1¬î¬ì¬ì¬ì¬îùë•ùëö¬∫¬î ¬ì¬ì¬ì¬î ùê∂ùëô,¬πùë•ùëõ\u0000ùëö¬∏1¬î¬ì¬ì¬ì¬îùë•ùëõ¬∫2ùïçùëö.", "case_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens_001", "slice": "natural_answerable_20docs"}
{"query": "From 2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt, what exact term fills [BLANK] in: \"2 These three novel designs significantly boost the overall performance of RAG on open-domain question- answering tasks like NQ (Kwiatkowski et al., 2019), HotpotQA (Yang et al., 2018), Qasper (Dasigi et al., 2021) and [BLANK] (Bai et al., 2023).\"?", "gold": "multifieldqa-en", "relevant_doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "relevant_chunk_id": 14, "relevant_text": "2 These three novel designs significantly boost the overall performance of RAG on open-domain question- answering tasks like NQ (Kwiatkowski et al., 2019), HotpotQA (Yang et al., 2018), Qasper (Dasigi et al., 2021) and MultiFieldQA-en (Bai et al., 2023).", "case_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms_016", "slice": "natural_answerable_20docs"}
{"query": "From 2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt, what exact term fills [BLANK] in: \"Then we feed these retrieved units ( ‚âà30K tokens) to an existing [BLANK] LLM to perform zero-sho\"?", "gold": "long-context", "relevant_doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "relevant_chunk_id": 3, "relevant_text": "Then we feed these retrieved units ( ‚âà30K tokens) to an existing long-context LLM to perform zero-sho", "case_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms_018", "slice": "natural_answerable_20docs"}
{"query": "From 2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt, what exact term fills [BLANK] in: \"[BLANK] trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq.\"?", "gold": "flashattention", "relevant_doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "relevant_chunk_id": 3, "relevant_text": "FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq.", "case_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness_018", "slice": "natural_answerable_20docs"}
{"query": "From 2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt, what exact term fills [BLANK] in: \"Other works (Wu et al., 2022; Bulatov et al., 2022) utilize memory mechanisms as a [BLANK] on past inputs, to look up relevant tokens.\"?", "gold": "compression", "relevant_doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "relevant_chunk_id": 23, "relevant_text": "Other works (Wu et al., 2022; Bulatov et al., 2022) utilize memory mechanisms as a compression on past inputs, to look up relevant tokens.", "case_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models_014", "slice": "natural_answerable_20docs"}
{"query": "From 2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt, what exact term fills [BLANK] in: \"ls with larger context windows (e.g., 4096, 32K, and even 100K tokens), but it remains unclear how these [BLANK] language models make use of their input contexts when performing downstream tasks.\"?", "gold": "extended-context", "relevant_doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "relevant_chunk_id": 8, "relevant_text": "ls with larger context windows (e.g., 4096, 32K, and even 100K tokens), but it remains unclear how these extended-context language models make use of their input contexts when performing downstream tasks.", "case_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts_007", "slice": "natural_answerable_20docs"}
{"query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"How this is done depends entirely on the API itself ‚Äì for example, it can involve call- ing another neural network, [BLANK] a Python script or using a retrieval system to perform search over a large corpus.\"?", "gold": "executing", "relevant_doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "relevant_chunk_id": 22, "relevant_text": "How this is done depends entirely on the API itself ‚Äì for example, it can involve call- ing another neural network, executing a Python script or using a retrieval system to perform search over a large corpus.", "case_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools_001", "slice": "natural_answerable_20docs"}
{"query": "From 2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt, what exact term fills [BLANK] in: \"2 Published as a conference paper at ICLR 2023 We conduct empirical evaluations of ReAct and [BLANK] baselines on four diverse benchmarks: question answering (HotPotQA, Yang et al., 2018),\"?", "gold": "state-of-the-art", "relevant_doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "relevant_chunk_id": 20, "relevant_text": "2 Published as a conference paper at ICLR 2023 We conduct empirical evaluations of ReAct and state-of-the-art baselines on four diverse benchmarks: question answering (HotPotQA, Yang et al., 2018),", "case_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models_017", "slice": "natural_answerable_20docs"}
{"query": "From 2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt, what exact term fills [BLANK] in: \"[BLANK] autoregressive token models Our approach uses retrieval as a way to augment input examples at the granularity of small chunks of tokens.\"?", "gold": "retrieval-enhanced", "relevant_doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "relevant_chunk_id": 21, "relevant_text": "Retrieval-enhanced autoregressive token models Our approach uses retrieval as a way to augment input examples at the granularity of small chunks of tokens.", "case_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens_020", "slice": "natural_answerable_20docs"}
{"query": "From 2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"explain how these three quality aspects can be measured in a fully automated way, by [BLANK] an LLM.\"?", "gold": "prompting", "relevant_doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "relevant_chunk_id": 23, "relevant_text": "explain how these three quality aspects can be measured in a fully automated way, by prompting an LLM.", "case_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation_014", "slice": "natural_answerable_20docs"}
{"query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \"Such re- strictions are sub-optimal for [BLANK] tasks, and could be very harmful when applying Ô¨Åne- tuning based approaches to token-level tasks such as question answering, where it is crucial to incor- porate context from both directions.\"?", "gold": "sentence-level", "relevant_doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "relevant_chunk_id": 7, "relevant_text": "Such re- strictions are sub-optimal for sentence-level tasks, and could be very harmful when applying Ô¨Åne- tuning based approaches to token-level tasks such as question answering, where it is crucial to incor- porate context from both directions.", "case_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding_010", "slice": "natural_answerable_20docs"}
{"query": "From 2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt, what exact term fills [BLANK] in: \"FlashAttention enables the Ô¨Årst Transformer that can achieve [BLANK] performance on the Path-X [ 80] challenge, solely from using a longer sequence length (16K).\"?", "gold": "better-than-chance", "relevant_doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "relevant_chunk_id": 19, "relevant_text": "FlashAttention enables the Ô¨Årst Transformer that can achieve better-than-chance performance on the Path-X [ 80] challenge, solely from using a longer sequence length (16K).", "case_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness_019", "slice": "natural_answerable_20docs"}
{"query": "From 2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt, what exact term fills [BLANK] in: \"progress on aligning language models by training them to act in [BLANK] with the user‚Äôs intention (Leike et al., 2018).\"?", "gold": "accordance", "relevant_doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "relevant_chunk_id": 8, "relevant_text": "progress on aligning language models by training them to act in accordance with the user‚Äôs intention (Leike et al., 2018).", "case_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback_005", "slice": "natural_answerable_20docs"}
{"query": "From 2302.13971__llama_open_and_efficient_foundation_language_models.txt, what exact term fills [BLANK] in: \"We also removed the comments from the .tex Ô¨Åles, and [BLANK] deÔ¨Ånitions and macros written by users to i\"?", "gold": "inline-expanded", "relevant_doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "relevant_chunk_id": 16, "relevant_text": "We also removed the comments from the .tex Ô¨Åles, and inline-expanded deÔ¨Ånitions and macros written by users to i", "case_id": "2302.13971__llama_open_and_efficient_foundation_language_models_016", "slice": "natural_answerable_20docs"}
{"query": "From 2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt, what exact term fills [BLANK] in: \"nces, since the time and memory complexity of [BLANK] are quadratic in sequence length.\"?", "gold": "self-attention", "relevant_doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "relevant_chunk_id": 1, "relevant_text": "nces, since the time and memory complexity of self-attention are quadratic in sequence length.", "case_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness_013", "slice": "natural_answerable_20docs"}
{"query": "From 2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt, what exact term fills [BLANK] in: \"The more that [BLANK] thinking and analysis is required for a problem (Evans, 2010), the greater the diversity of reasoning paths that can recover the answer.\"?", "gold": "deliberate", "relevant_doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "relevant_chunk_id": 6, "relevant_text": "The more that deliberate thinking and analysis is required for a problem (Evans, 2010), the greater the diversity of reasoning paths that can recover the answer.", "case_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models_010", "slice": "natural_answerable_20docs"}
{"query": "From 2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt, what exact term fills [BLANK] in: \"Chi‚Ä† Sharan Narang‚Ä†Aakanksha Chowdhery‚Ä†Denny Zhou‚Ä†¬ß ‚Ä†Google Research, Brain Team ‚Ä°xuezhiw@google.com ,¬ßdennyzhou@google.com ABSTRACT [BLANK] prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks.\"?", "gold": "chain-of-thought", "relevant_doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "relevant_chunk_id": 0, "relevant_text": "Chi‚Ä† Sharan Narang‚Ä†Aakanksha Chowdhery‚Ä†Denny Zhou‚Ä†¬ß ‚Ä†Google Research, Brain Team ‚Ä°xuezhiw@google.com ,¬ßdennyzhou@google.com ABSTRACT Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks.", "case_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models_013", "slice": "natural_answerable_20docs"}
{"query": "From 2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt, what exact term fills [BLANK] in: \"For each attention head, the [BLANK] attention enforces different sparsity patterns over KV cache.\"?", "gold": "blocksparse", "relevant_doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "relevant_chunk_id": 12, "relevant_text": "For each attention head, the blocksparse attention enforces different sparsity patterns over KV cache.", "case_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone_017", "slice": "natural_answerable_20docs"}
{"query": "From 2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt, what exact term fills [BLANK] in: \"On the other hand, block-sparse [BLANK] is faster than all existing approximate attention methods that we know of.\"?", "gold": "flashattention", "relevant_doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "relevant_chunk_id": 20, "relevant_text": "On the other hand, block-sparse FlashAttention is faster than all existing approximate attention methods that we know of.", "case_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness_020", "slice": "natural_answerable_20docs"}
{"query": "From 2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt, what exact term fills [BLANK] in: \"Research on training small LMs has often relied on imitation learning to [BLANK] the output of more capable models.\"?", "gold": "replicate", "relevant_doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "relevant_chunk_id": 1, "relevant_text": "Research on training small LMs has often relied on imitation learning to replicate the output of more capable models.", "case_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason_013", "slice": "natural_answerable_20docs"}
{"query": "From 2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt, what exact term fills [BLANK] in: \"It has been observed that [BLANK] prompting signiÔ¨Åcantly improves model performance across a variety of multi-step reasoning tasks\"?", "gold": "chain-of-thought", "relevant_doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "relevant_chunk_id": 4, "relevant_text": "It has been observed that chain-of-thought prompting signiÔ¨Åcantly improves model performance across a variety of multi-step reasoning tasks", "case_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models_018", "slice": "natural_answerable_20docs"}
{"query": "From 2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt, what exact term fills [BLANK] in: \"2.1 Long Retriever The [BLANK] RAG framework employs smaller retrieval units and prioritizes retrieving the exact fine- grained short context containing the answer.\"?", "gold": "traditional", "relevant_doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "relevant_chunk_id": 21, "relevant_text": "2.1 Long Retriever The traditional RAG framework employs smaller retrieval units and prioritizes retrieving the exact fine- grained short context containing the answer.", "case_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms_003", "slice": "natural_answerable_20docs"}
{"query": "From 2302.13971__llama_open_and_efficient_foundation_language_models.txt, what exact term fills [BLANK] in: \"We train large [BLANK] on a large quantity of textual data using a standard optimizer.\"?", "gold": "transformers", "relevant_doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "relevant_chunk_id": 9, "relevant_text": "We train large transformers on a large quantity of textual data using a standard optimizer.", "case_id": "2302.13971__llama_open_and_efficient_foundation_language_models_004", "slice": "natural_answerable_20docs"}
{"query": "From 2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt, what exact term fills [BLANK] in: \"On the contrary, the reader only needs to extract answers from 1arXiv:2406.15319v3 [cs.CL] 1 Sep 2024 [BLANK] RAG Retrieval Ranker Reader LongRAG Long Retrieval Long Reader Figure 1: Traditional RAG vs.\"?", "gold": "traditional", "relevant_doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "relevant_chunk_id": 7, "relevant_text": "On the contrary, the reader only needs to extract answers from 1arXiv:2406.15319v3 [cs.CL] 1 Sep 2024 Traditional RAG Retrieval Ranker Reader LongRAG Long Retrieval Long Reader Figure 1: Traditional RAG vs.", "case_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms_015", "slice": "natural_answerable_20docs"}
{"query": "From 2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt, what exact term fills [BLANK] in: \"### [BLANK](GPT-3.5-Turbo) : John and Mark will likely think that the ball is in the box, since that is where John left it before he left for work.\"?", "gold": "chatgpt", "relevant_doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "relevant_chunk_id": 10, "relevant_text": "### ChatGPT(GPT-3.5-Turbo) : John and Mark will likely think that the ball is in the box, since that is where John left it before he left for work.", "case_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason_015", "slice": "natural_answerable_20docs"}
{"query": "From 2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt, what exact term fills [BLANK] in: \"Le Denny Zhou Google Research, Brain Team {jasonwei,dennyzhou}@google.com Abstract We explore how generating a chain of thought ‚Äîa series of [BLANK] reasoning steps‚ÄîsigniÔ¨Åcantly improves the ability of large language models to perform complex reasoning.\"?", "gold": "intermediate", "relevant_doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "relevant_chunk_id": 0, "relevant_text": "Le Denny Zhou Google Research, Brain Team {jasonwei,dennyzhou}@google.com Abstract We explore how generating a chain of thought ‚Äîa series of intermediate reasoning steps‚ÄîsigniÔ¨Åcantly improves the ability of large language models to perform complex reasoning.", "case_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models_013", "slice": "natural_answerable_20docs"}
{"query": "From 2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt, what exact term fills [BLANK] in: \"We also provide [BLANK] results with a 7B, 14B models trained for 4.8T tokens, called phi- 3-small ,phi-3-medium , both significantly more capable than phi-3-mini (e.g., respectively 75%, 78% on MMLU, and 8.7, 8.9 on MT-bench).\"?", "gold": "parameter-scaling", "relevant_doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "relevant_chunk_id": 1, "relevant_text": "We also provide parameter-scaling results with a 7B, 14B models trained for 4.8T tokens, called phi- 3-small ,phi-3-medium , both significantly more capable than phi-3-mini (e.g., respectively 75%, 78% on MMLU, and 8.7, 8.9 on MT-bench).", "case_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone_013", "slice": "natural_answerable_20docs"}
{"query": "From 2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt, what exact term fills [BLANK] in: \"In this work, we present ReAct , a general paradigm to combine [BLANK] and acting with language models for solving diverse language reasoning and decision making tasks (Figure 1).\"?", "gold": "reasoning", "relevant_doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "relevant_chunk_id": 19, "relevant_text": "In this work, we present ReAct , a general paradigm to combine reasoning and acting with language models for solving diverse language reasoning and decision making tasks (Figure 1).", "case_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models_004", "slice": "natural_answerable_20docs"}
{"query": "From 2005.14165__language_models_are_few_shot_learners.txt, what exact term fills [BLANK] in: \"There is evidence that suggests that the [BLANK] achieved under this paradigm can be poor because th\"?", "gold": "generalization", "relevant_doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "relevant_chunk_id": 15, "relevant_text": "There is evidence that suggests that the generalization achieved under this paradigm can be poor because th", "case_id": "2005.14165__language_models_are_few_shot_learners_004", "slice": "natural_answerable_20docs"}
{"query": "From 2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt, what exact term fills [BLANK] in: \"This kind of [BLANK] design, with a ‚Äúheavy‚Äù retriever and a ‚Äúlight‚Äù reader, puts too much pressure on the retriever.\"?", "gold": "imbalanced", "relevant_doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "relevant_chunk_id": 9, "relevant_text": "This kind of imbalanced design, with a ‚Äúheavy‚Äù retriever and a ‚Äúlight‚Äù reader, puts too much pressure on the retriever.", "case_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms_004", "slice": "natural_answerable_20docs"}
{"query": "From 2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt, what exact term fills [BLANK] in: \"Scaling up the size of lan- guage models has been shown to confer a range of beneÔ¨Åts, such as improved [BLANK] and sample efÔ¨Åciency (Ka- plan et al., 2020; Brown et al., 2020, inter alia ).\"?", "gold": "performance", "relevant_doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "relevant_chunk_id": 6, "relevant_text": "Scaling up the size of lan- guage models has been shown to confer a range of beneÔ¨Åts, such as improved performance and sample efÔ¨Åciency (Ka- plan et al., 2020; Brown et al., 2020, inter alia ).", "case_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models_010", "slice": "natural_answerable_20docs"}
{"query": "From 2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt, what exact term fills [BLANK] in: \"Index Terms ‚ÄîLarge language model, [BLANK] gen- eration, natural language processing\"?", "gold": "retrieval-augmented", "relevant_doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "relevant_chunk_id": 3, "relevant_text": "Index Terms ‚ÄîLarge language model, retrieval-augmented gen- eration, natural language processing", "case_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey_018", "slice": "natural_answerable_20docs"}
{"query": "From 1706.03762__attention_is_all_you_need.txt, what exact term fills [BLANK] in: \"3.1 Encoder and Decoder Stacks Encoder: The encoder is composed of a stack of N= 6 [BLANK] layers.\"?", "gold": "identical", "relevant_doc_id": "1706.03762__attention_is_all_you_need.txt", "relevant_chunk_id": 16, "relevant_text": "3.1 Encoder and Decoder Stacks Encoder: The encoder is composed of a stack of N= 6 identical layers.", "case_id": "1706.03762__attention_is_all_you_need_006", "slice": "natural_answerable_20docs"}
{"query": "From 2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt, what exact term fills [BLANK] in: \"We [BLANK] probe InstructGPT‚Äôs capabilities, and Ô¨Ånd that it is able to follow instructions for summarizing code, answer questions about code, and sometimes follows instructions\"?", "gold": "qualitatively", "relevant_doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "relevant_chunk_id": 23, "relevant_text": "We qualitatively probe InstructGPT‚Äôs capabilities, and Ô¨Ånd that it is able to follow instructions for summarizing code, answer questions about code, and sometimes follows instructions", "case_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback_014", "slice": "natural_answerable_20docs"}
{"query": "From 2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt, what exact term fills [BLANK] in: \"o generate [BLANK]Ô¨Åc actions or plans, and then use a controller to choose or execute them.\"?", "gold": "domain-speci", "relevant_doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "relevant_chunk_id": 18, "relevant_text": "o generate domain-speciÔ¨Åc actions or plans, and then use a controller to choose or execute them.", "case_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models_015", "slice": "natural_answerable_20docs"}
{"query": "From 1706.03762__attention_is_all_you_need.txt, what exact term fills [BLANK] in: \"3.2.1 Scaled [BLANK] Attention We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2).\"?", "gold": "dot-product", "relevant_doc_id": "1706.03762__attention_is_all_you_need.txt", "relevant_chunk_id": 20, "relevant_text": "3.2.1 Scaled Dot-Product Attention We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2).", "case_id": "1706.03762__attention_is_all_you_need_020", "slice": "natural_answerable_20docs"}
{"query": "From 2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt, what exact term fills [BLANK] in: \"We Ô¨Årst split each of the [BLANK] into text passages of equal lengths as the basic retrieval units3and getM total passages in our corpus C={p1,\"?", "gold": "documents", "relevant_doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "relevant_chunk_id": 14, "relevant_text": "We Ô¨Årst split each of the documents into text passages of equal lengths as the basic retrieval units3and getM total passages in our corpus C={p1,", "case_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering_012", "slice": "natural_answerable_20docs"}
{"query": "From 2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt, what exact term fills [BLANK] in: \"This behavior cloning has been shown to be very effective in [BLANK] the style of the teacher model.\"?", "gold": "mimicking", "relevant_doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "relevant_chunk_id": 23, "relevant_text": "This behavior cloning has been shown to be very effective in mimicking the style of the teacher model.", "case_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason_001", "slice": "natural_answerable_20docs"}
{"query": "From 2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt, what exact term fills [BLANK] in: \"The training retrieval database is made of the same subsets as the training data, in [BLANK] that matc\"?", "gold": "proportion", "relevant_doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "relevant_chunk_id": 19, "relevant_text": "The training retrieval database is made of the same subsets as the training data, in proportion that matc", "case_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens_011", "slice": "natural_answerable_20docs"}
{"query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \"2 Related Work There is a long history of pre-training general lan- guage [BLANK], and we brieÔ¨Çy review the most widely-used approaches in this section.\"?", "gold": "representations", "relevant_doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "relevant_chunk_id": 11, "relevant_text": "2 Related Work There is a long history of pre-training general lan- guage representations, and we brieÔ¨Çy review the most widely-used approaches in this section.", "case_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding_004", "slice": "natural_answerable_20docs"}
{"query": "From 2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt, what exact term fills [BLANK] in: \"Our models generalize to the [BLANK] of ‚Äúheld-out‚Äù labelers that did not produce any train- ing data.\"?", "gold": "preferences", "relevant_doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "relevant_chunk_id": 20, "relevant_text": "Our models generalize to the preferences of ‚Äúheld-out‚Äù labelers that did not produce any train- ing data.", "case_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback_019", "slice": "natural_answerable_20docs"}
{"query": "From 2305.06983__active_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"Most existing retrieval aug- mented LMs employ a [BLANK] setup that only retrieves information once based on the input.\"?", "gold": "retrieve-and-generate", "relevant_doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "relevant_chunk_id": 1, "relevant_text": "Most existing retrieval aug- mented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input.", "case_id": "2305.06983__active_retrieval_augmented_generation_013", "slice": "natural_answerable_20docs"}
{"query": "From 2305.06983__active_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"and previously generated output y<t= [y0, ...,yt‚àí1]: qt=qry(x,y<t), where qry(¬∑)is the query [BLANK] function.\"?", "gold": "formulation", "relevant_doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "relevant_chunk_id": 23, "relevant_text": "and previously generated output y<t= [y0, ...,yt‚àí1]: qt=qry(x,y<t), where qry(¬∑)is the query formulation function.", "case_id": "2305.06983__active_retrieval_augmented_generation_002", "slice": "natural_answerable_20docs"}
{"query": "From 2302.13971__llama_open_and_efficient_foundation_language_models.txt, what exact term fills [BLANK] in: \"We train our models on trillions of tokens, and show that it is possible to train [BLANK] models using pu\"?", "gold": "state-of-the-art", "relevant_doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "relevant_chunk_id": 0, "relevant_text": "We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using pu", "case_id": "2302.13971__llama_open_and_efficient_foundation_language_models_009", "slice": "natural_answerable_20docs"}
{"query": "From 2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt, what exact term fills [BLANK] in: \"ignment tax‚Äù since our alignment procedure comes at the cost of 3 lower [BLANK] on certain tasks that we may care about.\"?", "gold": "performance", "relevant_doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "relevant_chunk_id": 20, "relevant_text": "ignment tax‚Äù since our alignment procedure comes at the cost of 3 lower performance on certain tasks that we may care about.", "case_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback_011", "slice": "natural_answerable_20docs"}
{"query": "From 2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt, what exact term fills [BLANK] in: \"However, more work is needed to study how these models perform on broader groups of users, and how they perform on inputs where humans [BLANK] about the desired behavior.\"?", "gold": "disagree", "relevant_doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "relevant_chunk_id": 21, "relevant_text": "However, more work is needed to study how these models perform on broader groups of users, and how they perform on inputs where humans disagree about the desired behavior.", "case_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback_008", "slice": "natural_answerable_20docs"}
{"query": "From 2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt, what exact term fills [BLANK] in: \"This assumption is now [BLANK] disrupted by the existence of frontier LLMs themselves, which allow us to inte\"?", "gold": "significantly", "relevant_doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "relevant_chunk_id": 4, "relevant_text": "This assumption is now significantly disrupted by the existence of frontier LLMs themselves, which allow us to inte", "case_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone_018", "slice": "natural_answerable_20docs"}
{"query": "From 2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt, what exact term fills [BLANK] in: \"Knowledge in the form of natural language can be entirely offloaded from the [BLANK] knowledge of LLMs by leveragi\"?", "gold": "parametric", "relevant_doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "relevant_chunk_id": 5, "relevant_text": "Knowledge in the form of natural language can be entirely offloaded from the parametric knowledge of LLMs by leveragi", "case_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms_007", "slice": "natural_answerable_20docs"}
{"query": "From 2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt, what exact term fills [BLANK] in: \"Scaling LLMs like GPT-4 [ 44] and PaLM-2 [ 1] to ever more parameters led to emergent abilities [ 63] unseen in smaller models (less than ‚àº10B parameters), most notably the [BLANK] ability to reason zero-shot [ 23].\"?", "gold": "remarkable", "relevant_doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "relevant_chunk_id": 12, "relevant_text": "Scaling LLMs like GPT-4 [ 44] and PaLM-2 [ 1] to ever more parameters led to emergent abilities [ 63] unseen in smaller models (less than ‚àº10B parameters), most notably the remarkable ability to reason zero-shot [ 23].", "case_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason_017", "slice": "natural_answerable_20docs"}
{"query": "From 2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt, what exact term fills [BLANK] in: \"SFT leverages highly curated [BLANK] data across diverse domains, e.g., math, coding, reasoning, conversation, model identity, and safety.\"?", "gold": "high-quality", "relevant_doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "relevant_chunk_id": 24, "relevant_text": "SFT leverages highly curated high-quality data across diverse domains, e.g., math, coding, reasoning, conversation, model identity, and safety.", "case_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone_003", "slice": "natural_answerable_20docs"}
{"query": "From 2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt, what exact term fills [BLANK] in: \"1arXiv:2404.14219v4 [cs.CL] 30 Aug 2024 User: Explain why it is [BLANK] that one can build a language model small enough to fit on a phone, yet almost as powerful as ChatGPT.\"?", "gold": "surprising", "relevant_doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "relevant_chunk_id": 7, "relevant_text": "1arXiv:2404.14219v4 [cs.CL] 30 Aug 2024 User: Explain why it is surprising that one can build a language model small enough to fit on a phone, yet almost as powerful as ChatGPT.", "case_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone_007", "slice": "natural_answerable_20docs"}
{"query": "From 2302.13971__llama_open_and_efficient_foundation_language_models.txt, what exact term fills [BLANK] in: \"Here are the main difference with the original architec- ture, and where we were found the inspiration for this change (in bracket): [BLANK] [GPT3].\"?", "gold": "pre-normalization", "relevant_doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "relevant_chunk_id": 20, "relevant_text": "Here are the main difference with the original architec- ture, and where we were found the inspiration for this change (in bracket): Pre-normalization [GPT3].", "case_id": "2302.13971__llama_open_and_efficient_foundation_language_models_019", "slice": "natural_answerable_20docs"}
{"query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"However, all of these models have several inherent [BLANK] that can at best be partially addressed by further scal- ing.\"?", "gold": "limitations", "relevant_doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "relevant_chunk_id": 3, "relevant_text": "However, all of these models have several inherent limitations that can at best be partially addressed by further scal- ing.", "case_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools_018", "slice": "natural_answerable_20docs"}
{"query": "From 2005.14165__language_models_are_few_shot_learners.txt, what exact term fills [BLANK] in: \"While it has shown some initial promise, this approach still achieves results far inferior to Ô¨Åne-tuning ‚Äì for example [RWC+19] achieves only 4% on Natural [BLANK], and even its 55 F1 CoQa result is now more than 35 points\"?", "gold": "questions", "relevant_doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "relevant_chunk_id": 22, "relevant_text": "While it has shown some initial promise, this approach still achieves results far inferior to Ô¨Åne-tuning ‚Äì for example [RWC+19] achieves only 4% on Natural Questions, and even its 55 F1 CoQa result is now more than 35 points", "case_id": "2005.14165__language_models_are_few_shot_learners_006", "slice": "natural_answerable_20docs"}
{"query": "From 2305.06983__active_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"e to avoid unneces- sary or [BLANK] retrieval that occurs in passive retrieval augmented LMs (Khandelwal et al., 2020; Borgeaud et al., 2022; Ram et al., 2023; Trivedi et al., 2022).\"?", "gold": "inappropriate", "relevant_doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "relevant_chunk_id": 15, "relevant_text": "e to avoid unneces- sary or inappropriate retrieval that occurs in passive retrieval augmented LMs (Khandelwal et al., 2020; Borgeaud et al., 2022; Ram et al., 2023; Trivedi et al., 2022).", "case_id": "2305.06983__active_retrieval_augmented_generation_004", "slice": "natural_answerable_20docs"}
{"query": "From 2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt, what exact term fills [BLANK] in: \"[BLANK] is up to 3\u0002faster than the standard attention implemen- tation across common sequence lengths from 128 to 2K and scales up to 64K.\"?", "gold": "flashattention", "relevant_doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "relevant_chunk_id": 20, "relevant_text": "FlashAttention is up to 3\u0002faster than the standard attention implemen- tation across common sequence lengths from 128 to 2K and scales up to 64K.", "case_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness_008", "slice": "natural_answerable_20docs"}
{"query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"After Ô¨Åltering, we merge API calls for different tools, [BLANK] in the augmented dataset C‚àó, and Ô¨Ånetune 1In practice, we use the token sequences ‚Äú [‚Äù, ‚Äú]‚Äù and ‚Äú->‚Äù to represent ‚Äú <API> ‚Äù, ‚Äú</API> ‚Äù and ‚Äú‚Üí‚Äù, respec- tively.\"?", "gold": "resulting", "relevant_doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "relevant_chunk_id": 17, "relevant_text": "After Ô¨Åltering, we merge API calls for different tools, resulting in the augmented dataset C‚àó, and Ô¨Ånetune 1In practice, we use the token sequences ‚Äú [‚Äù, ‚Äú]‚Äù and ‚Äú->‚Äù to represent ‚Äú <API> ‚Äù, ‚Äú</API> ‚Äù and ‚Äú‚Üí‚Äù, respec- tively.", "case_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools_006", "slice": "natural_answerable_20docs"}
{"query": "From 2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt, what exact term fills [BLANK] in: \"During evaluation the retrieval database consists in the full union of these datasets, with the exception of books for which we use a [BLANK] of 4%.\"?", "gold": "sub-sample", "relevant_doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "relevant_chunk_id": 20, "relevant_text": "During evaluation the retrieval database consists in the full union of these datasets, with the exception of books for which we use a sub-sample of 4%.", "case_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens_019", "slice": "natural_answerable_20docs"}
{"query": "From 2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt, what exact term fills [BLANK] in: \"just the order of the [BLANK] to change the position of the document that contains the answer (Figure 3).\"?", "gold": "documents", "relevant_doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "relevant_chunk_id": 27, "relevant_text": "just the order of the documents to change the position of the document that contains the answer (Figure 3).", "case_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts_008", "slice": "natural_answerable_20docs"}
{"query": "From 2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt, what exact term fills [BLANK] in: \"They both come back [BLANK] later in the day, and they do not know what happened in the room after each of them left the room.\"?", "gold": "together", "relevant_doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "relevant_chunk_id": 6, "relevant_text": "They both come back together later in the day, and they do not know what happened in the room after each of them left the room.", "case_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason_010", "slice": "natural_answerable_20docs"}
{"query": "From 2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"Evaluating RAG [BLANK] is, however, chal- lenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability\"?", "gold": "architectures", "relevant_doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "relevant_chunk_id": 1, "relevant_text": "Evaluating RAG architectures is, however, chal- lenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability", "case_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation_013", "slice": "natural_answerable_20docs"}
{"query": "From 2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt, what exact term fills [BLANK] in: \"Hence instruction tuned models will be always limited by the knowledge learned during [BLANK].\"?", "gold": "pre-training", "relevant_doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "relevant_chunk_id": 24, "relevant_text": "Hence instruction tuned models will be always limited by the knowledge learned during pre-training.", "case_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason_003", "slice": "natural_answerable_20docs"}
{"query": "From 2305.06983__active_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"The [BLANK] of the prompt is shown in Prompt 3.1, and full details can be found in Prompt D.3.\"?", "gold": "structure", "relevant_doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "relevant_chunk_id": 29, "relevant_text": "The structure of the prompt is shown in Prompt 3.1, and full details can be found in Prompt D.3.", "case_id": "2305.06983__active_retrieval_augmented_generation_014", "slice": "natural_answerable_20docs"}
{"query": "From 2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt, what exact term fills [BLANK] in: \"[BLANK] does not signiÔ¨Åcantly improve over GPT-3 on the Winogender (Rudinger et al., 2018) and CrowSPairs (Nangia et al., 2020) datasets.\"?", "gold": "instructgpt", "relevant_doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "relevant_chunk_id": 18, "relevant_text": "InstructGPT does not signiÔ¨Åcantly improve over GPT-3 on the Winogender (Rudinger et al., 2018) and CrowSPairs (Nangia et al., 2020) datasets.", "case_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback_002", "slice": "natural_answerable_20docs"}
{"query": "From 2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt, what exact term fills [BLANK] in: \"Achain of thought is a series of intermediate natural language reasoning steps that lead to the Ô¨Ånal output, and we refer to this approach as [BLANK] prompting .\"?", "gold": "chain-of-thought", "relevant_doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "relevant_chunk_id": 11, "relevant_text": "Achain of thought is a series of intermediate natural language reasoning steps that lead to the Ô¨Ånal output, and we refer to this approach as chain-of-thought prompting .", "case_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models_004", "slice": "natural_answerable_20docs"}
{"query": "From 2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt, what exact term fills [BLANK] in: \"These methods range from [BLANK] [ 51,74] to low-rank approximation [ 12,50,84], and their combinations [ 3,9,92].\"?", "gold": "sparse-approximation", "relevant_doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "relevant_chunk_id": 6, "relevant_text": "These methods range from sparse-approximation [ 51,74] to low-rank approximation [ 12,50,84], and their combinations [ 3,9,92].", "case_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness_007", "slice": "natural_answerable_20docs"}
{"query": "From 2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt, what exact term fills [BLANK] in: \"In [BLANK], we take the 2655 queries where the annotated long answer is a paragraph (as opposed to a list or a table).\"?", "gold": "particular", "relevant_doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "relevant_chunk_id": 24, "relevant_text": "In particular, we take the 2655 queries where the annotated long answer is a paragraph (as opposed to a list or a table).", "case_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts_011", "slice": "natural_answerable_20docs"}
{"query": "From 1706.03762__attention_is_all_you_need.txt, what exact term fills [BLANK] in: \"This masking, combined with fact that the output embeddings are offset by one position, ensures that the [BLANK] for position ican depend only on the known outputs at positions less than i.\"?", "gold": "predictions", "relevant_doc_id": "1706.03762__attention_is_all_you_need.txt", "relevant_chunk_id": 19, "relevant_text": "This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position ican depend only on the known outputs at positions less than i.", "case_id": "1706.03762__attention_is_all_you_need_019", "slice": "natural_answerable_20docs"}
{"query": "From 2302.13971__llama_open_and_efficient_foundation_language_models.txt, what exact term fills [BLANK] in: \"Data mixtures used for pre- training, for each subset we list the sampling propor- tion, number of epochs [BLANK] on the subset when training on 1.4T tokens, and disk size.\"?", "gold": "performed", "relevant_doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "relevant_chunk_id": 14, "relevant_text": "Data mixtures used for pre- training, for each subset we list the sampling propor- tion, number of epochs performed on the subset when training on 1.4T tokens, and disk size.", "case_id": "2302.13971__llama_open_and_efficient_foundation_language_models_012", "slice": "natural_answerable_20docs"}
{"query": "From 2302.13971__llama_open_and_efficient_foundation_language_models.txt, what exact term fills [BLANK] in: \"For instance, LLaMA-13B [BLANK] GPT-3 on most bench- marks, despite being 10 √ósmaller.\"?", "gold": "outperforms", "relevant_doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "relevant_chunk_id": 6, "relevant_text": "For instance, LLaMA-13B outperforms GPT-3 on most bench- marks, despite being 10 √ósmaller.", "case_id": "2302.13971__llama_open_and_efficient_foundation_language_models_007", "slice": "natural_answerable_20docs"}
{"query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"For each position i‚ààI, we then obtain up to m API callsc1 i,...,cm iby [BLANK] from Mgiven the sequence [P(x),x1,...,x i‚àí1,<API> ]as a p\"?", "gold": "sampling", "relevant_doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "relevant_chunk_id": 21, "relevant_text": "For each position i‚ààI, we then obtain up to m API callsc1 i,...,cm iby sampling from Mgiven the sequence [P(x),x1,...,x i‚àí1,<API> ]as a p", "case_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools_020", "slice": "natural_answerable_20docs"}
{"query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \"Unlike recent language repre- sentation models (Peters et al., 2018a; Rad- ford et al., 2018), BERT is designed to pre- train deep [BLANK] repre\"?", "gold": "bidirectional", "relevant_doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "relevant_chunk_id": 0, "relevant_text": "Unlike recent language repre- sentation models (Peters et al., 2018a; Rad- ford et al., 2018), BERT is designed to pre- train deep bidirectional repre", "case_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding_009", "slice": "natural_answerable_20docs"}
{"query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"In contrast to existing approaches, this enables a much more [BLANK] use of tools that is not tied to speciÔ¨Åc tasks.\"?", "gold": "comprehensive", "relevant_doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "relevant_chunk_id": 11, "relevant_text": "In contrast to existing approaches, this enables a much more comprehensive use of tools that is not tied to speciÔ¨Åc tasks.", "case_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools_004", "slice": "natural_answerable_20docs"}
{"query": "From 2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt, what exact term fills [BLANK] in: \"The development trajectory of RAG in the era of large models exhibits several distinct stage [BLANK].\"?", "gold": "characteristics", "relevant_doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "relevant_chunk_id": 6, "relevant_text": "The development trajectory of RAG in the era of large models exhibits several distinct stage characteristics.", "case_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey_007", "slice": "natural_answerable_20docs"}
{"query": "From 2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt, what exact term fills [BLANK] in: \"her [BLANK] prompting in this form can successfully elicit successful reasoning across a range of 3 Q: Roger has 5 tennis balls.\"?", "gold": "chain-of-thought", "relevant_doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "relevant_chunk_id": 24, "relevant_text": "her chain-of-thought prompting in this form can successfully elicit successful reasoning across a range of 3 Q: Roger has 5 tennis balls.", "case_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models_001", "slice": "natural_answerable_20docs"}
{"query": "From 2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt, what exact term fills [BLANK] in: \"4.Finally, [BLANK] reasoning can be readily elicited in sufÔ¨Åciently large off-the-shelf language models simply by including examples of chain of thought sequences into the exemplars of few-shot prompting.\"?", "gold": "chain-of-thought", "relevant_doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "relevant_chunk_id": 18, "relevant_text": "4.Finally, chain-of-thought reasoning can be readily elicited in sufÔ¨Åciently large off-the-shelf language models simply by including examples of chain of thought sequences into the exemplars of few-shot prompting.", "case_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models_002", "slice": "natural_answerable_20docs"}
{"query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"The New England Journal of Medicine is a registered trademark of [QA(‚ÄúWho is the publisher of The New England Journal of Medicine?‚Äù) ‚Üí [BLANK] Medical Society] the MMS.\"?", "gold": "massachusetts", "relevant_doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "relevant_chunk_id": 4, "relevant_text": "The New England Journal of Medicine is a registered trademark of [QA(‚ÄúWho is the publisher of The New England Journal of Medicine?‚Äù) ‚Üí Massachusetts Medical Society] the MMS.", "case_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools_010", "slice": "natural_answerable_20docs"}
{"query": "From 2305.06983__active_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"It [BLANK] generates search queries (shown in gray italic ) to retrieve relevant information to aid future generations.\"?", "gold": "iteratively", "relevant_doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "relevant_chunk_id": 28, "relevant_text": "It iteratively generates search queries (shown in gray italic ) to retrieve relevant information to aid future generations.", "case_id": "2305.06983__active_retrieval_augmented_generation_001", "slice": "natural_answerable_20docs"}
{"query": "From 2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt, what exact term fills [BLANK] in: \"This highlights the potential of endowing smaller models with better reasoning [BLANK].\"?", "gold": "capabilities", "relevant_doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "relevant_chunk_id": 20, "relevant_text": "This highlights the potential of endowing smaller models with better reasoning capabilities.", "case_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason_011", "slice": "natural_answerable_20docs"}
{"query": "From 2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt, what exact term fills [BLANK] in: \"This tight synergy between ‚Äúacting‚Äù and ‚Äúreasoning‚Äù allows humans to learn new tasks quickly and perform robust decision making or reasoning, even under previously unseen circumstances or facing information [BLANK].\"?", "gold": "uncertainties", "relevant_doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "relevant_chunk_id": 6, "relevant_text": "This tight synergy between ‚Äúacting‚Äù and ‚Äúreasoning‚Äù allows humans to learn new tasks quickly and perform robust decision making or reasoning, even under previously unseen circumstances or facing information uncertainties.", "case_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models_018", "slice": "natural_answerable_20docs"}
{"query": "From 2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt, what exact term fills [BLANK] in: \"Only a few top retrieval units (1 to 8 retrieval units in the four datasets we tested on), without [BLANK], are used for the next step.\"?", "gold": "re-ranking", "relevant_doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "relevant_chunk_id": 13, "relevant_text": "Only a few top retrieval units (1 to 8 retrieval units in the four datasets we tested on), without re-ranking, are used for the next step.", "case_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms_012", "slice": "natural_answerable_20docs"}
