{"query": "From 2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt, what exact term fills [BLANK] in: \"In additional experiments, we show [BLANK] can robustly boost performance on NLP tasks where adding a chain-of-thought might hurt performance compared to standard prompting (Ye & Durrett, 2022).\"?", "gold": "self-consistency", "relevant_doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "relevant_chunk_id": 15, "relevant_text": "In additional experiments, we show self-consistency can robustly boost performance on NLP tasks where adding a chain-of-thought might hurt performance compared to standard prompting (Ye & Durrett, 2022).", "case_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models_017", "slice": "natural_answerable_20docs"}
{"query": "From 2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt, what exact term fills [BLANK] in: \"As compute has gotten faster relative to memory speed [ 61,62,63], operations are [BLANK] bottlenecked by me\"?", "gold": "increasingly", "relevant_doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "relevant_chunk_id": 22, "relevant_text": "As compute has gotten faster relative to memory speed [ 61,62,63], operations are increasingly bottlenecked by me", "case_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness_014", "slice": "natural_answerable_20docs"}
{"query": "From 2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"Evaluating RAG [BLANK] is, however, chal- lenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability\"?", "gold": "architectures", "relevant_doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "relevant_chunk_id": 1, "relevant_text": "Evaluating RAG architectures is, however, chal- lenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability", "case_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation_013", "slice": "natural_answerable_20docs"}
{"query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \"More recently, sentence or document encoders which produce contextual token [BLANK] have been pre-trained from unlabeled text and ﬁne-tuned for a supervised downstream task (Dai and Le, 2015; Howard and R\"?", "gold": "representations", "relevant_doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "relevant_chunk_id": 17, "relevant_text": "More recently, sentence or document encoders which produce contextual token representations have been pre-trained from unlabeled text and ﬁne-tuned for a supervised downstream task (Dai and Le, 2015; Howard and R", "case_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding_012", "slice": "natural_answerable_20docs"}
{"query": "From 2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt, what exact term fills [BLANK] in: \"[BLANK] is up to 3\u0002faster than the standard attention implemen- tation across common sequence lengths from 128 to 2K and scales up to 64K.\"?", "gold": "flashattention", "relevant_doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "relevant_chunk_id": 20, "relevant_text": "FlashAttention is up to 3\u0002faster than the standard attention implemen- tation across common sequence lengths from 128 to 2K and scales up to 64K.", "case_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness_008", "slice": "natural_answerable_20docs"}
{"query": "From 2302.13971__llama_open_and_efficient_foundation_language_models.txt, what exact term fills [BLANK] in: \"and [BLANK] deﬁnitions and macros written by users to increase consistency across papers.\"?", "gold": "inline-expanded", "relevant_doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "relevant_chunk_id": 17, "relevant_text": "and inline-expanded deﬁnitions and macros written by users to increase consistency across papers.", "case_id": "2302.13971__llama_open_and_efficient_foundation_language_models_006", "slice": "natural_answerable_20docs"}
{"query": "From 2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt, what exact term fills [BLANK] in: \"It is designed to equip readers and professionals with a detailed and structured [BLANK] of both large models a\"?", "gold": "understanding", "relevant_doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "relevant_chunk_id": 10, "relevant_text": "It is designed to equip readers and professionals with a detailed and structured understanding of both large models a", "case_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey_017", "slice": "natural_answerable_20docs"}
{"query": "From 2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt, what exact term fills [BLANK] in: \"For example, large language models can generate outputs that are [BLANK], toxic, or simply not helpful to the user.\"?", "gold": "untruthful", "relevant_doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "relevant_chunk_id": 1, "relevant_text": "For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user.", "case_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback_013", "slice": "natural_answerable_20docs"}
{"query": "From 2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt, what exact term fills [BLANK] in: \"Training language models to follow [BLANK] with human feedback Long Ouyang∗Jeff Wu∗Xu Jiang∗Diogo Almeida∗Carroll L.\"?", "gold": "instructions", "relevant_doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "relevant_chunk_id": 0, "relevant_text": "Training language models to follow instructions with human feedback Long Ouyang∗Jeff Wu∗Xu Jiang∗Diogo Almeida∗Carroll L.", "case_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback_009", "slice": "natural_answerable_20docs"}
{"query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"How this is done depends entirely on the API itself – for example, it can involve call- ing another neural network, [BLANK] a Python script or using a retrieval system to perform search over a large corpus.\"?", "gold": "executing", "relevant_doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "relevant_chunk_id": 22, "relevant_text": "How this is done depends entirely on the API itself – for example, it can involve call- ing another neural network, executing a Python script or using a retrieval system to perform search over a large corpus.", "case_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools_001", "slice": "natural_answerable_20docs"}
{"query": "From 2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt, what exact term fills [BLANK] in: \"Our work is [BLANK] to these works, as our attention mechanism is unmodified during inference.\"?", "gold": "complementary", "relevant_doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "relevant_chunk_id": 22, "relevant_text": "Our work is complementary to these works, as our attention mechanism is unmodified during inference.", "case_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models_001", "slice": "natural_answerable_20docs"}
{"query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \"Dur- ing [BLANK], the model is trained on unlabeled data over different pre-training tasks.\"?", "gold": "pre-training", "relevant_doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "relevant_chunk_id": 22, "relevant_text": "Dur- ing pre-training, the model is trained on unlabeled data over different pre-training tasks.", "case_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding_011", "slice": "natural_answerable_20docs"}
{"query": "From 2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt, what exact term fills [BLANK] in: \"Since decision making and reasoning [BLANK] are integrated into a large language model, ReAct enjoys several unique features: A) Intuitive and easy to design : Designing ReAct promp\"?", "gold": "capabilities", "relevant_doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "relevant_chunk_id": 32, "relevant_text": "Since decision making and reasoning capabilities are integrated into a large language model, ReAct enjoys several unique features: A) Intuitive and easy to design : Designing ReAct promp", "case_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models_020", "slice": "natural_answerable_20docs"}
{"query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \"rs, and all of the param- eters are ﬁne-tuned using labeled data from the [BLANK] tasks.\"?", "gold": "downstream", "relevant_doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "relevant_chunk_id": 23, "relevant_text": "rs, and all of the param- eters are ﬁne-tuned using labeled data from the downstream tasks.", "case_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding_019", "slice": "natural_answerable_20docs"}
{"query": "From 2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt, what exact term fills [BLANK] in: \"cusing on enhancing language models by [BLANK] additional knowledge through Pre- Training Models (PTM).\"?", "gold": "incorporating", "relevant_doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "relevant_chunk_id": 7, "relevant_text": "cusing on enhancing language models by incorporating additional knowledge through Pre- Training Models (PTM).", "case_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey_005", "slice": "natural_answerable_20docs"}
{"query": "From 2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt, what exact term fills [BLANK] in: \"Chi† Sharan Narang†Aakanksha Chowdhery†Denny Zhou†§ †Google Research, Brain Team ‡xuezhiw@google.com ,§dennyzhou@google.com ABSTRACT [BLANK] prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks.\"?", "gold": "chain-of-thought", "relevant_doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "relevant_chunk_id": 0, "relevant_text": "Chi† Sharan Narang†Aakanksha Chowdhery†Denny Zhou†§ †Google Research, Brain Team ‡xuezhiw@google.com ,§dennyzhou@google.com ABSTRACT Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks.", "case_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models_013", "slice": "natural_answerable_20docs"}
{"query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"her the ob- tained responses are helpful for [BLANK] future tokens; this is used as a ﬁltering criterion.\"?", "gold": "predicting", "relevant_doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "relevant_chunk_id": 17, "relevant_text": "her the ob- tained responses are helpful for predicting future tokens; this is used as a ﬁltering criterion.", "case_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools_016", "slice": "natural_answerable_20docs"}
{"query": "From 2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt, what exact term fills [BLANK] in: \"lock-sparse FlashAttention , a sparse attention algorithm that is 2-4 \u0002faster than [BLANK] , scaling up to sequence length of 64k.\"?", "gold": "evenflashattention", "relevant_doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "relevant_chunk_id": 16, "relevant_text": "lock-sparse FlashAttention , a sparse attention algorithm that is 2-4 \u0002faster than evenFlashAttention , scaling up to sequence length of 64k.", "case_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness_016", "slice": "natural_answerable_20docs"}
{"query": "From 2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt, what exact term fills [BLANK] in: \"Write a [BLANK] answer for the given question using only the provided search results (some of which might be irrelevant).\"?", "gold": "high-quality", "relevant_doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "relevant_chunk_id": 29, "relevant_text": "Write a high-quality answer for the given question using only the provided search results (some of which might be irrelevant).", "case_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts_014", "slice": "natural_answerable_20docs"}
{"query": "From 2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt, what exact term fills [BLANK] in: \"2 These three novel designs significantly boost the overall performance of RAG on open-domain question- answering tasks like NQ (Kwiatkowski et al., 2019), HotpotQA (Yang et al., 2018), Qasper (Dasigi et al., 2021) and [BLANK] (Bai et al., 2023).\"?", "gold": "multifieldqa-en", "relevant_doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "relevant_chunk_id": 14, "relevant_text": "2 These three novel designs significantly boost the overall performance of RAG on open-domain question- answering tasks like NQ (Kwiatkowski et al., 2019), HotpotQA (Yang et al., 2018), Qasper (Dasigi et al., 2021) and MultiFieldQA-en (Bai et al., 2023).", "case_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms_016", "slice": "natural_answerable_20docs"}
{"query": "From 2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt, what exact term fills [BLANK] in: \"The reason behind this is that short attention resembles the attention scheme in the [BLANK] stage of LLMs.\"?", "gold": "pre-training", "relevant_doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "relevant_chunk_id": 15, "relevant_text": "The reason behind this is that short attention resembles the attention scheme in the pre-training stage of LLMs.", "case_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models_016", "slice": "natural_answerable_20docs"}
{"query": "From 2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt, what exact term fills [BLANK] in: \"ng low-rank matrices, which are generally efficient and reduce the number of trainable [BLANK].\"?", "gold": "parameters", "relevant_doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "relevant_chunk_id": 11, "relevant_text": "ng low-rank matrices, which are generally efficient and reduce the number of trainable parameters.", "case_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models_017", "slice": "natural_answerable_20docs"}
{"query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"You can call the API by writing \"[QA([BLANK])]\" where \"question\" is the question you want to ask.\"?", "gold": "question", "relevant_doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "relevant_chunk_id": 18, "relevant_text": "You can call the API by writing \"[QA(question)]\" where \"question\" is the question you want to ask.", "case_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools_002", "slice": "natural_answerable_20docs"}
{"query": "From 2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt, what exact term fills [BLANK] in: \"FlashAttention enables the ﬁrst Transformer that can achieve [BLANK] performance on the Path-X [ 80] challenge, solely from using a longer sequence length (16K).\"?", "gold": "better-than-chance", "relevant_doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "relevant_chunk_id": 19, "relevant_text": "FlashAttention enables the ﬁrst Transformer that can achieve better-than-chance performance on the Path-X [ 80] challenge, solely from using a longer sequence length (16K).", "case_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness_019", "slice": "natural_answerable_20docs"}
{"query": "From 2005.14165__language_models_are_few_shot_learners.txt, what exact term fills [BLANK] in: \"There is evidence that suggests that the [BLANK] achieved under this paradigm can be poor because th\"?", "gold": "generalization", "relevant_doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "relevant_chunk_id": 15, "relevant_text": "There is evidence that suggests that the generalization achieved under this paradigm can be poor because th", "case_id": "2005.14165__language_models_are_few_shot_learners_004", "slice": "natural_answerable_20docs"}
{"query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"After ﬁltering, we merge API calls for different tools, [BLANK] in the augmented dataset C∗, and ﬁnetune 1In practice, we use the token sequences “ [”, “]” and “->” to represent “ <API> ”, “</API> ” and “→”, respec- tively.\"?", "gold": "resulting", "relevant_doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "relevant_chunk_id": 17, "relevant_text": "After ﬁltering, we merge API calls for different tools, resulting in the augmented dataset C∗, and ﬁnetune 1In practice, we use the token sequences “ [”, “]” and “->” to represent “ <API> ”, “</API> ” and “→”, respec- tively.", "case_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools_006", "slice": "natural_answerable_20docs"}
{"query": "From 2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt, what exact term fills [BLANK] in: \"It has been observed that [BLANK] prompting signiﬁcantly improves model performance across a variety of multi-step reasoning tasks\"?", "gold": "chain-of-thought", "relevant_doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "relevant_chunk_id": 4, "relevant_text": "It has been observed that chain-of-thought prompting signiﬁcantly improves model performance across a variety of multi-step reasoning tasks", "case_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models_018", "slice": "natural_answerable_20docs"}
{"query": "From 2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt, what exact term fills [BLANK] in: \"ural networks have proven to be powerful language models, ﬁrst in the form of recurrent architectures (Graves, 2013; Jozefowicz et al., 2016; Mikolov et al., 2010) and more recently in the form of Transformers (Vaswani et al., 2017), that use attention to [BLANK] the past.\"?", "gold": "contextualise", "relevant_doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "relevant_chunk_id": 4, "relevant_text": "ural networks have proven to be powerful language models, ﬁrst in the form of recurrent architectures (Graves, 2013; Jozefowicz et al., 2016; Mikolov et al., 2010) and more recently in the form of Transformers (Vaswani et al., 2017), that use attention to contextualise the past.", "case_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens_018", "slice": "natural_answerable_20docs"}
{"query": "From 2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt, what exact term fills [BLANK] in: \"Remarkably, this has been successful for a range of simple [BLANK] tasks (Brown et al., 2020).\"?", "gold": "question-answering", "relevant_doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "relevant_chunk_id": 9, "relevant_text": "Remarkably, this has been successful for a range of simple question-answering tasks (Brown et al., 2020).", "case_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models_015", "slice": "natural_answerable_20docs"}
{"query": "From 2305.06983__active_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"If so (step 2 and 3), the system retrieves relevant documents and [BLANK] the sentence.\"?", "gold": "regenerates", "relevant_doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "relevant_chunk_id": 11, "relevant_text": "If so (step 2 and 3), the system retrieves relevant documents and regenerates the sentence.", "case_id": "2305.06983__active_retrieval_augmented_generation_005", "slice": "natural_answerable_20docs"}
{"query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"We represent each API call as a tuple c= (ac,ic) whereacis the name of the API and icis the cor- [BLANK] input.\"?", "gold": "responding", "relevant_doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "relevant_chunk_id": 15, "relevant_text": "We represent each API call as a tuple c= (ac,ic) whereacis the name of the API and icis the cor- responding input.", "case_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools_012", "slice": "natural_answerable_20docs"}
{"query": "From 2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt, what exact term fills [BLANK] in: \"Outputs from our 175B [BLANK] are preferred to 175B GPT-3 outputs 85 ±3% of the time, and preferred 71 ±4% of the time to few-shot 175B GPT-3.\"?", "gold": "instructgpt", "relevant_doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "relevant_chunk_id": 16, "relevant_text": "Outputs from our 175B InstructGPT are preferred to 175B GPT-3 outputs 85 ±3% of the time, and preferred 71 ±4% of the time to few-shot 175B GPT-3.", "case_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback_012", "slice": "natural_answerable_20docs"}
{"query": "From 2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt, what exact term fills [BLANK] in: \"nstruction Tuning [BLANK] tuning [ 46,38,62,61] has emerged as a crucial step in training language models.\"?", "gold": "instruction", "relevant_doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "relevant_chunk_id": 22, "relevant_text": "nstruction Tuning Instruction tuning [ 46,38,62,61] has emerged as a crucial step in training language models.", "case_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason_008", "slice": "natural_answerable_20docs"}
{"query": "From 2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natu- ral language layer between a user and textual databases, reducing the risk of [BLANK].\"?", "gold": "hallucinations", "relevant_doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "relevant_chunk_id": 1, "relevant_text": "RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natu- ral language layer between a user and textual databases, reducing the risk of hallucinations.", "case_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation_009", "slice": "natural_answerable_20docs"}
{"query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \"r [BLANK] such models (Howard and Ruder, 2018; Radford et al., 2018; Dai and Le, 2015).\"?", "gold": "pre-training", "relevant_doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "relevant_chunk_id": 21, "relevant_text": "r pre-training such models (Howard and Ruder, 2018; Radford et al., 2018; Dai and Le, 2015).", "case_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding_002", "slice": "natural_answerable_20docs"}
{"query": "From 2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt, what exact term fills [BLANK] in: \"Next, we collect a dataset of [BLANK] comparisons between outputs from our models on a larger set of API prompts.\"?", "gold": "human-labeled", "relevant_doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "relevant_chunk_id": 11, "relevant_text": "Next, we collect a dataset of human-labeled comparisons between outputs from our models on a larger set of API prompts.", "case_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback_004", "slice": "natural_answerable_20docs"}
{"query": "From 2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt, what exact term fills [BLANK] in: \"1.First, chain of thought, in principle, allows models to decompose multi-step problems into [BLANK] steps, which means that additional computation can be allocated to problems that require more reason\"?", "gold": "intermediate", "relevant_doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "relevant_chunk_id": 16, "relevant_text": "1.First, chain of thought, in principle, allows models to decompose multi-step problems into intermediate steps, which means that additional computation can be allocated to problems that require more reason", "case_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models_016", "slice": "natural_answerable_20docs"}
{"query": "From 2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt, what exact term fills [BLANK] in: \"The existing RAG framework tends to use short retrieval units, such as 100-word passages in popular open- domain [BLANK] tasks (Chen et al., 2017; Lewis et al., 2020; Karpukhin et al., 2020).\"?", "gold": "question-answering", "relevant_doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "relevant_chunk_id": 6, "relevant_text": "The existing RAG framework tends to use short retrieval units, such as 100-word passages in popular open- domain question-answering tasks (Chen et al., 2017; Lewis et al., 2020; Karpukhin et al., 2020).", "case_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms_005", "slice": "natural_answerable_20docs"}
{"query": "From 2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt, what exact term fills [BLANK] in: \"The question encoder and the reader model are then ﬁne- tuned using pairs of [BLANK] and answers jointly.\"?", "gold": "questions", "relevant_doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "relevant_chunk_id": 8, "relevant_text": "The question encoder and the reader model are then ﬁne- tuned using pairs of questions and answers jointly.", "case_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering_005", "slice": "natural_answerable_20docs"}
{"query": "From 2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt, what exact term fills [BLANK] in: \"As most of the datasets only have an [BLANK] split, we manually composed a set of eight few-shot exemplars with chains of thought for prompting—Figure 1 (right) shows one chain of thought exemplar, and the full set of exemplars is given in Appendix Table 20.\"?", "gold": "evaluation", "relevant_doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "relevant_chunk_id": 23, "relevant_text": "As most of the datasets only have an evaluation split, we manually composed a set of eight few-shot exemplars with chains of thought for prompting—Figure 1 (right) shows one chain of thought exemplar, and the full set of exemplars is given in Appendix Table 20.", "case_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models_020", "slice": "natural_answerable_20docs"}
{"query": "From 2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt, what exact term fills [BLANK] in: \"This highlights the potential of endowing smaller models with better reasoning [BLANK].\"?", "gold": "capabilities", "relevant_doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "relevant_chunk_id": 20, "relevant_text": "This highlights the potential of endowing smaller models with better reasoning capabilities.", "case_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason_011", "slice": "natural_answerable_20docs"}
{"query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \"In addi- tion to the masked language model, we also use a “next sentence prediction” task that jointly pre- trains text-pair [BLANK].\"?", "gold": "representations", "relevant_doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "relevant_chunk_id": 9, "relevant_text": "In addi- tion to the masked language model, we also use a “next sentence prediction” task that jointly pre- trains text-pair representations.", "case_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding_005", "slice": "natural_answerable_20docs"}
{"query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \"The same [BLANK] model parameters are used to initialize models for different down-stream tasks.\"?", "gold": "pre-trained", "relevant_doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "relevant_chunk_id": 20, "relevant_text": "The same pre-trained model parameters are used to initialize models for different down-stream tasks.", "case_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding_016", "slice": "natural_answerable_20docs"}
{"query": "From 2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"For models that do not provide access to token [BLANK], such as ChatGPT and GPT-4, differ- ent methods are needed.\"?", "gold": "probabilities", "relevant_doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "relevant_chunk_id": 14, "relevant_text": "For models that do not provide access to token probabilities, such as ChatGPT and GPT-4, differ- ent methods are needed.", "case_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation_012", "slice": "natural_answerable_20docs"}
{"query": "From 2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt, what exact term fills [BLANK] in: \"LongLoRA can [BLANK] Llama2 7B up to 100k context, or a 70B model up to 32k, on a single 8×A100 machine.\"?", "gold": "fine-tune", "relevant_doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "relevant_chunk_id": 20, "relevant_text": "LongLoRA can fine-tune Llama2 7B up to 100k context, or a 70B model up to 32k, on a single 8×A100 machine.", "case_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models_019", "slice": "natural_answerable_20docs"}
{"query": "From 2305.06983__active_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"We discard [BLANK] retrieved documents ∪t′<tDqt′and only use the retrieved documents from the current step to condition the next generation to prevent reaching the input length limit of LMs.\"?", "gold": "previously", "relevant_doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "relevant_chunk_id": 24, "relevant_text": "We discard previously retrieved documents ∪t′<tDqt′and only use the retrieved documents from the current step to condition the next generation to prevent reaching the input length limit of LMs.", "case_id": "2305.06983__active_retrieval_augmented_generation_011", "slice": "natural_answerable_20docs"}
{"query": "From 2305.06983__active_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"These attempts include methods that passively use the past context to retrieve additional [BLANK] at a fixed interval (Khandelwal et\"?", "gold": "information", "relevant_doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "relevant_chunk_id": 12, "relevant_text": "These attempts include methods that passively use the past context to retrieve additional information at a fixed interval (Khandelwal et", "case_id": "2305.06983__active_retrieval_augmented_generation_015", "slice": "natural_answerable_20docs"}
{"query": "From 2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt, what exact term fills [BLANK] in: \"With a 2 trillion token database, our [BLANK] Transformer ( R/e.sc/t.sc/r.sc/o.sc) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25 \u0002fewer parameters.\"?", "gold": "retrieval-enhanced", "relevant_doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "relevant_chunk_id": 1, "relevant_text": "With a 2 trillion token database, our Retrieval-Enhanced Transformer ( R/e.sc/t.sc/r.sc/o.sc) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25 \u0002fewer parameters.", "case_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens_013", "slice": "natural_answerable_20docs"}
{"query": "From 2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt, what exact term fills [BLANK] in: \"Knowledge in the form of natural language can be entirely offloaded from the [BLANK] knowledge of LLMs by leveragi\"?", "gold": "parametric", "relevant_doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "relevant_chunk_id": 5, "relevant_text": "Knowledge in the form of natural language can be entirely offloaded from the parametric knowledge of LLMs by leveragi", "case_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms_007", "slice": "natural_answerable_20docs"}
{"query": "From 2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt, what exact term fills [BLANK] in: \"In Orca 2, we continue exploring how improved training signals can enhance smaller LMs’ [BLANK] abilities.\"?", "gold": "reasoning", "relevant_doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "relevant_chunk_id": 1, "relevant_text": "In Orca 2, we continue exploring how improved training signals can enhance smaller LMs’ reasoning abilities.", "case_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason_009", "slice": "natural_answerable_20docs"}
{"query": "From 2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt, what exact term fills [BLANK] in: \"In addition to training LoRA weights in linear layers, LongLoRA further makes embedding and [BLANK] layers trainable.\"?", "gold": "normalization", "relevant_doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "relevant_chunk_id": 8, "relevant_text": "In addition to training LoRA weights in linear layers, LongLoRA further makes embedding and normalization layers trainable.", "case_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models_005", "slice": "natural_answerable_20docs"}
{"query": "From 2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt, what exact term fills [BLANK] in: \"Each kernel loads inputs from HBM to [BLANK] and SRAM, computes, then writes outputs to HBM.\"?", "gold": "registers", "relevant_doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "relevant_chunk_id": 23, "relevant_text": "Each kernel loads inputs from HBM to registers and SRAM, computes, then writes outputs to HBM.", "case_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness_003", "slice": "natural_answerable_20docs"}
{"query": "From 2302.13971__llama_open_and_efficient_foundation_language_models.txt, what exact term fills [BLANK] in: \"2.3 Optimizer Our models are trained using the AdamW opti- mizer (Loshchilov and Hutter, 2017), with the fol- lowing [BLANK]: β1= 0.9,β2= 0.95.\"?", "gold": "hyper-parameters", "relevant_doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "relevant_chunk_id": 22, "relevant_text": "2.3 Optimizer Our models are trained using the AdamW opti- mizer (Loshchilov and Hutter, 2017), with the fol- lowing hyper-parameters: β1= 0.9,β2= 0.95.", "case_id": "2302.13971__llama_open_and_efficient_foundation_language_models_020", "slice": "natural_answerable_20docs"}
{"query": "From 2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt, what exact term fills [BLANK] in: \"In this paper, we mainly focus on the setup where a frozen large language model, PaLM-540B (Chowdhery et al., 2022)1, is prompted with few-shot in-context examples to generate both [BLANK]ﬁc actions and free-form language thoughts for task solving (Figure 1 (1d), (2b)).\"?", "gold": "domain-speci", "relevant_doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "relevant_chunk_id": 30, "relevant_text": "In this paper, we mainly focus on the setup where a frozen large language model, PaLM-540B (Chowdhery et al., 2022)1, is prompted with few-shot in-context examples to generate both domain-speciﬁc actions and free-form language thoughts for task solving (Figure 1 (1d), (2b)).", "case_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models_011", "slice": "natural_answerable_20docs"}
{"query": "From 2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt, what exact term fills [BLANK] in: \"They both come back [BLANK] later in the day, and they do not know what happened in the room after each of them left the room.\"?", "gold": "together", "relevant_doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "relevant_chunk_id": 6, "relevant_text": "They both come back together later in the day, and they do not know what happened in the room after each of them left the room.", "case_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason_010", "slice": "natural_answerable_20docs"}
{"query": "From 2305.06983__active_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"2.3 Active Retrieval Augmented Generation To aid [BLANK] generation with retrieval, we pro- pose active retrieval augmented generation.\"?", "gold": "long-form", "relevant_doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "relevant_chunk_id": 22, "relevant_text": "2.3 Active Retrieval Augmented Generation To aid long-form generation with retrieval, we pro- pose active retrieval augmented generation.", "case_id": "2305.06983__active_retrieval_augmented_generation_016", "slice": "natural_answerable_20docs"}
{"query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"In contrast to existing approaches, this enables a much more [BLANK] use of tools that is not tied to speciﬁc tasks.\"?", "gold": "comprehensive", "relevant_doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "relevant_chunk_id": 11, "relevant_text": "In contrast to existing approaches, this enables a much more comprehensive use of tools that is not tied to speciﬁc tasks.", "case_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools_004", "slice": "natural_answerable_20docs"}
{"query": "From 2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt, what exact term fills [BLANK] in: \"dels (LLMs) showcase impres- sive capabilities but encounter challenges like hallucination, outdated knowledge, and [BLANK], untraceable reasoning processes.\"?", "gold": "non-transparent", "relevant_doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "relevant_chunk_id": 1, "relevant_text": "dels (LLMs) showcase impres- sive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes.", "case_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey_009", "slice": "natural_answerable_20docs"}
{"query": "From 2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt, what exact term fills [BLANK] in: \"It [BLANK] MoE layer as its feedforward models, employing the top2 routing among 16 expert networks.\"?", "gold": "incorporates", "relevant_doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "relevant_chunk_id": 14, "relevant_text": "It incorporates MoE layer as its feedforward models, employing the top2 routing among 16 expert networks.", "case_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone_012", "slice": "natural_answerable_20docs"}
{"query": "From 2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt, what exact term fills [BLANK] in: \"We open-source [BLANK] to make it easier to build on this primitive.1 We empirically va\"?", "gold": "flashattention", "relevant_doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "relevant_chunk_id": 16, "relevant_text": "We open-source FlashAttention to make it easier to build on this primitive.1 We empirically va", "case_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness_002", "slice": "natural_answerable_20docs"}
{"query": "From 2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt, what exact term fills [BLANK] in: \"So the answer is no.Q: The concert was [BLANK] to be on 06/01/1943, but was delayed by one day to today.\"?", "gold": "scheduled", "relevant_doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "relevant_chunk_id": 25, "relevant_text": "So the answer is no.Q: The concert was scheduled to be on 06/01/1943, but was delayed by one day to today.", "case_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models_003", "slice": "natural_answerable_20docs"}
{"query": "From 2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt, what exact term fills [BLANK] in: \"operates on long retrieval units, with only a few ([BLANK] fewer than 10) being fed into the reader.\"?", "gold": "typically", "relevant_doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "relevant_chunk_id": 21, "relevant_text": "operates on long retrieval units, with only a few (typically fewer than 10) being fed into the reader.", "case_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms_014", "slice": "natural_answerable_20docs"}
{"query": "From 2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt, what exact term fills [BLANK] in: \"Like Orca 1, we utilize more capable LLMs to [BLANK] various reasoning strategies across various tasks.\"?", "gold": "demonstrate", "relevant_doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "relevant_chunk_id": 16, "relevant_text": "Like Orca 1, we utilize more capable LLMs to demonstrate various reasoning strategies across various tasks.", "case_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason_012", "slice": "natural_answerable_20docs"}
{"query": "From 2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt, what exact term fills [BLANK] in: \"1arXiv:2309.12307v3 [cs.CL] 8 Mar 2024 Published as a conference paper at ICLR 2024 !Trainable ❄[BLANK](-headSelf-A1en(onFeed ForwardNorminput++Lora !\"?", "gold": "frozennormpostmul", "relevant_doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "relevant_chunk_id": 6, "relevant_text": "1arXiv:2309.12307v3 [cs.CL] 8 Mar 2024 Published as a conference paper at ICLR 2024 !Trainable ❄FrozenNormpostMul(-headSelf-A1en(onFeed ForwardNorminput++Lora !", "case_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models_007", "slice": "natural_answerable_20docs"}
{"query": "From 2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt, what exact term fills [BLANK] in: \"progress on aligning language models by training them to act in [BLANK] with the user’s intention (Leike et al., 2018).\"?", "gold": "accordance", "relevant_doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "relevant_chunk_id": 8, "relevant_text": "progress on aligning language models by training them to act in accordance with the user’s intention (Leike et al., 2018).", "case_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback_005", "slice": "natural_answerable_20docs"}
{"query": "From 1706.03762__attention_is_all_you_need.txt, what exact term fills [BLANK] in: \"ransformer follows this overall architecture using stacked [BLANK] and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.\"?", "gold": "self-attention", "relevant_doc_id": "1706.03762__attention_is_all_you_need.txt", "relevant_chunk_id": 16, "relevant_text": "ransformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.", "case_id": "1706.03762__attention_is_all_you_need_016", "slice": "natural_answerable_20docs"}
{"query": "From 2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt, what exact term fills [BLANK] in: \"HU\u0003RQ\u0003D\u0003GUDZHU\u0011 \u0015 \u0003$OI:RUOG \u0014 \u0003+RWVSRW\u00034$ Figure 1: (1) Comparison of 4 prompting methods, (a) Standard , (b) [BLANK] ( CoT, Reason Only), (c) Act-only, and (d) ReAct (Reason+Act), solving a HotpotQA (Yang et al., 2018) question.\"?", "gold": "chain-of-thought", "relevant_doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "relevant_chunk_id": 15, "relevant_text": "HU\u0003RQ\u0003D\u0003GUDZHU\u0011 \u0015 \u0003$OI:RUOG \u0014 \u0003+RWVSRW\u00034$ Figure 1: (1) Comparison of 4 prompting methods, (a) Standard , (b) Chain-of-thought ( CoT, Reason Only), (c) Act-only, and (d) ReAct (Reason+Act), solving a HotpotQA (Yang et al., 2018) question.", "case_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models_010", "slice": "natural_answerable_20docs"}
{"query": "From 2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt, what exact term fills [BLANK] in: \"The model is already [BLANK], and the chat template is as follows: <|user|> /n Question <|end|> /n <|assistant|> Thephi-3-small model (7B parameters) leverages the tiktoken tokenizer (for better multilingual tokenization) with a vocabulary size of 1003522and has default context length 8192.\"?", "gold": "chat-finetuned", "relevant_doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "relevant_chunk_id": 10, "relevant_text": "The model is already chat-finetuned, and the chat template is as follows: <|user|> /n Question <|end|> /n <|assistant|> Thephi-3-small model (7B parameters) leverages the tiktoken tokenizer (for better multilingual tokenization) with a vocabulary size of 1003522and has default context length 8192.", "case_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone_004", "slice": "natural_answerable_20docs"}
{"query": "From 2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt, what exact term fills [BLANK] in: \"[BLANK] and closed-domain QA), InstructGPT models make up information not present in the input about half as often as GPT-3 (a 21% vs.\"?", "gold": "summarization", "relevant_doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "relevant_chunk_id": 17, "relevant_text": "summarization and closed-domain QA), InstructGPT models make up information not present in the input about half as often as GPT-3 (a 21% vs.", "case_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback_016", "slice": "natural_answerable_20docs"}
{"query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"For each position i∈I, we then obtain up to m API callsc1 i,...,cm iby [BLANK] from Mgiven the sequence [P(x),x1,...,x i−1,<API> ]as a p\"?", "gold": "sampling", "relevant_doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "relevant_chunk_id": 21, "relevant_text": "For each position i∈I, we then obtain up to m API callsc1 i,...,cm iby sampling from Mgiven the sequence [P(x),x1,...,x i−1,<API> ]as a p", "case_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools_020", "slice": "natural_answerable_20docs"}
{"query": "From 2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt, what exact term fills [BLANK] in: \"rformance across tasks in language [BLANK] and interactive decision making, their abilities for reasoning (e.g.\"?", "gold": "understanding", "relevant_doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "relevant_chunk_id": 1, "relevant_text": "rformance across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g.", "case_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models_009", "slice": "natural_answerable_20docs"}
{"query": "From 2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt, what exact term fills [BLANK] in: \"Le Denny Zhou Google Research, Brain Team {jasonwei,dennyzhou}@google.com Abstract We explore how generating a chain of thought —a series of [BLANK] reasoning steps—signiﬁcantly improves the ability of large language models to perform complex reasoning.\"?", "gold": "intermediate", "relevant_doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "relevant_chunk_id": 0, "relevant_text": "Le Denny Zhou Google Research, Brain Team {jasonwei,dennyzhou}@google.com Abstract We explore how generating a chain of thought —a series of intermediate reasoning steps—signiﬁcantly improves the ability of large language models to perform complex reasoning.", "case_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models_013", "slice": "natural_answerable_20docs"}
{"query": "From 2305.06983__active_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"t al., 2017), where the informa- tion needs are clear in the user’s input, and it is [BLANK] to retrieve relevant knowledge once solely based on the input .\"?", "gold": "sufficient", "relevant_doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "relevant_chunk_id": 7, "relevant_text": "t al., 2017), where the informa- tion needs are clear in the user’s input, and it is sufficient to retrieve relevant knowledge once solely based on the input .", "case_id": "2305.06983__active_retrieval_augmented_generation_010", "slice": "natural_answerable_20docs"}
{"query": "From 2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt, what exact term fills [BLANK] in: \"First, a language model is prompted with a set of manually written [BLANK] exemplars (Wei et al., 2022).\"?", "gold": "chain-of-thought", "relevant_doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "relevant_chunk_id": 18, "relevant_text": "First, a language model is prompted with a set of manually written chain-of-thought exemplars (Wei et al., 2022).", "case_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models_012", "slice": "natural_answerable_20docs"}
{"query": "From 2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt, what exact term fills [BLANK] in: \"Token likelihoods are provided by a model, [BLANK] by 𝜃, that takes as input both previous tokens and their retrieved neighbours.\"?", "gold": "parameterized", "relevant_doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "relevant_chunk_id": 23, "relevant_text": "Token likelihoods are provided by a model, parameterized by 𝜃, that takes as input both previous tokens and their retrieved neighbours.", "case_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens_014", "slice": "natural_answerable_20docs"}
{"query": "From 2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt, what exact term fills [BLANK] in: \"The more that [BLANK] thinking and analysis is required for a problem (Evans, 2010), the greater the diversity of reasoning paths that can recover the answer.\"?", "gold": "deliberate", "relevant_doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "relevant_chunk_id": 6, "relevant_text": "The more that deliberate thinking and analysis is required for a problem (Evans, 2010), the greater the diversity of reasoning paths that can recover the answer.", "case_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models_010", "slice": "natural_answerable_20docs"}
{"query": "From 2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt, what exact term fills [BLANK] in: \"Naive RAG The Naive RAG research paradigm represents the earli- est [BLANK], which gained prominence shortly after the 3 Fig.\"?", "gold": "methodology", "relevant_doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "relevant_chunk_id": 18, "relevant_text": "Naive RAG The Naive RAG research paradigm represents the earli- est methodology, which gained prominence shortly after the 3 Fig.", "case_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey_019", "slice": "natural_answerable_20docs"}
{"query": "From 2005.14165__language_models_are_few_shot_learners.txt, what exact term fills [BLANK] in: \"Figure 1.2 [BLANK] the conditions we study, and shows few-shot learning of a simple task requiring the model to remove extraneous symbols from a word.\"?", "gold": "illustrates", "relevant_doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "relevant_chunk_id": 30, "relevant_text": "Figure 1.2 illustrates the conditions we study, and shows few-shot learning of a simple task requiring the model to remove extraneous symbols from a word.", "case_id": "2005.14165__language_models_are_few_shot_learners_014", "slice": "natural_answerable_20docs"}
{"query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"We ﬁrst sample up to kcandidate posi- tions for doing API calls by computing, for each i∈{1,...,n}, the probability pi=pM(<API>|P(x),x1:i−1) [BLANK] to starting an API call at position i.\"?", "gold": "thatmassigns", "relevant_doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "relevant_chunk_id": 21, "relevant_text": "We ﬁrst sample up to kcandidate posi- tions for doing API calls by computing, for each i∈{1,...,n}, the probability pi=pM(<API>|P(x),x1:i−1) thatMassigns to starting an API call at position i.", "case_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools_008", "slice": "natural_answerable_20docs"}
{"query": "From 2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"3 Evaluation [BLANK] We consider a standard RAG setting, where given a question q, the system first retrieves some context c(q)and then uses the retrieved context to generate an answer as(q).\"?", "gold": "strategies", "relevant_doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "relevant_chunk_id": 19, "relevant_text": "3 Evaluation Strategies We consider a standard RAG setting, where given a question q, the system first retrieves some context c(q)and then uses the retrieved context to generate an answer as(q).", "case_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation_019", "slice": "natural_answerable_20docs"}
{"query": "From 2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt, what exact term fills [BLANK] in: \"Published as a conference paper at ICLR 2023 [BLANK] IMPROVES CHAIN OF THOUGHT REASONING IN LANGUAGE MODELS Xuezhi Wang†‡Jason Wei†Dale Schuurmans†Quoc Le†Ed H.\"?", "gold": "self-consistency", "relevant_doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "relevant_chunk_id": 0, "relevant_text": "Published as a conference paper at ICLR 2023 SELF-CONSISTENCY IMPROVES CHAIN OF THOUGHT REASONING IN LANGUAGE MODELS Xuezhi Wang†‡Jason Wei†Dale Schuurmans†Quoc Le†Ed H.", "case_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models_009", "slice": "natural_answerable_20docs"}
{"query": "From 2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt, what exact term fills [BLANK] in: \"Orca 2 [BLANK] surpasses models of similar size and attains performance levels similar or better to those of models 5-10x larger, as assessed on complex tasks that test advanced reasoning abilities in zero-shot settings.\"?", "gold": "significantly", "relevant_doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "relevant_chunk_id": 3, "relevant_text": "Orca 2 significantly surpasses models of similar size and attains performance levels similar or better to those of models 5-10x larger, as assessed on complex tasks that test advanced reasoning abilities in zero-shot settings.", "case_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason_018", "slice": "natural_answerable_20docs"}
{"query": "From 2005.14165__language_models_are_few_shot_learners.txt, what exact term fills [BLANK] in: \"To avoid this confusion, we use the term “meta-learning” to capture the inner-loop / outer-loop structure of the general method, and the term “in [BLANK]” to refer to the inner loop of meta-learning.\"?", "gold": "context-learning", "relevant_doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "relevant_chunk_id": 26, "relevant_text": "To avoid this confusion, we use the term “meta-learning” to capture the inner-loop / outer-loop structure of the general method, and the term “in context-learning” to refer to the inner loop of meta-learning.", "case_id": "2005.14165__language_models_are_few_shot_learners_019", "slice": "natural_answerable_20docs"}
{"query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"In this paper, we show that LMs can teach [BLANK] to use external tools via simple APIs and achieve the best of both worlds.\"?", "gold": "themselves", "relevant_doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "relevant_chunk_id": 1, "relevant_text": "In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds.", "case_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools_013", "slice": "natural_answerable_20docs"}
{"query": "From 2302.13971__llama_open_and_efficient_foundation_language_models.txt, what exact term fills [BLANK] in: \"This leads to the following mixture of data and the per- centage they represent in the training set: English [BLANK] [67%].\"?", "gold": "commoncrawl", "relevant_doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "relevant_chunk_id": 10, "relevant_text": "This leads to the following mixture of data and the per- centage they represent in the training set: English CommonCrawl [67%].", "case_id": "2302.13971__llama_open_and_efficient_foundation_language_models_017", "slice": "natural_answerable_20docs"}
{"query": "From 2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt, what exact term fills [BLANK] in: \"Our models generalize to the [BLANK] of “held-out” labelers that did not produce any train- ing data.\"?", "gold": "preferences", "relevant_doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "relevant_chunk_id": 20, "relevant_text": "Our models generalize to the preferences of “held-out” labelers that did not produce any train- ing data.", "case_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback_019", "slice": "natural_answerable_20docs"}
{"query": "From 2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt, what exact term fills [BLANK] in: \"3 Published as a conference paper at ICLR 2023 sampling, as commonly used for [BLANK] text generation (Radford et al., 2019; Brown et al., 2020; Thoppilan et al., 2022), to achieve this goal.\"?", "gold": "open-ended", "relevant_doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "relevant_chunk_id": 30, "relevant_text": "3 Published as a conference paper at ICLR 2023 sampling, as commonly used for open-ended text generation (Radford et al., 2019; Brown et al., 2020; Thoppilan et al., 2022), to achieve this goal.", "case_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models_001", "slice": "natural_answerable_20docs"}
{"query": "From 2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt, what exact term fills [BLANK] in: \"We use a chunked [BLANK] module to incorporate the retrieved text (§2.4), with time complexity linear in the amount of retrieved data.\"?", "gold": "cross-attention", "relevant_doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "relevant_chunk_id": 11, "relevant_text": "We use a chunked cross-attention module to incorporate the retrieved text (§2.4), with time complexity linear in the amount of retrieved data.", "case_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens_004", "slice": "natural_answerable_20docs"}
{"query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \"ELMo and its predecessor (Peters et al., 2017, 2018a) generalize [BLANK] word embedding re- search along a diffe\"?", "gold": "traditional", "relevant_doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "relevant_chunk_id": 14, "relevant_text": "ELMo and its predecessor (Peters et al., 2017, 2018a) generalize traditional word embedding re- search along a diffe", "case_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding_017", "slice": "natural_answerable_20docs"}
{"query": "From 2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt, what exact term fills [BLANK] in: \"For example our previous model trained on this data recipe, phi-2 (2.7B parameters), matched the [BLANK] of models 25 times large\"?", "gold": "performance", "relevant_doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "relevant_chunk_id": 5, "relevant_text": "For example our previous model trained on this data recipe, phi-2 (2.7B parameters), matched the performance of models 25 times large", "case_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone_010", "slice": "natural_answerable_20docs"}
{"query": "From 2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt, what exact term fills [BLANK] in: \"3.1 Experimental Setup We explore [BLANK] prompting for various language models on multiple benchmarks.\"?", "gold": "chain-of-thought", "relevant_doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "relevant_chunk_id": 20, "relevant_text": "3.1 Experimental Setup We explore chain-of-thought prompting for various language models on multiple benchmarks.", "case_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models_019", "slice": "natural_answerable_20docs"}
{"query": "From 2005.14165__language_models_are_few_shot_learners.txt, what exact term fills [BLANK] in: \"Ziegler Jeffrey Wu Clemens Winter [BLANK] Hesse Mark Chen Eric Sigler Mateusz Litwin Scott Gray Benjamin Chess Jack Clark Christopher Berner Sam McCandlish Alec Radford Ilya Sutskever Dario Amodei OpenAI Abstract Recent wo\"?", "gold": "christopher", "relevant_doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "relevant_chunk_id": 0, "relevant_text": "Ziegler Jeffrey Wu Clemens Winter Christopher Hesse Mark Chen Eric Sigler Mateusz Litwin Scott Gray Benjamin Chess Jack Clark Christopher Berner Sam McCandlish Alec Radford Ilya Sutskever Dario Amodei OpenAI Abstract Recent wo", "case_id": "2005.14165__language_models_are_few_shot_learners_013", "slice": "natural_answerable_20docs"}
{"query": "From 2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt, what exact term fills [BLANK] in: \"Sample a diverse set of [BLANK] paths She eats 3 for breakfast, so she has 16 - 3 = 13 left.\"?", "gold": "reasoning", "relevant_doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "relevant_chunk_id": 9, "relevant_text": "Sample a diverse set of reasoning paths She eats 3 for breakfast, so she has 16 - 3 = 13 left.", "case_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models_005", "slice": "natural_answerable_20docs"}
{"query": "From 2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt, what exact term fills [BLANK] in: \"One limitation of these works is that these [BLANK] have a large gap to full attention, making it infeasible to\"?", "gold": "compressions", "relevant_doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "relevant_chunk_id": 23, "relevant_text": "One limitation of these works is that these compressions have a large gap to full attention, making it infeasible to", "case_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models_003", "slice": "natural_answerable_20docs"}
{"query": "From 2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt, what exact term fills [BLANK] in: \"We study this question with a [BLANK] key- value retrieval task, which is designed to be a mini- mal testbed for the basic ability to retrieve matching tokens from the input context.\"?", "gold": "synthetic", "relevant_doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "relevant_chunk_id": 14, "relevant_text": "We study this question with a synthetic key- value retrieval task, which is designed to be a mini- mal testbed for the basic ability to retrieve matching tokens from the input context.", "case_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts_004", "slice": "natural_answerable_20docs"}
{"query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \", 2018), BERT is designed to pre- train deep bidirectional [BLANK] from unlabeled text by jointly conditioning on both left and right context in all layers.\"?", "gold": "representations", "relevant_doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "relevant_chunk_id": 1, "relevant_text": ", 2018), BERT is designed to pre- train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.", "case_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding_013", "slice": "natural_answerable_20docs"}
{"query": "From 2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt, what exact term fills [BLANK] in: \"Document [1](Title: Asian Americans in science and [BLANK]) Prize in physics for discovery of the subatomic particle J/ψ.\"?", "gold": "technology", "relevant_doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "relevant_chunk_id": 29, "relevant_text": "Document [1](Title: Asian Americans in science and technology) Prize in physics for discovery of the subatomic particle J/ψ.", "case_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts_003", "slice": "natural_answerable_20docs"}
{"query": "From 2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt, what exact term fills [BLANK] in: \"Scaling LLMs like GPT-4 [ 44] and PaLM-2 [ 1] to ever more parameters led to emergent abilities [ 63] unseen in smaller models (less than ∼10B parameters), most notably the [BLANK] ability to reason zero-shot [ 23].\"?", "gold": "remarkable", "relevant_doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "relevant_chunk_id": 12, "relevant_text": "Scaling LLMs like GPT-4 [ 44] and PaLM-2 [ 1] to ever more parameters led to emergent abilities [ 63] unseen in smaller models (less than ∼10B parameters), most notably the remarkable ability to reason zero-shot [ 23].", "case_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason_017", "slice": "natural_answerable_20docs"}
{"query": "From 2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt, what exact term fills [BLANK] in: \"remove all training documents with high similarity (0.8 or higher) to a [BLANK] or test set document.\"?", "gold": "validation", "relevant_doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "relevant_chunk_id": 21, "relevant_text": "remove all training documents with high similarity (0.8 or higher) to a validation or test set document.", "case_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens_008", "slice": "natural_answerable_20docs"}
{"query": "From 2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt, what exact term fills [BLANK] in: \"Note that Mcan be very large (e.g., 21 million passages in our [BLANK], de- scribed in Section 4.1) and kis usually small, such as20–100.\"?", "gold": "experiments", "relevant_doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "relevant_chunk_id": 18, "relevant_text": "Note that Mcan be very large (e.g., 21 million passages in our experiments, de- scribed in Section 4.1) and kis usually small, such as20–100.", "case_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering_002", "slice": "natural_answerable_20docs"}
{"query": "From 2005.14165__language_models_are_few_shot_learners.txt, what exact term fills [BLANK] in: \"Since in-context learning involves absorbing many skills and tasks within the [BLANK] of the model, it is plaus\"?", "gold": "parameters", "relevant_doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "relevant_chunk_id": 24, "relevant_text": "Since in-context learning involves absorbing many skills and tasks within the parameters of the model, it is plaus", "case_id": "2005.14165__language_models_are_few_shot_learners_011", "slice": "natural_answerable_20docs"}
{"query": "From 2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt, what exact term fills [BLANK] in: \"GPUs, compute speed has out-paced memory speed [ 61,62,63], and most operations in [BLANK] are bottlenecked by memory accesses [ 43].\"?", "gold": "transformers", "relevant_doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "relevant_chunk_id": 10, "relevant_text": "GPUs, compute speed has out-paced memory speed [ 61,62,63], and most operations in Transformers are bottlenecked by memory accesses [ 43].", "case_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness_004", "slice": "natural_answerable_20docs"}
{"query": "From 2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"n this way, although it comes with the [BLANK] of being sensitive to the design of the prompt.\"?", "gold": "limitation", "relevant_doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "relevant_chunk_id": 17, "relevant_text": "n this way, although it comes with the limitation of being sensitive to the design of the prompt.", "case_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation_002", "slice": "natural_answerable_20docs"}
{"query": "From 2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt, what exact term fills [BLANK] in: \"They often rely on small number of tasks or on using other models for auto-evaluation by asking [BLANK]“givenresponsesfromsystem1 (reference)andsystem2(target), whichoneisbetter?”.\"?", "gold": "themtocomparetheoutputsoftwosystemswithapromptlike", "relevant_doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "relevant_chunk_id": 18, "relevant_text": "They often rely on small number of tasks or on using other models for auto-evaluation by asking themtocomparetheoutputsoftwosystemswithapromptlike“givenresponsesfromsystem1 (reference)andsystem2(target), whichoneisbetter?”.", "case_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason_006", "slice": "natural_answerable_20docs"}
{"query": "From 2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt, what exact term fills [BLANK] in: \"We ﬁrst split each of the [BLANK] into text passages of equal lengths as the basic retrieval units3and getM total passages in our corpus C={p1,\"?", "gold": "documents", "relevant_doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "relevant_chunk_id": 14, "relevant_text": "We ﬁrst split each of the documents into text passages of equal lengths as the basic retrieval units3and getM total passages in our corpus C={p1,", "case_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering_012", "slice": "natural_answerable_20docs"}
{"query": "From 2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt, what exact term fills [BLANK] in: \"1 [BLANK] Transformer models [ 82] have emerged as the most widely used architecture in applications such as natural language processing and image classiﬁcation.\"?", "gold": "introduction", "relevant_doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "relevant_chunk_id": 4, "relevant_text": "1 Introduction Transformer models [ 82] have emerged as the most widely used architecture in applications such as natural language processing and image classiﬁcation.", "case_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness_010", "slice": "natural_answerable_20docs"}
{"query": "From 2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt, what exact term fills [BLANK] in: \"[BLANK] Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases.\"?", "gold": "retrieval-augmented", "relevant_doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "relevant_chunk_id": 1, "relevant_text": "Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases.", "case_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey_013", "slice": "natural_answerable_20docs"}
{"query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"They also, [BLANK], struggle with basic functionality, such as arithmetic or fac- tual lookup, where much simpler and smaller models\"?", "gold": "paradoxically", "relevant_doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "relevant_chunk_id": 0, "relevant_text": "They also, paradoxically, struggle with basic functionality, such as arithmetic or fac- tual lookup, where much simpler and smaller models", "case_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools_009", "slice": "natural_answerable_20docs"}
{"query": "From 2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt, what exact term fills [BLANK] in: \"Ourmethodﬁrst [BLANK] a key-value database, where values store raw chunks of text tokens and keys\"?", "gold": "constructs", "relevant_doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "relevant_chunk_id": 14, "relevant_text": "Ourmethodﬁrst constructs a key-value database, where values store raw chunks of text tokens and keys", "case_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens_012", "slice": "natural_answerable_20docs"}
{"query": "From 1706.03762__attention_is_all_you_need.txt, what exact term fills [BLANK] in: \"Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer [BLANK].\"?", "gold": "normalization", "relevant_doc_id": "1706.03762__attention_is_all_you_need.txt", "relevant_chunk_id": 18, "relevant_text": "Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization.", "case_id": "1706.03762__attention_is_all_you_need_011", "slice": "natural_answerable_20docs"}
{"query": "From 2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt, what exact term fills [BLANK] in: \"We note that instruction tuning, while very beneficial for teaching the model how to solve a task, does not [BLANK] teach the model new knowledge.\"?", "gold": "necessarily", "relevant_doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "relevant_chunk_id": 24, "relevant_text": "We note that instruction tuning, while very beneficial for teaching the model how to solve a task, does not necessarily teach the model new knowledge.", "case_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason_014", "slice": "natural_answerable_20docs"}
{"query": "From 2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt, what exact term fills [BLANK] in: \"By meticulously curating and optimizing the training dataset, researchers can [BLANK] reduce the model’s size without compromising its performance.\"?", "gold": "significantly", "relevant_doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "relevant_chunk_id": 8, "relevant_text": "By meticulously curating and optimizing the training dataset, researchers can significantly reduce the model’s size without compromising its performance.", "case_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone_015", "slice": "natural_answerable_20docs"}
{"query": "From 1706.03762__attention_is_all_you_need.txt, what exact term fills [BLANK] in: \"3.2.1 Scaled [BLANK] Attention We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2).\"?", "gold": "dot-product", "relevant_doc_id": "1706.03762__attention_is_all_you_need.txt", "relevant_chunk_id": 20, "relevant_text": "3.2.1 Scaled Dot-Product Attention We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2).", "case_id": "1706.03762__attention_is_all_you_need_020", "slice": "natural_answerable_20docs"}
{"query": "From 1706.03762__attention_is_all_you_need.txt, what exact term fills [BLANK] in: \"Numerous efforts have since continued to push the boundaries of recurrent language models and [BLANK] architectures [38, 24, 15].\"?", "gold": "encoder-decoder", "relevant_doc_id": "1706.03762__attention_is_all_you_need.txt", "relevant_chunk_id": 7, "relevant_text": "Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15].", "case_id": "1706.03762__attention_is_all_you_need_007", "slice": "natural_answerable_20docs"}
{"query": "From 2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"explain how these three quality aspects can be measured in a fully automated way, by [BLANK] an LLM.\"?", "gold": "prompting", "relevant_doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "relevant_chunk_id": 23, "relevant_text": "explain how these three quality aspects can be measured in a fully automated way, by prompting an LLM.", "case_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation_014", "slice": "natural_answerable_20docs"}
{"query": "From 2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt, what exact term fills [BLANK] in: \"For the tasks where reasoning is of primary importance (Figure 1(1)), we alternate the generation of thoughts and actions so that the task-solving trajectory consists of multiple [BLANK] steps.\"?", "gold": "thought-action-observation", "relevant_doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "relevant_chunk_id": 31, "relevant_text": "For the tasks where reasoning is of primary importance (Figure 1(1)), we alternate the generation of thoughts and actions so that the task-solving trajectory consists of multiple thought-action-observation steps.", "case_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models_019", "slice": "natural_answerable_20docs"}
{"query": "From 2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt, what exact term fills [BLANK] in: \"[BLANK] During inference time, we apply the passage encoder EPto all the passages and index them using FAISS (Johnson et al., 2017) ofﬂine.\"?", "gold": "inference", "relevant_doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "relevant_chunk_id": 23, "relevant_text": "Inference During inference time, we apply the passage encoder EPto all the passages and index them using FAISS (Johnson et al., 2017) ofﬂine.", "case_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering_003", "slice": "natural_answerable_20docs"}
{"query": "From 2302.13971__llama_open_and_efficient_foundation_language_models.txt, what exact term fills [BLANK] in: \"We train large [BLANK] on a large quantity of textual data using a standard optimizer.\"?", "gold": "transformers", "relevant_doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "relevant_chunk_id": 9, "relevant_text": "We train large transformers on a large quantity of textual data using a standard optimizer.", "case_id": "2302.13971__llama_open_and_efficient_foundation_language_models_004", "slice": "natural_answerable_20docs"}
{"query": "From 2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt, what exact term fills [BLANK] in: \"If language models can robustly use information within long input con- texts, then their [BLANK] should be minimally affected by the position of the relevant information in the input context.\"?", "gold": "performance", "relevant_doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "relevant_chunk_id": 9, "relevant_text": "If language models can robustly use information within long input con- texts, then their performance should be minimally affected by the position of the relevant information in the input context.", "case_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts_005", "slice": "natural_answerable_20docs"}
{"query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \"Model Architecture BERT’s model architec- ture is a multi-layer bidirectional Transformer en- coder based on the original [BLANK] de- scribed in Vaswani et al.\"?", "gold": "implementation", "relevant_doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "relevant_chunk_id": 24, "relevant_text": "Model Architecture BERT’s model architec- ture is a multi-layer bidirectional Transformer en- coder based on the original implementation de- scribed in Vaswani et al.", "case_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding_020", "slice": "natural_answerable_20docs"}
