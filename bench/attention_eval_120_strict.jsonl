{"case_id": "CHUNK_009_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of [MASK] without regard to their distance in the input or output sequences [ 2,19].\"", "gold": "dependencies", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 9, "relevant_text": "Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [ 2,19]."}
{"case_id": "CHUNK_072_01", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"In International Conference on Learning [MASK] (ICLR) , 2016.\"", "gold": "representations", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 72, "relevant_text": "In International Conference on Learning Representations (ICLR) , 2016."}
{"case_id": "CHUNK_029_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"n layers in the decoder allow each position in the decoder to attend to all [MASK] in the decoder up to and including that position.\"", "gold": "positions", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 29, "relevant_text": "n layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position."}
{"case_id": "CHUNK_026_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"MultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO where head i= Attention( QWQ i, KWK i, V WV i) Where the [MASK] are parameter matrices WQ i∈Rdmodel×dk,WK i∈Rdmodel×dk,WV i∈Rdmodel×dv andWO∈Rhdv×dmodel.\"", "gold": "projections", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 26, "relevant_text": "MultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO where head i= Attention( QWQ i, KWK i, V WV i) Where the projections are parameter matrices WQ i∈Rdmodel×dk,WK i∈Rdmodel×dk,WV i∈Rdmodel×dv andWO∈Rhdv×dmodel."}
{"case_id": "CHUNK_060_01", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the [MASK] setting.\"", "gold": "semi-supervised", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 60, "relevant_text": "We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting."}
{"case_id": "CHUNK_003_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"h-to-French translation task, our model establishes a new single-model [MASK] BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.\"", "gold": "state-of-the-art", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 3, "relevant_text": "h-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature."}
{"case_id": "CHUNK_042_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"uous kernels, orO(logk(n))in the case of dilated [MASK] [ 18], increasing the length of the longest paths between any two positions in the network.\"", "gold": "convolutions", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 42, "relevant_text": "uous kernels, orO(logk(n))in the case of dilated convolutions [ 18], increasing the length of the longest paths between any two positions in the network."}
{"case_id": "CHUNK_011_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"of which use convolutional neural networks as basic building block, computing hidden [MASK] in parallel for all input and output positions.\"", "gold": "representations", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 11, "relevant_text": "of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions."}
{"case_id": "CHUNK_027_01", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"This allows every position in the decoder to attend over all [MASK] in the input sequence.\"", "gold": "positions", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 27, "relevant_text": "This allows every position in the decoder to attend over all positions in the input sequence."}
{"case_id": "CHUNK_000_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in [MASK] or scholarly works.\"", "gold": "journalistic", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 0, "relevant_text": "Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works."}
{"case_id": "CHUNK_056_01", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"In Table 3 rows (A), we vary the [MASK] of attention heads and the attention key an\"", "gold": "number", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 56, "relevant_text": "In Table 3 rows (A), we vary the number of attention heads and the attention key an"}
{"case_id": "CHUNK_063_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"We used a beam size of 21andα= 0.3 for both WSJ only and the [MASK] setting.\"", "gold": "semi-supervised", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 63, "relevant_text": "We used a beam size of 21andα= 0.3 for both WSJ only and the semi-supervised setting."}
{"case_id": "CHUNK_084_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"tion heads attend to a distant [MASK] of the verb ‘making’, completing the phrase ‘making...more difficult’.\"", "gold": "dependency", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 84, "relevant_text": "tion heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’."}
{"case_id": "CHUNK_027_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"ead, the total computational cost is similar to that of single-head attention with full [MASK].\"", "gold": "dimensionality", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 27, "relevant_text": "ead, the total computational cost is similar to that of single-head attention with full dimensionality."}
{"case_id": "CHUNK_047_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"corresponds to increasing the learning rate linearly for the first warmup _steps training steps, and decreasing it thereafter [MASK] to the inverse square root of the step number.\"", "gold": "proportionally", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 47, "relevant_text": "corresponds to increasing the learning rate linearly for the first warmup _steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number."}
{"case_id": "CHUNK_040_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"To improve computational performance for tasks involving very long sequences, [MASK] could be restricted to considering only a neighborhood\"", "gold": "self-attention", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 40, "relevant_text": "To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood"}
{"case_id": "CHUNK_025_01", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"These are [MASK] and once again projected, resulting in the final values, as depicted in Figure 2.\"", "gold": "concatenated", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 25, "relevant_text": "These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2."}
{"case_id": "CHUNK_062_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"(2016) [8] WSJ only, [MASK] 91.7 Transformer (4 layers) WSJ only, discriminative 91.3 Zhu et al.\"", "gold": "discriminative", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 62, "relevant_text": "(2016) [8] WSJ only, discriminative 91.7 Transformer (4 layers) WSJ only, discriminative 91.3 Zhu et al."}
{"case_id": "CHUNK_014_01", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"In the following sections, we will describe the Transformer, motivate [MASK] and discuss its advantages over models such as [17, 18] and [9].\"", "gold": "self-attention", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 14, "relevant_text": "In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9]."}
{"case_id": "CHUNK_034_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"r of the sequence, we must inject some [MASK] about the relative or absolute position of the tokens in the sequence.\"", "gold": "information", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 34, "relevant_text": "r of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence."}
{"case_id": "CHUNK_051_01", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"On the WMT 2014 [MASK] translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4the training cost of the previous state-of-the-art model.\"", "gold": "english-to-french", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 51, "relevant_text": "On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4the training cost of the previous state-of-the-art model."}
{"case_id": "CHUNK_017_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"We employ a residual connection [ 11] around each of the two sub-layers, followed by layer [MASK] [ 1].\"", "gold": "normalization", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 17, "relevant_text": "We employ a residual connection [ 11] around each of the two sub-layers, followed by layer normalization [ 1]."}
{"case_id": "CHUNK_083_01", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"<EOS> <pad> <pad> <pad> <pad> <pad> <pad> Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder [MASK] in layer 5 of 6.\"", "gold": "self-attention", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 83, "relevant_text": "<EOS> <pad> <pad> <pad> <pad> <pad> <pad> Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6."}
{"case_id": "CHUNK_019_01", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"3.2 Attention An attention function can be described as mapping a query and a set of [MASK] pairs to an output, where the query, keys, values, and output are all vectors.\"", "gold": "key-value", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 19, "relevant_text": "3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors."}
{"case_id": "CHUNK_055_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"Listed perplexities are [MASK], according to our byte-pair encoding, and should not be compared to per-word perplexities.\"", "gold": "per-wordpiece", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 55, "relevant_text": "Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities."}
{"case_id": "CHUNK_018_01", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer [MASK].\"", "gold": "normalization", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 18, "relevant_text": "Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization."}
{"case_id": "CHUNK_079_01", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"Garnett, editors, Advances in Neural [MASK] Processing Systems 28 , pages 2440–2448.\"", "gold": "information", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 79, "relevant_text": "Garnett, editors, Advances in Neural Information Processing Systems 28 , pages 2440–2448."}
{"case_id": "CHUNK_062_01", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"(2013) [40] [MASK] 91.3 Huang & Harper (2009) [14] semi-supervised 91.3 McClosky et al.\"", "gold": "semi-supervised", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 62, "relevant_text": "(2013) [40] semi-supervised 91.3 Huang & Harper (2009) [14] semi-supervised 91.3 McClosky et al."}
{"case_id": "CHUNK_083_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"<EOS> <pad> <pad> <pad> <pad> <pad> <pad> It is in this spirit that a majority of American governments have passed new laws since 2009 making the [MASK] or voting process more difficult .\"", "gold": "registration", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 83, "relevant_text": "<EOS> <pad> <pad> <pad> <pad> <pad> <pad> It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult ."}
{"case_id": "CHUNK_049_01", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"In addition, we apply dropout to the sums of the embeddings and the [MASK] encodings in both the encoder and decoder stacks.\"", "gold": "positional", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 49, "relevant_text": "In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks."}
{"case_id": "CHUNK_001_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"The best [MASK] models also connect the encoder and decoder through an attention mechanism.\"", "gold": "performing", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 1, "relevant_text": "The best performing models also connect the encoder and decoder through an attention mechanism."}
{"case_id": "CHUNK_057_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \", we vary the number of attention heads and the attention key and value dimensions, keeping the amount of [MASK] constant, as described in Section 3.2.2.\"", "gold": "computation", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 57, "relevant_text": ", we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2."}
{"case_id": "CHUNK_017_01", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"That is, the output of each sub-layer is LayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function [MASK] by the sub-layer itself.\"", "gold": "implemented", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 17, "relevant_text": "That is, the output of each sub-layer is LayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function implemented by the sub-layer itself."}
{"case_id": "CHUNK_013_01", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"End-to-end memory networks are based on a recurrent attention mechanism instead of sequence- aligned recurrence and have been shown to perform well on [MASK] question answering and language modeling tasks [34].\"", "gold": "simple-language", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 13, "relevant_text": "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence- aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34]."}
{"case_id": "CHUNK_064_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"rforms the Berkeley- Parser [29] even when training only on the WSJ training set of 40K [MASK].\"", "gold": "sentences", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 64, "relevant_text": "rforms the Berkeley- Parser [29] even when training only on the WSJ training set of 40K sentences."}
{"case_id": "CHUNK_043_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"tion layer and a point-wise [MASK] layer, the approach we take in our model.\"", "gold": "feed-forward", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 43, "relevant_text": "tion layer and a point-wise feed-forward layer, the approach we take in our model."}
{"case_id": "CHUNK_026_01", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"Due to the reduced dimension of each head, the total [MASK] cost is similar to that of single-h\"", "gold": "computational", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 26, "relevant_text": "Due to the reduced dimension of each head, the total computational cost is similar to that of single-h"}
{"case_id": "CHUNK_065_01", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"In the former task our best model [MASK] even all previously reported ensembles.\"", "gold": "outperforms", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 65, "relevant_text": "In the former task our best model outperforms even all previously reported ensembles."}
{"case_id": "CHUNK_035_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"sine functions of different [MASK]: PE(pos,2i)=sin(pos/100002i/d model) PE(pos,2i+1)=cos(pos/100002i/d model) where posis the position and iis the dimension.\"", "gold": "frequencies", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 35, "relevant_text": "sine functions of different frequencies: PE(pos,2i)=sin(pos/100002i/d model) PE(pos,2i+1)=cos(pos/100002i/d model) where posis the position and iis the dimension."}
{"case_id": "CHUNK_050_01", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"The [MASK] of this model is listed in the bottom line of Table 3.\"", "gold": "configuration", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 50, "relevant_text": "The configuration of this model is listed in the bottom line of Table 3."}
{"case_id": "CHUNK_060_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"We also trained it in a [MASK] setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences [37].\"", "gold": "semi-supervised", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 60, "relevant_text": "We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences [37]."}
{"case_id": "CHUNK_068_01", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"Learning phrase [MASK] using rnn encoder-decoder for statistical machine translation.\"", "gold": "representations", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 68, "relevant_text": "Learning phrase representations using rnn encoder-decoder for statistical machine translation."}
{"case_id": "CHUNK_061_01", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"During inference, we 9 Table 4: The Transformer generalizes well to English [MASK] parsing (Results are on Section 23 of WSJ) Parser Training WSJ 23 F1 Vinyals & Kaiser el al.\"", "gold": "constituency", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 61, "relevant_text": "During inference, we 9 Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ) Parser Training WSJ 23 F1 Vinyals & Kaiser el al."}
{"case_id": "CHUNK_045_01", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"5.2 Hardware and [MASK] We trained our models on one machine with 8 NVIDIA P100 GPUs.\"", "gold": "schedule", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 45, "relevant_text": "5.2 Hardware and Schedule We trained our models on one machine with 8 NVIDIA P100 GPUs."}
{"case_id": "CHUNK_015_01", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"Given z, the decoder then [MASK] an output sequence (y1, ..., y m)of symbols one element at a time.\"", "gold": "generates", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 15, "relevant_text": "Given z, the decoder then generates an output sequence (y1, ..., y m)of symbols one element at a time."}
{"case_id": "CHUNK_000_01", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"Gomez∗ † [MASK] of Toronto aidan@cs.toronto.eduŁukasz Kaiser∗ Google Br\"", "gold": "university", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 0, "relevant_text": "Gomez∗ † University of Toronto aidan@cs.toronto.eduŁukasz Kaiser∗ Google Br"}
{"case_id": "CHUNK_081_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, [MASK] Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al.\"", "gold": "wolfgang", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 81, "relevant_text": "[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al."}
{"case_id": "CHUNK_031_01", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"The [MASK] of input and output is dmodel = 512 , and the inner-layer has dimensionality dff= 2048 .\"", "gold": "dimensionality", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 31, "relevant_text": "The dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality dff= 2048 ."}
{"case_id": "CHUNK_063_01", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"Our results in Table 4 show that despite the lack of [MASK] tuning our model performs sur- prisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8].\"", "gold": "task-specific", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 63, "relevant_text": "Our results in Table 4 show that despite the lack of task-specific tuning our model performs sur- prisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8]."}
{"case_id": "CHUNK_023_01", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"While for small values of dkthe two mechanisms perform similarly, additive attention [MASK] dot product attention without scaling for larger values of dk[3].\"", "gold": "outperforms", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 23, "relevant_text": "While for small values of dkthe two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk[3]."}
{"case_id": "CHUNK_085_01", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"<EOS> <pad> The Law will never be perfect , but its [MASK] should be just - this is what we are missing , in my opinion .\"", "gold": "application", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 85, "relevant_text": "<EOS> <pad> The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion ."}
{"case_id": "CHUNK_015_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"Here, the encoder maps an input sequence of symbol [MASK] (x1, ..., x n)to a sequence of continuous representations z= (z1, ..., z n).\"", "gold": "representations", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 15, "relevant_text": "Here, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence of continuous representations z= (z1, ..., z n)."}
{"case_id": "CHUNK_036_01", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"We chose the sinusoidal version because it may allow the model to [MASK] to sequence lengths longer than the ones encountered during training.\"", "gold": "extrapolate", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 36, "relevant_text": "We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training."}
{"case_id": "CHUNK_045_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"Each training batch contained a set of sentence pairs containing [MASK] 25000 source tokens and 25000 target tokens.\"", "gold": "approximately", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 45, "relevant_text": "Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens."}
{"case_id": "CHUNK_021_01", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"In practice, we compute the attention function on a set of queries [MASK], packed together into a matrix Q.\"", "gold": "simultaneously", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 21, "relevant_text": "In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q."}
{"case_id": "CHUNK_041_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"attention could be restricted to considering only a [MASK] of size rin the input sequence centered around the respective output position.\"", "gold": "neighborhood", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 41, "relevant_text": "attention could be restricted to considering only a neighborhood of size rin the input sequence centered around the respective output position."}
{"case_id": "CHUNK_079_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"[34] Sainbayar [MASK], Arthur Szlam, Jason Weston, and Rob Fergus.\"", "gold": "sukhbaatar", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 79, "relevant_text": "[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus."}
{"case_id": "CHUNK_053_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"imum output length during inference to input length + 50, but [MASK] early when possible [38].\"", "gold": "terminate", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 53, "relevant_text": "imum output length during inference to input length + 50, but terminate early when possible [38]."}
{"case_id": "CHUNK_001_01", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"We propose a new simple network [MASK], the Transformer, based solely on attention mechanisms, dispensing with recurr\"", "gold": "architecture", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 1, "relevant_text": "We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurr"}
{"case_id": "CHUNK_002_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"er, based solely on attention mechanisms, dispensing with recurrence and [MASK] entirely.\"", "gold": "convolutions", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 2, "relevant_text": "er, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely."}
{"case_id": "CHUNK_022_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"k)V (1) The two most commonly used attention functions are additive attention [ 2], and [MASK] (multi- plicative) attention.\"", "gold": "dot-product", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 22, "relevant_text": "k)V (1) The two most commonly used attention functions are additive attention [ 2], and dot-product (multi- plicative) attention."}
{"case_id": "CHUNK_067_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"Neural machine [MASK] by jointly learning to align and translate.\"", "gold": "translation", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 67, "relevant_text": "Neural machine translation by jointly learning to align and translate."}
{"case_id": "CHUNK_069_01", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"[9]Jonas Gehring, Michael Auli, David [MASK], Denis Yarats, and Yann N.\"", "gold": "grangier", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 69, "relevant_text": "[9]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N."}
{"case_id": "CHUNK_071_01", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"In [MASK] of the 2009 Conference on Empirical Methods in Natural Language Processing , pages 832–841.\"", "gold": "proceedings", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 71, "relevant_text": "In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing , pages 832–841."}
{"case_id": "CHUNK_072_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"[15] Rafal [MASK], Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu.\"", "gold": "jozefowicz", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 72, "relevant_text": "[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu."}
{"case_id": "CHUNK_080_01", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"[36] Christian Szegedy, Vincent [MASK], Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\"", "gold": "vanhoucke", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 80, "relevant_text": "[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna."}
{"case_id": "CHUNK_053_01", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"Table 2 summarizes our results and compares our translation quality and training costs to other model [MASK] from the literature.\"", "gold": "architectures", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 53, "relevant_text": "Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature."}
{"case_id": "CHUNK_058_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding [MASK].\"", "gold": "over-fitting", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 58, "relevant_text": "We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting."}
{"case_id": "CHUNK_023_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"in practice, since it can be implemented using highly optimized matrix [MASK] code.\"", "gold": "multiplication", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 23, "relevant_text": "in practice, since it can be implemented using highly optimized matrix multiplication code."}
{"case_id": "CHUNK_044_01", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"Sentences were encoded using byte-pair encoding [ 3], which has a shared source- target [MASK] of about 37000 tokens.\"", "gold": "vocabulary", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 44, "relevant_text": "Sentences were encoded using byte-pair encoding [ 3], which has a shared source- target vocabulary of about 37000 tokens."}
{"case_id": "CHUNK_074_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"[22] Zhouhan Lin, Minwei Feng, Cicero [MASK] dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio.\"", "gold": "nogueira", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 74, "relevant_text": "[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio."}
{"case_id": "CHUNK_068_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"10 [5]Kyunghyun Cho, Bart van [MASK], Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio.\"", "gold": "merrienboer", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 68, "relevant_text": "10 [5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio."}
{"case_id": "CHUNK_087_01", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"<EOS> <pad> The Law will never be perfect , but its [MASK] should be just - this is what we are missing , in my opinion .\"", "gold": "application", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 87, "relevant_text": "<EOS> <pad> The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion ."}
{"case_id": "CHUNK_002_01", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"Experiments on two machine translation tasks show these models to be superior in quality while being more [MASK] and requiring significantly less time to train.\"", "gold": "parallelizable", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 2, "relevant_text": "Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train."}
{"case_id": "CHUNK_078_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"[32] Noam Shazeer, Azalia [MASK], Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean.\"", "gold": "mirhoseini", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 78, "relevant_text": "[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean."}
{"case_id": "CHUNK_032_01", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"5 Table 1: Maximum path lengths, per-layer complexity and minimum number of [MASK] operations for different layer types.\"", "gold": "sequential", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 32, "relevant_text": "5 Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types."}
{"case_id": "CHUNK_010_01", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"The Transformer allows for significantly more [MASK] and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\"", "gold": "parallelization", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 10, "relevant_text": "The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs."}
{"case_id": "CHUNK_046_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"For our big models,([MASK] on the bottom line of table 3), step time was 1.0 seconds.\"", "gold": "described", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 46, "relevant_text": "For our big models,(described on the bottom line of table 3), step time was 1.0 seconds."}
{"case_id": "CHUNK_011_01", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and [MASK] for ByteNet.\"", "gold": "logarithmically", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 11, "relevant_text": "In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet."}
{"case_id": "CHUNK_028_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"This mimics the typical encoder-decoder attention mechanisms in [MASK] models such as [38, 2, 9].\"", "gold": "sequence-to-sequence", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 28, "relevant_text": "This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9]."}
{"case_id": "CHUNK_034_01", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"To this end, we add \"[MASK] encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks.\"", "gold": "positional", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 34, "relevant_text": "To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks."}
{"case_id": "CHUNK_075_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"11 [25] Mitchell P Marcus, Mary Ann [MASK], and Beatrice Santorini.\"", "gold": "marcinkiewicz", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 75, "relevant_text": "11 [25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini."}
{"case_id": "CHUNK_081_01", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"Google’s neural machine [MASK] system: Bridging the gap between human and machine translation.\"", "gold": "translation", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 81, "relevant_text": "Google’s neural machine translation system: Bridging the gap between human and machine translation."}
{"case_id": "CHUNK_041_01", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"A single [MASK] layer with kernel width k < n does not connect all pairs of input and output positions.\"", "gold": "convolutional", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 41, "relevant_text": "A single convolutional layer with kernel width k < n does not connect all pairs of input and output positions."}
{"case_id": "CHUNK_070_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"In Proceedings of the IEEE Conference on Computer Vision and Pattern [MASK] , pages 770–778, 2016.\"", "gold": "recognition", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 70, "relevant_text": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 770–778, 2016."}
{"case_id": "CHUNK_085_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"<EOS> <pad> [MASK] Layer5 The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion .\"", "gold": "input-input", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 85, "relevant_text": "<EOS> <pad> Input-Input Layer5 The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion ."}
{"case_id": "CHUNK_022_01", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"[MASK] attention is identical to our algorithm, except for the scaling factor of1√dk.\"", "gold": "dot-product", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 22, "relevant_text": "Dot-product attention is identical to our algorithm, except for the scaling factor of1√dk."}
{"case_id": "CHUNK_058_01", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"In row (E) we replace our [MASK] positional encoding with learned positional embeddings [ 9], and observe nearly identical results to the base model.\"", "gold": "sinusoidal", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 58, "relevant_text": "In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [ 9], and observe nearly identical results to the base model."}
{"case_id": "CHUNK_035_01", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"That is, each dimension of the positional encoding [MASK] to a sinusoid.\"", "gold": "corresponds", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 35, "relevant_text": "That is, each dimension of the positional encoding corresponds to a sinusoid."}
{"case_id": "CHUNK_052_01", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"For the base models, we used a single model obtained by averaging the last 5 [MASK], which were written at 10-minute intervals.\"", "gold": "checkpoints", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 52, "relevant_text": "For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals."}
{"case_id": "CHUNK_064_01", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"7 Conclusion In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in [MASK] architectures with multi-headed self-attention.\"", "gold": "encoder-decoder", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 64, "relevant_text": "7 Conclusion In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention."}
{"case_id": "CHUNK_005_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and [MASK].\"", "gold": "visualizations", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 5, "relevant_text": "Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations."}
{"case_id": "CHUNK_052_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"mer (big) model trained for [MASK] used dropout rate Pdrop= 0.1, instead of 0.3.\"", "gold": "english-to-french", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 52, "relevant_text": "mer (big) model trained for English-to-French used dropout rate Pdrop= 0.1, instead of 0.3."}
{"case_id": "CHUNK_049_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"l) 27.3 38.1 3.3·1018 Transformer (big) 28.4 41.8 2.3·1019 Residual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the sub-layer input and [MASK].\"", "gold": "normalized", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 49, "relevant_text": "l) 27.3 38.1 3.3·1018 Transformer (big) 28.4 41.8 2.3·1019 Residual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the sub-layer input and normalized."}
{"case_id": "CHUNK_066_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"ate local, restricted attention mechanisms to [MASK] handle large inputs and outputs such as images, audio and video.\"", "gold": "efficiently", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 66, "relevant_text": "ate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video."}
{"case_id": "CHUNK_076_01", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob [MASK].\"", "gold": "uszkoreit", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 76, "relevant_text": "[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit."}
{"case_id": "CHUNK_087_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"Law will never be perfect , but its [MASK] should be just - this is what we are missing , in my opinion .\"", "gold": "application", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 87, "relevant_text": "Law will never be perfect , but its application should be just - this is what we are missing , in my opinion ."}
{"case_id": "CHUNK_024_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding [MASK] 4To illustrate why the dot products get large, assu\"", "gold": "dv-dimensional", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 24, "relevant_text": "On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional 4To illustrate why the dot products get large, assu"}
{"case_id": "CHUNK_030_01", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"This consists of two linear [MASK] with a ReLU activation in between.\"", "gold": "transformations", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 30, "relevant_text": "This consists of two linear transformations with a ReLU activation in between."}
{"case_id": "CHUNK_065_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"On both WMT 2014 [MASK] and WMT 2014 English-to-French translation tasks, we achieve a new state of the art.\"", "gold": "english-to-german", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 65, "relevant_text": "On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art."}
{"case_id": "CHUNK_031_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"other way of describing this is as two [MASK] with kernel size 1.\"", "gold": "convolutions", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 31, "relevant_text": "other way of describing this is as two convolutions with kernel size 1."}
{"case_id": "CHUNK_021_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"We compute the dot [MASK] of the query with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the values.\"", "gold": "products", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 21, "relevant_text": "We compute the dot products of the query with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the values."}
{"case_id": "CHUNK_054_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"All metrics are on the [MASK] translation development set, newstest2013.\"", "gold": "english-to-german", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 54, "relevant_text": "All metrics are on the English-to-German translation development set, newstest2013."}
{"case_id": "CHUNK_082_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"In [MASK] of the 51st Annual Meeting of the ACL (Volume 1: Long Papers) , pages 434–443.\"", "gold": "proceedings", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 82, "relevant_text": "In Proceedings of the 51st Annual Meeting of the ACL (Volume 1: Long Papers) , pages 434–443."}
{"case_id": "CHUNK_036_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"We also [MASK] with using learned positional embeddings [ 9] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)).\"", "gold": "experimented", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 36, "relevant_text": "We also experimented with using learned positional embeddings [ 9] instead, and found that the two versions produced nearly identical results (see Table 3 row (E))."}
{"case_id": "CHUNK_066_01", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"The code we used to train and evaluate our models is available at https://github.com/ [MASK]/tensor2tensor .\"", "gold": "tensorflow", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 66, "relevant_text": "The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor ."}
{"case_id": "CHUNK_033_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"he kernel size of convolutions and rthe size of the neighborhood in restricted [MASK].\"", "gold": "self-attention", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 33, "relevant_text": "he kernel size of convolutions and rthe size of the neighborhood in restricted self-attention."}
{"case_id": "CHUNK_014_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"the Transformer is the first transduction model relying entirely on self-attention to compute [MASK] of its input and output without using sequence- aligned RNNs or convolution.\"", "gold": "representations", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 14, "relevant_text": "the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution."}
{"case_id": "CHUNK_042_01", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"[MASK] layers are generally more expensive than recurrent layers, by a factor of k.\"", "gold": "convolutional", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 42, "relevant_text": "Convolutional layers are generally more expensive than recurrent layers, by a factor of k."}
{"case_id": "CHUNK_003_01", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"We show that the Transformer generalizes well to other tasks by applying it [MASK] to English constituency parsing both with large and limited training data.\"", "gold": "successfully", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 3, "relevant_text": "We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."}
{"case_id": "CHUNK_038_01", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"The third is the path length between long-range [MASK] in the network.\"", "gold": "dependencies", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 38, "relevant_text": "The third is the path length between long-range dependencies in the network."}
{"case_id": "CHUNK_004_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"posed replacing RNNs with [MASK] and started the effort to evaluate this idea.\"", "gold": "self-attention", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 4, "relevant_text": "posed replacing RNNs with self-attention and started the effort to evaluate this idea."}
{"case_id": "CHUNK_050_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"This hurts [MASK], as the model learns to be more unsure, but improves accuracy and BLEU score.\"", "gold": "perplexity", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 50, "relevant_text": "This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score."}
{"case_id": "CHUNK_037_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"Another is the amount of computation that can be [MASK], as measured by the minimu\"", "gold": "parallelized", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 37, "relevant_text": "Another is the amount of computation that can be parallelized, as measured by the minimu"}
{"case_id": "CHUNK_073_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"[18] Nal [MASK], Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko- ray Kavukcuoglu.\"", "gold": "kalchbrenner", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 73, "relevant_text": "[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko- ray Kavukcuoglu."}
{"case_id": "CHUNK_025_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"-dimensional 4To illustrate why the dot products get large, assume that the components of qandkare [MASK] random variables with mean 0and variance 1.\"", "gold": "independent", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 25, "relevant_text": "-dimensional 4To illustrate why the dot products get large, assume that the components of qandkare independent random variables with mean 0and variance 1."}
{"case_id": "CHUNK_084_01", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"13 [MASK] Layer5 The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion .\"", "gold": "input-input", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 84, "relevant_text": "13 Input-Input Layer5 The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion ."}
{"case_id": "CHUNK_043_01", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"As side benefit, [MASK] could yield more interpretable models.\"", "gold": "self-attention", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 43, "relevant_text": "As side benefit, self-attention could yield more interpretable models."}
{"case_id": "CHUNK_069_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"Empirical [MASK] of gated recurrent neural networks on sequence modeling.\"", "gold": "evaluation", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 69, "relevant_text": "Empirical evaluation of gated recurrent neural networks on sequence modeling."}
{"case_id": "CHUNK_071_00", "query": "Using the provided context only, what exact missing word fills [MASK] in this sentence: \"Gradient flow in recurrent nets: the difficulty of learning long-term [MASK], 2001.\"", "gold": "dependencies", "relevant_doc_id": "attention_is_all_you_need.txt", "relevant_chunk_id": 71, "relevant_text": "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies, 2001."}
