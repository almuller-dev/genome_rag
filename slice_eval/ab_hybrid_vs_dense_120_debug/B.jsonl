{"case_index": 1, "query": "From 2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt, what exact term fills [BLANK] in: \"In additional experiments, we show [BLANK] can robustly boost performance on NLP tasks where adding a chain-of-thought might hurt performance compared to standard prompting (Ye & Durrett, 2022).\"?", "gold": "self-consistency", "answer": "The exact term is **‚Äúself-consistency‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term and is explicitly supported by the provided context sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 94.096, "llm_ms": 1414.979, "top_contexts": [{"rank": 1, "score": 0.7572433, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.45564609018824376, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 15, "text_snippet": "asoning tasks, including GSM8K (Cobbe et al., 2021) (+17.9% absolute accuracy gains), SV AMP (Patel et al., 2021) (+11.0%), AQuA (Ling et al., 2017) (+12.2%), and across commonsense reasoning tasks such as StrategyQA (Geva et al., 2021) (+6"}, {"rank": 2, "score": 0.7466625, "source": "dense+rrf", "rrf_score": 0.03125763125763126, "rerank_score": 0.29720871323284415, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 57, "text_snippet": "urrett (2022) show that sometimes chain-of-thought prompting could hurt performance compared to standard prompting in few-shot in-context learning. Here we perform a study using self-consistency to see if it can help Ô¨Åll in the gap, over a "}, {"rank": 3, "score": 45.28789191001252, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.28865008046166657, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 1, "text_snippet": "such reasoning abilities emerge naturally in sufÔ¨Åciently large language models via a simple method called chain-of- thought prompting , where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three"}, {"rank": 4, "score": 0.7239978, "source": "dense+rrf", "rrf_score": 0.029857397504456328, "rerank_score": 0.2863987531072752, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 5, "text_snippet": "odel performance across a variety of multi-step reasoning tasks (Wei et al., 2022). In this paper, we introduce a novel decoding strategy called self-consistency to replace the greedy decoding strategy used in chain-of-thought prompting (We"}, {"rank": 5, "score": 0.74765867, "source": "dense+rrf", "rrf_score": 0.031754032258064516, "rerank_score": 0.2819520472979506, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 58, "text_snippet": " al., 2020) and RTE (Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009). The results over PaLM-540B are shown in Table 5. For some tasks (e.g., ANLI-R1, e-SNLI, RTE), adding chain-of-thought does h"}]}
{"case_index": 2, "query": "From 2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt, what exact term fills [BLANK] in: \"As compute has gotten faster relative to memory speed [ 61,62,63], operations are [BLANK] bottlenecked by me\"?", "gold": "increasingly", "answer": "The exact term is **‚Äúincreasingly‚Äù** (i.e., ‚Äúoperations are increasingly bottlenecked by ‚Ä¶‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected blank term 'increasingly' and is directly supported by the provided context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 70.063, "llm_ms": 1767.148, "top_contexts": [{"rank": 1, "score": 69.23282788238959, "source": "dense+rrf", "rrf_score": 0.01639344262295082, "rerank_score": 0.3361788399551749, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 22, "text_snippet": "forms of memory of diÔ¨Äerent sizes and speeds, with smaller memory being faster. As an example, the A100 GPU has 40-80GB of high bandwidth memory (HBM) with bandwidth 1.5-2.0TB/s and 192KB of on-chip SRAM per each of 108 streaming multiproce"}, {"rank": 2, "score": 0.6748017, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.30381744690640394, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 10, "text_snippet": "mplementation of attention on GPT-2. FlashAttention does not read and write the large ùëÅ\u0002ùëÅattention matrix to HBM, resulting in an 7.6 \u0002 speedup on the attention computation. GPUs, compute speed has out-paced memory speed [ 61,62,63], and mo"}, {"rank": 3, "score": 54.73686116546747, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.24168852077457564, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 100, "text_snippet": "n, Atri Rudra, and Christopher R√©. 2022. FlashAttention: Fast and memory-efficient exact attention with IO- awareness. ArXiv:2205.14135. Hermann Ebbinghaus. 1913. Memory: A contribu- tion to experimental psychology. H. A. Ruger & C. E. Buss"}, {"rank": 4, "score": 44.59592527226366, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.2360604090317123, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 23, "text_snippet": "peed [ 61,62,63], operations are increasingly bottlenecked by memory (HBM) accesses. Thus exploiting fast SRAM becomes more important. Execution Model. GPUs have a massive number of threads to execute an operation (called a kernel). Each ke"}, {"rank": 5, "score": 44.868492064018824, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.22281702930633276, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 84, "text_snippet": "mputer/RedPajama-Data . Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. CoRR , abs/2307.08691, 2023. Tri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra, and Christopher R ¬¥e. Flashattention: Fast an"}]}
{"case_index": 3, "query": "From 2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"Evaluating RAG [BLANK] is, however, chal- lenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability\"?", "gold": "architectures", "answer": "The exact term is **‚Äúarchitectures‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "The blank is correctly filled with 'architectures', which appears verbatim in the provided context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 89.847, "llm_ms": 1207.08, "top_contexts": [{"rank": 1, "score": 0.76248586, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.4438951959215983, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 1, "text_snippet": "ented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natu- ral language layer between"}, {"rank": 2, "score": 0.64568734, "source": "dense+rrf", "rrf_score": 0.027972027972027972, "rerank_score": 0.2670825857786453, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 48, "text_snippet": "actory results. III. R ETRIEVAL In the context of RAG, it is crucial to efficiently retrieve relevant documents from the data source. There are several key issues involved, such as the retrieval source, retrieval granularity, pre-processing"}, {"rank": 3, "score": 0.68412435, "source": "dense+rrf", "rrf_score": 0.030158730158730156, "rerank_score": 0.258325034987083, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 2, "text_snippet": "m to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the genera- tion itself. With Ragas, we put forward a suite of metrics which can be used to evaluate t"}, {"rank": 4, "score": 0.7332462, "source": "dense+rrf", "rrf_score": 0.031054405392392875, "rerank_score": 0.21540142522587036, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 125, "text_snippet": "atic evaluation of RAG applications, similarly base their assessments on these task- specific metrics [160]. Despite this, there is a notable paucity of research dedicated to evaluating the distinct characteristics of RAG models.The main ev"}, {"rank": 5, "score": 0.727911, "source": "dense+rrf", "rrf_score": 0.029957522915269395, "rerank_score": 0.21327159278506402, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 126, "text_snippet": "insof search engines, recommendation systems, and information retrieval systems are employed to measure the performance of the RAG retrieval module. Metrics such as Hit Rate, MRR, and NDCG are commonly utilized for this purpose [161], [162]"}]}
{"case_index": 4, "query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \"More recently, sentence or document encoders which produce contextual token [BLANK] have been pre-trained from unlabeled text and Ô¨Åne-tuned for a supervised downstream task (Dai and Le, 2015; Howard and R\"?", "gold": "representations", "answer": "The exact term is **‚Äúrepresentations‚Äù** (i.e., ‚Äúcontextual token representations‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected blank ('representations') and is directly supported by the provided context phrase 'contextual token representations'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 51.899, "llm_ms": 1214.226, "top_contexts": [{"rank": 1, "score": 0.6000752, "source": "dense+rrf", "rrf_score": 0.031544957774465976, "rerank_score": 0.4591404396683092, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 17, "text_snippet": "hows that the cloze task can be used to improve the robustness of text generation mod- els. 2.2 Unsupervised Fine-tuning Approaches As with the feature-based approaches, the Ô¨Årst works in this direction only pre-trained word em- bedding par"}, {"rank": 2, "score": 0.59164083, "source": "dense+rrf", "rrf_score": 0.028991596638655463, "rerank_score": 0.25185394621338625, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 20, "text_snippet": "or BERT. Apart from output layers, the same architec- tures are used in both pre-training and Ô¨Åne-tuning. The same pre-trained model parameters are used to initialize models for different down-stream tasks. During Ô¨Åne-tuning, all parameters"}, {"rank": 3, "score": 0.584324, "source": "dense+rrf", "rrf_score": 0.029513888888888888, "rerank_score": 0.25106132770036244, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 11, "text_snippet": "for downstream transfer. First, single-layer representations were learned using word vectors [ MCCD13 ,PSM14 ] and fed to task-speciÔ¨Åc architectures, then RNNs with multiple layers of representations and contextual state were used to form s"}, {"rank": 4, "score": 0.59888506, "source": "dense+rrf", "rrf_score": 0.030309988518943745, "rerank_score": 0.23985081352244458, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 1, "text_snippet": ", 2018), BERT is designed to pre- train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a re- sult, the pre-trained BERT model can be Ô¨Åne- tuned with just one a"}, {"rank": 5, "score": 37.25051827728281, "source": "dense+rrf", "rrf_score": 0.014084507042253521, "rerank_score": 0.2326530419608738, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 10, "text_snippet": "ford et al. (2018), which uses unidirec- tional language models for pre-training, BERT uses masked language models to enable pre- trained deep bidirectional representations. This is also in contrast to Peters et al. (2018a), which uses a sh"}]}
{"case_index": 5, "query": "From 2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt, what exact term fills [BLANK] in: \"[BLANK] is up to 3\u0002faster than the standard attention implemen- tation across common sequence lengths from 128 to 2K and scales up to 64K.\"?", "gold": "flashattention", "answer": "FlashAttention", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "The blank is 'FlashAttention', explicitly stated in the provided context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 101.148, "llm_ms": 1395.386, "top_contexts": [{"rank": 1, "score": 0.8191259, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.4107821701998185, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 20, "text_snippet": "n. FlashAttention is up to 3\u0002faster than the standard attention implemen- tation across common sequence lengths from 128 to 2K and scales up to 64K. Up to sequence length of 512, FlashAttention is both faster and more memory-eÔ¨Écient than an"}, {"rank": 2, "score": 0.7717747, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.39013252307215135, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 55, "text_snippet": "th-256 (sequence length 64K). ‚Ä¢Benchmarking Attention. We measure the runtime and memory performance of FlashAttention and block-sparse FlashAttention based on sequence length. We conÔ¨Årm that the memory footprint ofFlashAttention scales lin"}, {"rank": 3, "score": 0.72865736, "source": "dense+rrf", "rrf_score": 0.02862400327131466, "rerank_score": 0.28651563857004003, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 76, "text_snippet": "nce lengths. Memory Footprint. Figure 3 (right) shows the memory footprint of FlashAttention and block-sparse FlashAttention compared to various exact, approximate, and sparse attention baselines. FlashAttention and block-sparse FlashAttent"}, {"rank": 4, "score": 0.76334804, "source": "dense+rrf", "rrf_score": 0.031009615384615385, "rerank_score": 0.27221258218980077, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 75, "text_snippet": "tion mechanisms grow linearly with se- quence length, but FlashAttention still runs faster than approximate and sparse attention for short sequences due to fewer memory accesses. The approximate attention runtimes begin to cross over with F"}, {"rank": 5, "score": 39.68564476909715, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.2633358872567374, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 19, "text_snippet": "lift from modeling longer sequences on long-document classiÔ¨Åcation [13]. FlashAttention enables the Ô¨Årst Transformer that can achieve better-than-chance performance on the Path-X [ 80] challenge, solely from using a longer sequence length ("}]}
{"case_index": 6, "query": "From 2302.13971__llama_open_and_efficient_foundation_language_models.txt, what exact term fills [BLANK] in: \"and [BLANK] deÔ¨Ånitions and macros written by users to increase consistency across papers.\"?", "gold": "inline-expanded", "answer": "The exact term is **‚Äúinline-expanded‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'inline-expanded' and is directly supported by the provided context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 62.023, "llm_ms": 1346.613, "top_contexts": [{"rank": 1, "score": 0.5310615, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.254176424668222, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 17, "text_snippet": " and inline-expanded deÔ¨Ånitions and macros written by users to increase consistency across papers. Stack Exchange [2%]. We include a dump of Stack Exchange, a website of high quality ques- tions and answers that covers a diverse set of do- "}, {"rank": 2, "score": 33.25089982376228, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.20328385536196614, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 16, "text_snippet": " Books3 section of TheP- ile (Gao et al., 2020), a publicly available dataset for training large language models. We perform deduplication at the book level, removing books with more than 90% content overlap. ArXiv [2.5%]. We process arXiv "}, {"rank": 3, "score": 0.457594, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.17263296438047188, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 116, "text_snippet": "Marie-Anne Lachaux, Timoth¬¥ ee Lacroix, Baptiste Rozi` ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. arXiv preprint"}, {"rank": 4, "score": 0.4321198, "source": "dense+rrf", "rrf_score": 0.02886002886002886, "rerank_score": 0.17200456052626156, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 118, "text_snippet": "nal Linguistics. Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and Denny Zhou. 2022. Recitation-augmented language models. CoRR , abs/2210.01296. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth√©e "}, {"rank": 5, "score": 0.43648094, "source": "dense+rrf", "rrf_score": 0.029236022193768675, "rerank_score": 0.17085293390382797, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 190, "text_snippet": "2022. [57]Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo- th√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lamp"}]}
{"case_index": 7, "query": "From 2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt, what exact term fills [BLANK] in: \"It is designed to equip readers and professionals with a detailed and structured [BLANK] of both large models a\"?", "gold": "understanding", "answer": "The exact term that fills **[BLANK]** is:\n\n**‚Äúunderstanding‚Äù**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'understanding' and is directly supported by the provided context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 116.88, "llm_ms": 1095.801, "top_contexts": [{"rank": 1, "score": 0.5571157, "source": "dense+rrf", "rrf_score": 0.031099324975891997, "rerank_score": 0.3113278935537607, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 10, "text_snippet": "lacking analysis and summarization of how to evaluate RAG. This paper compre- hensively reviews the downstream tasks, datasets, benchmarks, and evaluation methods applicable to RAG. Overall, this paper sets out to meticulously compile and c"}, {"rank": 2, "score": 0.61513096, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.257899035740705, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 11, "text_snippet": "h a detailed and structured understanding of both large models and RAG. It aims to illuminate the evolution of retrieval augmentation techniques, assess the strengths and weaknesses of various approaches in their respective contexts, and sp"}, {"rank": 3, "score": 0.5978256, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.24147682861244452, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 79, "text_snippet": "on the long-context understanding ability of LLMs, and we still have a long way to go in harnessing open-source LLMs within our framework. 10  4 Related Work 4.1 Retrieval-Augmented Generation. Augmenting language models with information re"}, {"rank": 4, "score": 24.616847154088116, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.232984307060226, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 22, "text_snippet": " generate the complete answer at once y=LM([Dx,x]). 2.3 Active Retrieval Augmented Generation To aid long-form generation with retrieval, we pro- pose active retrieval augmented generation. It is a generic framework that actively decides wh"}, {"rank": 5, "score": 0.5607655, "source": "dense+rrf", "rrf_score": 0.03007688828584351, "rerank_score": 0.22330116815920825, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 49, "text_snippet": " units both affect the final generation results. 1) Data Structure: Initially, text is s the mainstream source of retrieval. Subsequently, the retrieval source expanded to in- clude semi-structured data (PDF) and structured data (Knowl- edg"}]}
{"case_index": 8, "query": "From 2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt, what exact term fills [BLANK] in: \"For example, large language models can generate outputs that are [BLANK], toxic, or simply not helpful to the user.\"?", "gold": "untruthful", "answer": "The exact term is **‚Äúuntruthful‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'untruthful' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 45.317, "llm_ms": 1411.435, "top_contexts": [{"rank": 1, "score": 0.60765636, "source": "dense+rrf", "rrf_score": 0.031544957774465976, "rerank_score": 0.3806923457890202, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 1, "text_snippet": ". For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language m"}, {"rank": 2, "score": 0.5654949, "source": "dense+rrf", "rrf_score": 0.028629032258064516, "rerank_score": 0.2947682389769392, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 0, "text_snippet": "Training language models to follow instructions with human feedback Long Ouyang‚àóJeff Wu‚àóXu Jiang‚àóDiogo Almeida‚àóCarroll L. Wainwright‚àó Pamela Mishkin‚àóChong Zhang Sandhini Agarwal Katarina Slama Alex Ray John Schulman Jacob Hilton Fraser Kelt"}, {"rank": 3, "score": 31.654469278830955, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.26346133679484063, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 75, "text_snippet": " models that act in accordance with user intentions. More practically, for the purpose of our language tasks, we use a framework similar to Askell et al. (2021), who deÔ¨Åne models to be aligned if they are helpful, honest, and harmless. To b"}, {"rank": 4, "score": 0.67780244, "source": "dense+rrf", "rrf_score": 0.03177805800756621, "rerank_score": 0.25967581015395735, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 157, "text_snippet": "lly harmful or dishonest response, we allow our model to generate these outputs. Training our model to be harmless despite user instructions is important, but is also difÔ¨Åcult because whether an output is harmful depends on the context in w"}, {"rank": 5, "score": 0.61933213, "source": "dense+rrf", "rrf_score": 0.030798389007344232, "rerank_score": 0.25572677704377456, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 155, "text_snippet": "ignment techniques to Ô¨Åne-tune language models to follow a wide range of instructions. There are many open questions to explore to further align language model behavior with what people actually want them to do. Many methods could be tried "}]}
{"case_index": 9, "query": "From 2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt, what exact term fills [BLANK] in: \"Training language models to follow [BLANK] with human feedback Long Ouyang‚àóJeff Wu‚àóXu Jiang‚àóDiogo Almeida‚àóCarroll L.\"?", "gold": "instructions", "answer": "The exact term that fills **[BLANK]** is **‚Äúinstructions‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term and is directly supported by context showing the paper title 'Training language models to follow instructions with human feedback'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.25, "hit_rank": 4, "matched_by": "doc_id+chunk_id", "retrieval_ms": 46.489, "llm_ms": 1695.135, "top_contexts": [{"rank": 1, "score": 0.63118005, "source": "dense+rrf", "rrf_score": 0.031054405392392875, "rerank_score": 0.3992538286308834, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 99, "text_snippet": "ustin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. 2021. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114 . Long Ouyang, Jeff Wu, Xu Jiang, Diog"}, {"rank": 2, "score": 0.75432456, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.37937777473571405, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 180, "text_snippet": "low instructions with human feedback, 2022. [47]Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke "}, {"rank": 3, "score": 0.6703131, "source": "dense+rrf", "rrf_score": 0.03149801587301587, "rerank_score": 0.34609699702211644, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 179, "text_snippet": " technical report, 2023. [45]OpenAI. Chatgpt (sep 25 version). https://chat.openai.com/chat , 2023. [Large language model]. [46]Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agar"}, {"rank": 4, "score": 0.76762986, "source": "dense+rrf", "rrf_score": 0.032018442622950824, "rerank_score": 0.33676041874465973, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 0, "text_snippet": "Training language models to follow instructions with human feedback Long Ouyang‚àóJeff Wu‚àóXu Jiang‚àóDiogo Almeida‚àóCarroll L. Wainwright‚àó Pamela Mishkin‚àóChong Zhang Sandhini Agarwal Katarina Slama Alex Ray John Schulman Jacob Hilton Fraser Kelt"}, {"rank": 5, "score": 63.28866491216394, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.3007048375589544, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 122, "text_snippet": "avarese, and Caiming Xiong. 2022. Codegen: An open large lan- guage model for code with multi-turn program syn- thesis. arXiv preprint arXiv:2203.13474 . Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, "}]}
{"case_index": 10, "query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"How this is done depends entirely on the API itself ‚Äì for example, it can involve call- ing another neural network, [BLANK] a Python script or using a retrieval system to perform search over a large corpus.\"?", "gold": "executing", "answer": "The exact term is **‚Äúexecuting‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'executing' and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 357.607, "llm_ms": 952.911, "top_contexts": [{"rank": 1, "score": 0.58759654, "source": "dense+rrf", "rrf_score": 0.031544957774465976, "rerank_score": 0.459642324888219, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 22, "text_snippet": "mpling from Mgiven the sequence [P(x),x1,...,x i‚àí1,<API> ]as a preÔ¨Åx and</API> as an end-of-sequence token.2 2We discard all examples where Mdoes not generate the </API> token.Executing API Calls As a next step, we execute all API calls gen"}, {"rank": 2, "score": 0.63002837, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.2769538564893238, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 1, "text_snippet": "metic or fac- tual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer , a model traine"}, {"rank": 3, "score": 0.5816829, "source": "dense+rrf", "rrf_score": 0.03057889822595705, "rerank_score": 0.2461157796451874, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 97, "text_snippet": "rred from making an API call. 8 Conclusion We have introduced Toolformer, a language model that learns in a self-supervised way how to use different tools such as search engines, calculators, and translation systems via simple API calls. Th"}, {"rank": 4, "score": 0.61744213, "source": "dense+rrf", "rrf_score": 0.0315136476426799, "rerank_score": 0.23784450589491166, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 12, "text_snippet": " how an API can be used, we let a LM annotate a huge language modeling dataset with potential API calls. We then use a self-supervised loss to determine which of these API calls actually help the model in predicting future tokens. Finally, "}, {"rank": 5, "score": 35.529564578098025, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.22584970265359236, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 93, "text_snippet": "mitations While our approach enables LMs to learn how to use a variety of tools in a self-supervised way, there are some clear limitations to what can be achieved with our method in its current form. One such limi- tation is the inability o"}]}
{"case_index": 11, "query": "From 2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt, what exact term fills [BLANK] in: \"Our work is [BLANK] to these works, as our attention mechanism is unmodified during inference.\"?", "gold": "complementary", "answer": "The exact term is **‚Äúcomplementary‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'complementary' and is directly supported by Context 1 sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 52.804, "llm_ms": 1461.319, "top_contexts": [{"rank": 1, "score": 0.6452027, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.275934620598022, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 22, "text_snippet": "etrieval-based (Karpukhin et al., 2020; Izacard et al., 2022; Guu et al., 2020), which augment language models via fetching related documents and including the retrieved results into contexts. Our work is complementary to these works, as ou"}, {"rank": 2, "score": 0.64196855, "source": "dense+rrf", "rrf_score": 0.031754032258064516, "rerank_score": 0.26195466662068456, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 24, "text_snippet": "ons have a large gap to full attention, making it infeasible to fine-tune pre-trained LLMs. Although our work also involves an approximation of attention mechanism, it has a similar shape and a small gap to standard attention. This enables "}, {"rank": 3, "score": 0.6614015, "source": "dense+rrf", "rrf_score": 0.03177805800756621, "rerank_score": 0.2416949737673283, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 29, "text_snippet": "g (Zhu et al., 2023), and methods based on out-of-distribution analysis (Han et al., 2023). Our method focuses on efficient fine-tuning and retaining the original architecture during inference, which is orthogonal to these position embeddin"}, {"rank": 4, "score": 0.608719, "source": "dense+rrf", "rrf_score": 0.029631255487269532, "rerank_score": 0.23941314373225603, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 75, "text_snippet": "ed in training-from- scratch transformers. This experiment is to examine their capability of fine-tuning on pre-trained LLMs (Touvron et al., 2023b), toward long context adaptation. Dilated attention performs well in full fine-tuning but is"}, {"rank": 5, "score": 34.00412176553493, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.23284000691592585, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 12, "text_snippet": "e. In terms of efficiency , regardless of whether LoRA is employed or not, computational cost increases dramatically as the context size expands, primarily due to the standard self-attention mechanism (Vaswani et al., 2017). As shown in Fig"}]}
{"case_index": 12, "query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \"Dur- ing [BLANK], the model is trained on unlabeled data over different pre-training tasks.\"?", "gold": "pre-training", "answer": "The exact term is **‚Äúpre-training‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context 1 explicitly states: 'During pre-training, the model is trained on unlabeled data...' so the blank is 'pre-training'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 55.021, "llm_ms": 940.641, "top_contexts": [{"rank": 1, "score": 0.5435539, "source": "dense+rrf", "rrf_score": 0.031544957774465976, "rerank_score": 0.3295985352030807, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 22, "text_snippet": "dels, where an effective recipe is to Ô¨Åne-tune models pre-trained with Ima- geNet (Deng et al., 2009; Yosinski et al., 2014). 3 BERT We introduce BERT and its detailed implementa- tion in this section. There are two steps in our framework: "}, {"rank": 2, "score": 33.294640937765706, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.2740099147447783, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 109, "text_snippet": "reading books. In Proceedings of the IEEE international conference on computer vision , pages 19‚Äì27. Appendix for ‚ÄúBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding‚Äù We organize the appendix into three section"}, {"rank": 3, "score": 0.551055, "source": "dense+rrf", "rrf_score": 0.03076923076923077, "rerank_score": 0.2728205128205073, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 1, "text_snippet": ", 2018), BERT is designed to pre- train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a re- sult, the pre-trained BERT model can be Ô¨Åne- tuned with just one a"}, {"rank": 4, "score": 0.51607203, "source": "dense+rrf", "rrf_score": 0.027425373134328357, "rerank_score": 0.2627329523406234, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 83, "text_snippet": "ted that rich, unsupervised pre-training is an integral part of many language understanding systems. In particular, these results enable even low-resource tasks to beneÔ¨Åt from deep unidirectional architec- tures. Our major contribution is f"}, {"rank": 5, "score": 31.7731327892335, "source": "dense+rrf", "rrf_score": 0.014285714285714285, "rerank_score": 0.25823410208476194, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 44, "text_snippet": "U, or a few hours on a GPU, starting from the exact same pre-trained model.7We de- scribe the task-speciÔ¨Åc details in the correspond- ing subsections of Section 4. More details can be found in Appendix A.5. 4 Experiments In this section, we"}]}
{"case_index": 13, "query": "From 2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt, what exact term fills [BLANK] in: \"Since decision making and reasoning [BLANK] are integrated into a large language model, ReAct enjoys several unique features: A) Intuitive and easy to design : Designing ReAct promp\"?", "gold": "capabilities", "answer": "The exact term is **‚Äúcapabilities‚Äù**:\n\n‚ÄúSince decision making and reasoning **capabilities** are integrated into a large language model, ReAct enjoys several unique features: A) Intuitive and easy to design : Designing ReAct promp‚Äù", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "The blank is correctly filled with 'capabilities', which appears verbatim in the provided context sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 51.798, "llm_ms": 1537.518, "top_contexts": [{"rank": 1, "score": 0.68371993, "source": "dense+rrf", "rrf_score": 0.032266458495966696, "rerank_score": 0.391688021573647, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 32, "text_snippet": "ughts only need to 1We show some GPT-3 (Brown et al., 2020) results in Appendix A.1, which outperforms PaLM-540B. 3  Published as a conference paper at ICLR 2023 appear sparsely in the most relevant positions of a trajectory, so we let the "}, {"rank": 2, "score": 0.6590309, "source": "dense+rrf", "rrf_score": 0.030798389007344232, "rerank_score": 0.2211532177560924, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 88, "text_snippet": " the development of versatile and generalist agents like Reed et al. (2022). 6 C ONCLUSION We have proposed ReAct ‚Äì a simple yet effective method for synergizing reasoning and acting in large language models. Through a diverse set of experi"}, {"rank": 3, "score": 0.67652375, "source": "dense+rrf", "rrf_score": 0.030330882352941176, "rerank_score": 0.20764722874217187, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 24, "text_snippet": "o understand the decision basis of model actions. To summarize, our key contributions are the following: (1) we introduce ReAct , a novel prompt- based paradigm to synergize reasoning and acting in language models for general task solving; "}, {"rank": 4, "score": 0.7228553, "source": "dense+rrf", "rrf_score": 0.031544957774465976, "rerank_score": 0.200573737257762, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 19, "text_snippet": "udies on how reasoning and acting can be combined in a synergistic manner for general task solving, and if such a combination can bring systematic beneÔ¨Åts compared to reasoning or acting alone. In this work, we present ReAct , a general par"}, {"rank": 5, "score": 0.6739448, "source": "dense+rrf", "rrf_score": 0.03076923076923077, "rerank_score": 0.20018234618290862, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 7, "text_snippet": "circumstances or facing information uncertainties. Recent results have hinted at the possibility of combining verbal reasoning with interactive decision making in autonomous systems. On one hand, properly prompted large language models (LLM"}]}
{"case_index": 14, "query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \"rs, and all of the param- eters are Ô¨Åne-tuned using labeled data from the [BLANK] tasks.\"?", "gold": "downstream", "answer": "The exact term is **‚Äúdownstream‚Äù** (i.e., ‚Äú‚Ä¶using labeled data from the **downstream** tasks.‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'downstream' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 64.598, "llm_ms": 1187.174, "top_contexts": [{"rank": 1, "score": 0.6129801, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.32742962854894286, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 23, "text_snippet": "rs, and all of the param- eters are Ô¨Åne-tuned using labeled data from the downstream tasks. Each downstream task has sep- arate Ô¨Åne-tuned models, even though they are ini- tialized with the same pre-trained parameters. The question-answerin"}, {"rank": 2, "score": 0.5994194, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.324753945191684, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 22, "text_snippet": "dels, where an effective recipe is to Ô¨Åne-tune models pre-trained with Ima- geNet (Deng et al., 2009; Yosinski et al., 2014). 3 BERT We introduce BERT and its detailed implementa- tion in this section. There are two steps in our framework: "}, {"rank": 3, "score": 34.78367133429458, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.29049342434032904, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 109, "text_snippet": "reading books. In Proceedings of the IEEE international conference on computer vision , pages 19‚Äì27. Appendix for ‚ÄúBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding‚Äù We organize the appendix into three section"}, {"rank": 4, "score": 45.24568943893777, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.266355281158505, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 476, "text_snippet": "heng, and Jingjing Liu. Uniter: Learning universal image-text representations. arXiv preprint arXiv:1909.11740 , 2019. [Cra17] Kate Crawford. The trouble with bias. NIPS 2017 Keynote , 2017. [DCLT18] Jacob Devlin, Ming-Wei Chang, Kenton Lee"}, {"rank": 5, "score": 0.56122196, "source": "dense+rrf", "rrf_score": 0.028258706467661692, "rerank_score": 0.25183472325219275, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 1, "text_snippet": ", 2018), BERT is designed to pre- train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a re- sult, the pre-trained BERT model can be Ô¨Åne- tuned with just one a"}]}
{"case_index": 15, "query": "From 2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt, what exact term fills [BLANK] in: \"cusing on enhancing language models by [BLANK] additional knowledge through Pre- Training Models (PTM).\"?", "gold": "incorporating", "answer": "The exact term that fills **[BLANK]** is:\n\n**‚Äúincorporating‚Äù**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'incorporating' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 48.886, "llm_ms": 1296.78, "top_contexts": [{"rank": 1, "score": 0.68217826, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.28294944087210117, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 7, "text_snippet": "cusing on enhancing language models by incorporating additional knowledge through Pre- Training Models (PTM). This early stage was characterized by foundational work aimed at refining pre-training techniques [3]‚Äì[5].The subsequent arrival o"}, {"rank": 2, "score": 0.6141187, "source": "dense+rrf", "rrf_score": 0.027884615384615386, "rerank_score": 0.2201858034016683, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 79, "text_snippet": "on the long-context understanding ability of LLMs, and we still have a long way to go in harnessing open-source LLMs within our framework. 10  4 Related Work 4.1 Retrieval-Augmented Generation. Augmenting language models with information re"}, {"rank": 3, "score": 0.5988183, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.2025379580295234, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 312, "text_snippet": "p training curricula. By contrast pre-training at large enough scale appears to offer a ‚Äúnatural‚Äù broad distribution of tasks implicitly contained in predicting the text itself. One direction for future work might be attempting to generate "}, {"rank": 4, "score": 0.5872351, "source": "dense+rrf", "rrf_score": 0.02976190476190476, "rerank_score": 0.2010163205900445, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 4, "text_snippet": "l, retrieval-augmented gen- eration, natural language processing, information retrieval I. I NTRODUCTION LARGE language models (LLMs) have achieved remark- able success, though they still face significant limitations, especially in domain-s"}, {"rank": 5, "score": 22.21834607996083, "source": "dense+rrf", "rrf_score": 0.014285714285714285, "rerank_score": 0.19626093366902772, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 13, "text_snippet": "he fine-tuning of LLMs. Researchers have also been exploring ways to enhance language models in the pre-training stage through retrieval-augmented techniques. advanced RAG, and modular RAG. This review contex- tualizes the broader scope of "}]}
{"case_index": 16, "query": "From 2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt, what exact term fills [BLANK] in: \"Chi‚Ä† Sharan Narang‚Ä†Aakanksha Chowdhery‚Ä†Denny Zhou‚Ä†¬ß ‚Ä†Google Research, Brain Team ‚Ä°xuezhiw@google.com ,¬ßdennyzhou@google.com ABSTRACT [BLANK] prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks.\"?", "gold": "chain-of-thought", "answer": "The exact term is **‚ÄúChain-of-thought‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'chain-of-thought' and is directly supported by Context 1 ('Chain-of-thought prompting...').", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 59.922, "llm_ms": 1194.469, "top_contexts": [{"rank": 1, "score": 0.8123338, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.5652261280880743, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 0, "text_snippet": "Published as a conference paper at ICLR 2023 SELF-CONSISTENCY IMPROVES CHAIN OF THOUGHT REASONING IN LANGUAGE MODELS Xuezhi Wang‚Ä†‚Ä°Jason Wei‚Ä†Dale Schuurmans‚Ä†Quoc Le‚Ä†Ed H. Chi‚Ä† Sharan Narang‚Ä†Aakanksha Chowdhery‚Ä†Denny Zhou‚Ä†¬ß ‚Ä†Google Research, "}, {"rank": 2, "score": 0.75624573, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.3151384036213943, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 0, "text_snippet": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models Jason Wei Xuezhi Wang Dale Schuurmans Maarten Bosma Brian Ichter Fei Xia Ed H. Chi Quoc V . Le Denny Zhou Google Research, Brain Team {jasonwei,dennyzhou}@google.com Abst"}, {"rank": 3, "score": 0.699513, "source": "dense+rrf", "rrf_score": 0.03057889822595705, "rerank_score": 0.29724155767826066, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 113, "text_snippet": "on. arXiv preprint arXiv:1803.05355 , 2018. Lev S Vygotsky. Thinking and speech. The collected works of LS Vygotsky , 1:39‚Äì285, 1987. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdh- ery, and Denny "}, {"rank": 4, "score": 0.7114429, "source": "dense+rrf", "rrf_score": 0.030776515151515152, "rerank_score": 0.2644193627796953, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 120, "text_snippet": "Zhou. 2022. Self- consistency improves chain of thought reasoning in language models. CoRR , abs/2203.11171. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicit"}, {"rank": 5, "score": 43.31583305827246, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.24534040611461685, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 40, "text_snippet": " 4  Published as a conference paper at ICLR 2023 3.2 M AINRESULTS We report the results of self-consistency averaged over 10 runs, where we sampled 40 outputs independently from the decoder in each run. The baseline we compare to is chain-o"}]}
{"case_index": 17, "query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"her the ob- tained responses are helpful for [BLANK] future tokens; this is used as a Ô¨Åltering criterion.\"?", "gold": "predicting", "answer": "The exact term is **‚Äúpredicting‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Model answer matches expected term 'predicting' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 53.684, "llm_ms": 930.486, "top_contexts": [{"rank": 1, "score": 0.634847, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.3710334457225021, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 17, "text_snippet": "her the ob- tained responses are helpful for predicting future tokens; this is used as a Ô¨Åltering criterion. After Ô¨Åltering, we merge API calls for different tools, resulting in the augmented dataset C‚àó, and Ô¨Ånetune 1In practice, we use the"}, {"rank": 2, "score": 0.5797177, "source": "dense+rrf", "rrf_score": 0.031746031746031744, "rerank_score": 0.2613313096738719, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 1, "text_snippet": "metic or fac- tual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer , a model traine"}, {"rank": 3, "score": 31.300328566449693, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.25980822787650143, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 12, "text_snippet": " how an API can be used, we let a LM annotate a huge language modeling dataset with potential API calls. We then use a self-supervised loss to determine which of these API calls actually help the model in predicting future tokens. Finally, "}, {"rank": 4, "score": 0.64327615, "source": "dense+rrf", "rrf_score": 0.032018442622950824, "rerank_score": 0.24158158284143774, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 88, "text_snippet": "ng criterion. High values typically correspond to API calls that are intuitively useful for predicting future tokens. approaches, additional information is always pro- vided, regardless of whether it is helpful or not. In contrast, Toolform"}, {"rank": 5, "score": 0.53920853, "source": "dense+rrf", "rrf_score": 0.02797339593114241, "rerank_score": 0.23536546234764485, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 93, "text_snippet": "mitations While our approach enables LMs to learn how to use a variety of tools in a self-supervised way, there are some clear limitations to what can be achieved with our method in its current form. One such limi- tation is the inability o"}]}
{"case_index": 18, "query": "From 2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt, what exact term fills [BLANK] in: \"lock-sparse FlashAttention , a sparse attention algorithm that is 2-4 \u0002faster than [BLANK] , scaling up to sequence length of 64k.\"?", "gold": "evenflashattention", "answer": "The term that fills **[BLANK]** is **‚Äúeven FlashAttention‚Äù**.", "answer_score": 0.3, "answer_correctness": 0.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context shows the phrase 'faster than evenFlashAttention'. The expected exact term is 'evenflashattention' (lowercase, no space). Model answered 'even FlashAttention' with a space/case mismatch, so it does not match the required exact term.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 58.642, "llm_ms": 2651.105, "top_contexts": [{"rank": 1, "score": 0.7808286, "source": "dense+rrf", "rrf_score": 0.032266458495966696, "rerank_score": 0.3709822424454923, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 16, "text_snippet": "lock-sparse FlashAttention , a sparse attention algorithm that is 2-4 \u0002faster than evenFlashAttention , scaling up to sequence length of 64k. We prove that block-sparse FlashAttention has better IO complexity than FlashAttention by a factor"}, {"rank": 2, "score": 0.78146803, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.3370277236881024, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 55, "text_snippet": "th-256 (sequence length 64K). ‚Ä¢Benchmarking Attention. We measure the runtime and memory performance of FlashAttention and block-sparse FlashAttention based on sequence length. We conÔ¨Årm that the memory footprint ofFlashAttention scales lin"}, {"rank": 3, "score": 0.794475, "source": "dense+rrf", "rrf_score": 0.032018442622950824, "rerank_score": 0.30929976269199777, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 20, "text_snippet": "n. FlashAttention is up to 3\u0002faster than the standard attention implemen- tation across common sequence lengths from 128 to 2K and scales up to 64K. Up to sequence length of 512, FlashAttention is both faster and more memory-eÔ¨Écient than an"}, {"rank": 4, "score": 39.31814244018681, "source": "dense+rrf", "rrf_score": 0.013888888888888888, "rerank_score": 0.2958136475349743, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 50, "text_snippet": "o blocks of the attention matrix. The algorithm is identical to Algorithm 1, except we skip zero blocks. We reproduce the algorithm description in Algorithm 5 in Appendix B. We also analyze the IO complexity of block-sparse FlashAttention ."}, {"rank": 5, "score": 0.7661847, "source": "dense+rrf", "rrf_score": 0.030117753623188408, "rerank_score": 0.29068754867882585, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 76, "text_snippet": "nce lengths. Memory Footprint. Figure 3 (right) shows the memory footprint of FlashAttention and block-sparse FlashAttention compared to various exact, approximate, and sparse attention baselines. FlashAttention and block-sparse FlashAttent"}]}
{"case_index": 19, "query": "From 2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt, what exact term fills [BLANK] in: \"Write a [BLANK] answer for the given question using only the provided search results (some of which might be irrelevant).\"?", "gold": "high-quality", "answer": "The exact term is **‚Äúhigh-quality‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Model answer matches expected term 'high-quality' and is directly supported by multiple context snippets containing the exact phrase.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.3333333333333333, "hit_rank": 3, "matched_by": "doc_id+chunk_id", "retrieval_ms": 56.093, "llm_ms": 964.591, "top_contexts": [{"rank": 1, "score": 0.5938185, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.3703397164378738, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 34, "text_snippet": "grained changes in the position of relevant information. 2.2 Models We analyze several state-of-the-art open and closed language models. We use greedy decoding when generating outputs and leave exploration of other decoding methods to futur"}, {"rank": 2, "score": 46.45182456893161, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.3417567128740096, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 31, "text_snippet": "ut Context  Wilhelm Conrad R√∂ntgen Desired Answer Figure 2: Example of the multi-document question answering task, with an input context and the desired model answer. The document containing the answer is bolded within the input context her"}, {"rank": 3, "score": 44.97461179414658, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.32234460260461056, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 29, "text_snippet": "ranked order, we explored randomly ordering the k‚àí1 distractor documents and mentioning that the documents are randomly ordered in the task description, but found the same trends. See Appendix C for more details.  Write a high-quality answe"}, {"rank": 4, "score": 0.57428324, "source": "dense+rrf", "rrf_score": 0.031009615384615385, "rerank_score": 0.3016215014273156, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 126, "text_snippet": "orted by decreasing relevance (i.e., the documents near the beginning of the input context are more likely to be useful than those at the end). To validate that our conclusions are not simply a byproduct of this bias, we run experiments wit"}, {"rank": 5, "score": 0.5539367, "source": "dense+rrf", "rrf_score": 0.028309409888357256, "rerank_score": 0.2729231238900899, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 49, "text_snippet": "UIDs. The relevant key-value pair for answering the query is bolded here within the input context for clarity. indicate that extended-context models are not nec- essarily better than their non-extended counterparts at using their input cont"}]}
{"case_index": 20, "query": "From 2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt, what exact term fills [BLANK] in: \"2 These three novel designs significantly boost the overall performance of RAG on open-domain question- answering tasks like NQ (Kwiatkowski et al., 2019), HotpotQA (Yang et al., 2018), Qasper (Dasigi et al., 2021) and [BLANK] (Bai et al., 2023).\"?", "gold": "multifieldqa-en", "answer": "The exact term that fills **[BLANK]** is **‚ÄúMultiFieldQA-en‚Äù**.", "answer_score": 0.9299999999999999, "answer_correctness": 0.9, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.9, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context shows the term as ‚ÄúMultiFieldQA-en‚Äù. Expected answer is lowercase ‚Äúmultifieldqa-en‚Äù; answer matches content but differs in capitalization.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 105.968, "llm_ms": 1675.932, "top_contexts": [{"rank": 1, "score": 0.82617646, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.514070605152747, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 14, "text_snippet": "further extract answers from the concatenation of retrievals, which is normally around 30K tokens. We simply prompt an existing long-context LM (like Gemini or GPT4) with the question to produce the answers in a zero-shot fashion. 2  These "}, {"rank": 2, "score": 43.033260645844265, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.27028682849226987, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 79, "text_snippet": "on the long-context understanding ability of LLMs, and we still have a long way to go in harnessing open-source LLMs within our framework. 10  4 Related Work 4.1 Retrieval-Augmented Generation. Augmenting language models with information re"}, {"rank": 3, "score": 0.6850983, "source": "dense+rrf", "rrf_score": 0.029116045245077504, "rerank_score": 0.26803174371153343, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 11, "text_snippet": " even millions of tokens as input (Reid et al., 2024; Achiam et al., 2023). In this paper, we propose to revisit this design choice for open-domain question answering and propose the LongRAG framework as a solution to balance the workload b"}, {"rank": 4, "score": 0.737152, "source": "dense+rrf", "rrf_score": 0.03128054740957967, "rerank_score": 0.24345589081346397, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 5, "text_snippet": " training corpus (Kandpal et al., 2022; Mallen et al., 2023). The standard solution to these issues is to rely on Retrieval Augmented Generation (RAG) (Lee et al., 2019; Lewis et al., 2020; Guu et al., 2020). Answering a question then essen"}, {"rank": 5, "score": 40.58865751883006, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.24247142944216055, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 6, "text_snippet": "7; Guu et al., 2020; Lewis et al., 2020; Izacard and Grave, 2021; Sachan et al., 2021; Lee et al., 2021; Jiang et al., 2022; Izacard et al., 2022; Nakano et al., 2021; Qian et al., 2023; Lazaridou et al., 2022; Shi et al., 2023). These sing"}]}
{"case_index": 21, "query": "From 2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt, what exact term fills [BLANK] in: \"The reason behind this is that short attention resembles the attention scheme in the [BLANK] stage of LLMs.\"?", "gold": "pre-training", "answer": "The exact term is **‚Äúpre-training‚Äù**, i.e., ‚Äúthe pre-training stage of LLMs.‚Äù", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'pre-training' and is directly supported by context stating 'pre-training stage of LLMs.'", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 50.735, "llm_ms": 1501.318, "top_contexts": [{"rank": 1, "score": 0.6047591, "source": "dense+rrf", "rrf_score": 0.031054405392392875, "rerank_score": 0.29357936432131193, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 75, "text_snippet": "ed in training-from- scratch transformers. This experiment is to examine their capability of fine-tuning on pre-trained LLMs (Touvron et al., 2023b), toward long context adaptation. Dilated attention performs well in full fine-tuning but is"}, {"rank": 2, "score": 0.57858646, "source": "dense+rrf", "rrf_score": 0.029051670471052088, "rerank_score": 0.2743483687491452, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 15, "text_snippet": "s most existing optimization and infrastructure. Techniques for common LLMs can also be applied to ours. For example, Flash-Attention2 (Dao et al., 2022; Dao, 2023) is compatible with our method in both training and inference time. The reas"}, {"rank": 3, "score": 40.738327544195045, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.26093549618612777, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 0, "text_snippet": "Published as a conference paper at ICLR 2024 LONG LORA: E FFICIENT FINE-TUNING OF LONG - CONTEXT LARGE LANGUAGE MODELS Yukang Chen1Shengju Qian1Haotian Tang2Xin Lai1 Zhijian Liu2Song Han2,3Jiaya Jia1 1CUHK2MIT3NVIDIA ABSTRACT We present Lon"}, {"rank": 4, "score": 0.59172976, "source": "dense+rrf", "rrf_score": 0.02871794871794872, "rerank_score": 0.23555329362921257, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 12, "text_snippet": "e. In terms of efficiency , regardless of whether LoRA is employed or not, computational cost increases dramatically as the context size expands, primarily due to the standard self-attention mechanism (Vaswani et al., 2017). As shown in Fig"}, {"rank": 5, "score": 0.5968442, "source": "dense+rrf", "rrf_score": 0.029236022193768675, "rerank_score": 0.23045299508990244, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 41, "text_snippet": "eads respectively. This manner does not increase additional computation costs but enables the information flow between different groups. We show that it gets close to the standard attention baseline in Table 1. Consistency to Full Attention"}]}
{"case_index": 22, "query": "From 2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt, what exact term fills [BLANK] in: \"ng low-rank matrices, which are generally efficient and reduce the number of trainable [BLANK].\"?", "gold": "parameters", "answer": "The exact term is **‚Äúparameters‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'parameters' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 82.047, "llm_ms": 1287.577, "top_contexts": [{"rank": 1, "score": 0.67786264, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.33248606280223963, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 11, "text_snippet": "ng low-rank matrices, which are generally efficient and reduce the number of trainable parameters. However, our empirical findings indicate that training long context models in this manner is neither sufficiently effective nor efficient. In"}, {"rank": 2, "score": 0.61244476, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.246549632102733, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 10, "text_snippet": "al., 2023) used 32 TPUs for standard transformer training and 128 TPUs for LongLLaMA. These computation resources are typically unaffordable for common researchers, which naturally leads us to question: can we extend the context window of L"}, {"rank": 3, "score": 35.74207978896411, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.2366614958094053, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 75, "text_snippet": "ed in training-from- scratch transformers. This experiment is to examine their capability of fine-tuning on pre-trained LLMs (Touvron et al., 2023b), toward long context adaptation. Dilated attention performs well in full fine-tuning but is"}, {"rank": 4, "score": 34.23552320145783, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.2240629082955925, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 0, "text_snippet": "Published as a conference paper at ICLR 2024 LONG LORA: E FFICIENT FINE-TUNING OF LONG - CONTEXT LARGE LANGUAGE MODELS Yukang Chen1Shengju Qian1Haotian Tang2Xin Lai1 Zhijian Liu2Song Han2,3Jiaya Jia1 1CUHK2MIT3NVIDIA ABSTRACT We present Lon"}, {"rank": 5, "score": 33.928981690991684, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.21688591660685777, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 45, "text_snippet": "edding layers is crucial for low-rank long-context adaptation . Llama2 7B (Touvron et al., 2023b) models with the proposed S2-Attn are trained on the RedPajama (Computer, 2023) dataset. The target context length is 32768. ‚Äò+ Normal / Embed‚Äô"}]}
{"case_index": 23, "query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"You can call the API by writing \"[QA([BLANK])]\" where \"question\" is the question you want to ask.\"?", "gold": "question", "answer": "The exact term is **`question`**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context explicitly shows the syntax \"[QA(question)]\"; model answer matches expected term.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 75.915, "llm_ms": 6116.295, "top_contexts": [{"rank": 1, "score": 0.5652521, "source": "dense+rrf", "rrf_score": 0.03177805800756621, "rerank_score": 0.32516231968900483, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 142, "text_snippet": " sample API calls for each tool considered. Question Answering We use the following prompt for the question answering tool: Your task is to add calls to a Question Answering API to a piece of text. The questions should help you get informat"}, {"rank": 2, "score": 0.5624056, "source": "dense+rrf", "rrf_score": 0.03128054740957967, "rerank_score": 0.3023043363767777, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 18, "text_snippet": "still refer to them as ‚Äú <API> ‚Äù, ‚Äú</API> ‚Äù and ‚Äú‚Üí‚Äù through- out this section.  Your task is to add calls to a Question Answering API to a  piece of text. The questions should help you get  information required to complete the text. You can"}, {"rank": 3, "score": 0.54009986, "source": "dense+rrf", "rrf_score": 0.030117753623188408, "rerank_score": 0.26362682516928726, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 1, "text_snippet": "metic or fac- tual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer , a model traine"}, {"rank": 4, "score": 37.064135457799495, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.26206075335138995, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 142, "text_snippet": "trieval instructions for 2WikiMultihopQA Skill 1. Use the Search API to look up relevant information by writing ‚Äú[Search(term)]‚Äù where ‚Äúterm‚Äù is the search term you want to look up. For example: Question: But what are the risks during produ"}, {"rank": 5, "score": 37.27246083266279, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.24625615732029438, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 147, "text_snippet": "tput: From this, we have 4 *30 minutes = [Calculator(4 *30)] 120 minutes. Input: x Output: Wikipedia Search We use the following prompt for the Wikipedia search tool: Your task is to complete a given piece of text. You can use a Wikipedia S"}]}
{"case_index": 24, "query": "From 2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt, what exact term fills [BLANK] in: \"FlashAttention enables the Ô¨Årst Transformer that can achieve [BLANK] performance on the Path-X [ 80] challenge, solely from using a longer sequence length (16K).\"?", "gold": "better-than-chance", "answer": "**better-than-chance**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'better-than-chance' and is directly supported by the provided context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 68.747, "llm_ms": 987.689, "top_contexts": [{"rank": 1, "score": 0.8024845, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.44135175816069516, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 19, "text_snippet": "lift from modeling longer sequences on long-document classiÔ¨Åcation [13]. FlashAttention enables the Ô¨Årst Transformer that can achieve better-than-chance performance on the Path-X [ 80] challenge, solely from using a longer sequence length ("}, {"rank": 2, "score": 0.74171203, "source": "dense+rrf", "rrf_score": 0.0315136476426799, "rerank_score": 0.3144342841363867, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 54, "text_snippet": "T-2 with context length 1K, while achieving 0.7 better perplexity. Modeling longer sequences yields 6.4 points of lift on two long- document classiÔ¨Åcation tasks. Finally, FlashAttention yields the Ô¨Årst Transformer that can achieve better-th"}, {"rank": 3, "score": 44.2787668574487, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.25766833107860515, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 69, "text_snippet": "s specialized medical text and thus may be more susceptible to a distribution shift in the document length, whereas ECtHR contains general language. Table 5: Long Document performance (mi- croùêπ1) at diÔ¨Äerent sequence lengths using FlashAtte"}, {"rank": 4, "score": 0.76958406, "source": "dense+rrf", "rrf_score": 0.0315136476426799, "rerank_score": 0.24596060533464206, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 72, "text_snippet": "a transformer on Path-64, and then transfer to Path-X by spatially interpolating the positional embeddings. FlashAttention achieves 61.4 accuracy on Path-X. Additionally, block-sparse FlashAttention enables the Transformers to scale to sequ"}, {"rank": 5, "score": 42.52381514538901, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.2413225113165481, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 4, "text_snippet": "T-2 and 6.4 points of lift on long-document classiÔ¨Åcation) and entirely new capabilities: the Ô¨Årst Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64"}]}
{"case_index": 25, "query": "From 2005.14165__language_models_are_few_shot_learners.txt, what exact term fills [BLANK] in: \"There is evidence that suggests that the [BLANK] achieved under this paradigm can be poor because th\"?", "gold": "generalization", "answer": "The exact term is **‚Äúgeneralization‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'generalization' and is explicitly present in the provided context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 59.23, "llm_ms": 1027.58, "top_contexts": [{"rank": 1, "score": 0.5540021, "source": "dense+rrf", "rrf_score": 0.031099324975891997, "rerank_score": 0.32422347900574083, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 15, "text_snippet": "with the expressiveness of the model and the narrowness of the training distribution. This can create problems for the pre-training plus Ô¨Åne-tuning paradigm, where models are designed to be large to absorb information during pre-training, b"}, {"rank": 2, "score": 0.61231625, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.2251250851300995, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 16, "text_snippet": "neralization achieved under this paradigm can be poor because the model is overly speciÔ¨Åc to the training distribution and does not generalize well outside it [YdC+19,MPL19 ]. Thus, the performance of Ô¨Åne-tuned models on speciÔ¨Åc benchmarks,"}, {"rank": 3, "score": 0.5475132, "source": "dense+rrf", "rrf_score": 0.0288981288981289, "rerank_score": 0.22475462898577747, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 25, "text_snippet": "skills and tasks within the parameters of the model, it is plausible that in-context learning abilities might show similarly strong gains with scale. 1In the context of language models this has sometimes been called ‚Äúzero-shot transfer‚Äù, bu"}, {"rank": 4, "score": 22.23511073349716, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.22196007783301736, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 48, "text_snippet": " The panels above show four methods for performing a task with a language model ‚Äì Ô¨Åne-tuning is the traditional method, whereas zero-, one-, and few-shot, which we study in this work, require the model to perform the task with only forward "}, {"rank": 5, "score": 20.760161180586714, "source": "dense+rrf", "rrf_score": 0.014285714285714285, "rerank_score": 0.1974065257027066, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 93, "text_snippet": "mitations While our approach enables LMs to learn how to use a variety of tools in a self-supervised way, there are some clear limitations to what can be achieved with our method in its current form. One such limi- tation is the inability o"}]}
{"case_index": 26, "query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"After Ô¨Åltering, we merge API calls for different tools, [BLANK] in the augmented dataset C‚àó, and Ô¨Ånetune 1In practice, we use the token sequences ‚Äú [‚Äù, ‚Äú]‚Äù and ‚Äú->‚Äù to represent ‚Äú <API> ‚Äù, ‚Äú</API> ‚Äù and ‚Äú‚Üí‚Äù, respec- tively.\"?", "gold": "resulting", "answer": "The exact term is **‚Äúresulting‚Äù** (i.e., ‚ÄúAfter Ô¨Åltering, we merge API calls for different tools, **resulting** in the augmented dataset C‚àó, ‚Ä¶‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches the expected term 'resulting' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 107.312, "llm_ms": 1736.284, "top_contexts": [{"rank": 1, "score": 0.7657813, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.41825086184548094, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 17, "text_snippet": "her the ob- tained responses are helpful for predicting future tokens; this is used as a Ô¨Åltering criterion. After Ô¨Åltering, we merge API calls for different tools, resulting in the augmented dataset C‚àó, and Ô¨Ånetune 1In practice, we use the"}, {"rank": 2, "score": 0.57659125, "source": "dense+rrf", "rrf_score": 0.031024531024531024, "rerank_score": 0.2491378052585761, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 27, "text_snippet": "atterns in the pretraining corpus, thus hurting perplexity.  x1:i‚àí1,e(ci,ri),xi:n; we proceed analogously for texts with multiple API calls. Doing this for all x‚àà Cresults in the new dataset C‚àóaugmented with API calls. We use this new datas"}, {"rank": 3, "score": 0.58997905, "source": "dense+rrf", "rrf_score": 0.031009615384615385, "rerank_score": 0.2480991858028239, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 1, "text_snippet": "metic or fac- tual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer , a model traine"}, {"rank": 4, "score": 0.5792437, "source": "dense+rrf", "rrf_score": 0.031009615384615385, "rerank_score": 0.234618019364983, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 12, "text_snippet": " how an API can be used, we let a LM annotate a huge language modeling dataset with potential API calls. We then use a self-supervised loss to determine which of these API calls actually help the model in predicting future tokens. Finally, "}, {"rank": 5, "score": 48.284577474512304, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.21376765229812683, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 212, "text_snippet": "line decision making: Benchmarks and additional opinions,‚Äù arXiv preprint arXiv:2306.02224 , 2023. [108] T. Schick, J. Dwivedi-Yu, R. Dess `ƒ±, R. Raileanu, M. Lomeli, L. Zettle- moyer, N. Cancedda, and T. Scialom, ‚ÄúToolformer: Language mode"}]}
{"case_index": 27, "query": "From 2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt, what exact term fills [BLANK] in: \"It has been observed that [BLANK] prompting signiÔ¨Åcantly improves model performance across a variety of multi-step reasoning tasks\"?", "gold": "chain-of-thought", "answer": "The exact term is **‚Äúchain-of-thought‚Äù** (as in ‚Äúchain-of-thought prompting‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'chain-of-thought' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 44.99, "llm_ms": 1370.688, "top_contexts": [{"rank": 1, "score": 60.79702748142635, "source": "dense+rrf", "rrf_score": 0.01639344262295082, "rerank_score": 0.41889232386099634, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 4, "text_snippet": ". For example, given the question ‚ÄúIf there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?‚Äù , instead of directly responding with ‚Äú5‚Äù, a language model would be prompted to respond with the entir"}, {"rank": 2, "score": 0.71795386, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.36780357078830844, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 5, "text_snippet": "odel performance across a variety of multi-step reasoning tasks (Wei et al., 2022). In this paper, we introduce a novel decoding strategy called self-consistency to replace the greedy decoding strategy used in chain-of-thought prompting (We"}, {"rank": 3, "score": 0.6824957, "source": "dense+rrf", "rrf_score": 0.02946912242686891, "rerank_score": 0.29501635341504767, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 87, "text_snippet": "rved that it signiÔ¨Åcantly improves accuracy in a range of arithmetic and commonsense reasoning tasks, across four large language models with varying scales. Beyond accuracy gains, self-consistency is also useful for collecting rationales wh"}, {"rank": 4, "score": 0.68885857, "source": "dense+rrf", "rrf_score": 0.03057889822595705, "rerank_score": 0.2890809083611884, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 41, "text_snippet": "The results are shown in Table 2.7Self-consistency improves the arithmetic reasoning performance over all four language models signiÔ¨Åcantly over chain-of-thought prompting. More surprisingly, the gains become more signiÔ¨Åcant when the langua"}, {"rank": 5, "score": 40.03325874184095, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.27334791223775723, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 86, "text_snippet": "ength sequence in recurrent language models. Nye et al. (2021) improve the logical consistency of samples from a System 1 model by adding a System 2-inspired logical reasoning module. In this paper we focus on a slightly different notion of"}]}
{"case_index": 28, "query": "From 2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt, what exact term fills [BLANK] in: \"ural networks have proven to be powerful language models, Ô¨Årst in the form of recurrent architectures (Graves, 2013; Jozefowicz et al., 2016; Mikolov et al., 2010) and more recently in the form of Transformers (Vaswani et al., 2017), that use attention to [BLANK] the past.\"?", "gold": "contextualise", "answer": "The exact term is **‚Äúcontextualise‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'contextualise' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 65.642, "llm_ms": 929.36, "top_contexts": [{"rank": 1, "score": 0.75384533, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.5670059153669742, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 4, "text_snippet": "ural networks have proven to be powerful language models, Ô¨Årst in the form of recurrent architectures (Graves, 2013; Jozefowicz et al., 2016; Mikolov et al., 2010) and more recently in the form of Transformers (Vaswani et al., 2017), that u"}, {"rank": 2, "score": 0.6148703, "source": "dense+rrf", "rrf_score": 0.02946236559139785, "rerank_score": 0.30437347601612885, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 84, "text_snippet": " the national US average of 0.385 kg CO 2e per KWh. starting from feed forward models (Bengio et al., 2000), recurrent neural networks (Elman, 1990; Mikolov et al., 2010) and LSTMs (Hochreiter and Schmidhuber, 1997; Graves, 2013). More rece"}, {"rank": 3, "score": 30.074806697964668, "source": "dense+rrf", "rrf_score": 0.014084507042253521, "rerank_score": 0.24970226134779303, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 38, "text_snippet": "t token of chunk ùê∂ùë¢is the Ô¨Årst to be able to access the retrieved content ùê∏ùë¢while maintaining autoregressivity in(1). Hence, there is a one token overlap between chunk ùê∂ùë¢=\u0010 ùë•¬πùë¢\u00001¬∫ùëö¬∏ùëñ\u0011 ùëñ2¬ª1¬îùëö¬ºand the corresponding attending chunk ùê∂¬∏ ùë¢,¬πùë•ùë¢ùëö¬∏ùëñ"}, {"rank": 4, "score": 0.61149216, "source": "dense+rrf", "rrf_score": 0.028373015873015873, "rerank_score": 0.22649558216415272, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 162, "text_snippet": "ry, and R. Socher. Pointer sentinel mixture models. In International Conference on Learning Representations , 2017. URL https://openreview.net/forum?id= Byj72udxe . 21  Improving language models by retrieving from trillions of tokens T. Mik"}, {"rank": 5, "score": 40.37649103860977, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.22554037499420923, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 3, "text_snippet": "cratch, yet can also rapidly R/e.sc/t.sc/r.sc/o.scÔ¨Åt pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale. 1. "}]}
{"case_index": 29, "query": "From 2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt, what exact term fills [BLANK] in: \"Remarkably, this has been successful for a range of simple [BLANK] tasks (Brown et al., 2020).\"?", "gold": "question-answering", "answer": "The exact term is **‚Äúquestion-answering‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context 1 explicitly contains the phrase 'simple question-answering tasks', matching the expected blank. Model answer is exact and fully supported.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 117.0, "llm_ms": 1244.516, "top_contexts": [{"rank": 1, "score": 52.394047159724664, "source": "dense+rrf", "rrf_score": 0.01639344262295082, "rerank_score": 0.3141988208276055, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 9, "text_snippet": "mpt‚Äù the model with a few input‚Äìoutput exemplars demonstrating the task. Remarkably, this has been successful for a range of simple question-answering tasks (Brown et al., 2020). Both of the above ideas, however, have key limitations. For r"}, {"rank": 2, "score": 0.67292607, "source": "dense+rrf", "rrf_score": 0.030621785881252923, "rerank_score": 0.2962105069998195, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 71, "text_snippet": "revailing theme (Wei et al., 2022b). For many reasoning tasks where standard prompting has a Ô¨Çat scaling curve, chain- of-thought prompting leads to dramatically increasing scaling curves. Chain-of-thought prompting appears to expand the se"}, {"rank": 3, "score": 36.180837613093345, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.29307122445043454, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 79, "text_snippet": "pecialized approaches for improving reasoning (Andor et al., 2019; Ran et al., 2019; Geva et al., 2020; PiÀõ ekos et al., 2021). Compared to prior work, self-consistency is applicable to a wide range of reasoning tasks without any additional"}, {"rank": 4, "score": 0.66199154, "source": "dense+rrf", "rrf_score": 0.029631255487269532, "rerank_score": 0.2912172582603232, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 70, "text_snippet": "t generally applicable (Section 4). Finally, we showed that for symbolic reasoning, chain-of-thought prompting facilitates OOD generalization to longer sequence lengths (Section 5). In all experiments, chain-of-thought reasoning is elicited"}, {"rank": 5, "score": 0.63583237, "source": "dense+rrf", "rrf_score": 0.02658450704225352, "rerank_score": 0.2838411509375612, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 10, "text_snippet": " For the traditional few- shot prompting method used in Brown et al. (2020), it works poorly on tasks that require reasoning abilities, and often does not improve substantially with increasing language model scale (Rae et al., 2021). In thi"}]}
{"case_index": 30, "query": "From 2305.06983__active_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"If so (step 2 and 3), the system retrieves relevant documents and [BLANK] the sentence.\"?", "gold": "regenerates", "answer": "The exact term is **‚Äúregenerates‚Äù** (i.e., ‚Äúthe system retrieves relevant documents and **regenerates** the sentence.‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'regenerates' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 55.173, "llm_ms": 1273.892, "top_contexts": [{"rank": 1, "score": 0.6408826, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3348644707671543, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 11, "text_snippet": "eddocumentsLMGeneration$%$%% Figure 1: An illustration of forward-looking active retrieval augmented generation (FLARE). Starting with the user input xand initial retrieval results Dx, FLARE iteratively generates a temporary next sentence ("}, {"rank": 2, "score": 0.5900636, "source": "dense+rrf", "rrf_score": 0.031024531024531024, "rerank_score": 0.2362151390926256, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 32, "text_snippet": "iable. There- fore, we propose a more direct way of forward- looking active retrieval that uses the next sentence to decide when and what to retrieve. 3.2.1 Confidence-based Active Retrieval As shown in Figure 1, at step t, we first generat"}, {"rank": 3, "score": 0.6144545, "source": "dense+rrf", "rrf_score": 0.0315136476426799, "rerank_score": 0.23461544672268772, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 17, "text_snippet": "ng Active REtrieval augmented generation (FLARE ), as illustrated in Figure 1. FLARE iter- atively generates a temporary next sentence , use it as the query to retrieve relevant documents if it contains low-probability tokens and regenerate"}, {"rank": 4, "score": 0.613006, "source": "dense+rrf", "rrf_score": 0.03149801587301587, "rerank_score": 0.2218085865507435, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 16, "text_snippet": ". When deciding what to retrieve , it is important to consider what LMs intend to gen- erate in the future, as the goal of active retrieval is to benefit future generations. Therefore, we propose anticipating the future by generating a temp"}, {"rank": 5, "score": 0.54141927, "source": "dense+rrf", "rrf_score": 0.027651515151515153, "rerank_score": 0.2212580516823002, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 22, "text_snippet": " generate the complete answer at once y=LM([Dx,x]). 2.3 Active Retrieval Augmented Generation To aid long-form generation with retrieval, we pro- pose active retrieval augmented generation. It is a generic framework that actively decides wh"}]}
{"case_index": 31, "query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"We represent each API call as a tuple c= (ac,ic) whereacis the name of the API and icis the cor- [BLANK] input.\"?", "gold": "responding", "answer": "The exact term is **‚Äúcorresponding‚Äù** (as in: ‚Äúic is the corresponding input.‚Äù).", "answer_score": 0.3, "answer_correctness": 0.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context clearly shows the blank is 'corresponding' ('ic is the corresponding input'), but expected answer is 'responding'. Model answer matches context but not the expected answer key.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 100.672, "llm_ms": 1341.514, "top_contexts": [{"rank": 1, "score": 0.6174542, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3700060573885758, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 15, "text_snippet": "lls into any given text, using special tokens to mark the start and end of each such call. We represent each API call as a tuple c= (ac,ic) whereacis the name of the API and icis the cor- responding input. Given an API call cwith a cor- res"}, {"rank": 2, "score": 0.47460347, "source": "dense+rrf", "rrf_score": 0.031024531024531024, "rerank_score": 0.28540893056351413, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 1, "text_snippet": "metic or fac- tual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer , a model traine"}, {"rank": 3, "score": 30.68991125725103, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.2267193809495627, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 93, "text_snippet": "mitations While our approach enables LMs to learn how to use a variety of tools in a self-supervised way, there are some clear limitations to what can be achieved with our method in its current form. One such limi- tation is the inability o"}, {"rank": 4, "score": 0.4990061, "source": "dense+rrf", "rrf_score": 0.029513888888888888, "rerank_score": 0.22554485138743927, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 17, "text_snippet": "her the ob- tained responses are helpful for predicting future tokens; this is used as a Ô¨Åltering criterion. After Ô¨Åltering, we merge API calls for different tools, resulting in the augmented dataset C‚àó, and Ô¨Ånetune 1In practice, we use the"}, {"rank": 5, "score": 0.4740153, "source": "dense+rrf", "rrf_score": 0.02900988017658188, "rerank_score": 0.22452861475737404, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 12, "text_snippet": " how an API can be used, we let a LM annotate a huge language modeling dataset with potential API calls. We then use a self-supervised loss to determine which of these API calls actually help the model in predicting future tokens. Finally, "}]}
{"case_index": 32, "query": "From 2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt, what exact term fills [BLANK] in: \"Outputs from our 175B [BLANK] are preferred to 175B GPT-3 outputs 85 ¬±3% of the time, and preferred 71 ¬±4% of the time to few-shot 175B GPT-3.\"?", "gold": "instructgpt", "answer": "The exact term is **‚ÄúInstructGPT‚Äù**.", "answer_score": 0.3, "answer_correctness": 0.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Expected exact term is lowercase 'instructgpt'; model answered 'InstructGPT' (different casing). Context supports 'InstructGPT' but not the exact expected string.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 119.704, "llm_ms": 1330.663, "top_contexts": [{"rank": 1, "score": 0.6793328, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.423079835107606, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 16, "text_snippet": "owing instructions. Outputs from our 175B InstructGPT are preferred to 175B GPT-3 outputs 85 ¬±3% of the time, and preferred 71 ¬±4% of the time to few-shot 175B GPT-3. InstructGPT models also generate more appropriate outputs according to ou"}, {"rank": 2, "score": 0.7001572, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.3328019759394259, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 88, "text_snippet": " compared directly, 175B InstructGPT outputs are preferred to GPT-3 outputs 85 ¬±3% of the time, and preferred 71 ¬±4% of the time to few-shot GPT-3. We also found that our results do not change signiÔ¨Åcantly when evaluated on prompts submitte"}, {"rank": 3, "score": 0.6404319, "source": "dense+rrf", "rrf_score": 0.030090497737556562, "rerank_score": 0.2900467405461951, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 2, "text_snippet": "demonstrations of the desired model behavior, which we use to Ô¨Åne-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further Ô¨Åne-tune this supervised model using reinforcement learn"}, {"rank": 4, "score": 0.670968, "source": "dense+rrf", "rrf_score": 0.031746031746031744, "rerank_score": 0.27893654074917484, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 15, "text_snippet": "ollows: Labelers signiÔ¨Åcantly prefer InstructGPT outputs over outputs from GPT-3. On our test set, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having over 100x fewer parameters. Th"}, {"rank": 5, "score": 0.6480075, "source": "dense+rrf", "rrf_score": 0.029857397504456328, "rerank_score": 0.25194749800812255, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 87, "text_snippet": " shown in Figure 1. We Ô¨Ånd that GPT-3 outputs perform the worst, and one can obtain signiÔ¨Åcant step-size improvements by using a well-crafted few-shot prompt (GPT-3 (prompted)), then by training on demonstrations using supervised learning ("}]}
{"case_index": 33, "query": "From 2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt, what exact term fills [BLANK] in: \"nstruction Tuning [BLANK] tuning [ 46,38,62,61] has emerged as a crucial step in training language models.\"?", "gold": "instruction", "answer": "The exact term that fills **[BLANK]** is:\n\n**Instruction**", "answer_score": 0.3, "answer_correctness": 0.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Expected blank is 'instruction' (lowercase). Model answered 'Instruction' (capitalized), which does not exactly match the expected term. Context clearly supports 'Instruction tuning'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 53.794, "llm_ms": 1121.745, "top_contexts": [{"rank": 1, "score": 0.7727457, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3379642394925691, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 22, "text_snippet": "nstruction Tuning Instruction tuning [ 46,38,62,61] has emerged as a crucial step in training language models. Instruction tuning involves learning from input-output pairs where the input is natural language task description,and the output "}, {"rank": 2, "score": 0.63698715, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.23275256564562802, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 21, "text_snippet": " to the phenomenon that all models are to some extent constrained by their underlying pre-trained model (while Orca 2 training could be applied any base LLM, we report results on LLaMA-2 7B and 13B in this report). Orca 2 models have not un"}, {"rank": 3, "score": 31.2806411475747, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.22600900898706022, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 148, "text_snippet": "synthetic data for post-training. While Orca 2 models have not gone through RLHF training for safety, we believe that the use of synthetic data for post-training that has been filtered with various content safety filters could provide anoth"}, {"rank": 4, "score": 30.98894777957547, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.2089233021710062, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 0, "text_snippet": "Orca 2: Teaching Small Language Models How to Reason Arindam Mitra, Luciano Del Corro‚Ä†, Shweti Mahajan‚Ä†, Andres Codas‚Ä° Clarisse Simoes‚Ä°, Sahaj Agarwal, Xuxi Chen‚àó, Anastasia Razdaibiedina‚àó Erik Jones‚àó, Kriti Aggarwal‚àó, Hamid Palangi, Guoqin"}, {"rank": 5, "score": 25.8157385213325, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.1876968219021908, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 13, "text_snippet": "ity to reason zero-shot [ 23]. These abilities include answering complex questions, generating explanations, and solving multi-step problems, for instance, such as those on the US Medical Licensing exam, on which LLMs now achieve a passing "}]}
{"case_index": 34, "query": "From 2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natu- ral language layer between a user and textual databases, reducing the risk of [BLANK].\"?", "gold": "hallucinations", "answer": "The exact term is **‚Äúhallucinations.‚Äù**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Model answer matches expected term 'hallucinations' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 61.377, "llm_ms": 1093.046, "top_contexts": [{"rank": 1, "score": 0.8108169, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.5562155780532436, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 1, "text_snippet": "ented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natu- ral language layer between"}, {"rank": 2, "score": 32.181208410775064, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.22573295653709574, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 125, "text_snippet": "them for the given domain of interest or application using the models and annotation approaches for the given domain: Automated RAI Measurement Framework: We employ a recently proposed framework for automated measurement of Responsible AI m"}, {"rank": 3, "score": 0.6540929, "source": "dense+rrf", "rrf_score": 0.028612012987012988, "rerank_score": 0.22524059157981016, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 9, "text_snippet": "we present Ragas1, a framework for the automated assessment 1Ragas is available at https://github.com/ explodinggradients/ragas .arXiv:2309.15217v2 [cs.CL] 28 Apr 2025  of retrieval augmented generation systems. We fo- cus on settings where"}, {"rank": 4, "score": 55.79259803735379, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.22231482945367537, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 237, "text_snippet": "-auto-eval-best-practices-RAG, 2023. [164] S. Es, J. James, L. Espinosa-Anke, and S. Schockaert, ‚ÄúRagas: Au- tomated evaluation of retrieval augmented generation,‚Äù arXiv preprint arXiv:2309.15217 , 2023. [165] J. Saad-Falcon, O. Khattab, C."}, {"rank": 5, "score": 0.6814457, "source": "dense+rrf", "rrf_score": 0.014084507042253521, "rerank_score": 0.21836100190590685, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 48, "text_snippet": "actory results. III. R ETRIEVAL In the context of RAG, it is crucial to efficiently retrieve relevant documents from the data source. There are several key issues involved, such as the retrieval source, retrieval granularity, pre-processing"}]}
{"case_index": 35, "query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \"r [BLANK] such models (Howard and Ruder, 2018; Radford et al., 2018; Dai and Le, 2015).\"?", "gold": "pre-training", "answer": "The exact term is **‚Äúpre-training‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'pre-training' and is directly supported by the provided context snippet.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 85.818, "llm_ms": 1143.95, "top_contexts": [{"rank": 1, "score": 0.6294952, "source": "dense+rrf", "rrf_score": 0.030886196246139225, "rerank_score": 0.3271296223074558, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 21, "text_snippet": "r pre-training such models (Howard and Ruder, 2018; Radford et al., 2018; Dai and Le, 2015). 2.3 Transfer Learning from Supervised Data There has also been work showing effective trans- fer from supervised tasks with large datasets, such as"}, {"rank": 2, "score": 0.6686155, "source": "dense+rrf", "rrf_score": 0.032266458495966696, "rerank_score": 0.2855985548187281, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 0, "text_snippet": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova Google AI Language {jacobdevlin,mingweichang,kentonl,kristout }@google.com Abstract We introduce a ne"}, {"rank": 3, "score": 0.6649734, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.27277173404769356, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 18, "text_snippet": "for a supervised downstream task (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018). The advantage of these approaches is that few parameters need to be learned from scratch. At least partly due to this advantage, OpenAI GPT ("}, {"rank": 4, "score": 48.35271937445739, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.2662576010608249, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 476, "text_snippet": "heng, and Jingjing Liu. Uniter: Learning universal image-text representations. arXiv preprint arXiv:1909.11740 , 2019. [Cra17] Kate Crawford. The trouble with bias. NIPS 2017 Keynote , 2017. [DCLT18] Jacob Devlin, Ming-Wei Chang, Kenton Lee"}, {"rank": 5, "score": 0.6205182, "source": "dense+rrf", "rrf_score": 0.028814262023217248, "rerank_score": 0.2648629626571562, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 20, "text_snippet": "or BERT. Apart from output layers, the same architec- tures are used in both pre-training and Ô¨Åne-tuning. The same pre-trained model parameters are used to initialize models for different down-stream tasks. During Ô¨Åne-tuning, all parameters"}]}
{"case_index": 36, "query": "From 2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt, what exact term fills [BLANK] in: \"Next, we collect a dataset of [BLANK] comparisons between outputs from our models on a larger set of API prompts.\"?", "gold": "human-labeled", "answer": "The exact term is **‚Äúhuman-labeled‚Äù**, as in: ‚ÄúNext, we collect a dataset of **human-labeled** comparisons between outputs from our models on a larger set of API prompts.‚Äù", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'human-labeled' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 81.739, "llm_ms": 1482.407, "top_contexts": [{"rank": 1, "score": 0.6029743, "source": "dense+rrf", "rrf_score": 0.03131881575727918, "rerank_score": 0.36050971736748605, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 11, "text_snippet": "and use this to train our supervised learning baselines. Next, we collect a dataset of human-labeled comparisons between outputs from our models on a larger set of API prompts. We then train a reward model (RM) on this dataset to predict wh"}, {"rank": 2, "score": 38.13826179402499, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.29252203472343835, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 1, "text_snippet": ". For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language m"}, {"rank": 3, "score": 0.5803896, "source": "dense+rrf", "rrf_score": 0.02813852813852814, "rerank_score": 0.27459128522742066, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 2, "text_snippet": "demonstrations of the desired model behavior, which we use to Ô¨Åne-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further Ô¨Åne-tune this supervised model using reinforcement learn"}, {"rank": 4, "score": 0.5983362, "source": "dense+rrf", "rrf_score": 0.02857142857142857, "rerank_score": 0.27364085412774364, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 53, "text_snippet": "dataset in Appendix A. 3.3 Tasks Our training tasks are from two sources: (1) a dataset of prompts written by our labelers and (2) a dataset of prompts submitted to early InstructGPT models on our API (see Table 6). These prompts are very d"}, {"rank": 5, "score": 30.604644682352067, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.2664389015252177, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 81, "text_snippet": "on API distribution. Our main metric is human preference ratings on a held out set of prompts from the same source as our training distribution. When using prompts from the API for evaluation, we only select prompts by customers we haven‚Äôt "}]}
{"case_index": 37, "query": "From 2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt, what exact term fills [BLANK] in: \"1.First, chain of thought, in principle, allows models to decompose multi-step problems into [BLANK] steps, which means that additional computation can be allocated to problems that require more reason\"?", "gold": "intermediate", "answer": "The exact term that fills **[BLANK]** is:\n\n**‚Äúintermediate‚Äù**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'intermediate' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 90.141, "llm_ms": 2255.491, "top_contexts": [{"rank": 1, "score": 0.78216755, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.4864614058395741, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 16, "text_snippet": " for arriving at the answer (and also, solutions/explanations typically come after the Ô¨Ånal answer (Narang et al., 2020; Wiegreffe et al., 2022; Lampinen et al., 2022, inter alia )). Chain-of-thought prompting has several attractive propert"}, {"rank": 2, "score": 39.32544512620325, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.27268476774830946, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 42, "text_snippet": "tput only a mathematical equation before giving the answer. Figure 5 shows that equation only prompting does not help much for GSM8K, which implies that the semantics of the questions in GSM8K are too challenging to directly translate into "}, {"rank": 3, "score": 0.71638995, "source": "dense+rrf", "rrf_score": 0.03125763125763126, "rerank_score": 0.2601278474540874, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 18, "text_snippet": " used for tasks such as math word problems, commonsense reasoning, and symbolic manipulation, and is potentially applicable (at least in principle) to any task that humans can solve via language. 4.Finally, chain-of-thought reasoning can be"}, {"rank": 4, "score": 40.97459035143819, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.24497365459395168, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 44, "text_snippet": "t allows the model to spend more computation (i.e., intermediate tokens) on harder problems. To isolate the effect of variable computation from chain-of-thought reasoning, we test a conÔ¨Åguration where the model is prompted to output a only "}, {"rank": 5, "score": 0.6409943, "source": "dense+rrf", "rrf_score": 0.026171741361614778, "rerank_score": 0.23419028882789764, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 152, "text_snippet": "ting improve performance for my task of interest? While chain-of-thought prompting is in principle applicable for any text-to-text task, it is more helpful for some tasks than others. Based on the experiments in this paper, our intuition is"}]}
{"case_index": 38, "query": "From 2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt, what exact term fills [BLANK] in: \"The existing RAG framework tends to use short retrieval units, such as 100-word passages in popular open- domain [BLANK] tasks (Chen et al., 2017; Lewis et al., 2020; Karpukhin et al., 2020).\"?", "gold": "question-answering", "answer": "The exact term is **‚Äúquestion-answering‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term and is directly supported by Context 1 phrase 'open-domain question-answering tasks'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 99.782, "llm_ms": 1050.854, "top_contexts": [{"rank": 1, "score": 0.7431903, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.4698122389384811, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 6, "text_snippet": "rely offloaded from the parametric knowledge of LLMs by leveraging a standalone retrieval component from an external corpus. The existing RAG framework tends to use short retrieval units, such as 100-word passages in popular open- domain qu"}, {"rank": 2, "score": 55.886312402689725, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.3080752868036613, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 79, "text_snippet": "on the long-context understanding ability of LLMs, and we still have a long way to go in harnessing open-source LLMs within our framework. 10  4 Related Work 4.1 Retrieval-Augmented Generation. Augmenting language models with information re"}, {"rank": 3, "score": 0.7282675, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.3011354003767233, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 0, "text_snippet": "LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs ‚ô†Ziyan Jiang,‚ô†Xueguang Ma,‚ô†Wenhu Chen ‚ô†University of Waterloo ziyanjiang528@gmail.com ,{x93ma ,wenhuchen}@uwaterloo.ca Project Website: https://tiger-ai-lab.github.io/"}, {"rank": 4, "score": 38.259536785588665, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.24687586492510577, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 6, "text_snippet": "7; Guu et al., 2020; Lewis et al., 2020; Izacard and Grave, 2021; Sachan et al., 2021; Lee et al., 2021; Jiang et al., 2022; Izacard et al., 2022; Nakano et al., 2021; Qian et al., 2023; Lazaridou et al., 2022; Shi et al., 2023). These sing"}, {"rank": 5, "score": 0.67994404, "source": "dense+rrf", "rrf_score": 0.028577260665441927, "rerank_score": 0.22742157522325482, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 11, "text_snippet": " even millions of tokens as input (Reid et al., 2024; Achiam et al., 2023). In this paper, we propose to revisit this design choice for open-domain question answering and propose the LongRAG framework as a solution to balance the workload b"}]}
{"case_index": 39, "query": "From 2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt, what exact term fills [BLANK] in: \"The question encoder and the reader model are then Ô¨Åne- tuned using pairs of [BLANK] and answers jointly.\"?", "gold": "questions", "answer": "The exact term that fills **[BLANK]** is **‚Äúquestions‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'questions' and is directly supported by Context 1 sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 112.601, "llm_ms": 1222.842, "top_contexts": [{"rank": 1, "score": 0.5695934, "source": "dense+rrf", "rrf_score": 0.029051670471052088, "rerank_score": 0.36863507283837377, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 8, "text_snippet": "/BM25 for open- domain QA before ORQA (Lee et al., 2019), which proposes a sophisticated inverse cloze task (ICT) objective, predicting the blocks that contain the masked sentence, for additional pretraining. The question encoder and the re"}, {"rank": 2, "score": 0.59565973, "source": "dense+rrf", "rrf_score": 0.031054405392392875, "rerank_score": 0.2977584758207373, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 9, "text_snippet": "open-domainarXiv:2004.04906v3 [cs.CL] 30 Sep 2020  QA datasets, it also suffers from two weaknesses. First, ICT pretraining is computationally intensive and it is not completely clear that regular sentences are good surrogates of questions "}, {"rank": 3, "score": 29.837043528502832, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.23479891247031265, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 98, "text_snippet": "Scaling in-context learning to 1, 000 examples. ArXiv, abs/2212.06713, 2022. Gautier Izacard and Edouard Grave. Distilling knowledge from reader to retriever for question answering. arXiv preprint arXiv:2012.04584 , 2020a. Gautier Izacard a"}, {"rank": 4, "score": 31.139428997107863, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.23106538805012564, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 17, "text_snippet": "rieval accuracy , which is the fraction of ques- tions for whichCFcontains a span that answers the question. 3 Dense Passage Retriever (DPR) We focus our research in this work on improv- ing the retrieval component in open-domain QA. Given "}, {"rank": 5, "score": 0.585601, "source": "dense+rrf", "rrf_score": 0.028991596638655463, "rerank_score": 0.2304041099788798, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 12, "text_snippet": " vs. 33.3%) in the open Natural Questions setting (Lee et al., 2019; Kwiatkowski et al., 2019). Our contributions are twofold. First, we demon- strate that with the proper training setup, sim- ply Ô¨Åne-tuning the question and passage encoder"}]}
{"case_index": 40, "query": "From 2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt, what exact term fills [BLANK] in: \"As most of the datasets only have an [BLANK] split, we manually composed a set of eight few-shot exemplars with chains of thought for prompting‚ÄîFigure 1 (right) shows one chain of thought exemplar, and the full set of exemplars is given in Appendix Table 20.\"?", "gold": "evaluation", "answer": "The exact term is **‚Äúevaluation‚Äù** (i.e., ‚ÄúAs most of the datasets only have an **evaluation** split, ‚Ä¶‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'evaluation' and is directly supported by the provided context quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 95.083, "llm_ms": 1344.008, "top_contexts": [{"rank": 1, "score": 0.8126261, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.5376883609882763, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 23, "text_snippet": "ed answer, as illustrated in Figure 1 (right). As most of the datasets only have an evaluation split, we manually composed a set of eight few-shot exemplars with chains of thought for prompting‚ÄîFigure 1 (right) shows one chain of thought ex"}, {"rank": 2, "score": 0.6885617, "source": "dense+rrf", "rrf_score": 0.03149801587301587, "rerank_score": 0.3357124452327364, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 22, "text_snippet": "rd few-shot prompting, popularized by Brown et al. (2020), in which a language model is given in-context exemplars of input‚Äìoutput pairs before outputting a prediction for a test-time example. Exemplars are formatted as questions and answer"}, {"rank": 3, "score": 0.6880075, "source": "dense+rrf", "rrf_score": 0.031009615384615385, "rerank_score": 0.33025810176803694, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 30, "text_snippet": " output ‚ü©triples for arithmetic, commonsense, and symbolic reasoning benchmarks. Chains of thought are highlighted. Full prompts in Appendix G. math word problems, we used this single set of eight chain of thought exemplars for all benchmar"}, {"rank": 4, "score": 0.71679515, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.3297276905863037, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 56, "text_snippet": "ete set. Figure 3 shows examples with chain of thought annotations for all datasets. Prompts. We follow the same experimental setup as the prior section. For CSQA and StrategyQA, we randomly selected examples from the training set and manua"}, {"rank": 5, "score": 0.61195123, "source": "dense+rrf", "rrf_score": 0.02813852813852814, "rerank_score": 0.31153766273397654, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 143, "text_snippet": "hain of thought still works with reasoning chains from an independent source without a background in machine learning. So we randomly sampled three sets of eight exemplars with chains of thought from GSM8K. These chain of thought annotation"}]}
{"case_index": 41, "query": "From 2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt, what exact term fills [BLANK] in: \"This highlights the potential of endowing smaller models with better reasoning [BLANK].\"?", "gold": "capabilities", "answer": "The exact term that fills **[BLANK]** is **‚Äúcapabilities‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'capabilities' and is directly supported by Context 1 sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 55.848, "llm_ms": 1186.235, "top_contexts": [{"rank": 1, "score": 0.6418452, "source": "dense+rrf", "rrf_score": 0.03131881575727918, "rerank_score": 0.27476321198326453, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 20, "text_snippet": "problem solving, reading comprehension, summarization, groundedness, truthfulness and toxic content generation and identification. Our preliminary results indicate that Orca 2 significantly surpasses models of a similar size, even matching "}, {"rank": 2, "score": 31.72843894149969, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.25680892798057553, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 148, "text_snippet": "synthetic data for post-training. While Orca 2 models have not gone through RLHF training for safety, we believe that the use of synthetic data for post-training that has been filtered with various content safety filters could provide anoth"}, {"rank": 3, "score": 0.67597806, "source": "dense+rrf", "rrf_score": 0.0315136476426799, "rerank_score": 0.25357158739857666, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 1, "text_snippet": "n benchmarks like BigBench Hard and AGIEval. In Orca 2, we continue exploring how improved training signals can enhance smaller LMs‚Äô reasoning abilities. Research on training small LMs has often relied on imitation learning to replicate the"}, {"rank": 4, "score": 25.37330006149412, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.2311893529660104, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 136, "text_snippet": "g ). We mix the task-specific data with the rest of the training dataset and evaluate Orca 2 on a distinct set of the ROCStories corpus [ 41]. While sampling the test instances, we remove any instances from ROCStories that are in FLAN train"}, {"rank": 5, "score": 25.594041224158033, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.22581948043932276, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 281, "text_snippet": " you always used to look like this , did n‚Äôt you , _‚Äù What is the word in the blank space (_)? The answer is Orca-v2-13B-Cautious The answer is izzy. The word in the blank space is the name of the girl who is crying. Figure 24: Demonstrativ"}]}
{"case_index": 42, "query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \"In addi- tion to the masked language model, we also use a ‚Äúnext sentence prediction‚Äù task that jointly pre- trains text-pair [BLANK].\"?", "gold": "representations", "answer": "The exact term is **‚Äúrepresentations‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected blank term and is directly supported by context ('text-pair representations').", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 59.428, "llm_ms": 945.276, "top_contexts": [{"rank": 1, "score": 0.6919973, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.44582136783149673, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 9, "text_snippet": "ng, the MLM ob- jective enables the representation to fuse the left and the right context, which allows us to pre- train a deep bidirectional Transformer. In addi- tion to the masked language model, we also use a ‚Äúnext sentence prediction‚Äù "}, {"rank": 2, "score": 38.90622327907607, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.3058026391727463, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 10, "text_snippet": "ford et al. (2018), which uses unidirec- tional language models for pre-training, BERT uses masked language models to enable pre- trained deep bidirectional representations. This is also in contrast to Peters et al. (2018a), which uses a sh"}, {"rank": 3, "score": 0.6185641, "source": "dense+rrf", "rrf_score": 0.030776515151515152, "rerank_score": 0.282728926856206, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 66, "text_snippet": " be found in Appendix C. 5.1 Effect of Pre-training Tasks We demonstrate the importance of the deep bidi- rectionality of BERT by evaluating two pre- training objectives using exactly the same pre- training data, Ô¨Åne-tuning scheme, and hype"}, {"rank": 4, "score": 0.67710465, "source": "dense+rrf", "rrf_score": 0.030621785881252923, "rerank_score": 0.2783422186448933, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 36, "text_snippet": "ela- tionship between two sentences, which is not di- rectly captured by language modeling. In order to train a model that understands sentence rela- tionships, we pre-train for a binarized next sen- tence prediction task that can be trivia"}, {"rank": 5, "score": 47.36718970511736, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.27706931838158155, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 0, "text_snippet": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova Google AI Language {jacobdevlin,mingweichang,kentonl,kristout }@google.com Abstract We introduce a ne"}]}
{"case_index": 43, "query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \"The same [BLANK] model parameters are used to initialize models for different down-stream tasks.\"?", "gold": "pre-trained", "answer": "pre-trained", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches the exact phrase in context: 'The same pre-trained model parameters...'", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 57.969, "llm_ms": 1521.105, "top_contexts": [{"rank": 1, "score": 0.6439496, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.35202419071919727, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 20, "text_snippet": "or BERT. Apart from output layers, the same architec- tures are used in both pre-training and Ô¨Åne-tuning. The same pre-trained model parameters are used to initialize models for different down-stream tasks. During Ô¨Åne-tuning, all parameters"}, {"rank": 2, "score": 0.60362345, "source": "dense+rrf", "rrf_score": 0.031746031746031744, "rerank_score": 0.29400781433351647, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 39, "text_snippet": " objectives used in Jernite et al. (2017) and Logeswaran and Lee (2018). However, in prior work, only sentence embeddings are transferred to down-stream tasks, where BERT transfers all pa- rameters to initialize end-task model parameters. P"}, {"rank": 3, "score": 34.53129762296854, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.2699603442467558, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 109, "text_snippet": "reading books. In Proceedings of the IEEE international conference on computer vision , pages 19‚Äì27. Appendix for ‚ÄúBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding‚Äù We organize the appendix into three section"}, {"rank": 4, "score": 0.54292816, "source": "dense+rrf", "rrf_score": 0.029709507042253523, "rerank_score": 0.26647748032001645, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 0, "text_snippet": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova Google AI Language {jacobdevlin,mingweichang,kentonl,kristout }@google.com Abstract We introduce a ne"}, {"rank": 5, "score": 32.907838240761684, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.2562613750230323, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 83, "text_snippet": "ted that rich, unsupervised pre-training is an integral part of many language understanding systems. In particular, these results enable even low-resource tasks to beneÔ¨Åt from deep unidirectional architec- tures. Our major contribution is f"}]}
{"case_index": 44, "query": "From 2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"For models that do not provide access to token [BLANK], such as ChatGPT and GPT-4, differ- ent methods are needed.\"?", "gold": "probabilities", "answer": "The exact term is **‚Äúprobabilities‚Äù** (i.e., ‚Äútoken probabilities‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'probabilities' and is directly supported by Context 1 ('token probabilities').", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 78.738, "llm_ms": 1264.116, "top_contexts": [{"rank": 1, "score": 0.5925308, "source": "dense+rrf", "rrf_score": 0.032266458495966696, "rerank_score": 0.3208442884559309, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 14, "text_snippet": "s it unsuitable for systems that access LLMs through an API. For models that do not provide access to token probabilities, such as ChatGPT and GPT-4, differ- ent methods are needed. SelfCheckGPT (Manakul et al., 2023) addresses this problem"}, {"rank": 2, "score": 24.679330069990957, "source": "dense+rrf", "rrf_score": 0.014285714285714285, "rerank_score": 0.22780391169368783, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 66, "text_snippet": "on par of the strongest fine-tuned RAG model like Atlas. On the HotpotQA dataset, LongRAG achieves a 64.3 exact match rate, which is also close to the SoTA fully-supervised RAG frameworks. 3.4 Full QA Performance on non-Wikipedia-based Data"}, {"rank": 3, "score": 25.841539757022954, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.2058950054968896, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 8, "text_snippet": "e not always predictive of downstream performance (Wang et al., 2023c). Moreover, this evaluation strategy relies on the LM probabilities, which are not accessible for some closed models (e.g. ChatGPT and GPT-4). Ques- tion answering is ano"}, {"rank": 4, "score": 24.845541938584617, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.19716526876855847, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 140, "text_snippet": "ments furnish quantitative metrics that not only gauge RAG model perfor- mance but also enhance comprehension of the model‚Äôs capabil- ities across various evaluation aspects. Prominent benchmarks such as RGB, RECALL and CRUD [167]‚Äì[169] foc"}, {"rank": 5, "score": 26.305302206317087, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.19465186780640945, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 2, "text_snippet": "is work, we provide a generalized view of ac- tive retrieval augmented generation , methods that actively decide when and what to retrieve across the course of the generation. We propose Forward- Looking Active REtrieval augmented generatio"}]}
{"case_index": 45, "query": "From 2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt, what exact term fills [BLANK] in: \"LongLoRA can [BLANK] Llama2 7B up to 100k context, or a 70B model up to 32k, on a single 8√óA100 machine.\"?", "gold": "fine-tune", "answer": "The exact term is **‚Äúfine-tune‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'fine-tune' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 66.649, "llm_ms": 1047.733, "top_contexts": [{"rank": 1, "score": 0.70689374, "source": "dense+rrf", "rrf_score": 0.032266458495966696, "rerank_score": 0.3839338640358059, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 20, "text_snippet": "embeddings. The trained models achieve comparable performance to the full-attention and fully fine-tuned results, while the computational cost is much less as shown in Figure 1. LongLoRA can fine-tune Llama2 7B up to 100k context, or a 70B "}, {"rank": 2, "score": 0.7114869, "source": "dense+rrf", "rrf_score": 0.032018442622950824, "rerank_score": 0.32978452190636093, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 58, "text_snippet": "lama2 13B model, we observe that the perplexity reduces by -0.28. In Table 4, we further examine the maximum context length that we can fine-tune on a single 8 √ó A100 machine. We extend Llama2 7B, 13B, and 70B to 100k, 65536, and 32768 cont"}, {"rank": 3, "score": 0.6706176, "source": "dense+rrf", "rrf_score": 0.03125763125763126, "rerank_score": 0.32798518131866866, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 3, "text_snippet": "ll under the premise of trainable embedding and normalization. LongLoRA combines this improved LoRA with S2-Attn. LongLoRA demonstrates strong empirical results on various tasks on Llama2 models from 7B/13B to 70B. LongLoRA extends Llama2 7"}, {"rank": 4, "score": 0.7083305, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.3256216123332332, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 77, "text_snippet": "l standard attention architecture during inference, making most pre-existing infrastructure and optimization reusable. At the training level, we bridge the gap between LoRA and full fine-tuning with trainable normalization and embedding. Ou"}, {"rank": 5, "score": 0.64042354, "source": "dense+rrf", "rrf_score": 0.02758751902587519, "rerank_score": 0.23959979819743166, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 51, "text_snippet": " We use AdamW (Loshchilov & Hutter, 2019) with Œ≤1= 0.9andŒ≤2= 0.95. The learning rate is set to 2√ó10‚àí5for 7B and 13B models, and 10‚àí5for 70B models. We also use a linear learning rate warmup. The weight decay is 6  Published as a conference "}]}
{"case_index": 46, "query": "From 2305.06983__active_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"We discard [BLANK] retrieved documents ‚à™t‚Ä≤<tDqt‚Ä≤and only use the retrieved documents from the current step to condition the next generation to prevent reaching the input length limit of LMs.\"?", "gold": "previously", "answer": "The exact term is **‚Äúpreviously‚Äù**, i.e., ‚ÄúWe discard **previously** retrieved documents ‚à™t‚Ä≤<tDqt‚Ä≤ ‚Ä¶‚Äù.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'previously' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 64.533, "llm_ms": 1330.375, "top_contexts": [{"rank": 1, "score": 0.7206211, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.48762761249153347, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 24, "text_snippet": "t, and the input to LMs is the concatena- tion of the retrieved documents Dqt, the user input x, and the previous generation y<t. We discard previously retrieved documents ‚à™t‚Ä≤<tDqt‚Ä≤and only use the retrieved documents from the current step "}, {"rank": 2, "score": 0.5883627, "source": "dense+rrf", "rrf_score": 0.030834914611005692, "rerank_score": 0.2808881108195729, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 16, "text_snippet": ". When deciding what to retrieve , it is important to consider what LMs intend to gen- erate in the future, as the goal of active retrieval is to benefit future generations. Therefore, we propose anticipating the future by generating a temp"}, {"rank": 3, "score": 27.907237180038532, "source": "dense+rrf", "rrf_score": 0.013888888888888888, "rerank_score": 0.24790812858910846, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 11, "text_snippet": "r of documents in the input context (akin to retrieving more or less documents in retrieval-augmented generation), and (ii) control the position of the relevant information within the input context by changing the order of the documents to "}, {"rank": 4, "score": 0.64943784, "source": "dense+rrf", "rrf_score": 0.0315136476426799, "rerank_score": 0.24217951957431819, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 23, "text_snippet": "and previously generated output y<t= [y0, ...,yt‚àí1]: qt=qry(x,y<t), where qry(¬∑)is the query formulation function. At the beginning ( t= 1), the previous generation is empty ( y<1=‚àÖ), and the user input is used as the initial query ( q1=x)."}, {"rank": 5, "score": 0.60054195, "source": "dense+rrf", "rrf_score": 0.03125763125763126, "rerank_score": 0.23958485958485473, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 22, "text_snippet": " generate the complete answer at once y=LM([Dx,x]). 2.3 Active Retrieval Augmented Generation To aid long-form generation with retrieval, we pro- pose active retrieval augmented generation. It is a generic framework that actively decides wh"}]}
{"case_index": 47, "query": "From 2305.06983__active_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"These attempts include methods that passively use the past context to retrieve additional [BLANK] at a fixed interval (Khandelwal et\"?", "gold": "information", "answer": "The exact term is **‚Äúinformation‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context explicitly contains the phrase 'retrieve additional information at a fixed interval', matching the expected blank.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 74.665, "llm_ms": 830.955, "top_contexts": [{"rank": 1, "score": 0.58428854, "source": "dense+rrf", "rrf_score": 0.032018442622950824, "rerank_score": 0.35187050219460714, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 12, "text_snippet": "Biden) may not cover all aspects and de- tails. It is crucial to retrieve extra information as needed during generation, such as when generat- ing a certain aspect (e.g., Joe Biden‚Äôs education history) or a specific detail (e.g., the date o"}, {"rank": 2, "score": 0.5557694, "source": "dense+rrf", "rrf_score": 0.02964254577157803, "rerank_score": 0.22349312152199227, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 13, "text_snippet": "rieve additional information at a fixed interval (Khandelwal et al., 2020; Borgeaud et al., 2022; Ram et al., 2023; Trivedi et al., 2022) which might not accurately reflect what LMs intend to gener- ate in the future or retrieve at inapprop"}, {"rank": 3, "score": 28.951075239511347, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.22244272280342256, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 17, "text_snippet": "ng Active REtrieval augmented generation (FLARE ), as illustrated in Figure 1. FLARE iter- atively generates a temporary next sentence , use it as the query to retrieve relevant documents if it contains low-probability tokens and regenerate"}, {"rank": 4, "score": 0.5949315, "source": "dense+rrf", "rrf_score": 0.030798389007344232, "rerank_score": 0.21839536817844746, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 15, "text_snippet": "e to avoid unneces- sary or inappropriate retrieval that occurs in passive retrieval augmented LMs (Khandelwal et al., 2020; Borgeaud et al., 2022; Ram et al., 2023; Trivedi et al., 2022). Given the observation that large LMs tend to be wel"}, {"rank": 5, "score": 0.5573829, "source": "dense+rrf", "rrf_score": 0.029083245521601686, "rerank_score": 0.21309673730633052, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 21, "text_snippet": "ing when and what to retrieve, we follow exist- ing methods (Ram et al., 2023; Trivedi et al., 2022) to prepend the retrieved documents before the user input to aid future generation for both baselines and our method for fair comparisons: y"}]}
{"case_index": 48, "query": "From 2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt, what exact term fills [BLANK] in: \"With a 2 trillion token database, our [BLANK] Transformer ( R/e.sc/t.sc/r.sc/o.sc) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25 \u0002fewer parameters.\"?", "gold": "retrieval-enhanced", "answer": "The exact term is **‚ÄúRetrieval-Enhanced‚Äù**, i.e., **‚ÄúRetrieval-Enhanced Transformer (RETRO)‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'retrieval-enhanced' and is explicitly present in the provided context ('Retrieval-Enhanced Transformer').", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 107.767, "llm_ms": 1366.305, "top_contexts": [{"rank": 1, "score": 0.65249664, "source": "dense+rrf", "rrf_score": 0.03177805800756621, "rerank_score": 0.4826452838093454, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 1, "text_snippet": ", Karen Simonyan, Jack W. Raez, Erich Elsenzand Laurent Sifrey,z All authors from DeepMind,yEqual contributions,zEqual senior authorship We enhance auto-regressive language models by conditioning on document chunks retrieved from a large co"}, {"rank": 2, "score": 52.14631664514275, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.36392084132043273, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 228, "text_snippet": "baseline models, measured in bits-per-bytes on evaluation set, in Table 14. F.2. The Pile In Fig. 4, we compare R/e.sc/t.sc/r.sc/o.scagainst Jurassic-1 (Lieber et al., 2021). The full bits-per-bytes results are reported in Table 15. F.3. Fi"}, {"rank": 3, "score": 48.56982293714528, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.3577962556374423, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 38, "text_snippet": "t token of chunk ùê∂ùë¢is the Ô¨Årst to be able to access the retrieved content ùê∏ùë¢while maintaining autoregressivity in(1). Hence, there is a one token overlap between chunk ùê∂ùë¢=\u0010 ùë•¬πùë¢\u00001¬∫ùëö¬∏ùëñ\u0011 ùëñ2¬ª1¬îùëö¬ºand the corresponding attending chunk ùê∂¬∏ ùë¢,¬πùë•ùë¢ùëö¬∏ùëñ"}, {"rank": 4, "score": 55.464011812196155, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.3549453951094579, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 22, "text_snippet": " ùïç=¬ª1¬îùë£¬º, obtained using a text tokenizer1. Wespliteach ùëõ-token-longexample ùëã=¬πùë•1¬î¬ì¬ì¬ì¬îùë•ùëõ¬∫intoasequenceof ùëôchunks¬πùê∂1¬î¬ì¬ì¬ì¬îùê∂ùëô¬∫ of sizeùëö=ùëõ ùëô, i.e.ùê∂1,¬πùë•1¬î¬ì¬ì¬ì¬îùë•ùëö¬∫¬î ¬ì¬ì¬ì¬î ùê∂ùëô,¬πùë•ùëõ\u0000ùëö¬∏1¬î¬ì¬ì¬ì¬îùë•ùëõ¬∫2ùïçùëö. We useùëõ=2048andùëö=64. We augment each chunk ùê∂ùë¢with a se"}, {"rank": 5, "score": 0.6147511, "source": "dense+rrf", "rrf_score": 0.02928692699490662, "rerank_score": 0.3370888538833057, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 235, "text_snippet": "of the Ô¨Åltered curves is the same: R/e.sc/t.sc/r.sc/o.scmodels exploit leakage more, but the performance improvement they provide remains signiÔ¨Åcant even on original chunks that haven‚Äôt been observed in the training set. 34  Improving langu"}]}
{"case_index": 49, "query": "From 2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt, what exact term fills [BLANK] in: \"Knowledge in the form of natural language can be entirely offloaded from the [BLANK] knowledge of LLMs by leveragi\"?", "gold": "parametric", "answer": "The exact term is **‚Äúparametric‚Äù**, i.e., ‚Äú‚Ä¶offloaded from the **parametric** knowledge of LLMs‚Ä¶‚Äù", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'parametric' and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 47.756, "llm_ms": 1319.256, "top_contexts": [{"rank": 1, "score": 56.04321040532367, "source": "dense+rrf", "rrf_score": 0.01639344262295082, "rerank_score": 0.3445287623754549, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 5, "text_snippet": "ather than chunking them into smaller units. By doing so, we achieve an F1 score of 25.9% on Qasper (previously 22.5%) and 57.5% on MultiFieldQA-en (previously 51.2%). Our study offers insights into the future roadmap for combining RAG with"}, {"rank": 2, "score": 0.65038294, "source": "dense+rrf", "rrf_score": 0.031544957774465976, "rerank_score": 0.23117714158838185, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 79, "text_snippet": "on the long-context understanding ability of LLMs, and we still have a long way to go in harnessing open-source LLMs within our framework. 10  4 Related Work 4.1 Retrieval-Augmented Generation. Augmenting language models with information re"}, {"rank": 3, "score": 32.33792798864114, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.21387169399136002, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 0, "text_snippet": "LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs ‚ô†Ziyan Jiang,‚ô†Xueguang Ma,‚ô†Wenhu Chen ‚ô†University of Waterloo ziyanjiang528@gmail.com ,{x93ma ,wenhuchen}@uwaterloo.ca Project Website: https://tiger-ai-lab.github.io/"}, {"rank": 4, "score": 28.1836293099908, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.21070792065469268, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 19, "text_snippet": "e end performance dips significantly due to the huge amount of ‚Äúhard negatives‚Äù, which confuses the reader. With ‚Äúlong retriever units‚Äù, we observe an entirely different trend. As we recall more units (from 1 to 8 units), both the recall an"}, {"rank": 5, "score": 27.653968693195836, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.1901859494179779, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 14, "text_snippet": " al., 2022). We ask the following question: can we create a simple and generic retrieval augmented LM that ac- tively decides when and what to retrieve throughout the generation process, and are applicable to a va- riety of long-form genera"}]}
{"case_index": 50, "query": "From 2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt, what exact term fills [BLANK] in: \"In Orca 2, we continue exploring how improved training signals can enhance smaller LMs‚Äô [BLANK] abilities.\"?", "gold": "reasoning", "answer": "The exact term that fills **[BLANK]** is **‚Äúreasoning‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context explicitly states 'enhance smaller LMs‚Äô reasoning abilities'; model answer matches expected term.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 54.839, "llm_ms": 1197.913, "top_contexts": [{"rank": 1, "score": 0.7421719, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.36187605953043217, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 1, "text_snippet": "n benchmarks like BigBench Hard and AGIEval. In Orca 2, we continue exploring how improved training signals can enhance smaller LMs‚Äô reasoning abilities. Research on training small LMs has often relied on imitation learning to replicate the"}, {"rank": 2, "score": 0.690933, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.23666027986663576, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 0, "text_snippet": "Orca 2: Teaching Small Language Models How to Reason Arindam Mitra, Luciano Del Corro‚Ä†, Shweti Mahajan‚Ä†, Andres Codas‚Ä° Clarisse Simoes‚Ä°, Sahaj Agarwal, Xuxi Chen‚àó, Anastasia Razdaibiedina‚àó Erik Jones‚àó, Kriti Aggarwal‚àó, Hamid Palangi, Guoqin"}, {"rank": 3, "score": 0.6227964, "source": "dense+rrf", "rrf_score": 0.029273504273504274, "rerank_score": 0.21681855350826482, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 148, "text_snippet": "synthetic data for post-training. While Orca 2 models have not gone through RLHF training for safety, we believe that the use of synthetic data for post-training that has been filtered with various content safety filters could provide anoth"}, {"rank": 4, "score": 0.70662117, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.21211122579957242, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 15, "text_snippet": ", we continue to pursue the question of how we can teach smaller LMs to reason. The objectives of Orca 2 are two-fold. Firstly, we aim to teach smaller models howto use a suite of reasoning techniques, such as step-by-step processing, recal"}, {"rank": 5, "score": 30.419597167935965, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.19940888416800265, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 144, "text_snippet": "ng training and safety guardrails (e.g., content filter) within the Azure OpenAI API. However, detailed studies are required for better quantification of such risks. Small Model Capacity: We note that post-training, while significantly bene"}]}
{"case_index": 51, "query": "From 2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt, what exact term fills [BLANK] in: \"In addition to training LoRA weights in linear layers, LongLoRA further makes embedding and [BLANK] layers trainable.\"?", "gold": "normalization", "answer": "The exact term is **‚Äúnormalization‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'normalization' and is explicitly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 61.297, "llm_ms": 1262.335, "top_contexts": [{"rank": 1, "score": 0.73342377, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3298469206973678, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 8, "text_snippet": "e time. In addition to training LoRA weights in linear layers, LongLoRA further makes embedding and normalization layers trainable. This extension is pivotal for context extension, and only introduces a minimal number of additional trainabl"}, {"rank": 2, "score": 0.58849686, "source": "dense+rrf", "rrf_score": 0.028693528693528692, "rerank_score": 0.2450881063922643, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 0, "text_snippet": "Published as a conference paper at ICLR 2024 LONG LORA: E FFICIENT FINE-TUNING OF LONG - CONTEXT LARGE LANGUAGE MODELS Yukang Chen1Shengju Qian1Haotian Tang2Xin Lai1 Zhijian Liu2Song Han2,3Jiaya Jia1 1CUHK2MIT3NVIDIA ABSTRACT We present Lon"}, {"rank": 3, "score": 0.66270864, "source": "dense+rrf", "rrf_score": 0.031754032258064516, "rerank_score": 0.23983769908641503, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 3, "text_snippet": "ll under the premise of trainable embedding and normalization. LongLoRA combines this improved LoRA with S2-Attn. LongLoRA demonstrates strong empirical results on various tasks on Llama2 models from 7B/13B to 70B. LongLoRA extends Llama2 7"}, {"rank": 4, "score": 0.63964957, "source": "dense+rrf", "rrf_score": 0.03128054740957967, "rerank_score": 0.23086190013306465, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 7, "text_snippet": "ding ! ! ‚ùÑ(b) Low-rank adaptx N ‚ùÑPa1ern 1 -w/o shi@(a) Shi3ed sparse a6en7onEach pa1ern in half heads a !Norm (0.004%)a !Lora (0.12%) a ‚ùÑ Linear Projec3on (96%) a !Embedding (1.94%)a ‚ùÑHead (1.94%) (c) Propor5ons of Parameters (LLaMA7B for e"}, {"rank": 5, "score": 34.991707453020105, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.21846753890271323, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 2, "text_snippet": " shifted sparse attention (S2-Attn) effectively enables context extension, leading to non-trivial computation saving with similar perfor- mance to fine-tuning with vanilla attention. Particularly, it can be implemented with only two lines o"}]}
{"case_index": 52, "query": "From 2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt, what exact term fills [BLANK] in: \"Each kernel loads inputs from HBM to [BLANK] and SRAM, computes, then writes outputs to HBM.\"?", "gold": "registers", "answer": "The exact term is **‚Äúregisters‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'registers' and is directly supported by Context 1 sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 60.129, "llm_ms": 1315.739, "top_contexts": [{"rank": 1, "score": 67.00851998126417, "source": "dense+rrf", "rrf_score": 0.01639344262295082, "rerank_score": 0.272958633509881, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 23, "text_snippet": "peed [ 61,62,63], operations are increasingly bottlenecked by memory (HBM) accesses. Thus exploiting fast SRAM becomes more important. Execution Model. GPUs have a massive number of threads to execute an operation (called a kernel). Each ke"}, {"rank": 2, "score": 54.73686116546747, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.26085028193941917, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 100, "text_snippet": "n, Atri Rudra, and Christopher R√©. 2022. FlashAttention: Fast and memory-efficient exact attention with IO- awareness. ArXiv:2205.14135. Hermann Ebbinghaus. 1913. Memory: A contribu- tion to experimental psychology. H. A. Ruger & C. E. Buss"}, {"rank": 3, "score": 0.6341281, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.2349856404034172, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 7, "text_snippet": " with wall-clock speed) and tend to ignore overheads from memory access (IO). In this paper, we argue that a missing principle is making attention algorithms IO-aware [1]‚Äîthat is, carefully accounting for reads and writes to diÔ¨Äerent levels"}, {"rank": 4, "score": 36.00596494635844, "source": "dense+rrf", "rrf_score": 0.014285714285714285, "rerank_score": 0.22726894231898614, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 32, "text_snippet": " techniques (tiling, recomputation) to overcome the technical challenge of computing exact attention in sub-quadratic HBM accesses. We describe this in Algorithm 1. The main idea is that we split the inputs Q¬ñK¬ñVinto blocks, load them from "}, {"rank": 5, "score": 39.956400076416315, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.22211932603236448, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 84, "text_snippet": "mputer/RedPajama-Data . Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. CoRR , abs/2307.08691, 2023. Tri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra, and Christopher R ¬¥e. Flashattention: Fast an"}]}
{"case_index": 53, "query": "From 2302.13971__llama_open_and_efficient_foundation_language_models.txt, what exact term fills [BLANK] in: \"2.3 Optimizer Our models are trained using the AdamW opti- mizer (Loshchilov and Hutter, 2017), with the fol- lowing [BLANK]: Œ≤1= 0.9,Œ≤2= 0.95.\"?", "gold": "hyper-parameters", "answer": "The exact term is **‚Äúhyper-parameters‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term and is directly supported by Context 1, which contains the exact phrase 'following hyper-parameters'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 94.452, "llm_ms": 1047.225, "top_contexts": [{"rank": 1, "score": 0.6472821, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.38426432522419707, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 22, "text_snippet": " (2021), at each layer of the network. The details of the hyper-parameters for our dif- ferent models are given in Table 2. 2.3 Optimizer Our models are trained using the AdamW opti- mizer (Loshchilov and Hutter, 2017), with the fol- lowing"}, {"rank": 2, "score": 0.5926455, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.23180783114717074, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 189, "text_snippet": "ximum learning rate, which is shown in Table 10. The learning rate is decayed using a cosine cycle length that matches the total number of training tokens. All models are trained using AdamW (Loshchilov and Hutter, 2019) with a weight decay"}, {"rank": 3, "score": 0.48534322, "source": "dense+rrf", "rrf_score": 0.027984344422700584, "rerank_score": 0.22756407876557275, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 283, "text_snippet": "air encodings as in Brown et al. (2020) are used for all models. All our language models and RL policies have a context length of 2k tokens. We Ô¨Ålter out prompts that are longer than 1k tokens and limit the maximum response length to 1k tok"}, {"rank": 4, "score": 34.44308771591394, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.2251418130556358, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 50, "text_snippet": "es are up to 100k for 7B models, 65536 for 13B models, and 32768 for 70B models. The position indices for these models are re-scaled with Position Interpolation (Chen et al., 2023). Training Procedure We follow most training hyper-parameter"}, {"rank": 5, "score": 0.50430954, "source": "dense+rrf", "rrf_score": 0.029083245521601686, "rerank_score": 0.21192905827956496, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 187, "text_snippet": "In particular, we use the LAMB optimizer with learning rate 3.75e-3, with batch size 448, trained for at most 7100 steps. The training is stopped once the validation accuracy (for masked language modeling) reaches the target 72.0%, and the "}]}
{"case_index": 54, "query": "From 2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt, what exact term fills [BLANK] in: \"In this paper, we mainly focus on the setup where a frozen large language model, PaLM-540B (Chowdhery et al., 2022)1, is prompted with few-shot in-context examples to generate both [BLANK]Ô¨Åc actions and free-form language thoughts for task solving (Figure 1 (1d), (2b)).\"?", "gold": "domain-speci", "answer": "The exact term is **‚Äúdomain-speciÔ¨Åc‚Äù**.", "answer_score": 0.3, "answer_correctness": 0.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Blank asks for the exact term filling [BLANK], which is 'domain-speci' (as in 'domain-speciÔ¨Åc'). Model answered 'domain-specific' including the suffix, so it does not match the expected exact fill. Context clearly supports 'domain-speciÔ¨Åc'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 97.841, "llm_ms": 990.98, "top_contexts": [{"rank": 1, "score": 0.7973107, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.5670555617118356, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 30, "text_snippet": " Thought 3), and so on. However, as the language space Lis unlimited, learning in this augmented action space is difÔ¨Åcult and requires strong language priors. In this paper, we mainly focus on the setup where a frozen large language model, "}, {"rank": 2, "score": 0.62334275, "source": "dense+rrf", "rrf_score": 0.029116045245077504, "rerank_score": 0.3119217385127548, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 19, "text_snippet": "udies on how reasoning and acting can be combined in a synergistic manner for general task solving, and if such a combination can bring systematic beneÔ¨Åts compared to reasoning or acting alone. In this work, we present ReAct , a general par"}, {"rank": 3, "score": 0.64030004, "source": "dense+rrf", "rrf_score": 0.029437229437229435, "rerank_score": 0.2882202032821435, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 17, "text_snippet": " fact hallucination and error propagation over the reasoning process (Figure 1 (1b)). On the other hand, recent work has explored the use of pre-trained language models for planning and acting in interactive environments (Ahn et al., 2022; "}, {"rank": 4, "score": 42.97649981217655, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.2831476430559653, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 43, "text_snippet": " mod- els, respectively. 4.2 Downstream Tasks We evaluate all models on a variety of downstream tasks. In all cases, we consider a prompted zero- shot setup ‚Äì i.e., models are instructed to solve each task in natural language, but we do not"}, {"rank": 5, "score": 45.7207968488692, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.2721180556556301, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 31, "text_snippet": "xt example is a human trajectory of actions, thoughts, and environment observations to solve a task instance (see Appendix C). For the tasks where reasoning is of primary importance (Figure 1(1)), we alternate the generation of thoughts and"}]}
{"case_index": 55, "query": "From 2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt, what exact term fills [BLANK] in: \"They both come back [BLANK] later in the day, and they do not know what happened in the room after each of them left the room.\"?", "gold": "together", "answer": "The exact term that fills **[BLANK]** is:\n\n**together**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "The context explicitly contains the phrase 'They both come back together later in the day,' matching the expected blank.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 77.88, "llm_ms": 1197.289, "top_contexts": [{"rank": 1, "score": 0.6174563, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3970756282289402, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 6, "text_snippet": "then leaves for school. They both come back together later in the day, and they do not know what happened in the room after each of them left the room. Where do they think the ball is? ### Orca-2-13B : Let‚Äôs analyze the situation step by st"}, {"rank": 2, "score": 0.47784114, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.22959587601434392, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 7, "text_snippet": "John and Mark have not seen each other‚Äôs actions. Therefore, they only know what happened while they were in the room. John only saw the ball in the box before he left for work. Mark only saw the ball in the basket before he left for school"}, {"rank": 3, "score": 0.49786803, "source": "dense+rrf", "rrf_score": 0.0315136476426799, "rerank_score": 0.22889315081433265, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 10, "text_snippet": "ve still been in the box. 4. Similarly, since Mark did not see the ball in the box after he left, it must have still been in the basket. Therefore, the ball is still in the box. ### ChatGPT(GPT-3.5-Turbo) : John and Mark will likely think t"}, {"rank": 4, "score": 0.44591406, "source": "dense+rrf", "rrf_score": 0.03149801587301587, "rerank_score": 0.22001828667156328, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 5, "text_snippet": "rca 2 models match or surpass all other models including models 5-10x larger. Note that all models are using the same LLaMA-2 base models of the respective size. ‚àówork done while at Microsoft;‚Ä†,‚Ä°denote equal contributions.arXiv:2311.11045v2"}, {"rank": 5, "score": 22.822534128842015, "source": "dense+rrf", "rrf_score": 0.014285714285714285, "rerank_score": 0.20868679112233537, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 51, "text_snippet": " closest to how humans perform tasks ‚Äì for example, in the translation example in Figure 2.1, a human would likely know what to do from just the text instruction. Figure 2.1 shows the four methods using the example of translating English to"}]}
{"case_index": 56, "query": "From 2305.06983__active_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"2.3 Active Retrieval Augmented Generation To aid [BLANK] generation with retrieval, we pro- pose active retrieval augmented generation.\"?", "gold": "long-form", "answer": "The exact term is **‚Äúlong-form‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 72.658, "llm_ms": 972.445, "top_contexts": [{"rank": 1, "score": 0.6433887, "source": "dense+rrf", "rrf_score": 0.032266458495966696, "rerank_score": 0.30493429459703997, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 22, "text_snippet": " generate the complete answer at once y=LM([Dx,x]). 2.3 Active Retrieval Augmented Generation To aid long-form generation with retrieval, we pro- pose active retrieval augmented generation. It is a generic framework that actively decides wh"}, {"rank": 2, "score": 0.63326776, "source": "dense+rrf", "rrf_score": 0.03128054740957967, "rerank_score": 0.21781046362108838, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 21, "text_snippet": "ing when and what to retrieve, we follow exist- ing methods (Ram et al., 2023; Trivedi et al., 2022) to prepend the retrieved documents before the user input to aid future generation for both baselines and our method for fair comparisons: y"}, {"rank": 3, "score": 25.01895353771121, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.2162776394325278, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 28, "text_snippet": "University)][Search(Joe Biden degree)]the University of Pennsylvania, where he earneda law degree.Generate a summary about Joe Biden.Input$&$&#%$&%%%GenerationRetriever$%$%%Figure 2: An illustration of forward-looking active re- trieval aug"}, {"rank": 4, "score": 0.5900088, "source": "dense+rrf", "rrf_score": 0.027417840375586856, "rerank_score": 0.19443456526444444, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 11, "text_snippet": "eddocumentsLMGeneration$%$%% Figure 1: An illustration of forward-looking active retrieval augmented generation (FLARE). Starting with the user input xand initial retrieval results Dx, FLARE iteratively generates a temporary next sentence ("}, {"rank": 5, "score": 0.62238634, "source": "dense+rrf", "rrf_score": 0.029857397504456328, "rerank_score": 0.19339785155126105, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 16, "text_snippet": ". When deciding what to retrieve , it is important to consider what LMs intend to gen- erate in the future, as the goal of active retrieval is to benefit future generations. Therefore, we propose anticipating the future by generating a temp"}]}
{"case_index": 57, "query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"In contrast to existing approaches, this enables a much more [BLANK] use of tools that is not tied to speciÔ¨Åc tasks.\"?", "gold": "comprehensive", "answer": "The exact term that fills **[BLANK]** is **‚Äúcomprehensive‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'comprehensive' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 90.15, "llm_ms": 2315.049, "top_contexts": [{"rank": 1, "score": 0.6278786, "source": "dense+rrf", "rrf_score": 0.03177805800756621, "rerank_score": 0.35851275268294475, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 11, "text_snippet": "when andhow to use which tool. In contrast to existing approaches, this enables a much more comprehensive use of tools that is not tied to speciÔ¨Åc tasks. Our approach for achieving these goals is based on the recent idea of using large LMs "}, {"rank": 2, "score": 0.61476326, "source": "dense+rrf", "rrf_score": 0.03057889822595705, "rerank_score": 0.2808772717633871, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 1, "text_snippet": "metic or fac- tual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer , a model traine"}, {"rank": 3, "score": 0.5603307, "source": "dense+rrf", "rrf_score": 0.02690501986276634, "rerank_score": 0.26775542371359995, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 43, "text_snippet": " mod- els, respectively. 4.2 Downstream Tasks We evaluate all models on a variety of downstream tasks. In all cases, we consider a prompted zero- shot setup ‚Äì i.e., models are instructed to solve each task in natural language, but we do not"}, {"rank": 4, "score": 0.6268873, "source": "dense+rrf", "rrf_score": 0.029437229437229435, "rerank_score": 0.24661070217295286, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 13, "text_snippet": "h tool to use when and how. As our approach is agnostic of the dataset be- ing used, we can apply it to the exact same dataset that was used to pretrain a model in the Ô¨Årst place. This ensures that the model does not lose any of its general"}, {"rank": 5, "score": 0.67618537, "source": "dense+rrf", "rrf_score": 0.03177805800756621, "rerank_score": 0.23938571760292252, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 90, "text_snippet": "pilan et al., 2022) or they work by prompting the language model in a few-shot setup tailored towards a speciÔ¨Åc task where it is known a priori which tools needs to beused (Gao et al., 2022; Lazaridou et al., 2022; Yao et al., 2022). In con"}]}
{"case_index": 58, "query": "From 2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt, what exact term fills [BLANK] in: \"dels (LLMs) showcase impres- sive capabilities but encounter challenges like hallucination, outdated knowledge, and [BLANK], untraceable reasoning processes.\"?", "gold": "non-transparent", "answer": "The exact term is **‚Äúnon-transparent‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term and is explicitly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 60.185, "llm_ms": 1095.572, "top_contexts": [{"rank": 1, "score": 0.7327156, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.42779643982515037, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 1, "text_snippet": "dels (LLMs) showcase impres- sive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution b"}, {"rank": 2, "score": 63.441950273113555, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.2934427342995043, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 0, "text_snippet": "1 Retrieval-Augmented Generation for Large Language Models: A Survey Yunfan Gaoa, Yun Xiongb, Xinyu Gaob, Kangxiang Jiab, Jinliu Panb, Yuxi Bic, Yi Daia, Jiawei Suna, Meng Wangc, and Haofen Wanga,c aShanghai Research Institute for Intellige"}, {"rank": 3, "score": 0.6315018, "source": "dense+rrf", "rrf_score": 0.028484848484848488, "rerank_score": 0.21436578171229875, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 79, "text_snippet": "on the long-context understanding ability of LLMs, and we still have a long way to go in harnessing open-source LLMs within our framework. 10  4 Related Work 4.1 Retrieval-Augmented Generation. Augmenting language models with information re"}, {"rank": 4, "score": 26.40405032002434, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.20793593096955232, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 238, "text_snippet": " maximizing llm performance,‚Äù https://community.openai. com/t/openai-dev-day-2023-breakout-sessions/505213# a-survey-of-techniques-for-maximizing-llm-performance-2, 2023. [167] J. Chen, H. Lin, X. Han, and L. Sun, ‚ÄúBenchmarking large lan- g"}, {"rank": 5, "score": 0.6518001, "source": "dense+rrf", "rrf_score": 0.03057889822595705, "rerank_score": 0.181191023458151, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 4, "text_snippet": "l, retrieval-augmented gen- eration, natural language processing, information retrieval I. I NTRODUCTION LARGE language models (LLMs) have achieved remark- able success, though they still face significant limitations, especially in domain-s"}]}
{"case_index": 59, "query": "From 2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt, what exact term fills [BLANK] in: \"It [BLANK] MoE layer as its feedforward models, employing the top2 routing among 16 expert networks.\"?", "gold": "incorporates", "answer": "The exact term is **‚Äúincorporates‚Äù**:\n\n‚ÄúIt **incorporates** MoE layer as its feedforward models, employing the top2 routing among 16 expert networks.‚Äù", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "The context explicitly contains the sentence with the blank filled by 'incorporates', matching the expected answer.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 93.562, "llm_ms": 2498.296, "top_contexts": [{"rank": 1, "score": 0.72585475, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.30557537768773524, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 14, "text_snippet": "ion layers to optimize KV cache savings while maintaining long context retrieval performance. An additional 10% multilingual data was also used for this model. Thephi-3.5-MoE adopts an Mixture-of-Experts (MoE) architecture to selectively ac"}, {"rank": 2, "score": 0.63759923, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.26007722091882485, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 16, "text_snippet": "3. The table shows the Keys/values a query token in block 8 attended to. Blue=local blocks, orange=remote/vertical blocks, gray=blocks skipped. total parameters. Additionally, we utilize the SparseMixer approach [LGC23, LDL+23] for training"}, {"rank": 3, "score": 0.518697, "source": "dense+rrf", "rrf_score": 0.03021353930031804, "rerank_score": 0.2542240628454928, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 0, "text_snippet": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone Microsoft Abstract We introduce phi-3-mini , a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both"}, {"rank": 4, "score": 0.52256376, "source": "dense+rrf", "rrf_score": 0.030117753623188408, "rerank_score": 0.23080414550424, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 6, "text_snippet": "7B parameters), matched the performance of models 25 times larger trained on regular data. In this report we present a new model, phi-3-mini (3.8B parameters), trained for 3.3T tokens on larger and more advanced versions of the datasets use"}, {"rank": 5, "score": 0.56798005, "source": "dense+rrf", "rrf_score": 0.03057889822595705, "rerank_score": 0.20389633406846633, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 2, "text_snippet": ", 8.9 on MT-bench). To enhance multilingual, multimodal, and long-context capabilities, we introduce three models in the phi-3.5 series: phi-3.5-mini ,phi-3.5-MoE , and phi- 3.5-Vision . The phi-3.5-MoE , a 16 x 3.8B MoE model with 6.6 bill"}]}
{"case_index": 60, "query": "From 2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt, what exact term fills [BLANK] in: \"We open-source [BLANK] to make it easier to build on this primitive.1 We empirically va\"?", "gold": "flashattention", "answer": "The exact term is **‚ÄúFlashAttention‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context explicitly says 'We open-source FlashAttention...'; expected answer 'flashattention' matches aside from capitalization.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 50.356, "llm_ms": 953.739, "top_contexts": [{"rank": 1, "score": 0.6526054, "source": "dense+rrf", "rrf_score": 0.03131881575727918, "rerank_score": 0.3326106817029708, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 16, "text_snippet": "lock-sparse FlashAttention , a sparse attention algorithm that is 2-4 \u0002faster than evenFlashAttention , scaling up to sequence length of 64k. We prove that block-sparse FlashAttention has better IO complexity than FlashAttention by a factor"}, {"rank": 2, "score": 0.6619555, "source": "dense+rrf", "rrf_score": 0.03149801587301587, "rerank_score": 0.29638575299817205, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 17, "text_snippet": "to make it easier to build on this primitive.1 We empirically validate that FlashAttention speeds up model training and improves model quality by modeling longer context. We also benchmark the runtime and memory footprint of FlashAttention "}, {"rank": 3, "score": 0.6268672, "source": "dense+rrf", "rrf_score": 0.02946236559139785, "rerank_score": 0.25152777256992126, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 100, "text_snippet": "n, Atri Rudra, and Christopher R√©. 2022. FlashAttention: Fast and memory-efficient exact attention with IO- awareness. ArXiv:2205.14135. Hermann Ebbinghaus. 1913. Memory: A contribu- tion to experimental psychology. H. A. Ruger & C. E. Buss"}, {"rank": 4, "score": 0.6588968, "source": "dense+rrf", "rrf_score": 0.030309988518943745, "rerank_score": 0.24077465339623821, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 7, "text_snippet": " with wall-clock speed) and tend to ignore overheads from memory access (IO). In this paper, we argue that a missing principle is making attention algorithms IO-aware [1]‚Äîthat is, carefully accounting for reads and writes to diÔ¨Äerent levels"}, {"rank": 5, "score": 44.40141657645509, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.23518046435118237, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 88, "text_snippet": "ng, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821 ,"}]}
{"case_index": 61, "query": "From 2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt, what exact term fills [BLANK] in: \"So the answer is no.Q: The concert was [BLANK] to be on 06/01/1943, but was delayed by one day to today.\"?", "gold": "scheduled", "answer": "The exact term that fills **[BLANK]** is **‚Äúscheduled‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Model answer matches expected term 'scheduled' and is directly supported by the provided context sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 102.904, "llm_ms": 1065.289, "top_contexts": [{"rank": 1, "score": 0.54356444, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3734629923672459, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 277, "text_snippet": " days later. Sotodayis01/07/2019. So the answer is 01/07/2019. Q:The concert was scheduled to be on 06/01/1943, but was delayed by one day to today. What is the date 10 days ago in MM/DD/YYYY? A:One dayafter06/01/1943 is06/02/1943, sotodayi"}, {"rank": 2, "score": 0.49542928, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.35611997975245113, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 25, "text_snippet": " populated areas (c) desert (d) apartment (e) roadblock A: The answer must be a place with a lot of people. Race tracks, desert, apartments, and roadblocks don't have a lot of people, but populated areas do. So the answer is (b). Q: Yes or "}, {"rank": 3, "score": 0.3983619, "source": "dense+rrf", "rrf_score": 0.03036576949620428, "rerank_score": 0.3114898401276077, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 26, "text_snippet": "delayed by one day to today. What is the date 10 days ago in MM/DD/YYYY? A: One day after 06/01/1943 is 06/02/1943, so today is 06/02/1943. 10 days before today is 05/23/1943. So the answer is 05/23/1943. Q: Is the following sentence plausi"}, {"rank": 4, "score": 33.47057210189794, "source": "dense+rrf", "rrf_score": 0.0136986301369863, "rerank_score": 0.2708013455424428, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 139, "text_snippet": "Model OutputQuestion Question QuestionFigure 10: Examples of semantic understanding and one-step missing errors that were Ô¨Åxed by scaling PaLM from 62B to 540B. A.2 What is the role of prompt engineering? One of the key considerations of pr"}, {"rank": 5, "score": 0.42342788, "source": "dense+rrf", "rrf_score": 0.029437229437229435, "rerank_score": 0.25414060222111, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 246, "text_snippet": "w (Tue, 7/9/1972). What is the date one week ago from today in MM/DD/YYYY? MODEL ANSWER (INCORRECT): Tomorrow is 7/9/1972. One week ago from today is 7/2/1972. So the answer is 7/2/1972. \u0017(Ground truth is 07/01/1972.) QUESTION :Today is the"}]}
{"case_index": 62, "query": "From 2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt, what exact term fills [BLANK] in: \"operates on long retrieval units, with only a few ([BLANK] fewer than 10) being fed into the reader.\"?", "gold": "typically", "answer": "The exact term that fills **[BLANK]** is **‚Äútypically‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'typically' and is directly supported by Context 1/2 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 85.024, "llm_ms": 1324.403, "top_contexts": [{"rank": 1, "score": 0.68737274, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.3836281116090204, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 21, "text_snippet": "operates on long retrieval units, with only a few (typically fewer than 10) being fed into the reader. An illustrative example is shown in Figure 2. 2.1 Long Retriever The traditional RAG framework employs smaller retrieval units and priori"}, {"rank": 2, "score": 0.6485034, "source": "dense+rrf", "rrf_score": 0.03057889822595705, "rerank_score": 0.26665135855675715, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 20, "text_snippet": "ontext methods evolve, the performance of LongRAG will continue to improve. Therefore, we believe the modern RAG systems should re-consider the granularity of their retrieval units to exploit the advantages of the current long-context LLMs."}, {"rank": 3, "score": 0.67574155, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.25507902155279155, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 7, "text_snippet": "assive corpus with up to tens of millions of information units). Subsequently, the retrieved units are passed to the reader to generate the final response. On the contrary, the reader only needs to extract answers from 1arXiv:2406.15319v3 ["}, {"rank": 4, "score": 40.047803872884266, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.24683371833797169, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 27, "text_snippet": "stralian Football League‚Ä¶  Reader  Long  Reader Figure 2: LongRAG example. We form the long retrieval unit which is at least 4K tokens by using the entire document or grouping related documents, depending on the orinal docuemnt size. In thi"}, {"rank": 5, "score": 0.6186394, "source": "dense+rrf", "rrf_score": 0.028309409888357256, "rerank_score": 0.23938450096492345, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 8, "text_snippet": "ever needs to scan over a massive amount of units to find the relevant piece. In contrast, LongRAG operates on long retrieval units (30x longer). The retriever of LongRAG has a significantly reduced workload, achieving strong retrieval qual"}]}
{"case_index": 63, "query": "From 2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt, what exact term fills [BLANK] in: \"Like Orca 1, we utilize more capable LLMs to [BLANK] various reasoning strategies across various tasks.\"?", "gold": "demonstrate", "answer": "The exact term is **‚Äúdemonstrate‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 48.933, "llm_ms": 1204.101, "top_contexts": [{"rank": 1, "score": 0.705783, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.29847116591773865, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 16, "text_snippet": "owing them to perform at their best, irrespective of their size. Like Orca 1, we utilize more capable LLMs to demonstrate various reasoning strategies across various tasks. However, in Orca 2, the reasoning strategies are carefully tailored"}, {"rank": 2, "score": 0.69556665, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.2823536395171741, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 1, "text_snippet": "n benchmarks like BigBench Hard and AGIEval. In Orca 2, we continue exploring how improved training signals can enhance smaller LMs‚Äô reasoning abilities. Research on training small LMs has often relied on imitation learning to replicate the"}, {"rank": 3, "score": 0.6669314, "source": "dense+rrf", "rrf_score": 0.031024531024531024, "rerank_score": 0.25014473765155887, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 0, "text_snippet": "Orca 2: Teaching Small Language Models How to Reason Arindam Mitra, Luciano Del Corro‚Ä†, Shweti Mahajan‚Ä†, Andres Codas‚Ä° Clarisse Simoes‚Ä°, Sahaj Agarwal, Xuxi Chen‚àó, Anastasia Razdaibiedina‚àó Erik Jones‚àó, Kriti Aggarwal‚àó, Hamid Palangi, Guoqin"}, {"rank": 4, "score": 26.71705569960646, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.19179165225462685, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 148, "text_snippet": "synthetic data for post-training. While Orca 2 models have not gone through RLHF training for safety, we believe that the use of synthetic data for post-training that has been filtered with various content safety filters could provide anoth"}, {"rank": 5, "score": 0.67935026, "source": "dense+rrf", "rrf_score": 0.029910714285714284, "rerank_score": 0.19147770116120588, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 2, "text_snippet": "asks, potentially different from the one used by the larger model. For example, while larger models might provide a direct answer to a complex task, smaller models may not have the same capacity. In Orca 2, we teach the model various reason"}]}
{"case_index": 64, "query": "From 2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt, what exact term fills [BLANK] in: \"1arXiv:2309.12307v3 [cs.CL] 8 Mar 2024 Published as a conference paper at ICLR 2024 !Trainable ‚ùÑ[BLANK](-headSelf-A1en(onFeed ForwardNorminput++Lora !\"?", "gold": "frozennormpostmul", "answer": "The exact term that fills **[BLANK]** is:\n\n**FrozenNormpostMul**", "answer_score": 0.3, "answer_correctness": 0.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Expected exact term is 'frozennormpostmul' (lowercase). Model answered 'FrozenNormpostMul' with different casing, so not exact. Context explicitly shows '‚ùÑFrozenNormpostMul', so answer is grounded.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 47.792, "llm_ms": 1446.967, "top_contexts": [{"rank": 1, "score": 0.6994326, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3914786463519279, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 6, "text_snippet": "luated on the proof-pile (Azerbayev et al., 2022) test set in perplexity. 1 I NTRODUCTION Large language models (LLMs) are typically trained with a pre-defined context size, such as 2048 tokens for LLaMA (Touvron et al., 2023a) and 4096 tok"}, {"rank": 2, "score": 0.6465622, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.3193386613484143, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 0, "text_snippet": "Published as a conference paper at ICLR 2024 LONG LORA: E FFICIENT FINE-TUNING OF LONG - CONTEXT LARGE LANGUAGE MODELS Yukang Chen1Shengju Qian1Haotian Tang2Xin Lai1 Zhijian Liu2Song Han2,3Jiaya Jia1 1CUHK2MIT3NVIDIA ABSTRACT We present Lon"}, {"rank": 3, "score": 0.60508436, "source": "dense+rrf", "rrf_score": 0.0293236301369863, "rerank_score": 0.25288081690517766, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 44, "text_snippet": "ters and memory cost, compared to full fine-tuning. However, adapting LLMs from short context length to long is not easy. We empirically observe an obvious gap between LoRA and full fine-tuning. As shown in Table 2, the gap between LoRA and"}, {"rank": 4, "score": 47.26905316694357, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.24860339960575698, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 7, "text_snippet": "circumstances or facing information uncertainties. Recent results have hinted at the possibility of combining verbal reasoning with interactive decision making in autonomous systems. On one hand, properly prompted large language models (LLM"}, {"rank": 5, "score": 0.615745, "source": "dense+rrf", "rrf_score": 0.030090497737556562, "rerank_score": 0.23956778279596364, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 68, "text_snippet": "perplexity and fine-tuning steps for a Llama2 7B model extending to the 8192 context length on the PG19 validation set, in 8  Published as a conference paper at ICLR 2024 Table 6: Comparisons among S2-Attn and alternative attention patterns"}]}
{"case_index": 65, "query": "From 2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt, what exact term fills [BLANK] in: \"progress on aligning language models by training them to act in [BLANK] with the user‚Äôs intention (Leike et al., 2018).\"?", "gold": "accordance", "answer": "The exact term is **‚Äúaccordance‚Äù** (as in: ‚Äúact in **accordance** with the user‚Äôs intention‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'accordance' and is directly supported by the provided context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 112.201, "llm_ms": 1944.098, "top_contexts": [{"rank": 1, "score": 0.75045705, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3779698352088929, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 8, "text_snippet": "progress on aligning language models by training them to act in accordance with the user‚Äôs intention (Leike et al., 2018). This encompasses both explicit intentions such as following instructions and implicit intentions such as staying trut"}, {"rank": 2, "score": 0.63324386, "source": "dense+rrf", "rrf_score": 0.031054405392392875, "rerank_score": 0.3506881839939666, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 7, "text_snippet": "the next token on a webpage from the internet‚Äîis different from the objective ‚Äúfollow the user‚Äôs instructions helpfully and safely‚Äù (Radford et al., 2019; Brown et al., 2020; Fedus et al., 2021; Rae et al., 2021; Thoppilan et al., 2022). Th"}, {"rank": 3, "score": 0.6767722, "source": "dense+rrf", "rrf_score": 0.031024531024531024, "rerank_score": 0.2706428827675916, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 75, "text_snippet": " models that act in accordance with user intentions. More practically, for the purpose of our language tasks, we use a framework similar to Askell et al. (2021), who deÔ¨Åne models to be aligned if they are helpful, honest, and harmless. To b"}, {"rank": 4, "score": 0.5835104, "source": "dense+rrf", "rrf_score": 0.027692895339954164, "rerank_score": 0.26074222510345346, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 9, "text_snippet": "fabricate information or mislead the user), and harmless (they should not cause physical, psychological, or social harm to people or the environment). We elaborate on the evaluation of these criteria in Section 3.6. We focus on Ô¨Åne-tuning a"}, {"rank": 5, "score": 0.58520305, "source": "dense+rrf", "rrf_score": 0.027242401779095628, "rerank_score": 0.25937475263042337, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 168, "text_snippet": "e models by retrieving from trillions of tokens,‚Äù inInternational conference on machine learning . PMLR, 2022, pp. 2206‚Äì2240. [6] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray et a"}]}
{"case_index": 66, "query": "From 1706.03762__attention_is_all_you_need.txt, what exact term fills [BLANK] in: \"ransformer follows this overall architecture using stacked [BLANK] and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.\"?", "gold": "self-attention", "answer": "The exact term is **‚Äúself-attention‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected blank ('self-attention') and is directly supported by Context 1/5 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 78.777, "llm_ms": 1469.17, "top_contexts": [{"rank": 1, "score": 0.7662364, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.48231095130230645, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 16, "text_snippet": "ransformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively. 3.1 Encoder and Decoder Stacks "}, {"rank": 2, "score": 0.5416787, "source": "dense+rrf", "rrf_score": 0.028205128205128206, "rerank_score": 0.2287430321138958, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 114, "text_snippet": "  E1 E2 EN...Figure 3: Differences in pre-training model architectures. BERT uses a bidirectional Transformer. OpenAI GPT uses a left-to-right Transformer. ELMo uses the concatenation of independently trained left-to-right and right-to- lef"}, {"rank": 3, "score": 0.60020816, "source": "dense+rrf", "rrf_score": 0.03125763125763126, "rerank_score": 0.2248947928955705, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 30, "text_snippet": "rd Networks In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transf"}, {"rank": 4, "score": 29.587796706374135, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.21998498296430108, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 38, "text_snippet": "t token of chunk ùê∂ùë¢is the Ô¨Årst to be able to access the retrieved content ùê∏ùë¢while maintaining autoregressivity in(1). Hence, there is a one token overlap between chunk ùê∂ùë¢=\u0010 ùë•¬πùë¢\u00001¬∫ùëö¬∏ùëñ\u0011 ùëñ2¬ª1¬îùëö¬ºand the corresponding attending chunk ùê∂¬∏ ùë¢,¬πùë•ùë¢ùëö¬∏ùëñ"}, {"rank": 5, "score": 0.6017299, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.21966925325692782, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 15, "text_snippet": ",2,35]. Here, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence of continuous representations z= (z1, ..., z n). Given z, the decoder then generates an output sequence (y1, ..., y m)of symbols one elem"}]}
{"case_index": 67, "query": "From 2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt, what exact term fills [BLANK] in: \"HU\u0003RQ\u0003D\u0003GUDZHU\u0011 \u0015 \u0003$OI:RUOG \u0014 \u0003+RWVSRW\u00034$ Figure 1: (1) Comparison of 4 prompting methods, (a) Standard , (b) [BLANK] ( CoT, Reason Only), (c) Act-only, and (d) ReAct (Reason+Act), solving a HotpotQA (Yang et al., 2018) question.\"?", "gold": "chain-of-thought", "answer": "The exact term that fills **[BLANK]** is:\n\n**Chain-of-thought**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context explicitly shows '(b) Chain-of-thought (CoT, Reason Only)'. Model answer matches expected term.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 146.358, "llm_ms": 1593.439, "top_contexts": [{"rank": 1, "score": 0.81671864, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.5021753563530851, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 15, "text_snippet": "HU\u0003RQ\u0003D\u0003GUDZHU\u0011\u000b\u0015\f\u0003$OI:RUOG\u000b\u0014\f\u0003+RWVSRW\u00034$ Figure 1: (1) Comparison of 4 prompting methods, (a) Standard , (b) Chain-of-thought ( CoT, Reason Only), (c) Act-only, and (d) ReAct (Reason+Act), solving a HotpotQA (Yang et al., 2018) question. ("}, {"rank": 2, "score": 0.7442149, "source": "dense+rrf", "rrf_score": 0.030834914611005692, "rerank_score": 0.23458307921026106, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 44, "text_snippet": "lines (with formats as Figure 1(1a-1c)): (a) Standard prompting (Standard ), which removes all thoughts, actions, observations in ReAct trajectories. (b) Chain-of-thought prompting (CoT) (Wei et al., 2022), which removes actions and observa"}, {"rank": 3, "score": 35.21590512308687, "source": "dense+rrf", "rrf_score": 0.014084507042253521, "rerank_score": 0.2222719881962832, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 36, "text_snippet": "easoning tasks like multi-hop question answering and fact veriÔ¨Åcation. As shown in Figure 1(1d), by interacting with a Wikipedia API, ReAct is able to retrieve information to support reasoning, while also use reasoning to target what to ret"}, {"rank": 4, "score": 43.31309954659238, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.21923174537592519, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 20, "text_snippet": "pertaining to a task in an interleaved manner, which allows the model to perform dynamic reasoning to create, maintain, and adjust high-level plans for acting (reason to act), while also interact with the external environments (e.g. Wikiped"}, {"rank": 5, "score": 0.7122675, "source": "dense+rrf", "rrf_score": 0.031746031746031744, "rerank_score": 0.21607085450568111, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 57, "text_snippet": "rent number of samples, reaching CoT-SC performance with 21 samples using merely 3-5 samples. These results indicate the value of properly combining model internal knowledge and external knowledge for reasoning tasks. ReAct performs best fo"}]}
{"case_index": 68, "query": "From 2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt, what exact term fills [BLANK] in: \"The model is already [BLANK], and the chat template is as follows: <|user|> /n Question <|end|> /n <|assistant|> Thephi-3-small model (7B parameters) leverages the tiktoken tokenizer (for better multilingual tokenization) with a vocabulary size of 1003522and has default context length 8192.\"?", "gold": "chat-finetuned", "answer": "The exact term is **‚Äúchat-finetuned‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'chat-finetuned' and is explicitly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 66.845, "llm_ms": 852.735, "top_contexts": [{"rank": 1, "score": 0.75695896, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.5138585670285541, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 10, "text_snippet": "ectly adapted to phi-3-mini . The model uses 3072 hidden dimension, 32 heads and 32 layers. We trained using bfloat16 for a total of 3.3T tokens. The model is already chat-finetuned, and the chat template is as follows: <|user|> /n Question"}, {"rank": 2, "score": 0.6405668, "source": "dense+rrf", "rrf_score": 0.0315136476426799, "rerank_score": 0.2781559314474321, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 16, "text_snippet": "3. The table shows the Keys/values a query token in block 8 attended to. Blue=local blocks, orange=remote/vertical blocks, gray=blocks skipped. total parameters. Additionally, we utilize the SparseMixer approach [LGC23, LDL+23] for training"}, {"rank": 3, "score": 0.6421577, "source": "dense+rrf", "rrf_score": 0.03125, "rerank_score": 0.26319562193502516, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 6, "text_snippet": "7B parameters), matched the performance of models 25 times larger trained on regular data. In this report we present a new model, phi-3-mini (3.8B parameters), trained for 3.3T tokens on larger and more advanced versions of the datasets use"}, {"rank": 4, "score": 0.6438766, "source": "dense+rrf", "rrf_score": 0.03125763125763126, "rerank_score": 0.25045967238255484, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 8, "text_snippet": "ved solely by changing the training data. phi-3-mini: The development of a compact language model that rivals the capabilities of ChatGPT, while fitting on a phone, is a testament to the power of data-driven machine learning. By meticulousl"}, {"rank": 5, "score": 0.6041255, "source": "dense+rrf", "rrf_score": 0.031024531024531024, "rerank_score": 0.24683898921219777, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 0, "text_snippet": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone Microsoft Abstract We introduce phi-3-mini , a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both"}]}
{"case_index": 69, "query": "From 2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt, what exact term fills [BLANK] in: \"[BLANK] and closed-domain QA), InstructGPT models make up information not present in the input about half as often as GPT-3 (a 21% vs.\"?", "gold": "summarization", "answer": "The exact term is **‚Äúsummarization‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context explicitly says '(e.g. summarization and closed-domain QA)', matching expected term.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 85.918, "llm_ms": 1372.124, "top_contexts": [{"rank": 1, "score": 0.7199644, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.39847573589448076, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 17, "text_snippet": "nformative answers about twice as often as GPT-3. Our results are equally strong on the subset of questions that were not adversarially selected against GPT-3. On ‚Äúclosed-domain‚Äù tasks from our API prompt distribution, where the output shou"}, {"rank": 2, "score": 0.5894223, "source": "dense+rrf", "rrf_score": 0.029857397504456328, "rerank_score": 0.2539436474521124, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 16, "text_snippet": "owing instructions. Outputs from our 175B InstructGPT are preferred to 175B GPT-3 outputs 85 ¬±3% of the time, and preferred 71 ¬±4% of the time to few-shot 175B GPT-3. InstructGPT models also generate more appropriate outputs according to ou"}, {"rank": 3, "score": 29.778572446759597, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.2501947474421853, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 9, "text_snippet": "fabricate information or mislead the user), and harmless (they should not cause physical, psychological, or social harm to people or the environment). We elaborate on the evaluation of these criteria in Section 3.6. We focus on Ô¨Åne-tuning a"}, {"rank": 4, "score": 0.5988213, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.24549733477009972, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 89, "text_snippet": "e concrete axes. SpeciÔ¨Åcally, compared to GPT-3, InstructGPT outputs are more appropriate in the context of a customer assistant, more often follow explicit constraints deÔ¨Åned in the instruction (e.g. ‚ÄúWrite your answer in 2 paragraphs or l"}, {"rank": 5, "score": 0.5855695, "source": "dense+rrf", "rrf_score": 0.02900988017658188, "rerank_score": 0.21988258812939274, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 115, "text_snippet": " compared to GPT-3 175B with no additional preÔ¨Åxing. Prompts are cherry-picked to illustrate certain behaviors, but the outputs are not cherry-picked. (1) InstructGPT can follow instructions in other languages, though it sometimes generates"}]}
{"case_index": 70, "query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"For each position i‚ààI, we then obtain up to m API callsc1 i,...,cm iby [BLANK] from Mgiven the sequence [P(x),x1,...,x i‚àí1,<API> ]as a p\"?", "gold": "sampling", "answer": "The term is **‚Äúsampling‚Äù** (i.e., ‚Äúby **sampling** from *M* given the sequence ‚Ä¶‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'sampling' and is directly supported by the provided context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 68.445, "llm_ms": 1305.964, "top_contexts": [{"rank": 1, "score": 0.6837631, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.38137440558182756, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 21, "text_snippet": "n+1as a continuation for the sequence z1,...,z n. We Ô¨Årst sample up to kcandidate posi- tions for doing API calls by computing, for each i‚àà{1,...,n}, the probability pi=pM(<API>|P(x),x1:i‚àí1) thatMassigns to starting an API call at position "}, {"rank": 2, "score": 0.59102976, "source": "dense+rrf", "rrf_score": 0.03007688828584351, "rerank_score": 0.2779609939717252, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 27, "text_snippet": "atterns in the pretraining corpus, thus hurting perplexity.  x1:i‚àí1,e(ci,ri),xi:n; we proceed analogously for texts with multiple API calls. Doing this for all x‚àà Cresults in the new dataset C‚àóaugmented with API calls. We use this new datas"}, {"rank": 3, "score": 0.65738136, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.25676307200620674, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 22, "text_snippet": "mpling from Mgiven the sequence [P(x),x1,...,x i‚àí1,<API> ]as a preÔ¨Åx and</API> as an end-of-sequence token.2 2We discard all examples where Mdoes not generate the </API> token.Executing API Calls As a next step, we execute all API calls gen"}, {"rank": 4, "score": 0.5570811, "source": "dense+rrf", "rrf_score": 0.029273504273504274, "rerank_score": 0.2447837865975948, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 1, "text_snippet": "metic or fac- tual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer , a model traine"}, {"rank": 5, "score": 0.5769471, "source": "dense+rrf", "rrf_score": 0.029211087420042643, "rerank_score": 0.23145555613888785, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 12, "text_snippet": " how an API can be used, we let a LM annotate a huge language modeling dataset with potential API calls. We then use a self-supervised loss to determine which of these API calls actually help the model in predicting future tokens. Finally, "}]}
{"case_index": 71, "query": "From 2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt, what exact term fills [BLANK] in: \"rformance across tasks in language [BLANK] and interactive decision making, their abilities for reasoning (e.g.\"?", "gold": "understanding", "answer": "The exact term is **‚Äúunderstanding‚Äù** (i.e., ‚Äúperformance across tasks in language **understanding** and interactive decision making, their abilities for reasoning ‚Ä¶‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context 1 explicitly contains the phrase 'language understanding and interactive decision making', matching the expected blank.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 52.724, "llm_ms": 1392.147, "top_contexts": [{"rank": 1, "score": 0.66643083, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.3088760733835777, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 1, "text_snippet": "rformance across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In th"}, {"rank": 2, "score": 0.614823, "source": "dense+rrf", "rrf_score": 0.030621785881252923, "rerank_score": 0.24990411726426248, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 88, "text_snippet": " the development of versatile and generalist agents like Reed et al. (2022). 6 C ONCLUSION We have proposed ReAct ‚Äì a simple yet effective method for synergizing reasoning and acting in large language models. Through a diverse set of experi"}, {"rank": 3, "score": 0.63225424, "source": "dense+rrf", "rrf_score": 0.03055037313432836, "rerank_score": 0.24433007477988303, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 82, "text_snippet": "actions and their corresponding observations into a coherent stream of inputs for the model to reason more accurately and tackle tasks beyond reasoning (e.g. interactive decision making). Language model for decision making The strong capabi"}, {"rank": 4, "score": 0.66065866, "source": "dense+rrf", "rrf_score": 0.03125763125763126, "rerank_score": 0.22652704295309464, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 19, "text_snippet": "udies on how reasoning and acting can be combined in a synergistic manner for general task solving, and if such a combination can bring systematic beneÔ¨Åts compared to reasoning or acting alone. In this work, we present ReAct , a general par"}, {"rank": 5, "score": 0.59692746, "source": "dense+rrf", "rrf_score": 0.02761904761904762, "rerank_score": 0.20235094204585383, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 75, "text_snippet": "mbined reasoning and action using an LLM applied to an interactive environment within a closed-loop system. Perhaps the closest prior work is Inner Monologue (IM), from Huang et al. (2022b), in which actions from an embodied agent are motiv"}]}
{"case_index": 72, "query": "From 2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt, what exact term fills [BLANK] in: \"Le Denny Zhou Google Research, Brain Team {jasonwei,dennyzhou}@google.com Abstract We explore how generating a chain of thought ‚Äîa series of [BLANK] reasoning steps‚ÄîsigniÔ¨Åcantly improves the ability of large language models to perform complex reasoning.\"?", "gold": "intermediate", "answer": "The exact term is **‚Äúintermediate‚Äù**, as in: ‚Äúa series of **intermediate** reasoning steps.‚Äù", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'intermediate' and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 65.86, "llm_ms": 1506.278, "top_contexts": [{"rank": 1, "score": 0.8192335, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.5474561000754208, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 0, "text_snippet": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models Jason Wei Xuezhi Wang Dale Schuurmans Maarten Bosma Brian Ichter Fei Xia Ed H. Chi Quoc V . Le Denny Zhou Google Research, Brain Team {jasonwei,dennyzhou}@google.com Abst"}, {"rank": 2, "score": 0.6987939, "source": "dense+rrf", "rrf_score": 0.030017921146953404, "rerank_score": 0.36660273291355094, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 0, "text_snippet": "Published as a conference paper at ICLR 2023 SELF-CONSISTENCY IMPROVES CHAIN OF THOUGHT REASONING IN LANGUAGE MODELS Xuezhi Wang‚Ä†‚Ä°Jason Wei‚Ä†Dale Schuurmans‚Ä†Quoc Le‚Ä†Ed H. Chi‚Ä† Sharan Narang‚Ä†Aakanksha Chowdhery‚Ä†Denny Zhou‚Ä†¬ß ‚Ä†Google Research, "}, {"rank": 3, "score": 0.6931881, "source": "dense+rrf", "rrf_score": 0.029571646010002173, "rerank_score": 0.25432926669088396, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 120, "text_snippet": "Zhou. 2022. Self- consistency improves chain of thought reasoning in language models. CoRR , abs/2203.11171. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicit"}, {"rank": 4, "score": 50.93037641964235, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.2420045685118631, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 133, "text_snippet": " Proceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence , 32, Apr. 2018. URL https: //ojs.aaai.org/index.php/AAAI/article/view/12340 . Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and"}, {"rank": 5, "score": 53.249341511860784, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.24163891759997308, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 114, "text_snippet": "-augmented ensembles in language models. arXiv preprint arXiv:2207.00747 , 2022b. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models"}]}
{"case_index": 73, "query": "From 2305.06983__active_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"t al., 2017), where the informa- tion needs are clear in the user‚Äôs input, and it is [BLANK] to retrieve relevant knowledge once solely based on the input .\"?", "gold": "sufficient", "answer": "The exact term that fills **[BLANK]** is **‚Äúsufficient‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'sufficient' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 94.029, "llm_ms": 1210.403, "top_contexts": [{"rank": 1, "score": 0.57104063, "source": "dense+rrf", "rrf_score": 0.02938045560996381, "rerank_score": 0.4082979094147516, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 7, "text_snippet": "t al., 2017), where the informa- tion needs are clear in the user‚Äôs input, and it is sufficient to retrieve relevant knowledge once solely based on the input . Increasingly powerful large LMs have also demonstrated abilities in more complex"}, {"rank": 2, "score": 0.6049388, "source": "dense+rrf", "rrf_score": 0.031746031746031744, "rerank_score": 0.32736454470178844, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 5, "text_snippet": "u et al., 2021). Augmenting LMs with retrieval components that look up relevant in- formation from external knowledge resources is a promising direction to address hallucination (Khan- delwal et al., 2020; Izacard et al., 2022). Retrieval a"}, {"rank": 3, "score": 52.19571862912831, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.3114465066000555, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 22, "text_snippet": " generate the complete answer at once y=LM([Dx,x]). 2.3 Active Retrieval Augmented Generation To aid long-form generation with retrieval, we pro- pose active retrieval augmented generation. It is a generic framework that actively decides wh"}, {"rank": 4, "score": 0.65025014, "source": "dense+rrf", "rrf_score": 0.03177805800756621, "rerank_score": 0.2659098854107281, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 1, "text_snippet": "create factually inaccurate out- put. Augmenting LMs by retrieving informa- tion from external knowledge resources is one promising solution. Most existing retrieval aug- mented LMs employ a retrieve-and-generate setup that only retrieves i"}, {"rank": 5, "score": 0.5757001, "source": "dense+rrf", "rrf_score": 0.028258706467661692, "rerank_score": 0.26578473215076276, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 24, "text_snippet": "t, and the input to LMs is the concatena- tion of the retrieved documents Dqt, the user input x, and the previous generation y<t. We discard previously retrieved documents ‚à™t‚Ä≤<tDqt‚Ä≤and only use the retrieved documents from the current step "}]}
{"case_index": 74, "query": "From 2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt, what exact term fills [BLANK] in: \"First, a language model is prompted with a set of manually written [BLANK] exemplars (Wei et al., 2022).\"?", "gold": "chain-of-thought", "answer": "The exact term is **‚Äúchain-of-thought‚Äù** (as in ‚Äúmanually written **chain-of-thought exemplars**‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'chain-of-thought' and is directly supported by Context 1 phrase 'manually written chain-of-thought exemplars'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 80.4, "llm_ms": 1974.366, "top_contexts": [{"rank": 1, "score": 0.6496188, "source": "dense+rrf", "rrf_score": 0.03177805800756621, "rerank_score": 0.3725457781751564, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 18, "text_snippet": "., in Output 2), but such solutions are less likely to arrive at the same answer. That is, we hypothesize that correct reasoning processes, even if they are diverse, tend to have greater agreement in their Ô¨Ånal answer than incorrect process"}, {"rank": 2, "score": 38.76968870963002, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.3092746782728582, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 40, "text_snippet": " 4  Published as a conference paper at ICLR 2023 3.2 M AINRESULTS We report the results of self-consistency averaged over 10 runs, where we sampled 40 outputs independently from the decoder in each run. The baseline we compare to is chain-o"}, {"rank": 3, "score": 47.79768024744302, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.2924054020147941, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 109, "text_snippet": ", Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022. LaMDA: Language models for dialog applications. arXiv preprint arXiv:2201.08239 . Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and"}, {"rank": 4, "score": 0.63151175, "source": "dense+rrf", "rrf_score": 0.030117753623188408, "rerank_score": 0.2851688138441724, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 5, "text_snippet": "odel performance across a variety of multi-step reasoning tasks (Wei et al., 2022). In this paper, we introduce a novel decoding strategy called self-consistency to replace the greedy decoding strategy used in chain-of-thought prompting (We"}, {"rank": 5, "score": 0.6362821, "source": "dense+rrf", "rrf_score": 0.02862400327131466, "rerank_score": 0.2776805661138588, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 79, "text_snippet": "pecialized approaches for improving reasoning (Andor et al., 2019; Ran et al., 2019; Geva et al., 2020; PiÀõ ekos et al., 2021). Compared to prior work, self-consistency is applicable to a wide range of reasoning tasks without any additional"}]}
{"case_index": 75, "query": "From 2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt, what exact term fills [BLANK] in: \"Token likelihoods are provided by a model, [BLANK] by ùúÉ, that takes as input both previous tokens and their retrieved neighbours.\"?", "gold": "parameterized", "answer": "The exact term is **‚Äúparameterized‚Äù** (as in: ‚Äúa model, **parameterized** by ùúÉ, ‚Ä¶‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches the expected term 'parameterized' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 83.469, "llm_ms": 1778.122, "top_contexts": [{"rank": 1, "score": 0.6935352, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3667044058484411, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 23, "text_snippet": "brevity) is a non-trainable operator speciÔ¨Åed in ¬ß2.3. Token likelihoods are provided by a model, parameterized by ùúÉ, that takes as input both previous tokens and their retrieved neighbours. This deÔ¨Ånes the following retrieval-enhanced sequ"}, {"rank": 2, "score": 0.5660028, "source": "dense+rrf", "rrf_score": 0.031054405392392875, "rerank_score": 0.27047972556338, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 30, "text_snippet": " News 236,918 398 No 10% Wikipedia 13,288 23 Yes 5% GitHub 374,952 143 No 5% 4  Improving language models by retrieving from trillions of tokens 2.4.R/e.sc/t.sc/r.sc/o.scmodel architecture Our model relies on an encoder-decoder transformer "}, {"rank": 3, "score": 0.5473373, "source": "dense+rrf", "rrf_score": 0.0288981288981289, "rerank_score": 0.2630275034933835, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 242, "text_snippet": "37  Improving language models by retrieving from trillions of tokens Table 16jGreat Circle (novel) , from Wikipedia September 21. The article is about a recent novel and chunks ùê∂3andùê∂4are speciÔ¨Åcally about its reception. The name Publishers"}, {"rank": 4, "score": 30.337066973273522, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.2596606537947589, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 38, "text_snippet": "t token of chunk ùê∂ùë¢is the Ô¨Årst to be able to access the retrieved content ùê∏ùë¢while maintaining autoregressivity in(1). Hence, there is a one token overlap between chunk ùê∂ùë¢=\u0010 ùë•¬πùë¢\u00001¬∫ùëö¬∏ùëñ\u0011 ùëñ2¬ª1¬îùëö¬ºand the corresponding attending chunk ùê∂¬∏ ùë¢,¬πùë•ùë¢ùëö¬∏ùëñ"}, {"rank": 5, "score": 0.5465392, "source": "dense+rrf", "rrf_score": 0.027242401779095628, "rerank_score": 0.2555132775417132, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 210, "text_snippet": "valuating R/e.sc/t.sc/r.sc/o.scmodels on neighbours only and their continuation only aÔ¨Äects performance. Overall, attending to neighbours only provides 22%of the performance improvement due to retrieval in R/e.sc/t.sc/r.sc/o.sc, while atten"}]}
{"case_index": 76, "query": "From 2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt, what exact term fills [BLANK] in: \"The more that [BLANK] thinking and analysis is required for a problem (Evans, 2010), the greater the diversity of reasoning paths that can recover the answer.\"?", "gold": "deliberate", "answer": "The exact term is **‚Äúdeliberate‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'deliberate' and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 80.874, "llm_ms": 1237.77, "top_contexts": [{"rank": 1, "score": 0.7226412, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.474796760592195, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 6, "text_snippet": "aths that reach a correct answer (Stanovich & West, 2000). The more that deliberate thinking and analysis is required for a problem (Evans, 2010), the greater the diversity of reasoning paths that can recover the answer. Figure 1 illustrate"}, {"rank": 2, "score": 35.59489310440976, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.28037374097235007, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 87, "text_snippet": "rved that it signiÔ¨Åcantly improves accuracy in a range of arithmetic and commonsense reasoning tasks, across four large language models with varying scales. Beyond accuracy gains, self-consistency is also useful for collecting rationales wh"}, {"rank": 3, "score": 32.23987233373653, "source": "dense+rrf", "rrf_score": 0.014285714285714285, "rerank_score": 0.27788757011700255, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 11, "text_snippet": "mainder for $2 per egg, so in  total she sells 7 * $2 = $14 per day.  The answer is $14. The answer is $14. Greedy decode Figure 1: The self-consistency method contains three steps: (1) prompt a language model using chain-of-thought (CoT) p"}, {"rank": 4, "score": 39.21662143548177, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.2756794696493724, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 144, "text_snippet": "ed reasoning paths from the PaLM-540B model in Table 13. We see that the diversity in the additionally sampled reasoning paths indeed helps the model arrive at a more correct Ô¨Ånal answer after aggregation. A.1.2 R OBUSTNESS TO DIFFERENT SET"}, {"rank": 5, "score": 39.64072026199778, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.2667696127489417, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 77, "text_snippet": "much shorter and less opportunity remains for generating diversity in the decoding process. In addition, we tested self-consistency with zero-shot chain-of-thought (Kojima et al., 2022) and show that self-consistency works for zero-shot CoT"}]}
{"case_index": 77, "query": "From 2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt, what exact term fills [BLANK] in: \"Naive RAG The Naive RAG research paradigm represents the earli- est [BLANK], which gained prominence shortly after the 3 Fig.\"?", "gold": "methodology", "answer": "The exact term that fills **[BLANK]** is **‚Äúmethodology‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context 1 explicitly states: 'The Naive RAG research paradigm represents the earliest methodology'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 101.096, "llm_ms": 2199.297, "top_contexts": [{"rank": 1, "score": 80.30720177364567, "source": "dense+rrf", "rrf_score": 0.01639344262295082, "rerank_score": 0.3149561775141749, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 18, "text_snippet": "g, and we categorize it into three stages: Naive RAG, Advanced RAG, and Modular RAG, as showed in Figure 3. Despite RAG method are cost-effective and surpass the performance of the native LLM, they also exhibit several limitations. The deve"}, {"rank": 2, "score": 0.64906955, "source": "dense+rrf", "rrf_score": 0.02964426877470356, "rerank_score": 0.1845465660305793, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 19, "text_snippet": "hortly after the  3 Fig. 2. A representative instance of the RAG process applied to question answering. It mainly consists of 3 steps. 1) Indexing. Documents are split into chunks, encoded into vectors, and stored in a vector database. 2) R"}, {"rank": 3, "score": 0.68594074, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.1833255428190171, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 3, "text_snippet": "augmentation techniques. The paper highlights the state-of-the- art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-"}, {"rank": 4, "score": 0.623096, "source": "dense+rrf", "rrf_score": 0.027884615384615386, "rerank_score": 0.18124207311334273, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 0, "text_snippet": "1 Retrieval-Augmented Generation for Large Language Models: A Survey Yunfan Gaoa, Yun Xiongb, Xinyu Gaob, Kangxiang Jiab, Jinliu Panb, Yuxi Bic, Yi Daia, Jiawei Suna, Meng Wangc, and Haofen Wanga,c aShanghai Research Institute for Intellige"}, {"rank": 5, "score": 0.64856863, "source": "dense+rrf", "rrf_score": 0.02694394213381555, "rerank_score": 0.18105393846318277, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 20, "text_snippet": "er. widespread adoption of ChatGPT. The Naive RAG follows a traditional process that includes indexing, retrieval, and generation, which is also characterized as a ‚ÄúRetrieve-Read‚Äù framework [7]. Indexing starts with the cleaning and extract"}]}
{"case_index": 78, "query": "From 2005.14165__language_models_are_few_shot_learners.txt, what exact term fills [BLANK] in: \"Figure 1.2 [BLANK] the conditions we study, and shows few-shot learning of a simple task requiring the model to remove extraneous symbols from a word.\"?", "gold": "illustrates", "answer": "The exact term is **‚Äúillustrates‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'illustrates' and is directly supported by Context 1 sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 84.921, "llm_ms": 1097.243, "top_contexts": [{"rank": 1, "score": 0.679772, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.4539436085700351, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 30, "text_snippet": "ions are allowed and only an instruction in natural language is given to the model. GPT-3 could also in principle be evaluated in the traditional Ô¨Åne-tuning setting, but we leave this to future work. Figure 1.2 illustrates the conditions we"}, {"rank": 2, "score": 0.589439, "source": "dense+rrf", "rrf_score": 0.029827662395050816, "rerank_score": 0.32591236013933045, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 19, "text_snippet": "Larger models make increasingly efÔ¨Åcient use of in-context information. We show in-context learning performance on a simple task requiring the model to remove random symbols from a word, both with and without a natural language task descrip"}, {"rank": 3, "score": 0.6142164, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.2601589446044031, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 166, "text_snippet": "r to learn these tasks at test time, as the model cannot perform them zero-shot and their artiÔ¨Åcial nature makes them unlikely to appear in the pre-training data (although we cannot conÔ¨Årm this with certainty). We can further quantify perfo"}, {"rank": 4, "score": 26.51766763926476, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.2601589446044031, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 147, "text_snippet": "er study of test-time behavior of language models. 3.9.1 Arithmetic To test GPT-3‚Äôs ability to perform simple arithmetic operations without task-speciÔ¨Åc training, we developed a small battery of 10 tests that involve asking GPT-3 a simple a"}, {"rank": 5, "score": 0.5981031, "source": "dense+rrf", "rrf_score": 0.028790389395194696, "rerank_score": 0.257270875284429, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 43, "text_snippet": "on with human performance. In this work we do not Ô¨Åne-tune GPT-3 because our focus is on task-agnostic performance, but GPT-3 can be Ô¨Åne-tuned in principle and this is a promising direction for future work. ‚Ä¢Few-Shot (FS) is the term we wil"}]}
{"case_index": 79, "query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"We Ô¨Årst sample up to kcandidate posi- tions for doing API calls by computing, for each i‚àà{1,...,n}, the probability pi=pM(<API>|P(x),x1:i‚àí1) [BLANK] to starting an API call at position i.\"?", "gold": "thatmassigns", "answer": "The exact term is:\n\n**‚Äúthat M assigns‚Äù**", "answer_score": 0.3, "answer_correctness": 0.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context shows the blank is the concatenated term 'thatMassigns' (no spaces). Model answered 'that M assigns' with spaces, so it does not match the expected exact term, though it is supported by context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 93.09, "llm_ms": 1296.71, "top_contexts": [{"rank": 1, "score": 0.7549397, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.4540038356711323, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 21, "text_snippet": "n+1as a continuation for the sequence z1,...,z n. We Ô¨Årst sample up to kcandidate posi- tions for doing API calls by computing, for each i‚àà{1,...,n}, the probability pi=pM(<API>|P(x),x1:i‚àí1) thatMassigns to starting an API call at position "}, {"rank": 2, "score": 0.58430475, "source": "dense+rrf", "rrf_score": 0.030090497737556562, "rerank_score": 0.26824301209760143, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 134, "text_snippet": "niel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. Opt: Open pre- trained transformer language models.  A API Details When sampling and Ô¨Åltering API calls, by default we use values of œÑs= 0.05andœÑf= 1.0‚Äì"}, {"rank": 3, "score": 0.5847014, "source": "dense+rrf", "rrf_score": 0.029850746268656716, "rerank_score": 0.2573053813782815, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 27, "text_snippet": "atterns in the pretraining corpus, thus hurting perplexity.  x1:i‚àí1,e(ci,ri),xi:n; we proceed analogously for texts with multiple API calls. Doing this for all x‚àà Cresults in the new dataset C‚àóaugmented with API calls. We use this new datas"}, {"rank": 4, "score": 0.61764616, "source": "dense+rrf", "rrf_score": 0.031746031746031744, "rerank_score": 0.24797668488556054, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 20, "text_snippet": "te API calls for the question answering tool. Mitself on this dataset. Each of these steps is described in more detail below. Sampling API Calls For each API, we write a promptP(x)that encourages the LM to anno- tate an example x=x1,...,x n"}, {"rank": 5, "score": 0.5509189, "source": "dense+rrf", "rrf_score": 0.027692895339954164, "rerank_score": 0.22635955805754038, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 1, "text_snippet": "metic or fac- tual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer , a model traine"}]}
{"case_index": 80, "query": "From 2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"3 Evaluation [BLANK] We consider a standard RAG setting, where given a question q, the system first retrieves some context c(q)and then uses the retrieved context to generate an answer as(q).\"?", "gold": "strategies", "answer": "The exact term is **‚ÄúStrategies‚Äù**, i.e., the heading is:\n\n**‚Äú3 Evaluation Strategies‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Model correctly identifies the blank as 'strategies' from the context heading '3 Evaluation Strategies'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 84.313, "llm_ms": 1346.492, "top_contexts": [{"rank": 1, "score": 0.75159407, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.4781644413945809, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 19, "text_snippet": "Yuan et al., 2021) similarly uses refer- ence answers to compute aspects such as precision (estimated as the probability of generating the gen- erated answer given the reference) and recall (esti- mated as the probability of generating the "}, {"rank": 2, "score": 35.36322112702383, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.28098322016388355, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 42, "text_snippet": "r (Dasigi et al., 2021) is an information-seeking question answering dataset over academic research papers. Each question is written as a followup to the title and abstract of a particular paper, and the answer, if present, is identified in"}, {"rank": 3, "score": 0.6157304, "source": "dense+rrf", "rrf_score": 0.027884615384615386, "rerank_score": 0.27079233180307083, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 20, "text_snippet": "generate an answer as(q). When building a RAG system,  we usually do not have access to human-annotated datasets or reference answers. We therefore fo- cus on metrics that are fully self-contained and reference-free. We focus in particular "}, {"rank": 4, "score": 30.62503989906804, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.2513931606934072, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 16, "text_snippet": ". When deciding what to retrieve , it is important to consider what LMs intend to gen- erate in the future, as the goal of active retrieval is to benefit future generations. Therefore, we propose anticipating the future by generating a temp"}, {"rank": 5, "score": 30.607781370122584, "source": "dense+rrf", "rrf_score": 0.014285714285714285, "rerank_score": 0.2499264025314366, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 10, "text_snippet": "n in the input context. We first experiment with multi-document ques- tion answering, which requires models to reason over provided documents to find relevant informa- tion and use it to answer a given question; this task mimics the retriev"}]}
{"case_index": 81, "query": "From 2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt, what exact term fills [BLANK] in: \"Published as a conference paper at ICLR 2023 [BLANK] IMPROVES CHAIN OF THOUGHT REASONING IN LANGUAGE MODELS Xuezhi Wang‚Ä†‚Ä°Jason Wei‚Ä†Dale Schuurmans‚Ä†Quoc Le‚Ä†Ed H.\"?", "gold": "self-consistency", "answer": "SELF-CONSISTENCY", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "The blank is filled by the paper title term 'SELF-CONSISTENCY', matching expected 'self-consistency' and directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 54.905, "llm_ms": 1235.648, "top_contexts": [{"rank": 1, "score": 0.7432947, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.4704605869237562, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 0, "text_snippet": "Published as a conference paper at ICLR 2023 SELF-CONSISTENCY IMPROVES CHAIN OF THOUGHT REASONING IN LANGUAGE MODELS Xuezhi Wang‚Ä†‚Ä°Jason Wei‚Ä†Dale Schuurmans‚Ä†Quoc Le‚Ä†Ed H. Chi‚Ä† Sharan Narang‚Ä†Aakanksha Chowdhery‚Ä†Denny Zhou‚Ä†¬ß ‚Ä†Google Research, "}, {"rank": 2, "score": 0.67044425, "source": "dense+rrf", "rrf_score": 0.03125763125763126, "rerank_score": 0.3829430084274899, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 120, "text_snippet": "Zhou. 2022. Self- consistency improves chain of thought reasoning in language models. CoRR , abs/2203.11171. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicit"}, {"rank": 3, "score": 0.74783707, "source": "dense+rrf", "rrf_score": 0.03177805800756621, "rerank_score": 0.3783480555817278, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 133, "text_snippet": " Proceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence , 32, Apr. 2018. URL https: //ojs.aaai.org/index.php/AAAI/article/view/12340 . Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and"}, {"rank": 4, "score": 0.68064666, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.36905603871833853, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 113, "text_snippet": "on. arXiv preprint arXiv:1803.05355 , 2018. Lev S Vygotsky. Thinking and speech. The collected works of LS Vygotsky , 1:39‚Äì285, 1987. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdh- ery, and Denny "}, {"rank": 5, "score": 0.6296265, "source": "dense+rrf", "rrf_score": 0.030776515151515152, "rerank_score": 0.35964308263058803, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 109, "text_snippet": ", Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022. LaMDA: Language models for dialog applications. arXiv preprint arXiv:2201.08239 . Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and"}]}
{"case_index": 82, "query": "From 2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt, what exact term fills [BLANK] in: \"Orca 2 [BLANK] surpasses models of similar size and attains performance levels similar or better to those of models 5-10x larger, as assessed on complex tasks that test advanced reasoning abilities in zero-shot settings.\"?", "gold": "significantly", "answer": "The exact term is **‚Äúsignificantly‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'significantly' and is directly supported by Context 1/2.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 90.375, "llm_ms": 962.377, "top_contexts": [{"rank": 1, "score": 0.76268756, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.4711679425791736, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 3, "text_snippet": "valuate Orca 2 using a comprehensive set of 15 diverse benchmarks (corresponding to approximately 100 tasks and over 36K unique prompts). Orca 2 significantly surpasses models of similar size and attains performance levels similar or better"}, {"rank": 2, "score": 0.7466755, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.280499725171888, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 20, "text_snippet": "problem solving, reading comprehension, summarization, groundedness, truthfulness and toxic content generation and identification. Our preliminary results indicate that Orca 2 significantly surpasses models of a similar size, even matching "}, {"rank": 3, "score": 0.7601443, "source": "dense+rrf", "rrf_score": 0.03128054740957967, "rerank_score": 0.25690743494523643, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 147, "text_snippet": "ques and recognizing the most effective solution strategy for each task, achieve performance levels comparable to, and often exceeding, models that are much larger, especially on zero-shot reasoning tasks. Though these models still exhibit "}, {"rank": 4, "score": 0.7326516, "source": "dense+rrf", "rrf_score": 0.029513888888888888, "rerank_score": 0.24754128065490447, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 1, "text_snippet": "n benchmarks like BigBench Hard and AGIEval. In Orca 2, we continue exploring how improved training signals can enhance smaller LMs‚Äô reasoning abilities. Research on training small LMs has often relied on imitation learning to replicate the"}, {"rank": 5, "score": 37.30146525035493, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.24672075911752034, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 64, "text_snippet": "sks selected to assess open-ended generation, summarization, safety, bias, reasoning, and comprehension capacities of Orca 2. Except where specified otherwise, evaluations were conducted using the test split of each dataset. We conduct eval"}]}
{"case_index": 83, "query": "From 2005.14165__language_models_are_few_shot_learners.txt, what exact term fills [BLANK] in: \"To avoid this confusion, we use the term ‚Äúmeta-learning‚Äù to capture the inner-loop / outer-loop structure of the general method, and the term ‚Äúin [BLANK]‚Äù to refer to the inner loop of meta-learning.\"?", "gold": "context-learning", "answer": "The exact term is **‚Äúcontext-learning‚Äù**, i.e., the phrase is **‚Äúin context-learning‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'context-learning' and is directly supported by Context 1 quote ('in context-learning').", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 125.819, "llm_ms": 1273.826, "top_contexts": [{"rank": 1, "score": 0.7272552, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.46660108327740585, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 26, "text_snippet": " learning from zero examples. To avoid this confusion, we use the term ‚Äúmeta-learning‚Äù to capture the inner-loop / outer-loop structure of the general method, and the term ‚Äúin context-learning‚Äù to refer to the inner loop of meta-learning. W"}, {"rank": 2, "score": 0.5365667, "source": "dense+rrf", "rrf_score": 0.030798389007344232, "rerank_score": 0.3152836071350484, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 25, "text_snippet": "skills and tasks within the parameters of the model, it is plausible that in-context learning abilities might show similarly strong gains with scale. 1In the context of language models this has sometimes been called ‚Äúzero-shot transfer‚Äù, bu"}, {"rank": 3, "score": 0.5833504, "source": "dense+rrf", "rrf_score": 0.0315136476426799, "rerank_score": 0.2649017809338967, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 27, "text_snippet": "of whether the model learns new tasks from scratch at inference time or simply recognizes patterns seen during training ‚Äì this is an important issue which we discuss later in the paper, but ‚Äúmeta-learning‚Äù is intended to encompass both poss"}, {"rank": 4, "score": 0.6538862, "source": "dense+rrf", "rrf_score": 0.031754032258064516, "rerank_score": 0.22273590488149295, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 306, "text_snippet": "alearning in language models has been utilized in [ RWC+19], though with much more limited results and no systematic study. More broadly, language model metalearning has an inner-loop-outer-loop structure, making it structurally similar to "}, {"rank": 5, "score": 21.885790521102965, "source": "dense+rrf", "rrf_score": 0.013888888888888888, "rerank_score": 0.21961967447503317, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 51, "text_snippet": "s assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior. We will refer to it as the cautious system instruction . Few Shot Data : The dataset above does no"}]}
{"case_index": 84, "query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"In this paper, we show that LMs can teach [BLANK] to use external tools via simple APIs and achieve the best of both worlds.\"?", "gold": "themselves", "answer": "The exact term is **‚Äúthemselves‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'themselves' and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 53.179, "llm_ms": 1057.804, "top_contexts": [{"rank": 1, "score": 0.7062562, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.4367867943365763, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 1, "text_snippet": "metic or fac- tual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer , a model traine"}, {"rank": 2, "score": 0.6580318, "source": "dense+rrf", "rrf_score": 0.03076923076923077, "rerank_score": 0.2469255523253797, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 12, "text_snippet": " how an API can be used, we let a LM annotate a huge language modeling dataset with potential API calls. We then use a self-supervised loss to determine which of these API calls actually help the model in predicting future tokens. Finally, "}, {"rank": 3, "score": 0.5919379, "source": "dense+rrf", "rrf_score": 0.029513888888888888, "rerank_score": 0.23567356093919417, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 6, "text_snippet": "legislative bodies, like  city councils, to hold their meetings open to the public. Figure 1: Exemplary predictions of Toolformer. The model autonomously decides to call different APIs (from top to bottom: a question answering system, a cal"}, {"rank": 4, "score": 0.5826534, "source": "dense+rrf", "rrf_score": 0.028949545078577336, "rerank_score": 0.2277205008993971, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 212, "text_snippet": "line decision making: Benchmarks and additional opinions,‚Äù arXiv preprint arXiv:2306.02224 , 2023. [108] T. Schick, J. Dwivedi-Yu, R. Dess `ƒ±, R. Raileanu, M. Lomeli, L. Zettle- moyer, N. Cancedda, and T. Scialom, ‚ÄúToolformer: Language mode"}, {"rank": 5, "score": 0.5941192, "source": "dense+rrf", "rrf_score": 0.027783137179239824, "rerank_score": 0.2270892036320295, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 13, "text_snippet": "h tool to use when and how. As our approach is agnostic of the dataset be- ing used, we can apply it to the exact same dataset that was used to pretrain a model in the Ô¨Årst place. This ensures that the model does not lose any of its general"}]}
{"case_index": 85, "query": "From 2302.13971__llama_open_and_efficient_foundation_language_models.txt, what exact term fills [BLANK] in: \"This leads to the following mixture of data and the per- centage they represent in the training set: English [BLANK] [67%].\"?", "gold": "commoncrawl", "answer": "The exact term is **‚ÄúCommonCrawl‚Äù** (i.e., ‚ÄúEnglish CommonCrawl [67%].‚Äù).", "answer_score": 0.9299999999999999, "answer_correctness": 0.9, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.9, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context shows 'English CommonCrawl [67%]'. Expected answer is 'commoncrawl' (case-insensitive). Model answered 'CommonCrawl', which matches aside from casing.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 62.489, "llm_ms": 1789.668, "top_contexts": [{"rank": 1, "score": 0.65041304, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3750103262266428, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 10, "text_snippet": " mains. For the most part, we reuse data sources that have been leveraged to train other LLMs, with the restriction of only using data that is publicly available, and compatible with open sourcing. This leads to the following mixture of dat"}, {"rank": 2, "score": 20.445949691116784, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.2152207650038443, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 78, "text_snippet": "mits 0.231 kg CO 2eq/KWh, lead- ing to 82 tCO 2eq. In this study, we are interested in comparing the cost in carbon emission of training of these models if they were trained in the same data center. Hence, we do not take the location of dat"}, {"rank": 3, "score": 19.596525720510265, "source": "dense+rrf", "rrf_score": 0.014285714285714285, "rerank_score": 0.1974065257027066, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 5, "text_snippet": "at inference. For instance, although Hoffmann et al. (2022) recommends training a 10B model on 200B tokens, we Ô¨Ånd that the performance of a 7B model continues to improve even after 1T tokens. The focus of this work is to train a series of "}, {"rank": 4, "score": 20.069636498250592, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.17059307339376645, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 0, "text_snippet": "LLaMA: Open and EfÔ¨Åcient Foundation Language Models Hugo Touvron‚àó, Thibaut Lavril‚àó, Gautier Izacard‚àó, Xavier Martinet Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi√®re, Naman Goyal Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand "}, {"rank": 5, "score": 19.867983588980238, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.15717639389260837, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 22, "text_snippet": "trained on the same data for slightly more epochs (4.8T tokens total as for phi-3-small . The model has 40 heads and 40 layers, with embedding dimension 5120. We observe that some benchmarks improve much less from 7B to 14B than they do fro"}]}
{"case_index": 86, "query": "From 2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt, what exact term fills [BLANK] in: \"Our models generalize to the [BLANK] of ‚Äúheld-out‚Äù labelers that did not produce any train- ing data.\"?", "gold": "preferences", "answer": "The exact term is **‚Äúpreferences‚Äù**:\n\n‚ÄúOur models generalize to the **preferences** of ‚Äòheld-out‚Äô labelers that did not produce any training data.‚Äù", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'preferences' and is directly supported by the provided context quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 87.996, "llm_ms": 1256.66, "top_contexts": [{"rank": 1, "score": 0.6317911, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.36129499833626777, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 93, "text_snippet": "e preferences of \"held-out\" labelers that did not produce any train- ing data. Held-out labelers have similar ranking preferences as workers who we used to produce training data (see Figure 3). In particular, according to held-out workers, "}, {"rank": 2, "score": 49.482032885538295, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.3194230765171683, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 20, "text_snippet": "ignment tax‚Äù since our alignment procedure comes at the cost of 3  lower performance on certain tasks that we may care about. We can greatly reduce the performance regressions on these datasets by mixing PPO updates with updates that increa"}, {"rank": 3, "score": 0.6202112, "source": "dense+rrf", "rrf_score": 0.030834914611005692, "rerank_score": 0.2763751947114748, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 21, "text_snippet": " ing data. To test the generalization of our models, we conduct a preliminary experiment with held-out labelers, and Ô¨Ånd that they prefer InstructGPT outputs to outputs from GPT-3 at about the same rate as our training labelers. However, mo"}, {"rank": 4, "score": 39.88273113581902, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.27135875491543343, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 92, "text_snippet": "mpted)SFT PPO-ptx FLAN T0 Model246Likert score Figure 5: Comparing our models with FLAN and T0 in terms of Likert scores on a 1-7 scale, on the InstructGPT prompt distribution. FLAN and T0 perform better than default GPT-3, and comparably w"}, {"rank": 5, "score": 0.55702484, "source": "dense+rrf", "rrf_score": 0.02782608695652174, "rerank_score": 0.2608265952782091, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 12, "text_snippet": "We illustrate this process in Figure 2. This procedure aligns the behavior of GPT-3 to the stated preferences of a speciÔ¨Åc group of people (mostly our labelers and researchers), rather than any broader notion of ‚Äúhuman values‚Äù; we discuss t"}]}
{"case_index": 87, "query": "From 2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt, what exact term fills [BLANK] in: \"3 Published as a conference paper at ICLR 2023 sampling, as commonly used for [BLANK] text generation (Radford et al., 2019; Brown et al., 2020; Thoppilan et al., 2022), to achieve this goal.\"?", "gold": "open-ended", "answer": "The exact term that fills **[BLANK]** is:\n\n**open-ended**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'open-ended' and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 98.091, "llm_ms": 1081.29, "top_contexts": [{"rank": 1, "score": 0.7420769, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.4711679425791736, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 30, "text_snippet": "l., 2021; Thoppilan et al., 2022). 3  Published as a conference paper at ICLR 2023 sampling, as commonly used for open-ended text generation (Radford et al., 2019; Brown et al., 2020; Thoppilan et al., 2022), to achieve this goal. One shoul"}, {"rank": 2, "score": 57.8588329121743, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.37789416510321655, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 40, "text_snippet": " 4  Published as a conference paper at ICLR 2023 3.2 M AINRESULTS We report the results of self-consistency averaged over 10 runs, where we sampled 40 outputs independently from the decoder in each run. The baseline we compare to is chain-o"}, {"rank": 3, "score": 0.6459232, "source": "dense+rrf", "rrf_score": 0.031746031746031744, "rerank_score": 0.30356425305605567, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 0, "text_snippet": "Published as a conference paper at ICLR 2023 SELF-CONSISTENCY IMPROVES CHAIN OF THOUGHT REASONING IN LANGUAGE MODELS Xuezhi Wang‚Ä†‚Ä°Jason Wei‚Ä†Dale Schuurmans‚Ä†Quoc Le‚Ä†Ed H. Chi‚Ä† Sharan Narang‚Ä†Aakanksha Chowdhery‚Ä†Denny Zhou‚Ä†¬ß ‚Ä†Google Research, "}, {"rank": 4, "score": 45.81149688960653, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.29913802135401923, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 18, "text_snippet": "., in Output 2), but such solutions are less likely to arrive at the same answer. That is, we hypothesize that correct reasoning processes, even if they are diverse, tend to have greater agreement in their Ô¨Ånal answer than incorrect process"}, {"rank": 5, "score": 51.55074571569866, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.2900707221665879, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 14, "text_snippet": "et al., 2022) and GPT-3-175B (Brown et al., 2020), and two densely-activated decoder-only language models: LaMDA-137B (Thoppilan et al., 2022) and PaLM-540B (Chowdhery et al., 2022). On all four language models, self-consistency improves ov"}]}
{"case_index": 88, "query": "From 2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt, what exact term fills [BLANK] in: \"We use a chunked [BLANK] module to incorporate the retrieved text (¬ß2.4), with time complexity linear in the amount of retrieved data.\"?", "gold": "cross-attention", "answer": "cross-attention", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "The blank is explicitly 'cross-attention' in Context 1, matching the expected answer.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 107.164, "llm_ms": 1288.941, "top_contexts": [{"rank": 1, "score": 0.6159997, "source": "dense+rrf", "rrf_score": 0.031099324975891997, "rerank_score": 0.3700550864568491, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 11, "text_snippet": ".sc/t.sc/r.sc/o.sc, a retrieval-enhanced autoregressive language model (¬ß2.2). We use a chunked cross-attention module to incorporate the retrieved text (¬ß2.4), with time complexity linear in the amount of retrieved data. We show that retri"}, {"rank": 2, "score": 0.63859904, "source": "dense+rrf", "rrf_score": 0.031754032258064516, "rerank_score": 0.30657814071968287, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 22, "text_snippet": " ùïç=¬ª1¬îùë£¬º, obtained using a text tokenizer1. Wespliteach ùëõ-token-longexample ùëã=¬πùë•1¬î¬ì¬ì¬ì¬îùë•ùëõ¬∫intoasequenceof ùëôchunks¬πùê∂1¬î¬ì¬ì¬ì¬îùê∂ùëô¬∫ of sizeùëö=ùëõ ùëô, i.e.ùê∂1,¬πùë•1¬î¬ì¬ì¬ì¬îùë•ùëö¬∫¬î ¬ì¬ì¬ì¬î ùê∂ùëô,¬πùë•ùëõ\u0000ùëö¬∏1¬î¬ì¬ì¬ì¬îùë•ùëõ¬∫2ùïçùëö. We useùëõ=2048andùëö=64. We augment each chunk ùê∂ùë¢with a se"}, {"rank": 3, "score": 0.6000546, "source": "dense+rrf", "rrf_score": 0.02797339593114241, "rerank_score": 0.29067333722614147, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 38, "text_snippet": "t token of chunk ùê∂ùë¢is the Ô¨Årst to be able to access the retrieved content ùê∏ùë¢while maintaining autoregressivity in(1). Hence, there is a one token overlap between chunk ùê∂ùë¢=\u0010 ùë•¬πùë¢\u00001¬∫ùëö¬∏ùëñ\u0011 ùëñ2¬ª1¬îùëö¬ºand the corresponding attending chunk ùê∂¬∏ ùë¢,¬πùë•ùë¢ùëö¬∏ùëñ"}, {"rank": 4, "score": 0.62489873, "source": "dense+rrf", "rrf_score": 0.028484848484848488, "rerank_score": 0.27295821160938816, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 183, "text_snippet": "sc¬πùê∂ùë¢¬∫ùëóand the chunk ùê∂ùë¢to be relatively well aligned, and assume that they start at the same position. Therefore, when computing C/a.sc¬πùêª¬∏ ùë¢¬îùê∏ùë¢¬∫, we set the distance between the data token ùëñ2¬ª1¬îùëô¬ºof chunkùê∂¬∏ ùë¢and 5https://github.com/earwig/m"}, {"rank": 5, "score": 31.38707774535947, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.2697435897435842, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 216, "text_snippet": "ly sound. We choose to have cross-attention every 3 layer as this provides a good trade-oÔ¨Ä between performance and run-time. 31  Improving language models by retrieving from trillions of tokens E. Qualitative experiments We illustrate the u"}]}
{"case_index": 89, "query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \"ELMo and its predecessor (Peters et al., 2017, 2018a) generalize [BLANK] word embedding re- search along a diffe\"?", "gold": "traditional", "answer": "The exact term is **‚Äútraditional‚Äù**, i.e., the sentence reads:\n\n‚ÄúELMo and its predecessor (Peters et al., 2017, 2018a) generalize **traditional** word embedding re- search along a diffe‚Äù", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'traditional' and is directly supported by the provided context snippet.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 82.879, "llm_ms": 2796.906, "top_contexts": [{"rank": 1, "score": 0.631146, "source": "dense+rrf", "rrf_score": 0.032266458495966696, "rerank_score": 0.31207627088335327, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 14, "text_snippet": " Lee, 2018) or paragraph embeddings (Le and Mikolov, 2014). To train sentence representations, prior work has used objectives to rank candidate next sentences (Jernite et al., 2017; Logeswaran and Lee, 2018), left-to-right generation of nex"}, {"rank": 2, "score": 0.6837678, "source": "dense+rrf", "rrf_score": 0.032266458495966696, "rerank_score": 0.2975330733806198, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 15, "text_snippet": ") generalize traditional word embedding re- search along a different dimension. They extract context-sensitive features from a left-to-right and a right-to-left language model. The contextual rep- resentation of each token is the concatenat"}, {"rank": 3, "score": 0.6243123, "source": "dense+rrf", "rrf_score": 0.0315136476426799, "rerank_score": 0.2875025856333278, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 0, "text_snippet": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova Google AI Language {jacobdevlin,mingweichang,kentonl,kristout }@google.com Abstract We introduce a ne"}, {"rank": 4, "score": 0.6299764, "source": "dense+rrf", "rrf_score": 0.031009615384615385, "rerank_score": 0.2652416581427248, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 10, "text_snippet": "ford et al. (2018), which uses unidirec- tional language models for pre-training, BERT uses masked language models to enable pre- trained deep bidirectional representations. This is also in contrast to Peters et al. (2018a), which uses a sh"}, {"rank": 5, "score": 32.83493084446756, "source": "dense+rrf", "rrf_score": 0.014285714285714285, "rerank_score": 0.2549459489502709, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 109, "text_snippet": "reading books. In Proceedings of the IEEE international conference on computer vision , pages 19‚Äì27. Appendix for ‚ÄúBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding‚Äù We organize the appendix into three section"}]}
{"case_index": 90, "query": "From 2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt, what exact term fills [BLANK] in: \"For example our previous model trained on this data recipe, phi-2 (2.7B parameters), matched the [BLANK] of models 25 times large\"?", "gold": "performance", "answer": "The exact term that fills **[BLANK]** is **‚Äúperformance‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'performance' and is directly supported by Context 1/2 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 90.352, "llm_ms": 1170.355, "top_contexts": [{"rank": 1, "score": 0.6350107, "source": "dense+rrf", "rrf_score": 0.0304147465437788, "rerank_score": 0.3724124541976752, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 5, "text_snippet": "he existence of frontier LLMs themselves, which allow us to interact with data in novel ways. In our previous works on the phi models [GZA+23, LBE+23, JBA+23] it was shown that a combination of LLM-based filtering of publicly available web "}, {"rank": 2, "score": 0.739723, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3383718026067894, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 6, "text_snippet": "7B parameters), matched the performance of models 25 times larger trained on regular data. In this report we present a new model, phi-3-mini (3.8B parameters), trained for 3.3T tokens on larger and more advanced versions of the datasets use"}, {"rank": 3, "score": 0.69206697, "source": "dense+rrf", "rrf_score": 0.031746031746031744, "rerank_score": 0.275144427809962, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 0, "text_snippet": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone Microsoft Abstract We introduce phi-3-mini , a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both"}, {"rank": 4, "score": 0.6470582, "source": "dense+rrf", "rrf_score": 0.030117753623188408, "rerank_score": 0.250243220650584, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 16, "text_snippet": "3. The table shows the Keys/values a query token in block 8 attended to. Blue=local blocks, orange=remote/vertical blocks, gray=blocks skipped. total parameters. Additionally, we utilize the SparseMixer approach [LGC23, LDL+23] for training"}, {"rank": 5, "score": 0.65260345, "source": "dense+rrf", "rrf_score": 0.029418126757516764, "rerank_score": 0.24014426818440787, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 21, "text_snippet": "s an example, the result of a game in premier league in a particular day might be good training data for frontier models, but we need to remove such information to leave more model capacity for ‚Äúreasoning‚Äù for the mini size models. We compa"}]}
{"case_index": 91, "query": "From 2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt, what exact term fills [BLANK] in: \"3.1 Experimental Setup We explore [BLANK] prompting for various language models on multiple benchmarks.\"?", "gold": "chain-of-thought", "answer": "The exact term is **‚Äúchain-of-thought‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term and is directly supported by context sentence: 'We explore chain-of-thought prompting...'", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 61.479, "llm_ms": 1056.34, "top_contexts": [{"rank": 1, "score": 0.7477734, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.335439943765711, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 20, "text_snippet": "21, inter alia ). Strikingly, chain- of-thought prompting when used with the 540B parameter language model performs comparably with task-speciÔ¨Åc Ô¨Ånetuned models on several tasks, even achieving new state of the art on the challenging GSM8K "}, {"rank": 2, "score": 34.26988245499278, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.2858730155049154, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 160, "text_snippet": "e 1, we see that adding a calculator signiÔ¨Åcantly boosts performance of chain-of-thought prompting on most tasks. Table 1: Chain of thought prompting outperforms standard prompting for various large language models on Ô¨Åve arithmetic reasoni"}, {"rank": 3, "score": 29.886632614983675, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.26263495809627646, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 30, "text_snippet": " output ‚ü©triples for arithmetic, commonsense, and symbolic reasoning benchmarks. Chains of thought are highlighted. Full prompts in Appendix G. math word problems, we used this single set of eight chain of thought exemplars for all benchmar"}, {"rank": 4, "score": 40.56216141072533, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.24052104476016975, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 114, "text_snippet": "-augmented ensembles in language models. arXiv preprint arXiv:2207.00747 , 2022b. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models"}, {"rank": 5, "score": 0.69957554, "source": "dense+rrf", "rrf_score": 0.027783137179239824, "rerank_score": 0.23569901214882683, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 68, "text_snippet": "s (though performance is lower than in the in-domain setting). Hence, chain-of-thought prompting facilitates length generalization beyond seen chains of thought for language models of sufÔ¨Åcient scale. 6 Discussion We have explored chain-of-"}]}
{"case_index": 92, "query": "From 2005.14165__language_models_are_few_shot_learners.txt, what exact term fills [BLANK] in: \"Ziegler Jeffrey Wu Clemens Winter [BLANK] Hesse Mark Chen Eric Sigler Mateusz Litwin Scott Gray Benjamin Chess Jack Clark Christopher Berner Sam McCandlish Alec Radford Ilya Sutskever Dario Amodei OpenAI Abstract Recent wo\"?", "gold": "christopher", "answer": "The exact term that fills **[BLANK]** is **‚ÄúChristopher‚Äù** (as in **‚ÄúChristopher Hesse‚Äù**).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context shows the author list includes 'Christopher Hesse'; blank is 'Christopher'. Model answer matches expected.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 34.642, "llm_ms": 1396.546, "top_contexts": [{"rank": 1, "score": 0.68620145, "source": "dense+rrf", "rrf_score": 0.03177805800756621, "rerank_score": 0.523470112855731, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 0, "text_snippet": "Language Models are Few-Shot Learners Tom B. Brown‚àóBenjamin Mann‚àóNick Ryder‚àóMelanie Subbiah‚àó Jared Kaplan‚Ä†Prafulla Dhariwal Arvind Neelakantan Pranav Shyam Girish Sastry Amanda Askell Sandhini Agarwal Ariel Herbert-Voss Gretchen Krueger Tom"}, {"rank": 2, "score": 0.74014807, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.49298524570305857, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 99, "text_snippet": " Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris"}, {"rank": 3, "score": 0.73597574, "source": "dense+rrf", "rrf_score": 0.031746031746031744, "rerank_score": 0.48939509788884594, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 84, "text_snippet": "nav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Li"}, {"rank": 4, "score": 0.74182844, "source": "dense+rrf", "rrf_score": 0.032018442622950824, "rerank_score": 0.48603578540385417, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 102, "text_snippet": ", Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert- V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwi"}, {"rank": 5, "score": 0.705925, "source": "dense+rrf", "rrf_score": 0.030776515151515152, "rerank_score": 0.4356951783672714, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 97, "text_snippet": "ind Neelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric"}]}
{"case_index": 93, "query": "From 2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt, what exact term fills [BLANK] in: \"Sample a diverse set of [BLANK] paths She eats 3 for breakfast, so she has 16 - 3 = 13 left.\"?", "gold": "reasoning", "answer": "The exact term is **‚Äúreasoning‚Äù**, as in: ‚ÄúSample a diverse set of **reasoning** paths ‚Ä¶‚Äù.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'reasoning' and is directly supported by the provided context phrase 'Sample a diverse set of reasoning paths'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 64.938, "llm_ms": 1523.289, "top_contexts": [{"rank": 1, "score": 0.52639186, "source": "dense+rrf", "rrf_score": 0.031099324975891997, "rerank_score": 0.3198567911408747, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 9, "text_snippet": "rking lot? A: There are 3 cars in the parking lot  already. 2 more arrive. Now there are  3 + 2 = 5 cars. The answer is 5.  ‚Ä¶ Q: Janet‚Äôs ducks lay 16 eggs per day.  She eats three for breakfast every  morning and bakes muffins for her  frie"}, {"rank": 2, "score": 0.5073379, "source": "dense+rrf", "rrf_score": 0.029957522915269395, "rerank_score": 0.31492288287154, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 11, "text_snippet": "mainder for $2 per egg, so in  total she sells 7 * $2 = $14 per day.  The answer is $14. The answer is $14. Greedy decode Figure 1: The self-consistency method contains three steps: (1) prompt a language model using chain-of-thought (CoT) p"}, {"rank": 3, "score": 0.5957884, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.2897086679337331, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 10, "text_snippet": "g paths She eats 3 for breakfast, so  she has 16 - 3 = 13 left. Then  she bakes muffins, so she  has 13 - 4 = 9 eggs left. So  she has 9 eggs * $2 = $18. This means she she sells the  remainder for $2 * (16 - 4 - 3)  = $26 per day. The answ"}, {"rank": 4, "score": 0.57559115, "source": "dense+rrf", "rrf_score": 0.03128054740957967, "rerank_score": 0.24450006217636042, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 22, "text_snippet": " a prompt and a question, self-consistency introduces an additional latent variable ri, which is a sequence of tokens representing the reasoning path in the i-th output, then couples the generation of (ri,ai)where ri‚Üíai, i.e., generating a "}, {"rank": 5, "score": 38.731120659519064, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.23773805585014962, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 109, "text_snippet": ", Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022. LaMDA: Language models for dialog applications. arXiv preprint arXiv:2201.08239 . Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and"}]}
{"case_index": 94, "query": "From 2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt, what exact term fills [BLANK] in: \"One limitation of these works is that these [BLANK] have a large gap to full attention, making it infeasible to\"?", "gold": "compressions", "answer": "The exact term is **‚Äúcompressions‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'compressions' and is directly supported by Context 1/3 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 86.892, "llm_ms": 1075.119, "top_contexts": [{"rank": 1, "score": 0.62538546, "source": "dense+rrf", "rrf_score": 0.03028233151183971, "rerank_score": 0.3229420369600373, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 23, "text_snippet": " et al., 2022; Ding et al., 2023; Qiu et al., 2020). They alleviate the quadratic complexity of the self-attention computation. For example, Longformer (Beltagy et al., 2020) and BigBird (Zaheer et al., 2020) use sparse attention to handle "}, {"rank": 2, "score": 0.64390856, "source": "dense+rrf", "rrf_score": 0.03125763125763126, "rerank_score": 0.2982019039554625, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 75, "text_snippet": "ed in training-from- scratch transformers. This experiment is to examine their capability of fine-tuning on pre-trained LLMs (Touvron et al., 2023b), toward long context adaptation. Dilated attention performs well in full fine-tuning but is"}, {"rank": 3, "score": 0.71228254, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.2683659632593882, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 24, "text_snippet": "ons have a large gap to full attention, making it infeasible to fine-tune pre-trained LLMs. Although our work also involves an approximation of attention mechanism, it has a similar shape and a small gap to standard attention. This enables "}, {"rank": 4, "score": 0.6486234, "source": "dense+rrf", "rrf_score": 0.030798389007344232, "rerank_score": 0.25396453947078396, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 41, "text_snippet": "eads respectively. This manner does not increase additional computation costs but enables the information flow between different groups. We show that it gets close to the standard attention baseline in Table 1. Consistency to Full Attention"}, {"rank": 5, "score": 0.64203227, "source": "dense+rrf", "rrf_score": 0.030303030303030304, "rerank_score": 0.24845305305649162, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 42, "text_snippet": " Child et al., 2019), designed for training from scratch, have gaps to the standard full attention, which is used in pre-training. In Table 6, we show that S2-Attn not only enables efficient fine-tuning but also supports fullattention testi"}]}
{"case_index": 95, "query": "From 2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt, what exact term fills [BLANK] in: \"We study this question with a [BLANK] key- value retrieval task, which is designed to be a mini- mal testbed for the basic ability to retrieve matching tokens from the input context.\"?", "gold": "synthetic", "answer": "The exact term is **‚Äúsynthetic‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context explicitly says 'We study this question with a synthetic key-value retrieval task'; model answer matches expected term.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 99.722, "llm_ms": 1806.191, "top_contexts": [{"rank": 1, "score": 0.74514806, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.5023812064899007, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 14, "text_snippet": "r at using their input context (¬ß2.3). Given that language models struggle to retrieve and use relevant information in the multi-document question answering task, to what extent can lan- guage models even retrieve from their input con- text"}, {"rank": 2, "score": 0.7218275, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.41747808521705954, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 50, "text_snippet": "wering task, to what extent can they simply retrieve from input contexts? We study this question with a syn- thetic key-value retrieval task, which is designed to provide a minimal testbed for the basic ability to retrieve matching tokens f"}, {"rank": 3, "score": 0.6343785, "source": "dense+rrf", "rrf_score": 0.03149801587301587, "rerank_score": 0.32502723233043507, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 49, "text_snippet": "UIDs. The relevant key-value pair for answering the query is bolded here within the input context for clarity. indicate that extended-context models are not nec- essarily better than their non-extended counterparts at using their input cont"}, {"rank": 4, "score": 46.1751093580084, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.3237319902926832, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 81, "text_snippet": "ge model can take in 16K tokens, is it actually beneficial to provide 16K tokens of context? The answer to this question is ultimately downstream task-specific since it de- pends on the marginal value of the added context and the model‚Äôs ab"}, {"rank": 5, "score": 0.6421517, "source": "dense+rrf", "rrf_score": 0.03149801587301587, "rerank_score": 0.26839885302375033, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 15, "text_snippet": " are given a collection of JSON-formatted key-value pairs and must return the value associated with a specific key. Similar to the multi-document QA task, the key-value retrieval task admits controlled changes to the input context length (a"}]}
{"case_index": 96, "query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \", 2018), BERT is designed to pre- train deep bidirectional [BLANK] from unlabeled text by jointly conditioning on both left and right context in all layers.\"?", "gold": "representations", "answer": "The exact term is **‚Äúrepresentations‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'representations' and is directly supported by the provided context sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 75.821, "llm_ms": 1331.793, "top_contexts": [{"rank": 1, "score": 0.72286594, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.4195056980161836, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 1, "text_snippet": ", 2018), BERT is designed to pre- train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a re- sult, the pre-trained BERT model can be Ô¨Åne- tuned with just one a"}, {"rank": 2, "score": 0.6533799, "source": "dense+rrf", "rrf_score": 0.03125, "rerank_score": 0.32712269283153506, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 114, "text_snippet": "  E1 E2 EN...Figure 3: Differences in pre-training model architectures. BERT uses a bidirectional Transformer. OpenAI GPT uses a left-to-right Transformer. ELMo uses the concatenation of independently trained left-to-right and right-to- lef"}, {"rank": 3, "score": 0.6357291, "source": "dense+rrf", "rrf_score": 0.030309988518943745, "rerank_score": 0.2862511164700949, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 9, "text_snippet": "ng, the MLM ob- jective enables the representation to fuse the left and the right context, which allows us to pre- train a deep bidirectional Transformer. In addi- tion to the masked language model, we also use a ‚Äúnext sentence prediction‚Äù "}, {"rank": 4, "score": 0.67000735, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.2839268031509382, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 0, "text_snippet": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova Google AI Language {jacobdevlin,mingweichang,kentonl,kristout }@google.com Abstract We introduce a ne"}, {"rank": 5, "score": 51.13594816631855, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.2647807213548499, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 476, "text_snippet": "heng, and Jingjing Liu. Uniter: Learning universal image-text representations. arXiv preprint arXiv:1909.11740 , 2019. [Cra17] Kate Crawford. The trouble with bias. NIPS 2017 Keynote , 2017. [DCLT18] Jacob Devlin, Ming-Wei Chang, Kenton Lee"}]}
{"case_index": 97, "query": "From 2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt, what exact term fills [BLANK] in: \"Document [1](Title: Asian Americans in science and [BLANK]) Prize in physics for discovery of the subatomic particle J/œà.\"?", "gold": "technology", "answer": "The exact term that fills **[BLANK]** is **‚Äútechnology‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context explicitly shows the title 'Asian Americans in science and technology', so 'technology' correctly fills the blank and is fully supported.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 46.614, "llm_ms": 1236.415, "top_contexts": [{"rank": 1, "score": 0.7025592, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.30040973172615515, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 29, "text_snippet": "ranked order, we explored randomly ordering the k‚àí1 distractor documents and mentioning that the documents are randomly ordered in the task description, but found the same trends. See Appendix C for more details.  Write a high-quality answe"}, {"rank": 2, "score": 29.326203917867208, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.2595741298925309, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 49, "text_snippet": "UIDs. The relevant key-value pair for answering the query is bolded here within the input context for clarity. indicate that extended-context models are not nec- essarily better than their non-extended counterparts at using their input cont"}, {"rank": 3, "score": 44.682453898816405, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.25843598106150795, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 207, "text_snippet": "‚Äù arXiv preprint arXiv:2310.07554 , 2023. [98] N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni, and P. Liang, ‚ÄúLost in the middle: How language models use long contexts,‚Äù arXiv preprint arXiv:2307.03172 , 2023. [99] Y "}, {"rank": 4, "score": 0.5647736, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.2518292060573683, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 35, "text_snippet": " [1](Title: Asian Americans in science and  technology) ...  Document [2](Title: List of Nobel laureates in  Physics) ...  Document [3](Title: Scientist) ...  Document [4](Title: Norwegian Americans) ...  Document [5](Title: Maria Goeppert "}, {"rank": 5, "score": 26.15324657630917, "source": "dense+rrf", "rrf_score": 0.013888888888888888, "rerank_score": 0.24549299234262711, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 2, "text_snippet": "urs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts, even for explicitly long-context models. Our analysis provides a better understanding "}]}
{"case_index": 98, "query": "From 2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt, what exact term fills [BLANK] in: \"Scaling LLMs like GPT-4 [ 44] and PaLM-2 [ 1] to ever more parameters led to emergent abilities [ 63] unseen in smaller models (less than ‚àº10B parameters), most notably the [BLANK] ability to reason zero-shot [ 23].\"?", "gold": "remarkable", "answer": "The exact term that fills **[BLANK]** is **‚Äúremarkable‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'remarkable' and is directly supported by the provided context sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 53.993, "llm_ms": 1376.792, "top_contexts": [{"rank": 1, "score": 0.71367973, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.47353980791614053, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 12, "text_snippet": "humans and machines, enhancing user experience in existing applications like coding [3], web search [ 36], chatbots [ 45,56], customer service and content creation. This transformation brought by LLMs is also paving the way for new innovati"}, {"rank": 2, "score": 31.907737133457022, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.21229730211167888, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 1, "text_snippet": "n benchmarks like BigBench Hard and AGIEval. In Orca 2, we continue exploring how improved training signals can enhance smaller LMs‚Äô reasoning abilities. Research on training small LMs has often relied on imitation learning to replicate the"}, {"rank": 3, "score": 28.430119581574587, "source": "dense+rrf", "rrf_score": 0.014285714285714285, "rerank_score": 0.20868247218726166, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 75, "text_snippet": "any exemplars or CoT prompting. Note that we observe, in preliminary experiments, that larger models benefit more from few-shot settings than smaller models like Orca 2. We conduct evaluation only based on the zero-shot settings, we leave a"}, {"rank": 4, "score": 34.62173866742455, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.20133956500123243, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 0, "text_snippet": "Orca 2: Teaching Small Language Models How to Reason Arindam Mitra, Luciano Del Corro‚Ä†, Shweti Mahajan‚Ä†, Andres Codas‚Ä° Clarisse Simoes‚Ä°, Sahaj Agarwal, Xuxi Chen‚àó, Anastasia Razdaibiedina‚àó Erik Jones‚àó, Kriti Aggarwal‚àó, Hamid Palangi, Guoqin"}, {"rank": 5, "score": 0.66733825, "source": "dense+rrf", "rrf_score": 0.03149801587301587, "rerank_score": 0.20018268723477986, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 13, "text_snippet": "ity to reason zero-shot [ 23]. These abilities include answering complex questions, generating explanations, and solving multi-step problems, for instance, such as those on the US Medical Licensing exam, on which LLMs now achieve a passing "}]}
{"case_index": 99, "query": "From 2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt, what exact term fills [BLANK] in: \"remove all training documents with high similarity (0.8 or higher) to a [BLANK] or test set document.\"?", "gold": "validation", "answer": "The exact term is **‚Äúvalidation‚Äù** (i.e., ‚Äúremove all training documents with high similarity (0.8 or higher) to a **validation** or test set document.‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'validation' and is directly supported by context stating 'validation or test set document.'", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 50.367, "llm_ms": 1161.151, "top_contexts": [{"rank": 1, "score": 0.6367232, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3754541921256616, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 21, "text_snippet": "remove all training documents with high similarity (0.8 or higher) to a validation or test set document. Additionally, we remove all validation and test articles from Wikitext103 (Merity et al., 2017) from our Wikipedia training data. 2.2. "}, {"rank": 2, "score": 0.5892899, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.3002527643031909, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 20, "text_snippet": "f the same subsets as the training data, in proportion that matches the training sampling frequencies. During evaluation the retrieval database consists in the full union of these datasets, with the exception of books for which we use a sub"}, {"rank": 3, "score": 0.56419367, "source": "dense+rrf", "rrf_score": 0.031024531024531024, "rerank_score": 0.2827718282538237, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 78, "text_snippet": "struct the dataset with articles from the ‚Äúfuture‚Äù and manually remove new articles that strongly overlap documents in our training data. This guarantees that the evaluation documents are not leaked in our training data. 10  Improving langu"}, {"rank": 4, "score": 32.44795514915959, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.25975250224481633, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 69, "text_snippet": "e stem from their ability to memorise training data, their high training cost, the static nature of their training data (Lazaridou et al., 2021), their tendency of amplifying inherent biases in the training data, and their ability to genera"}, {"rank": 5, "score": 28.97712885403631, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.25304987181275973, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 113, "text_snippet": "formers ( R/e.sc/t.sc/r.sc/o.sc), a method for modelling arbitrary text se- quences whilstretrievingfromdatabaseswithtrillions oftokens‚Äîscalingthedataavailable to models by an order of magnitude compared to what is typically consumed during"}]}
{"case_index": 100, "query": "From 2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt, what exact term fills [BLANK] in: \"Note that Mcan be very large (e.g., 21 million passages in our [BLANK], de- scribed in Section 4.1) and kis usually small, such as20‚Äì100.\"?", "gold": "experiments", "answer": "The exact term is **‚Äúexperiments‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term and is directly supported by Context 1 ('21 million passages in our experiments').", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 47.366, "llm_ms": 1292.554, "top_contexts": [{"rank": 1, "score": 0.66080517, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.44299315752115714, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 18, "text_snippet": "evant to the input question for the reader at run-time. Note that Mcan be very large (e.g., 21 million passages in our experiments, de- scribed in Section 4.1) and kis usually small, such as20‚Äì100. 3.1 Overview Our dense passage retriever ("}, {"rank": 2, "score": 0.59808826, "source": "dense+rrf", "rrf_score": 0.03128054740957967, "rerank_score": 0.22061263014017862, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 17, "text_snippet": "rieval accuracy , which is the fraction of ques- tions for whichCFcontains a span that answers the question. 3 Dense Passage Retriever (DPR) We focus our research in this work on improv- ing the retrieval component in open-domain QA. Given "}, {"rank": 3, "score": 36.84989178551297, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.2184234041748519, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 1, "text_snippet": "where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented us- ingdense representations alone, where em- beddings are learned from a sma"}, {"rank": 4, "score": 0.6402319, "source": "dense+rrf", "rrf_score": 0.031054405392392875, "rerank_score": 0.20848110135109052, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 15, "text_snippet": "sic retrieval units3and getM total passages in our corpus C={p1,p2,...,p M}, where each passage pican be viewed as a sequence of tokensw(i) 1,w(i) 2,¬∑¬∑¬∑,w(i) |pi|. Given a question q, the task is to Ô¨Ånd a span w(i) s,w(i) s+1,¬∑¬∑¬∑,w(i) efrom"}, {"rank": 5, "score": 30.620263479847328, "source": "dense+rrf", "rrf_score": 0.014285714285714285, "rerank_score": 0.20190248346838677, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 152, "text_snippet": "1‚Äì1140, 2020. T. Hennigan, T. Cai, T. Norman, and I. Babuschkin. Haiku: Sonnet for JAX, 2020. URL http: //github.com/deepmind/dm-haiku . G. Izacard and E. Grave. Leveraging passage retrieval with generative models for open domain question a"}]}
{"case_index": 101, "query": "From 2005.14165__language_models_are_few_shot_learners.txt, what exact term fills [BLANK] in: \"Since in-context learning involves absorbing many skills and tasks within the [BLANK] of the model, it is plaus\"?", "gold": "parameters", "answer": "The exact term is **‚Äúparameters‚Äù** (i.e., ‚Äúwithin the **parameters** of the model‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'parameters' and is directly supported by the provided context snippet.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 73.267, "llm_ms": 1480.599, "top_contexts": [{"rank": 1, "score": 0.66146874, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.3477545688270677, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 25, "text_snippet": "skills and tasks within the parameters of the model, it is plausible that in-context learning abilities might show similarly strong gains with scale. 1In the context of language models this has sometimes been called ‚Äúzero-shot transfer‚Äù, bu"}, {"rank": 2, "score": 51.92012729802884, "source": "dense+rrf", "rrf_score": 0.01639344262295082, "rerank_score": 0.31627405861001295, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 24, "text_snippet": "8 ], to 1.5 billion parameters [ RWC+19], to 8 billion parameters [ SPP+19], 11 billion parameters [ RSR+19], and Ô¨Ånally 17 billion parameters [ Tur20 ]. Each increase has brought improvements in text synthesis and/or downstream NLP tasks, "}, {"rank": 3, "score": 0.53749484, "source": "dense+rrf", "rrf_score": 0.02886002886002886, "rerank_score": 0.27520213139382466, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 26, "text_snippet": " learning from zero examples. To avoid this confusion, we use the term ‚Äúmeta-learning‚Äù to capture the inner-loop / outer-loop structure of the general method, and the term ‚Äúin context-learning‚Äù to refer to the inner loop of meta-learning. W"}, {"rank": 4, "score": 25.072403976667164, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.2384232427408459, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 29, "text_snippet": " as well as several novel tasks designed to test rapid adaptation to tasks unlikely to be directly contained in the training set. For each task, we evaluate GPT-3 under 3 conditions: (a) ‚Äúfew-shot learning‚Äù, or in-context learning where we "}, {"rank": 5, "score": 21.75939256368654, "source": "dense+rrf", "rrf_score": 0.014285714285714285, "rerank_score": 0.2326668467427657, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 143, "text_snippet": "ostly simulate zero-shot settings. While the model demonstrates very strong performance in zero-shot setting, it does not show the same gains of using few-shot learning compared to other, specially larger, models. Synthetic data: As Orca 2 "}]}
{"case_index": 102, "query": "From 2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt, what exact term fills [BLANK] in: \"GPUs, compute speed has out-paced memory speed [ 61,62,63], and most operations in [BLANK] are bottlenecked by memory accesses [ 43].\"?", "gold": "transformers", "answer": "The exact term is **‚ÄúTransformers‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context explicitly states 'most operations in Transformers are bottlenecked by memory accesses'; expected answer 'transformers' matches (case-insensitive).", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 43.159, "llm_ms": 1124.225, "top_contexts": [{"rank": 1, "score": 0.6599, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.36995162542453197, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 10, "text_snippet": "mplementation of attention on GPT-2. FlashAttention does not read and write the large ùëÅ\u0002ùëÅattention matrix to HBM, resulting in an 7.6 \u0002 speedup on the attention computation. GPUs, compute speed has out-paced memory speed [ 61,62,63], and mo"}, {"rank": 2, "score": 0.6576372, "source": "dense+rrf", "rrf_score": 0.031746031746031744, "rerank_score": 0.23937931235061563, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 23, "text_snippet": "peed [ 61,62,63], operations are increasingly bottlenecked by memory (HBM) accesses. Thus exploiting fast SRAM becomes more important. Execution Model. GPUs have a massive number of threads to execute an operation (called a kernel). Each ke"}, {"rank": 3, "score": 0.5940644, "source": "dense+rrf", "rrf_score": 0.029273504273504274, "rerank_score": 0.23888480685611013, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 22, "text_snippet": "forms of memory of diÔ¨Äerent sizes and speeds, with smaller memory being faster. As an example, the A100 GPU has 40-80GB of high bandwidth memory (HBM) with bandwidth 1.5-2.0TB/s and 192KB of on-chip SRAM per each of 108 streaming multiproce"}, {"rank": 4, "score": 54.282533381794494, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.22470301565158735, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 100, "text_snippet": "n, Atri Rudra, and Christopher R√©. 2022. FlashAttention: Fast and memory-efficient exact attention with IO- awareness. ArXiv:2205.14135. Hermann Ebbinghaus. 1913. Memory: A contribu- tion to experimental psychology. H. A. Ruger & C. E. Buss"}, {"rank": 5, "score": 39.50654310766438, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.18896423907057275, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 84, "text_snippet": "mputer/RedPajama-Data . Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. CoRR , abs/2307.08691, 2023. Tri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra, and Christopher R ¬¥e. Flashattention: Fast an"}]}
{"case_index": 103, "query": "From 2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"n this way, although it comes with the [BLANK] of being sensitive to the design of the prompt.\"?", "gold": "limitation", "answer": "The exact term is **‚Äúlimitation‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context explicitly contains the phrase 'comes with the limitation of being sensitive to the design of the prompt.' Model answer matches expected term.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 54.149, "llm_ms": 857.075, "top_contexts": [{"rank": 1, "score": 0.568866, "source": "dense+rrf", "rrf_score": 0.029827662395050816, "rerank_score": 0.27771202067370704, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 17, "text_snippet": "n this way, although it comes with the limitation of being sensitive to the design of the prompt. Rather than scoring individual answers, some authors have also focused on using an LLM to select the best answer among a number of candidates "}, {"rank": 2, "score": 0.56654596, "source": "dense+rrf", "rrf_score": 0.02854251012145749, "rerank_score": 0.20675837800430114, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 16, "text_snippet": "ssive LM. This idea of using prompts was previously also considered by Yuan et al. (2021), although they used a smaller fine-tuned LM (i.e. BART) and did not observe a clear benefit from using prompts. An- other approach directly asks ChatG"}, {"rank": 3, "score": 22.517031670625194, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.20381667904542197, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 281, "text_snippet": " you always used to look like this , did n‚Äôt you , _‚Äù What is the word in the blank space (_)? The answer is Orca-v2-13B-Cautious The answer is izzy. The word in the blank space is the name of the girl who is crying. Figure 24: Demonstrativ"}, {"rank": 4, "score": 23.081880437832794, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.20079559601951624, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 44, "text_snippet": "ugmented LMs can also be formulated using our framework (subsection 2.3). In this section, we formally in- troduce three baseline categories based on when and what to retrieve. These baselines are not exact reproductions of the correspondin"}, {"rank": 5, "score": 31.090766375424387, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.19796027481881667, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 9, "text_snippet": "we present Ragas1, a framework for the automated assessment 1Ragas is available at https://github.com/ explodinggradients/ragas .arXiv:2309.15217v2 [cs.CL] 28 Apr 2025  of retrieval augmented generation systems. We fo- cus on settings where"}]}
{"case_index": 104, "query": "From 2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt, what exact term fills [BLANK] in: \"They often rely on small number of tasks or on using other models for auto-evaluation by asking [BLANK]‚Äúgivenresponsesfromsystem1 (reference)andsystem2(target), whichoneisbetter?‚Äù.\"?", "gold": "themtocomparetheoutputsoftwosystemswithapromptlike", "answer": "The exact term is **‚Äúthem‚Äù** (i.e., ‚Äúby asking **them** to compare the outputs of two systems‚Ä¶‚Äù).", "answer_score": 0.0, "answer_correctness": 0.0, "answer_groundedness": 0.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 0.0, "abstained": false, "should_abstain": false, "notes": "Blank requires the full phrase 'themtocomparetheoutputsoftwosystemswithapromptlike', not just 'them'. Answer is incomplete.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 62.259, "llm_ms": 1353.287, "top_contexts": [{"rank": 1, "score": 0.66709554, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.4034685277346391, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 18, "text_snippet": "roach a particular task. Rather than naively imitating powerful LLMs, we treat them as a reservoir of behaviors from which we carefully select those best suited for the task at hand. Some previous studies on training small models are limite"}, {"rank": 2, "score": 0.5770775, "source": "dense+rrf", "rrf_score": 0.03055037313432836, "rerank_score": 0.23573593769618525, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 1, "text_snippet": "n benchmarks like BigBench Hard and AGIEval. In Orca 2, we continue exploring how improved training signals can enhance smaller LMs‚Äô reasoning abilities. Research on training small LMs has often relied on imitation learning to replicate the"}, {"rank": 3, "score": 0.66101646, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.21626382422840637, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 19, "text_snippet": "ference)andsystem2(target), whichoneisbetter?‚Äù. However, previouswork[ 13,42,60,67] has demonstrated that this approach has several drawbacks. In this work, we provide a comprehensive evaluation comparing Orca 2 to several other models. We "}, {"rank": 4, "score": 22.390940819611348, "source": "dense+rrf", "rrf_score": 0.014084507042253521, "rerank_score": 0.21009698961046086, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 137, "text_snippet": "periment highlights the potential of specializing Orca 2 models for specific tasks using synthetic data generated with prompt erasing . 20  7 Limitations Orca 2, built upon the LLaMA 2 model family, retains many of its limitations, as well "}, {"rank": 5, "score": 30.586586302735824, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.1895911561459477, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 0, "text_snippet": "Orca 2: Teaching Small Language Models How to Reason Arindam Mitra, Luciano Del Corro‚Ä†, Shweti Mahajan‚Ä†, Andres Codas‚Ä° Clarisse Simoes‚Ä°, Sahaj Agarwal, Xuxi Chen‚àó, Anastasia Razdaibiedina‚àó Erik Jones‚àó, Kriti Aggarwal‚àó, Hamid Palangi, Guoqin"}]}
{"case_index": 105, "query": "From 2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt, what exact term fills [BLANK] in: \"We Ô¨Årst split each of the [BLANK] into text passages of equal lengths as the basic retrieval units3and getM total passages in our corpus C={p1,\"?", "gold": "documents", "answer": "The exact term that fills **[BLANK]** is **‚Äúdocuments‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'documents' and is directly supported by the provided context sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 48.375, "llm_ms": 966.115, "top_contexts": [{"rank": 1, "score": 0.5698712, "source": "dense+rrf", "rrf_score": 0.02889344262295082, "rerank_score": 0.34949197794496656, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 14, "text_snippet": "Who Ô¨Årst voiced Meg on Family Guy? ‚Äù or ‚Äú Where was the 8th Dalai Lama born? ‚Äù, a system is required to answer it using a large corpus of diversiÔ¨Åed topics. More speciÔ¨Åcally, we assumethe extractive QA setting, in which the answer is restri"}, {"rank": 2, "score": 0.69455755, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.29510052965594985, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 15, "text_snippet": "sic retrieval units3and getM total passages in our corpus C={p1,p2,...,p M}, where each passage pican be viewed as a sequence of tokensw(i) 1,w(i) 2,¬∑¬∑¬∑,w(i) |pi|. Given a question q, the task is to Ô¨Ånd a span w(i) s,w(i) s+1,¬∑¬∑¬∑,w(i) efrom"}, {"rank": 3, "score": 0.6372332, "source": "dense+rrf", "rrf_score": 0.031009615384615385, "rerank_score": 0.23404089782409157, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 32, "text_snippet": "ple, disjoint text blocks of 100 words as passages , serving as our basic retrieval units, following (Wang et al., 2019), which results in 21,015,324 passages in the end.5 Each passage is also prepended with the title of the Wikipedia artic"}, {"rank": 4, "score": 0.6021875, "source": "dense+rrf", "rrf_score": 0.029411764705882353, "rerank_score": 0.2295248108138161, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 31, "text_snippet": "a Pre-processing Following (Lee et al., 2019), we use the English Wikipedia dump from Dec. 20, 2018 as the source documents for answering questions. We Ô¨Årst apply the pre-processing code released in DrQA (Chen et al., 2017) to extract the c"}, {"rank": 5, "score": 0.64239395, "source": "dense+rrf", "rrf_score": 0.030776515151515152, "rerank_score": 0.2154146485425376, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 17, "text_snippet": "rieval accuracy , which is the fraction of ques- tions for whichCFcontains a span that answers the question. 3 Dense Passage Retriever (DPR) We focus our research in this work on improv- ing the retrieval component in open-domain QA. Given "}]}
{"case_index": 106, "query": "From 2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt, what exact term fills [BLANK] in: \"1 [BLANK] Transformer models [ 82] have emerged as the most widely used architecture in applications such as natural language processing and image classiÔ¨Åcation.\"?", "gold": "introduction", "answer": "The exact term is **‚ÄúIntroduction‚Äù** (i.e., ‚Äú**1 Introduction** Transformer models [82] have emerged‚Ä¶‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context explicitly shows the heading '1 Introduction' preceding the sentence; model answer matches expected term (case difference only).", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 47.001, "llm_ms": 1166.232, "top_contexts": [{"rank": 1, "score": 69.97348908458481, "source": "dense+rrf", "rrf_score": 0.01639344262295082, "rerank_score": 0.3687294247821033, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 4, "text_snippet": "T-2 and 6.4 points of lift on long-document classiÔ¨Åcation) and entirely new capabilities: the Ô¨Årst Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64"}, {"rank": 2, "score": 0.67913496, "source": "dense+rrf", "rrf_score": 0.032018442622950824, "rerank_score": 0.2902336385993442, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 134, "text_snippet": " well as the (dense) FlashAttention on the Long-range Arena tasks. EÔ¨Écient Transformer. Transformer-based models have become the most widely-used architecture in natural language processing [ 22] and computer vision [ 24,91]. However, one o"}, {"rank": 3, "score": 55.544890396986546, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.2291774186479612, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 100, "text_snippet": "n, Atri Rudra, and Christopher R√©. 2022. FlashAttention: Fast and memory-efficient exact attention with IO- awareness. ArXiv:2205.14135. Hermann Ebbinghaus. 1913. Memory: A contribu- tion to experimental psychology. H. A. Ruger & C. E. Buss"}, {"rank": 4, "score": 36.53787558929573, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.20643785741901385, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 10, "text_snippet": "mplementation of attention on GPT-2. FlashAttention does not read and write the large ùëÅ\u0002ùëÅattention matrix to HBM, resulting in an 7.6 \u0002 speedup on the attention computation. GPUs, compute speed has out-paced memory speed [ 61,62,63], and mo"}, {"rank": 5, "score": 0.624174, "source": "dense+rrf", "rrf_score": 0.03125763125763126, "rerank_score": 0.20529686686277016, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 106, "text_snippet": "o solve math word problems. arXiv preprint arXiv:2110.14168 . Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Car- bonell, Quoc V Le, and Ruslan Salakhutdinov. 2019. Transformer-xl: Attentive language mod- els beyond a Ô¨Åxed-length context. arXi"}]}
{"case_index": 107, "query": "From 2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt, what exact term fills [BLANK] in: \"[BLANK] Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases.\"?", "gold": "retrieval-augmented", "answer": "The exact term is **‚ÄúRetrieval-Augmented‚Äù**, i.e., **‚ÄúRetrieval-Augmented Generation (RAG)‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "The blank is 'Retrieval-Augmented' as shown in the provided context ('Retrieval-Augmented Generation (RAG) has emerged...'). Matches expected 'retrieval-augmented' aside from capitalization.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 46.617, "llm_ms": 1396.819, "top_contexts": [{"rank": 1, "score": 0.72982454, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.3475720015275476, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 1, "text_snippet": "dels (LLMs) showcase impres- sive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution b"}, {"rank": 2, "score": 0.7186743, "source": "dense+rrf", "rrf_score": 0.031746031746031744, "rerank_score": 0.2210472223978784, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 5, "text_snippet": "Generation (RAG) enhances LLMs by retrieving relevant document chunks from external knowledge base through semantic similarity calcu- lation. By referencing external knowledge, RAG effectively reduces the problem of generating factually inc"}, {"rank": 3, "score": 26.970126510987633, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.20865199264394457, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 238, "text_snippet": " maximizing llm performance,‚Äù https://community.openai. com/t/openai-dev-day-2023-breakout-sessions/505213# a-survey-of-techniques-for-maximizing-llm-performance-2, 2023. [167] J. Chen, H. Lin, X. Han, and L. Sun, ‚ÄúBenchmarking large lan- g"}, {"rank": 4, "score": 28.099888558184254, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.20549615381734232, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 239, "text_snippet": " against external counterfactual knowledge,‚Äù arXiv preprint arXiv:2311.08147 , 2023. [169] Y . Lyu, Z. Li, S. Niu, F. Xiong, B. Tang, W. Wang, H. Wu, H. Liu, T. Xu, and E. Chen, ‚ÄúCrud-rag: A comprehensive chinese benchmark for retrieval-aug"}, {"rank": 5, "score": 0.7595459, "source": "dense+rrf", "rrf_score": 0.030886196246139225, "rerank_score": 0.1979625598433769, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 4, "text_snippet": "l, retrieval-augmented gen- eration, natural language processing, information retrieval I. I NTRODUCTION LARGE language models (LLMs) have achieved remark- able success, though they still face significant limitations, especially in domain-s"}]}
{"case_index": 108, "query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"They also, [BLANK], struggle with basic functionality, such as arithmetic or fac- tual lookup, where much simpler and smaller models\"?", "gold": "paradoxically", "answer": "The exact term that fills **[BLANK]** is **‚Äúparadoxically‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'paradoxically' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 52.469, "llm_ms": 1189.756, "top_contexts": [{"rank": 1, "score": 0.6743913, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.44988277331267185, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 0, "text_snippet": "Toolformer: Language Models Can Teach Themselves to Use Tools Timo Schick Jane Dwivedi-Yu Roberto Dess√¨‚Ä†Roberta Raileanu Maria Lomeli Luke Zettlemoyer Nicola Cancedda Thomas Scialom Meta AI Research‚Ä†Universitat Pompeu Fabra Abstract Languag"}, {"rank": 2, "score": 0.5887572, "source": "dense+rrf", "rrf_score": 0.031754032258064516, "rerank_score": 0.3050222533705136, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 1, "text_snippet": "metic or fac- tual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer , a model traine"}, {"rank": 3, "score": 45.62711449889626, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.22208559052113808, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 212, "text_snippet": "line decision making: Benchmarks and additional opinions,‚Äù arXiv preprint arXiv:2306.02224 , 2023. [108] T. Schick, J. Dwivedi-Yu, R. Dess `ƒ±, R. Raileanu, M. Lomeli, L. Zettle- moyer, N. Cancedda, and T. Scialom, ‚ÄúToolformer: Language mode"}, {"rank": 4, "score": 28.573249116856278, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.20452621018625258, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 94, "text_snippet": "e are no examples of chained tool use in the Ô¨Ånetuning dataset. Our current approach also does not allow the LM to use a tool in an in- teractive way ‚Äì especially for tools such as search engines, that could potentially return hundreds of d"}, {"rank": 5, "score": 24.60187786558854, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.20177950159096814, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 14, "text_snippet": "r at using their input context (¬ß2.3). Given that language models struggle to retrieve and use relevant information in the multi-document question answering task, to what extent can lan- guage models even retrieve from their input con- text"}]}
{"case_index": 109, "query": "From 2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt, what exact term fills [BLANK] in: \"OurmethodÔ¨Årst [BLANK] a key-value database, where values store raw chunks of text tokens and keys\"?", "gold": "constructs", "answer": "The exact term is **‚Äúconstructs‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term and is directly supported by Context 1 ('Our method first constructs a key-value database...').", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 59.345, "llm_ms": 1074.344, "top_contexts": [{"rank": 1, "score": 0.5911414, "source": "dense+rrf", "rrf_score": 0.032266458495966696, "rerank_score": 0.339850721434098, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 14, "text_snippet": "show that the performance of R/e.sc/t.sc/r.sc/o.sc comes from both explicit neighbour copying and general knowledge extraction (¬ß4.4). 2. Method Wedesignourretrieval-enhancedarchitecturetobecapableofretrievingfromadatabasewithtrillions of t"}, {"rank": 2, "score": 0.58927524, "source": "dense+rrf", "rrf_score": 0.031009615384615385, "rerank_score": 0.2684459503197912, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 22, "text_snippet": " ùïç=¬ª1¬îùë£¬º, obtained using a text tokenizer1. Wespliteach ùëõ-token-longexample ùëã=¬πùë•1¬î¬ì¬ì¬ì¬îùë•ùëõ¬∫intoasequenceof ùëôchunks¬πùê∂1¬î¬ì¬ì¬ì¬îùê∂ùëô¬∫ of sizeùëö=ùëõ ùëô, i.e.ùê∂1,¬πùë•1¬î¬ì¬ì¬ì¬îùë•ùëö¬∫¬î ¬ì¬ì¬ì¬î ùê∂ùëô,¬πùë•ùëõ\u0000ùëö¬∏1¬î¬ì¬ì¬ì¬îùë•ùëõ¬∫2ùïçùëö. We useùëõ=2048andùëö=64. We augment each chunk ùê∂ùë¢with a se"}, {"rank": 3, "score": 29.489422242513612, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.2504724480421518, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 242, "text_snippet": "37  Improving language models by retrieving from trillions of tokens Table 16jGreat Circle (novel) , from Wikipedia September 21. The article is about a recent novel and chunks ùê∂3andùê∂4are speciÔ¨Åcally about its reception. The name Publishers"}, {"rank": 4, "score": 0.6379938, "source": "dense+rrf", "rrf_score": 0.031544957774465976, "rerank_score": 0.24565839220845176, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 25, "text_snippet": "irectly comparable with the largest language models that are evaluated by sampling. 2.3. Nearest neighbour retrieval Retrieval neighbours. Our database consists of a key-value memory. Each value consists of two contiguous chunks of tokens w"}, {"rank": 5, "score": 0.5639472, "source": "dense+rrf", "rrf_score": 0.02919863597612958, "rerank_score": 0.24518912784878447, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 113, "text_snippet": "formers ( R/e.sc/t.sc/r.sc/o.sc), a method for modelling arbitrary text se- quences whilstretrievingfromdatabaseswithtrillions oftokens‚Äîscalingthedataavailable to models by an order of magnitude compared to what is typically consumed during"}]}
{"case_index": 110, "query": "From 1706.03762__attention_is_all_you_need.txt, what exact term fills [BLANK] in: \"Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer [BLANK].\"?", "gold": "normalization", "answer": "The exact term is **‚Äúnormalization‚Äù** (i.e., ‚Äúfollowed by layer normalization‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context explicitly states 'followed by layer normalization'; model answer correctly fills blank with 'normalization'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 70.705, "llm_ms": 1664.833, "top_contexts": [{"rank": 1, "score": 0.61504054, "source": "dense+rrf", "rrf_score": 0.032266458495966696, "rerank_score": 0.388729837368355, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 18, "text_snippet": "s also composed of a stack of N= 6identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the enc"}, {"rank": 2, "score": 0.64789915, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.3389046223579291, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 17, "text_snippet": "ted feed-forward network. We employ a residual connection [ 11] around each of the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is LayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function"}, {"rank": 3, "score": 0.56105155, "source": "dense+rrf", "rrf_score": 0.03055037313432836, "rerank_score": 0.2557180622102658, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 29, "text_snippet": "n layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We"}, {"rank": 4, "score": 0.5744031, "source": "dense+rrf", "rrf_score": 0.03125763125763126, "rerank_score": 0.25151817418254113, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 16, "text_snippet": "ransformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively. 3.1 Encoder and Decoder Stacks "}, {"rank": 5, "score": 0.5728331, "source": "dense+rrf", "rrf_score": 0.030536130536130537, "rerank_score": 0.2443805849797279, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 28, "text_snippet": "the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9]. ‚Ä¢The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries c"}]}
{"case_index": 111, "query": "From 2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt, what exact term fills [BLANK] in: \"We note that instruction tuning, while very beneficial for teaching the model how to solve a task, does not [BLANK] teach the model new knowledge.\"?", "gold": "necessarily", "answer": "The exact term that fills **[BLANK]** is **‚Äúnecessarily‚Äù**:\n\n‚Äú‚Ä¶does not **necessarily** teach the model new knowledge.‚Äù", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'necessarily' and is directly supported by the provided context sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 61.897, "llm_ms": 1334.194, "top_contexts": [{"rank": 1, "score": 0.7019956, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.4254112853408671, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 24, "text_snippet": "not result in proportional improvement to small model performance when thoroughly evaluated on knowledge-intensive or reasoning-intensive tasks where correctness is not just judged by style. We note that instruction tuning, while very benef"}, {"rank": 2, "score": 63.84639528051362, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.367886222684738, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 144, "text_snippet": "ng training and safety guardrails (e.g., content filter) within the Azure OpenAI API. However, detailed studies are required for better quantification of such risks. Small Model Capacity: We note that post-training, while significantly bene"}, {"rank": 3, "score": 33.565405716298145, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.2711915013066795, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 2, "text_snippet": "asks, potentially different from the one used by the larger model. For example, while larger models might provide a direct answer to a complex task, smaller models may not have the same capacity. In Orca 2, we teach the model various reason"}, {"rank": 4, "score": 32.28974095913887, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.2586855167823006, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 148, "text_snippet": "synthetic data for post-training. While Orca 2 models have not gone through RLHF training for safety, we believe that the use of synthetic data for post-training that has been filtered with various content safety filters could provide anoth"}, {"rank": 5, "score": 29.306031024762145, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.2522741984686563, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 128, "text_snippet": " Evaluation results for HHH dataset. system message while assessing chat model‚Äôs safety. Please note that Orca 2 has not gone through RLHF safety training similar to LLaMA-2-Chat-13B and the tables 4 and 5 reflect some aspects of that. We h"}]}
{"case_index": 112, "query": "From 2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt, what exact term fills [BLANK] in: \"By meticulously curating and optimizing the training dataset, researchers can [BLANK] reduce the model‚Äôs size without compromising its performance.\"?", "gold": "significantly", "answer": "The exact term that fills **[BLANK]** is **‚Äúsignificantly‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'significantly' and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 92.425, "llm_ms": 1235.215, "top_contexts": [{"rank": 1, "score": 0.6710869, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.4114455421386301, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 8, "text_snippet": "ved solely by changing the training data. phi-3-mini: The development of a compact language model that rivals the capabilities of ChatGPT, while fitting on a phone, is a testament to the power of data-driven machine learning. By meticulousl"}, {"rank": 2, "score": 0.5963985, "source": "dense+rrf", "rrf_score": 0.031754032258064516, "rerank_score": 0.2936038059386201, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 0, "text_snippet": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone Microsoft Abstract We introduce phi-3-mini , a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both"}, {"rank": 3, "score": 0.57529646, "source": "dense+rrf", "rrf_score": 0.030090497737556562, "rerank_score": 0.25431899228318205, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 17, "text_snippet": " running locally on a cell-phone. Thanks to its small size, phi- 3-mini can be quantized to 4-bits so that it only occupies ‚âà1.8GB of memory. We tested the quantized model by deploying phi-3-mini on iPhone 14 with A16 Bionic chip running na"}, {"rank": 4, "score": 0.6134456, "source": "dense+rrf", "rrf_score": 0.03149801587301587, "rerank_score": 0.23999456104346975, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 6, "text_snippet": "7B parameters), matched the performance of models 25 times larger trained on regular data. In this report we present a new model, phi-3-mini (3.8B parameters), trained for 3.3T tokens on larger and more advanced versions of the datasets use"}, {"rank": 5, "score": 24.664300627324398, "source": "dense+rrf", "rrf_score": 0.014084507042253521, "rerank_score": 0.23998772592107445, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 25, "text_snippet": "DPO data covers chat format data, reasoning, and responsible AI (RAI) efforts. We use DPO to steer the model away from unwanted behavior, by using those outputs as ‚Äúrejected‚Äù responses. Besides improvement in math, coding, reasoning, robust"}]}
{"case_index": 113, "query": "From 1706.03762__attention_is_all_you_need.txt, what exact term fills [BLANK] in: \"3.2.1 Scaled [BLANK] Attention We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2).\"?", "gold": "dot-product", "answer": "The exact term is **‚ÄúDot-Product‚Äù**, i.e., **‚Äú3.2.1 Scaled Dot-Product Attention.‚Äù**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "The blank is 'dot-product' as shown in the context ('3.2.1 Scaled Dot-Product Attention'). Answer matches expected term.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 35.866, "llm_ms": 1222.78, "top_contexts": [{"rank": 1, "score": 0.6994212, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3141289974225821, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 20, "text_snippet": "t Attention  Multi-Head Attention Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel. of the values, where the weight assigned to each value is computed by a "}, {"rank": 2, "score": 32.16968799673107, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.23384847053042893, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 29, "text_snippet": "n layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We"}, {"rank": 3, "score": 0.62217045, "source": "dense+rrf", "rrf_score": 0.031746031746031744, "rerank_score": 0.18149073829698928, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 19, "text_snippet": "sequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position ican depend only on the known outputs at positions less than i. 3.2 Attention An attention "}, {"rank": 4, "score": 0.5803268, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.17218465978192618, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 69, "text_snippet": " dilate (Ding et al., 2023), block sparse (Qiu et al., 2020), stride sparse (Child et al., 2019) for comparison. ‚Äò cro. heads / layers ‚Äô means to swap different attention settings across attention heads or sequential layers . Taking S2-Attn"}, {"rank": 5, "score": 0.6136892, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.1718956031378544, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 37, "text_snippet": "dimension. 3.2 S HIFTED SPARSE ATTENTION Standard self-attention costs O(n2)computations, making LLMs on long sequences high memory cost and slow. To avoid this issue during training, we propose Shifted Sparse Attention (S2-Attn), as shown "}]}
{"case_index": 114, "query": "From 1706.03762__attention_is_all_you_need.txt, what exact term fills [BLANK] in: \"Numerous efforts have since continued to push the boundaries of recurrent language models and [BLANK] architectures [38, 24, 15].\"?", "gold": "encoder-decoder", "answer": "The exact term is **‚Äúencoder-decoder‚Äù**, as in: ‚Äúrecurrent language models and **encoder-decoder** architectures [38, 24, 15].‚Äù", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'encoder-decoder' and is directly supported by the provided context sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 65.282, "llm_ms": 1354.29, "top_contexts": [{"rank": 1, "score": 0.6953205, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3481519740305486, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 7, "text_snippet": " [ 35,2,5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15]. Recurrent models typically factor computation along the symbol positions of the input and "}, {"rank": 2, "score": 0.62040615, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.26353014823950055, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 6, "text_snippet": "performed while at Google Research. 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7 [cs.CL] 2 Aug 2023  1 Introduction Recurrent neural networks, long short-term memory [ 13] and "}, {"rank": 3, "score": 0.6178576, "source": "dense+rrf", "rrf_score": 0.031024531024531024, "rerank_score": 0.23795109000582906, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 4, "text_snippet": "ural networks have proven to be powerful language models, Ô¨Årst in the form of recurrent architectures (Graves, 2013; Jozefowicz et al., 2016; Mikolov et al., 2010) and more recently in the form of Transformers (Vaswani et al., 2017), that u"}, {"rank": 4, "score": 23.76223058869832, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.19461042155126346, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 281, "text_snippet": " you always used to look like this , did n‚Äôt you , _‚Äù What is the word in the blank space (_)? The answer is Orca-v2-13B-Cautious The answer is izzy. The word in the blank space is the name of the girl who is crying. Figure 24: Demonstrativ"}, {"rank": 5, "score": 0.6183656, "source": "dense+rrf", "rrf_score": 0.03076923076923077, "rerank_score": 0.18901397450683974, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 11, "text_snippet": "for downstream transfer. First, single-layer representations were learned using word vectors [ MCCD13 ,PSM14 ] and fed to task-speciÔ¨Åc architectures, then RNNs with multiple layers of representations and contextual state were used to form s"}]}
{"case_index": 115, "query": "From 2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"explain how these three quality aspects can be measured in a fully automated way, by [BLANK] an LLM.\"?", "gold": "prompting", "answer": "The exact term is **‚Äúprompting‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'prompting' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 42.734, "llm_ms": 976.218, "top_contexts": [{"rank": 1, "score": 0.6473378, "source": "dense+rrf", "rrf_score": 0.03177805800756621, "rerank_score": 0.3298330932898308, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 23, "text_snippet": " explain how these three quality aspects can be measured in a fully automated way, by prompting an LLM. In our implementation and experiments, all prompts are evaluated using the gpt-3.5-turbo-16k model, which is available through the OpenA"}, {"rank": 2, "score": 0.65914255, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.2254894326282775, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 237, "text_snippet": "-auto-eval-best-practices-RAG, 2023. [164] S. Es, J. James, L. Espinosa-Anke, and S. Schockaert, ‚ÄúRagas: Au- tomated evaluation of retrieval augmented generation,‚Äù arXiv preprint arXiv:2309.15217 , 2023. [165] J. Saad-Falcon, O. Khattab, C."}, {"rank": 3, "score": 32.096704020887756, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.2168392965602377, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 22, "text_snippet": "lly, Context Relevance refers to the idea that the retrieved context should be focused, containing as little irrelevant informa- tion as possible. This is important given the cost associated with feeding long context passages to LLMs. Moreo"}, {"rank": 4, "score": 0.635826, "source": "dense+rrf", "rrf_score": 0.029211087420042643, "rerank_score": 0.21212646673576188, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 128, "text_snippet": "Evaluation Aspects Contemporary evaluation practices of RAG models empha- size three primary quality scores and four essential abilities, which collectively inform the evaluation of the two principal targets of the RAG model: retrieval and "}, {"rank": 5, "score": 0.6431828, "source": "dense+rrf", "rrf_score": 0.029211087420042643, "rerank_score": 0.2058422174840035, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 127, "text_snippet": " on the content‚Äôs objectives: unlabeled and labeled content. For unlabeled content, the evaluation encompasses the faithfulness, relevance, and non-harmfulness of the generated answers. In contrast, for labeled content, the focus is on the "}]}
{"case_index": 116, "query": "From 2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt, what exact term fills [BLANK] in: \"For the tasks where reasoning is of primary importance (Figure 1(1)), we alternate the generation of thoughts and actions so that the task-solving trajectory consists of multiple [BLANK] steps.\"?", "gold": "thought-action-observation", "answer": "The exact term is **‚Äúthought-action-observation‚Äù**, i.e., the trajectory consists of multiple **thought-action-observation steps**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term exactly and is directly supported by the provided context quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 57.191, "llm_ms": 1295.926, "top_contexts": [{"rank": 1, "score": 0.6656178, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.42764106874220814, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 31, "text_snippet": "xt example is a human trajectory of actions, thoughts, and environment observations to solve a task instance (see Appendix C). For the tasks where reasoning is of primary importance (Figure 1(1)), we alternate the generation of thoughts and"}, {"rank": 2, "score": 0.6179539, "source": "dense+rrf", "rrf_score": 0.03021353930031804, "rerank_score": 0.29536976315827285, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 40, "text_snippet": " Wikipedia, and force models to retrieve via explicit reasoning in language. 3.2 M ETHODS ReAct Prompting For HotpotQA and Fever, we randomly select 6 and 3 cases2from the training set and manually compose ReAct -format trajectories to use "}, {"rank": 3, "score": 0.602576, "source": "dense+rrf", "rrf_score": 0.027809742999616416, "rerank_score": 0.27648043338238043, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 24, "text_snippet": "o understand the decision basis of model actions. To summarize, our key contributions are the following: (1) we introduce ReAct , a novel prompt- based paradigm to synergize reasoning and acting in language models for general task solving; "}, {"rank": 4, "score": 0.66045225, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.27446691093289555, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 19, "text_snippet": "udies on how reasoning and acting can be combined in a synergistic manner for general task solving, and if such a combination can bring systematic beneÔ¨Åts compared to reasoning or acting alone. In this work, we present ReAct , a general par"}, {"rank": 5, "score": 0.6116344, "source": "dense+rrf", "rrf_score": 0.0293236301369863, "rerank_score": 0.26663282223549817, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 30, "text_snippet": " Thought 3), and so on. However, as the language space Lis unlimited, learning in this augmented action space is difÔ¨Åcult and requires strong language priors. In this paper, we mainly focus on the setup where a frozen large language model, "}]}
{"case_index": 117, "query": "From 2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt, what exact term fills [BLANK] in: \"[BLANK] During inference time, we apply the passage encoder EPto all the passages and index them using FAISS (Johnson et al., 2017) ofÔ¨Çine.\"?", "gold": "inference", "answer": "**Inference**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "The blank is filled by the term 'Inference', as shown in Context 1 ('Inference During inference time...').", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 62.93, "llm_ms": 1017.323, "top_contexts": [{"rank": 1, "score": 0.72909117, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.39373938839514194, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 23, "text_snippet": "BERT (Devlin et al., 2019) networks (base, un- cased) and take the representation at the [CLS] token as the output, so d= 768 . Inference During inference time, we apply the passage encoder EPto all the passages and index them using FAISS ("}, {"rank": 2, "score": 0.6404074, "source": "dense+rrf", "rrf_score": 0.03125763125763126, "rerank_score": 0.26047812538718756, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 17, "text_snippet": "rieval accuracy , which is the fraction of ques- tions for whichCFcontains a span that answers the question. 3 Dense Passage Retriever (DPR) We focus our research in this work on improv- ing the retrieval component in open-domain QA. Given "}, {"rank": 3, "score": 0.60226, "source": "dense+rrf", "rrf_score": 0.028958333333333336, "rerank_score": 0.2371176394531583, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 81, "text_snippet": "the dual-encoder framework, they introduced a late-interaction operator on top of the BERT encoders. Dense retrieval for open-domain QA has been explored by Das et al. (2019), who propose to re- trieve relevant passages iteratively using re"}, {"rank": 4, "score": 0.62692297, "source": "dense+rrf", "rrf_score": 0.027583600982429624, "rerank_score": 0.22461971704956857, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 19, "text_snippet": "t passage are func- tions of both the retriever and reader. We also experimented with natural paragraphs in our preliminary trials and found that using Ô¨Åxed-length passages performs better in both retrieval and Ô¨Ånal QA accuracy, as observed"}, {"rank": 5, "score": 0.6128914, "source": "dense+rrf", "rrf_score": 0.02946912242686891, "rerank_score": 0.22337980295194243, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 116, "text_snippet": "he salient phrase ‚ÄúThoros of Myr‚Äù is critical, and DPR is unable to capture it. D Joint Training of Retriever and Reader We Ô¨Åx the passage encoder in our joint-training scheme while allowing only the question encoder to receive backpropagat"}]}
{"case_index": 118, "query": "From 2302.13971__llama_open_and_efficient_foundation_language_models.txt, what exact term fills [BLANK] in: \"We train large [BLANK] on a large quantity of textual data using a standard optimizer.\"?", "gold": "transformers", "answer": "The exact term is **‚Äútransformers‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context explicitly states: 'We train large transformers on a large quantity of textual data using a standard optimizer.' Model answer matches expected term.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 65.892, "llm_ms": 1222.193, "top_contexts": [{"rank": 1, "score": 0.57100403, "source": "dense+rrf", "rrf_score": 0.030679156908665108, "rerank_score": 0.2641017576164592, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 9, "text_snippet": "v1 [cs.CL] 27 Feb 2023  2 Approach Our training approach is similar to the methods described in previous work (Brown et al., 2020; Chowdhery et al., 2022), and is inspired by the Chinchilla scaling laws (Hoffmann et al., 2022). We train lar"}, {"rank": 2, "score": 0.58109593, "source": "dense+rrf", "rrf_score": 0.030309988518943745, "rerank_score": 0.2385549513889294, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 0, "text_snippet": "LLaMA: Open and EfÔ¨Åcient Foundation Language Models Hugo Touvron‚àó, Thibaut Lavril‚àó, Gautier Izacard‚àó, Xavier Martinet Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi√®re, Naman Goyal Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand "}, {"rank": 3, "score": 24.399793314353076, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.2257755822830453, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 29, "text_snippet": "t results on a total of 20 benchmarks: ‚Ä¢Zero-shot. We provide a textual description of the task and a test example. The model either provides an answer using open-ended generation, or ranks the proposed answers. ‚Ä¢Few-shot. We provide a few "}, {"rank": 4, "score": 20.300094092714946, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.22324310849253132, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 137, "text_snippet": "periment highlights the potential of specializing Orca 2 models for specific tasks using synthetic data generated with prompt erasing . 20  7 Limitations Orca 2, built upon the LLaMA 2 model family, retains many of its limitations, as well "}, {"rank": 5, "score": 0.5766657, "source": "dense+rrf", "rrf_score": 0.028594771241830064, "rerank_score": 0.20388498785722137, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 1, "text_snippet": "ow that it is possible to train state-of-the-art models using publicly avail- able datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and L"}]}
{"case_index": 119, "query": "From 2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt, what exact term fills [BLANK] in: \"If language models can robustly use information within long input con- texts, then their [BLANK] should be minimally affected by the position of the relevant information in the input context.\"?", "gold": "performance", "answer": "The exact term that fills **[BLANK]** is **‚Äúperformance‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'performance' and is directly supported by the provided context sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 60.308, "llm_ms": 1219.192, "top_contexts": [{"rank": 1, "score": 0.6854856, "source": "dense+rrf", "rrf_score": 0.032266458495966696, "rerank_score": 0.4607641421234377, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 9, "text_snippet": "at require accessing and using information within an input context. In particular, our experi- ments make controlled changes to the input context size and the position of the relevant information within the input context and study their eff"}, {"rank": 2, "score": 0.651479, "source": "dense+rrf", "rrf_score": 0.031054405392392875, "rerank_score": 0.3580481640434955, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 21, "text_snippet": "eir input context and introduces new evaluation protocols for future long- context models; to claim that a language model can robustly use information within long input con- texts, it is necessary to show that its performance is minimally a"}, {"rank": 3, "score": 0.64244455, "source": "dense+rrf", "rrf_score": 0.030158730158730156, "rerank_score": 0.3181913350604225, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 49, "text_snippet": "UIDs. The relevant key-value pair for answering the query is bolded here within the input context for clarity. indicate that extended-context models are not nec- essarily better than their non-extended counterparts at using their input cont"}, {"rank": 4, "score": 0.6407377, "source": "dense+rrf", "rrf_score": 0.029236022193768675, "rerank_score": 0.2994993437930661, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 11, "text_snippet": "r of documents in the input context (akin to retrieving more or less documents in retrieval-augmented generation), and (ii) control the position of the relevant information within the input context by changing the order of the documents to "}, {"rank": 5, "score": 40.102376211454335, "source": "dense+rrf", "rrf_score": 0.013888888888888888, "rerank_score": 0.28073427834428993, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 14, "text_snippet": "r at using their input context (¬ß2.3). Given that language models struggle to retrieve and use relevant information in the multi-document question answering task, to what extent can lan- guage models even retrieve from their input con- text"}]}
{"case_index": 120, "query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \"Model Architecture BERT‚Äôs model architec- ture is a multi-layer bidirectional Transformer en- coder based on the original [BLANK] de- scribed in Vaswani et al.\"?", "gold": "implementation", "answer": "The exact term is **‚Äúimplementation‚Äù** (as in ‚Äúbased on the original implementation described in Vaswani et al.‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'implementation' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 59.264, "llm_ms": 1410.94, "top_contexts": [{"rank": 1, "score": 0.7289696, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.4368203550169144, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 24, "text_snippet": "architec- ture and the Ô¨Ånal downstream architecture. Model Architecture BERT‚Äôs model architec- ture is a multi-layer bidirectional Transformer en- coder based on the original implementation de- scribed in Vaswani et al. (2017) and released "}, {"rank": 2, "score": 0.59698045, "source": "dense+rrf", "rrf_score": 0.03128054740957967, "rerank_score": 0.27338694824340865, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 0, "text_snippet": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova Google AI Language {jacobdevlin,mingweichang,kentonl,kristout }@google.com Abstract We introduce a ne"}, {"rank": 3, "score": 0.55484, "source": "dense+rrf", "rrf_score": 0.027242401779095628, "rerank_score": 0.2679332109782336, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 10, "text_snippet": "ford et al. (2018), which uses unidirec- tional language models for pre-training, BERT uses masked language models to enable pre- trained deep bidirectional representations. This is also in contrast to Peters et al. (2018a), which uses a sh"}, {"rank": 4, "score": 0.60613847, "source": "dense+rrf", "rrf_score": 0.0293236301369863, "rerank_score": 0.2632901181114782, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 114, "text_snippet": "  E1 E2 EN...Figure 3: Differences in pre-training model architectures. BERT uses a bidirectional Transformer. OpenAI GPT uses a left-to-right Transformer. ELMo uses the concatenation of independently trained left-to-right and right-to- lef"}, {"rank": 5, "score": 40.62366295726071, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.24724997295624937, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 19, "text_snippet": "1.5e‚àí44M 1.4T Table 2: Model sizes, architectures, and optimization hyper-parameters. Overall, our entire training dataset contains roughly 1.4T tokens after tokenization. For most of our training data, each token is used only once dur- ing"}]}
