{"query": "From 2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt, what exact term fills [BLANK] in: \"In additional experiments, we show [BLANK] can robustly boost performance on NLP tasks where adding a chain-of-thought might hurt performance compared to standard prompting (Ye & Durrett, 2022).\"?", "gold": "self-consistency", "relevant_doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "relevant_chunk_id": 15, "relevant_text": "In additional experiments, we show self-consistency can robustly boost performance on NLP tasks where adding a chain-of-thought might hurt performance compared to standard prompting (Ye & Durrett, 2022).", "case_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models_017", "slice": "natural_answerable_20docs"}
{"query": "From 2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt, what exact term fills [BLANK] in: \"As compute has gotten faster relative to memory speed [ 61,62,63], operations are [BLANK] bottlenecked by me\"?", "gold": "increasingly", "relevant_doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "relevant_chunk_id": 22, "relevant_text": "As compute has gotten faster relative to memory speed [ 61,62,63], operations are increasingly bottlenecked by me", "case_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness_014", "slice": "natural_answerable_20docs"}
{"query": "From 2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"Evaluating RAG [BLANK] is, however, chal- lenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability\"?", "gold": "architectures", "relevant_doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "relevant_chunk_id": 1, "relevant_text": "Evaluating RAG architectures is, however, chal- lenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability", "case_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation_013", "slice": "natural_answerable_20docs"}
{"query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \"More recently, sentence or document encoders which produce contextual token [BLANK] have been pre-trained from unlabeled text and ﬁne-tuned for a supervised downstream task (Dai and Le, 2015; Howard and R\"?", "gold": "representations", "relevant_doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "relevant_chunk_id": 17, "relevant_text": "More recently, sentence or document encoders which produce contextual token representations have been pre-trained from unlabeled text and ﬁne-tuned for a supervised downstream task (Dai and Le, 2015; Howard and R", "case_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding_012", "slice": "natural_answerable_20docs"}
{"query": "From 2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt, what exact term fills [BLANK] in: \"[BLANK] is up to 3\u0002faster than the standard attention implemen- tation across common sequence lengths from 128 to 2K and scales up to 64K.\"?", "gold": "flashattention", "relevant_doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "relevant_chunk_id": 20, "relevant_text": "FlashAttention is up to 3\u0002faster than the standard attention implemen- tation across common sequence lengths from 128 to 2K and scales up to 64K.", "case_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness_008", "slice": "natural_answerable_20docs"}
{"query": "From 2302.13971__llama_open_and_efficient_foundation_language_models.txt, what exact term fills [BLANK] in: \"and [BLANK] deﬁnitions and macros written by users to increase consistency across papers.\"?", "gold": "inline-expanded", "relevant_doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "relevant_chunk_id": 17, "relevant_text": "and inline-expanded deﬁnitions and macros written by users to increase consistency across papers.", "case_id": "2302.13971__llama_open_and_efficient_foundation_language_models_006", "slice": "natural_answerable_20docs"}
{"query": "From 2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt, what exact term fills [BLANK] in: \"It is designed to equip readers and professionals with a detailed and structured [BLANK] of both large models a\"?", "gold": "understanding", "relevant_doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "relevant_chunk_id": 10, "relevant_text": "It is designed to equip readers and professionals with a detailed and structured understanding of both large models a", "case_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey_017", "slice": "natural_answerable_20docs"}
{"query": "From 2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt, what exact term fills [BLANK] in: \"For example, large language models can generate outputs that are [BLANK], toxic, or simply not helpful to the user.\"?", "gold": "untruthful", "relevant_doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "relevant_chunk_id": 1, "relevant_text": "For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user.", "case_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback_013", "slice": "natural_answerable_20docs"}
{"query": "From 2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt, what exact term fills [BLANK] in: \"Training language models to follow [BLANK] with human feedback Long Ouyang∗Jeff Wu∗Xu Jiang∗Diogo Almeida∗Carroll L.\"?", "gold": "instructions", "relevant_doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "relevant_chunk_id": 0, "relevant_text": "Training language models to follow instructions with human feedback Long Ouyang∗Jeff Wu∗Xu Jiang∗Diogo Almeida∗Carroll L.", "case_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback_009", "slice": "natural_answerable_20docs"}
{"query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"How this is done depends entirely on the API itself – for example, it can involve call- ing another neural network, [BLANK] a Python script or using a retrieval system to perform search over a large corpus.\"?", "gold": "executing", "relevant_doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "relevant_chunk_id": 22, "relevant_text": "How this is done depends entirely on the API itself – for example, it can involve call- ing another neural network, executing a Python script or using a retrieval system to perform search over a large corpus.", "case_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools_001", "slice": "natural_answerable_20docs"}
{"query": "From 2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt, what exact term fills [BLANK] in: \"Our work is [BLANK] to these works, as our attention mechanism is unmodified during inference.\"?", "gold": "complementary", "relevant_doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "relevant_chunk_id": 22, "relevant_text": "Our work is complementary to these works, as our attention mechanism is unmodified during inference.", "case_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models_001", "slice": "natural_answerable_20docs"}
{"query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \"Dur- ing [BLANK], the model is trained on unlabeled data over different pre-training tasks.\"?", "gold": "pre-training", "relevant_doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "relevant_chunk_id": 22, "relevant_text": "Dur- ing pre-training, the model is trained on unlabeled data over different pre-training tasks.", "case_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding_011", "slice": "natural_answerable_20docs"}
