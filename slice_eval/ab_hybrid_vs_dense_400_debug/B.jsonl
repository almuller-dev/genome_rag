{"case_index": 1, "query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \"4We note that in the literature the bidirectional Trans- Input/Output [BLANK] To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g.,‚ü®Question, Answer‚ü©) in one token sequence.\"?", "gold": "representations", "answer": "The exact term that fills **[BLANK]** is:\n\n**Representations**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'representations' and is directly supported by Context 1 ('Input/Output Representations').", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 98.464, "llm_ms": 1507.712, "top_contexts": [{"rank": 1, "score": 0.7926122, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.5545475732497643, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 27, "text_snippet": " and 4096 for the H= 1024 . 4We note that in the literature the bidirectional Trans-  Input/Output Representations To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single"}, {"rank": 2, "score": 0.567089, "source": "dense+rrf", "rrf_score": 0.02976190476190476, "rerank_score": 0.28692969631475873, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 20, "text_snippet": "or BERT. Apart from output layers, the same architec- tures are used in both pre-training and Ô¨Åne-tuning. The same pre-trained model parameters are used to initialize models for different down-stream tasks. During Ô¨Åne-tuning, all parameters"}, {"rank": 3, "score": 36.60892290609003, "source": "dense+rrf", "rrf_score": 0.013888888888888888, "rerank_score": 0.25132435504151057, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 83, "text_snippet": "ted that rich, unsupervised pre-training is an integral part of many language understanding systems. In particular, these results enable even low-resource tasks to beneÔ¨Åt from deep unidirectional architec- tures. Our major contribution is f"}, {"rank": 4, "score": 0.6017592, "source": "dense+rrf", "rrf_score": 0.029418126757516764, "rerank_score": 0.24742748129371628, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 1, "text_snippet": ", 2018), BERT is designed to pre- train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a re- sult, the pre-trained BERT model can be Ô¨Åne- tuned with just one a"}, {"rank": 5, "score": 36.49288593012541, "source": "dense+rrf", "rrf_score": 0.0136986301369863, "rerank_score": 0.2399555990642206, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 43, "text_snippet": "tion or sequence tagging. At the output, the token rep- resentations are fed into an output layer for token- level tasks, such as sequence tagging or question answering, and the [CLS] representation is fed into an output layer for classiÔ¨Åca"}]}
{"case_index": 2, "query": "From 1706.03762__attention_is_all_you_need.txt, what exact term fills [BLANK] in: \"h-to-French translation task, our model establishes a new single-model [BLANK] BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.\"?", "gold": "state-of-the-art", "answer": "The exact term is **‚Äústate-of-the-art‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term and is explicitly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 110.196, "llm_ms": 1677.996, "top_contexts": [{"rank": 1, "score": 0.7010226, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.49471500767725646, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 3, "text_snippet": "h-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show tha"}, {"rank": 2, "score": 0.69397235, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.33103248658654405, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 51, "text_snippet": "tom line of Table 3. Training took 3.5days on 8P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models. On the WMT 2014 English-to-French tr"}, {"rank": 3, "score": 0.6468686, "source": "dense+rrf", "rrf_score": 0.03125, "rerank_score": 0.2688069846962054, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 50, "text_snippet": "6]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score. 6 Results 6.1 Machine Translation On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Ta"}, {"rank": 4, "score": 0.69309986, "source": "dense+rrf", "rrf_score": 0.031746031746031744, "rerank_score": 0.2624520225818027, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 2, "text_snippet": "er, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significa"}, {"rank": 5, "score": 0.5816767, "source": "dense+rrf", "rrf_score": 0.028404512489927477, "rerank_score": 0.24473496205418993, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 65, "text_snippet": "rent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles. We a"}]}
{"case_index": 3, "query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"her the ob- tained responses are helpful for [BLANK] future tokens; this is used as a Ô¨Åltering criterion.\"?", "gold": "predicting", "answer": "The exact term is **‚Äúpredicting‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'predicting' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 68.943, "llm_ms": 1067.279, "top_contexts": [{"rank": 1, "score": 0.63481885, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.3710334457225021, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 17, "text_snippet": "her the ob- tained responses are helpful for predicting future tokens; this is used as a Ô¨Åltering criterion. After Ô¨Åltering, we merge API calls for different tools, resulting in the augmented dataset C‚àó, and Ô¨Ånetune 1In practice, we use the"}, {"rank": 2, "score": 0.57982796, "source": "dense+rrf", "rrf_score": 0.031746031746031744, "rerank_score": 0.2613313096738719, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 1, "text_snippet": "metic or fac- tual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer , a model traine"}, {"rank": 3, "score": 31.300328566449693, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.25980822787650143, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 12, "text_snippet": " how an API can be used, we let a LM annotate a huge language modeling dataset with potential API calls. We then use a self-supervised loss to determine which of these API calls actually help the model in predicting future tokens. Finally, "}, {"rank": 4, "score": 0.64335597, "source": "dense+rrf", "rrf_score": 0.032018442622950824, "rerank_score": 0.24158158284143774, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 88, "text_snippet": "ng criterion. High values typically correspond to API calls that are intuitively useful for predicting future tokens. approaches, additional information is always pro- vided, regardless of whether it is helpful or not. In contrast, Toolform"}, {"rank": 5, "score": 0.53928846, "source": "dense+rrf", "rrf_score": 0.02797339593114241, "rerank_score": 0.23536546234764485, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 93, "text_snippet": "mitations While our approach enables LMs to learn how to use a variety of tools in a self-supervised way, there are some clear limitations to what can be achieved with our method in its current form. One such limi- tation is the inability o"}]}
{"case_index": 4, "query": "From 2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt, what exact term fills [BLANK] in: \"ural networks have proven to be powerful language models, Ô¨Årst in the form of recurrent architectures (Graves, 2013; Jozefowicz et al., 2016; Mikolov et al., 2010) and more recently in the form of Transformers (Vaswani et al., 2017), that use attention to [BLANK] the past.\"?", "gold": "contextualise", "answer": "The exact term is **‚Äúcontextualise‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'contextualise' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 104.196, "llm_ms": 1217.704, "top_contexts": [{"rank": 1, "score": 0.753896, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.5670059153669742, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 4, "text_snippet": "ural networks have proven to be powerful language models, Ô¨Årst in the form of recurrent architectures (Graves, 2013; Jozefowicz et al., 2016; Mikolov et al., 2010) and more recently in the form of Transformers (Vaswani et al., 2017), that u"}, {"rank": 2, "score": 0.6149497, "source": "dense+rrf", "rrf_score": 0.02946236559139785, "rerank_score": 0.30437347601612885, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 84, "text_snippet": " the national US average of 0.385 kg CO 2e per KWh. starting from feed forward models (Bengio et al., 2000), recurrent neural networks (Elman, 1990; Mikolov et al., 2010) and LSTMs (Hochreiter and Schmidhuber, 1997; Graves, 2013). More rece"}, {"rank": 3, "score": 30.074806697964668, "source": "dense+rrf", "rrf_score": 0.014084507042253521, "rerank_score": 0.24970226134779303, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 38, "text_snippet": "t token of chunk ùê∂ùë¢is the Ô¨Årst to be able to access the retrieved content ùê∏ùë¢while maintaining autoregressivity in(1). Hence, there is a one token overlap between chunk ùê∂ùë¢=\u0010 ùë•¬πùë¢\u00001¬∫ùëö¬∏ùëñ\u0011 ùëñ2¬ª1¬îùëö¬ºand the corresponding attending chunk ùê∂¬∏ ùë¢,¬πùë•ùë¢ùëö¬∏ùëñ"}, {"rank": 4, "score": 0.6115767, "source": "dense+rrf", "rrf_score": 0.028373015873015873, "rerank_score": 0.22649558216415272, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 162, "text_snippet": "ry, and R. Socher. Pointer sentinel mixture models. In International Conference on Learning Representations , 2017. URL https://openreview.net/forum?id= Byj72udxe . 21  Improving language models by retrieving from trillions of tokens T. Mik"}, {"rank": 5, "score": 40.37649103860977, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.22554037499420923, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 3, "text_snippet": "cratch, yet can also rapidly R/e.sc/t.sc/r.sc/o.scÔ¨Åt pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale. 1. "}]}
{"case_index": 5, "query": "From 2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt, what exact term fills [BLANK] in: \"At a high level, our Retrieval Transformer ( R/e.sc/t.sc/r.sc/o.sc) model splits the input sequence into chunks and retrieves text similar to the previous chunk to improve the [BLANK] in the current chunk.\"?", "gold": "predictions", "answer": "The exact term that fills **[BLANK]** is **‚Äúpredictions‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term and is explicitly supported by Context 1 ('improve the predictions in the current chunk').", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 85.018, "llm_ms": 947.207, "top_contexts": [{"rank": 1, "score": 0.7560036, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.48592114426397975, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 7, "text_snippet": "tly access a large database to perform predictions‚Äîa semi-parametric approach. At a high level, our Retrieval Transformer ( R/e.sc/t.sc/r.sc/o.sc) model splits the input sequence into chunks and retrieves text similar to the previous chunk "}, {"rank": 2, "score": 0.6695445, "source": "dense+rrf", "rrf_score": 0.030621785881252923, "rerank_score": 0.3286055266150102, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 113, "text_snippet": "formers ( R/e.sc/t.sc/r.sc/o.sc), a method for modelling arbitrary text se- quences whilstretrievingfromdatabaseswithtrillions oftokens‚Äîscalingthedataavailable to models by an order of magnitude compared to what is typically consumed during"}, {"rank": 3, "score": 0.7217361, "source": "dense+rrf", "rrf_score": 0.0304147465437788, "rerank_score": 0.32132711180438983, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 38, "text_snippet": "t token of chunk ùê∂ùë¢is the Ô¨Årst to be able to access the retrieved content ùê∏ùë¢while maintaining autoregressivity in(1). Hence, there is a one token overlap between chunk ùê∂ùë¢=\u0010 ùë•¬πùë¢\u00001¬∫ùëö¬∏ùëñ\u0011 ùëñ2¬ª1¬îùëö¬ºand the corresponding attending chunk ùê∂¬∏ ùë¢,¬πùë•ùë¢ùëö¬∏ùëñ"}, {"rank": 4, "score": 0.67632127, "source": "dense+rrf", "rrf_score": 0.028790389395194696, "rerank_score": 0.321002240374673, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 183, "text_snippet": "sc¬πùê∂ùë¢¬∫ùëóand the chunk ùê∂ùë¢to be relatively well aligned, and assume that they start at the same position. Therefore, when computing C/a.sc¬πùêª¬∏ ùë¢¬îùê∏ùë¢¬∫, we set the distance between the data token ùëñ2¬ª1¬îùëô¬ºof chunkùê∂¬∏ ùë¢and 5https://github.com/earwig/m"}, {"rank": 5, "score": 0.64395225, "source": "dense+rrf", "rrf_score": 0.028006267136701922, "rerank_score": 0.30779350697260116, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 101, "text_snippet": "he chunked cross-attention mechanism. 4https://github.com/facebookresearch/FiD 13  Improving language models by retrieving from trillions of tokens Figure5jR/e.sc/t.sc/r.sc/o.sc-Ô¨Åttingabaselinetransformer. AnytransformercanbeÔ¨Åne-tunedintoar"}]}
{"case_index": 6, "query": "From 2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt, what exact term fills [BLANK] in: \"Though simple for humans, [BLANK] reasoning is a task where language models often struggle (Hendrycks et al., 2021; Patel et al., 2021, inter alia ).\"?", "gold": "arithmetic", "answer": "The exact term that fills **[BLANK]** is **‚Äúarithmetic‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'arithmetic' and is directly supported by the provided context sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 102.901, "llm_ms": 1258.573, "top_contexts": [{"rank": 1, "score": 0.7271904, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.4688075405701939, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 19, "text_snippet": "rve the utility of chain-of-thought prompting for arithmetic reasoning (Section 3), commonsense reasoning (Section 4), and symbolic reasoning (Section 5). 3 Arithmetic Reasoning We begin by considering math word problems of the form in Figu"}, {"rank": 2, "score": 0.71753407, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.2932552758664648, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 3, "text_snippet": "asks, their ability to demonstrate reasoning is often seen as a limitation, which cannot be overcome solely by increasing model scale (Rae et al., 2021; BIG-bench collaboration, 2021, inter alia ). In an effort to address this shortcoming, "}, {"rank": 3, "score": 37.65358653827091, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.2502763430157724, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 20, "text_snippet": "21, inter alia ). Strikingly, chain- of-thought prompting when used with the 540B parameter language model performs comparably with task-speciÔ¨Åc Ô¨Ånetuned models on several tasks, even achieving new state of the art on the challenging GSM8K "}, {"rank": 4, "score": 0.6540686, "source": "dense+rrf", "rrf_score": 0.026856524873828405, "rerank_score": 0.24839060547229833, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 71, "text_snippet": "revailing theme (Wei et al., 2022b). For many reasoning tasks where standard prompting has a Ô¨Çat scaling curve, chain- of-thought prompting leads to dramatically increasing scaling curves. Chain-of-thought prompting appears to expand the se"}, {"rank": 5, "score": 35.58309862979066, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.24804651780873163, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 68, "text_snippet": "s (though performance is lower than in the in-domain setting). Hence, chain-of-thought prompting facilitates length generalization beyond seen chains of thought for language models of sufÔ¨Åcient scale. 6 Discussion We have explored chain-of-"}]}
{"case_index": 7, "query": "From 2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt, what exact term fills [BLANK] in: \"e end performance dips [BLANK] due to the huge amount of ‚Äúhard negatives‚Äù, which confuses the reader.\"?", "gold": "significantly", "answer": "The exact term is **‚Äúsignificantly‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term and is directly supported by Context 1 ('performance dips significantly').", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 56.128, "llm_ms": 1047.462, "top_contexts": [{"rank": 1, "score": 0.7052803, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.40288944426689094, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 19, "text_snippet": "e end performance dips significantly due to the huge amount of ‚Äúhard negatives‚Äù, which confuses the reader. With ‚Äúlong retriever units‚Äù, we observe an entirely different trend. As we recall more units (from 1 to 8 units), both the recall an"}, {"rank": 2, "score": 37.18376124043787, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.24716563789826557, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 28, "text_snippet": " then fed into a long reader. Compared to traditional RAG, which retrieves hundreds of short units, our proposed LongRAG reduces the likelihood of retrieving hard negatives during the retrieval stage and more effectively leverages recent ad"}, {"rank": 3, "score": 0.52915555, "source": "dense+rrf", "rrf_score": 0.027884615384615386, "rerank_score": 0.22364613128056035, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 71, "text_snippet": "demonstrating the effectiveness of our LongRAG design. The upper part of the figure shows the NQ dataset, while the lower part displays the HotpotQA dataset. On the left, it illustrates how the overall performance changes with different set"}, {"rank": 4, "score": 31.96307935107933, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.22299271391363132, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 20, "text_snippet": "ontext methods evolve, the performance of LongRAG will continue to improve. Therefore, we believe the modern RAG systems should re-consider the granularity of their retrieval units to exploit the advantages of the current long-context LLMs."}, {"rank": 5, "score": 32.95035517461559, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.22092992631972766, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 0, "text_snippet": "LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs ‚ô†Ziyan Jiang,‚ô†Xueguang Ma,‚ô†Wenhu Chen ‚ô†University of Waterloo ziyanjiang528@gmail.com ,{x93ma ,wenhuchen}@uwaterloo.ca Project Website: https://tiger-ai-lab.github.io/"}]}
{"case_index": 8, "query": "From 2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt, what exact term fills [BLANK] in: \"Outputs from our 175B [BLANK] are preferred to 175B GPT-3 outputs 85 ¬±3% of the time, and preferred 71 ¬±4% of the time to few-shot 175B GPT-3.\"?", "gold": "instructgpt", "answer": "The exact term that fills **[BLANK]** is **‚ÄúInstructGPT‚Äù**.", "answer_score": 0.3, "answer_correctness": 0.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context shows the blank is ‚ÄúInstructGPT‚Äù (capitalized), but expected answer is lowercase ‚Äúinstructgpt‚Äù; answer does not match expected exact term/casing. Fully supported by provided context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 79.445, "llm_ms": 1049.109, "top_contexts": [{"rank": 1, "score": 0.6794163, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.423079835107606, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 16, "text_snippet": "owing instructions. Outputs from our 175B InstructGPT are preferred to 175B GPT-3 outputs 85 ¬±3% of the time, and preferred 71 ¬±4% of the time to few-shot 175B GPT-3. InstructGPT models also generate more appropriate outputs according to ou"}, {"rank": 2, "score": 0.7022244, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.3328019759394259, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 88, "text_snippet": " compared directly, 175B InstructGPT outputs are preferred to GPT-3 outputs 85 ¬±3% of the time, and preferred 71 ¬±4% of the time to few-shot GPT-3. We also found that our results do not change signiÔ¨Åcantly when evaluated on prompts submitte"}, {"rank": 3, "score": 0.6413251, "source": "dense+rrf", "rrf_score": 0.030090497737556562, "rerank_score": 0.2900467405461951, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 2, "text_snippet": "demonstrations of the desired model behavior, which we use to Ô¨Åne-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further Ô¨Åne-tune this supervised model using reinforcement learn"}, {"rank": 4, "score": 0.6723382, "source": "dense+rrf", "rrf_score": 0.031746031746031744, "rerank_score": 0.27893654074917484, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 15, "text_snippet": "ollows: Labelers signiÔ¨Åcantly prefer InstructGPT outputs over outputs from GPT-3. On our test set, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having over 100x fewer parameters. Th"}, {"rank": 5, "score": 0.6487917, "source": "dense+rrf", "rrf_score": 0.029857397504456328, "rerank_score": 0.25194749800812255, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 87, "text_snippet": " shown in Figure 1. We Ô¨Ånd that GPT-3 outputs perform the worst, and one can obtain signiÔ¨Åcant step-size improvements by using a well-crafted few-shot prompt (GPT-3 (prompted)), then by training on demonstrations using supervised learning ("}]}
{"case_index": 9, "query": "From 2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt, what exact term fills [BLANK] in: \"B) General and Ô¨Çexible : Due to the Ô¨Çexible thought space and [BLANK] occurrence format, ReAct works for diverse tasks with distinct action spaces and reasoning needs, including but\"?", "gold": "thought-action", "answer": "The exact term is **‚Äúthought-action‚Äù**, as in: ‚ÄúDue to the flexible thought space and **thought-action** occurrence format, ‚Ä¶‚Äù.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches the expected term 'thought-action' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 96.811, "llm_ms": 2003.131, "top_contexts": [{"rank": 1, "score": 0.68791974, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.42189337024122536, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 33, "text_snippet": "eatures: A) Intuitive and easy to design : Designing ReAct prompts is straightforward as human annotators just type down their thoughts in language on top of their actions taken. No ad-hoc format choice, thought design, or example selection"}, {"rank": 2, "score": 0.6169752, "source": "dense+rrf", "rrf_score": 0.03057889822595705, "rerank_score": 0.26366574335255705, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 88, "text_snippet": " the development of versatile and generalist agents like Reed et al. (2022). 6 C ONCLUSION We have proposed ReAct ‚Äì a simple yet effective method for synergizing reasoning and acting in large language models. Through a diverse set of experi"}, {"rank": 3, "score": 0.6359079, "source": "dense+rrf", "rrf_score": 0.03076923076923077, "rerank_score": 0.24680547000622338, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 19, "text_snippet": "udies on how reasoning and acting can be combined in a synergistic manner for general task solving, and if such a combination can bring systematic beneÔ¨Åts compared to reasoning or acting alone. In this work, we present ReAct , a general par"}, {"rank": 4, "score": 0.68086636, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.2300940707758654, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 34, "text_snippet": " with distinct action spaces and reasoning needs, including but not limited to QA, fact veriÔ¨Åcation, text game, and web navigation. C) Performant and robust :ReAct shows strong generalization to new task instances while learning solely from"}, {"rank": 5, "score": 0.6122664, "source": "dense+rrf", "rrf_score": 0.029211087420042643, "rerank_score": 0.21839718850545903, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 24, "text_snippet": "o understand the decision basis of model actions. To summarize, our key contributions are the following: (1) we introduce ReAct , a novel prompt- based paradigm to synergize reasoning and acting in language models for general task solving; "}]}
{"case_index": 10, "query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"This is [BLANK]:2302.04761v1 [cs.CL] 9 Feb 2023 x1: i-1 = Pittsburgh is also known as xi: n = the Steel City x* = Pittsburgh is also known as [QA(What ‚Ä¶?\"?", "gold": "impor-arxiv", "answer": "The term is **‚ÄúarXiv‚Äù**.", "answer_score": 0.0, "answer_correctness": 0.0, "answer_groundedness": 0.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 0.0, "abstained": false, "should_abstain": false, "notes": "Expected term is \"impor-arxiv\" (shown in context as \"impor-arXiv\"), but model answered \"arXiv\".", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 342.038, "llm_ms": 1224.237, "top_contexts": [{"rank": 1, "score": 0.7943711, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.43847100296053476, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 8, "text_snippet": "tools in a novel way, which fulÔ¨Ålls the following desiderata: ‚Ä¢The use of tools should be learned in a self-supervised way without requiring large amounts of human annotations . This is impor-arXiv:2302.04761v1 [cs.CL] 9 Feb 2023  x1: i-1 ="}, {"rank": 2, "score": 34.5683115073501, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.24307701439519688, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 27, "text_snippet": "atterns in the pretraining corpus, thus hurting perplexity.  x1:i‚àí1,e(ci,ri),xi:n; we proceed analogously for texts with multiple API calls. Doing this for all x‚àà Cresults in the new dataset C‚àóaugmented with API calls. We use this new datas"}, {"rank": 3, "score": 0.43066102, "source": "dense+rrf", "rrf_score": 0.031054405392392875, "rerank_score": 0.2141692935543159, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 212, "text_snippet": "line decision making: Benchmarks and additional opinions,‚Äù arXiv preprint arXiv:2306.02224 , 2023. [108] T. Schick, J. Dwivedi-Yu, R. Dess `ƒ±, R. Raileanu, M. Lomeli, L. Zettle- moyer, N. Cancedda, and T. Scialom, ‚ÄúToolformer: Language mode"}, {"rank": 4, "score": 32.01216563337302, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.20435665022340932, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 8, "text_snippet": "inchilla. In the rest of this paper, we present an overview of the modiÔ¨Åcations we made to the transformer architecture (Vaswani et al., 2017), as well as our training method. We then report the performance of our models and compare with ot"}, {"rank": 5, "score": 32.20116412886491, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.19707105863668956, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 1, "text_snippet": "metic or fac- tual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer , a model traine"}]}
{"case_index": 11, "query": "From 2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt, what exact term fills [BLANK] in: \"In this paper, we mainly focus on the setup where a frozen large language model, PaLM-540B (Chowdhery et al., 2022)1, is prompted with few-shot in-context examples to generate both [BLANK]Ô¨Åc actions and free-form language thoughts for task solving (Figure 1 (1d), (2b)).\"?", "gold": "domain-speci", "answer": "The exact term is **‚Äúdomain-speciÔ¨Åc‚Äù**.", "answer_score": 0.3, "answer_correctness": 0.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Blank asks for the exact term filling [BLANK], which is 'domain-speci' (as in domain-speciÔ¨Åc). Model answered the full word 'domain-speciÔ¨Åc' instead of the exact substring. Context clearly supports 'domain-speciÔ¨Åc'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 155.102, "llm_ms": 1200.518, "top_contexts": [{"rank": 1, "score": 0.79734606, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.5670555617118356, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 30, "text_snippet": " Thought 3), and so on. However, as the language space Lis unlimited, learning in this augmented action space is difÔ¨Åcult and requires strong language priors. In this paper, we mainly focus on the setup where a frozen large language model, "}, {"rank": 2, "score": 0.62320733, "source": "dense+rrf", "rrf_score": 0.029116045245077504, "rerank_score": 0.3119217385127548, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 19, "text_snippet": "udies on how reasoning and acting can be combined in a synergistic manner for general task solving, and if such a combination can bring systematic beneÔ¨Åts compared to reasoning or acting alone. In this work, we present ReAct , a general par"}, {"rank": 3, "score": 0.64022315, "source": "dense+rrf", "rrf_score": 0.029437229437229435, "rerank_score": 0.2882202032821435, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 17, "text_snippet": " fact hallucination and error propagation over the reasoning process (Figure 1 (1b)). On the other hand, recent work has explored the use of pre-trained language models for planning and acting in interactive environments (Ahn et al., 2022; "}, {"rank": 4, "score": 42.97649981217655, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.2831476430559653, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 43, "text_snippet": " mod- els, respectively. 4.2 Downstream Tasks We evaluate all models on a variety of downstream tasks. In all cases, we consider a prompted zero- shot setup ‚Äì i.e., models are instructed to solve each task in natural language, but we do not"}, {"rank": 5, "score": 45.7207968488692, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.2721180556556301, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 31, "text_snippet": "xt example is a human trajectory of actions, thoughts, and environment observations to solve a task instance (see Appendix C). For the tasks where reasoning is of primary importance (Figure 1(1)), we alternate the generation of thoughts and"}]}
{"case_index": 12, "query": "From 2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt, what exact term fills [BLANK] in: \"limited support of reasoning and acting behaviors), and perform initial Ô¨Ånetuning [BLANK] showing the potential of ReAct to improve with additional training data.\"?", "gold": "experiments", "answer": "The exact term is **‚Äúexperiments‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'experiments' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 48.109, "llm_ms": 1066.932, "top_contexts": [{"rank": 1, "score": 0.69600105, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.40453204757964367, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 25, "text_snippet": "tion; (3) we present systematic ablations and analysis to understand the importance of acting in reasoning tasks, and reasoning in interactive tasks; (4) we analyze the limitations of ReAct under the prompting setup (i.e. limited support of"}, {"rank": 2, "score": 28.808407240812986, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.21501718496438502, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 18, "text_snippet": "o generate domain-speciÔ¨Åc actions or plans, and then use a controller to choose or execute them. However, they do not employ language models to reason abstractly about high-level goals or maintain a working memory to support acting, barring"}, {"rank": 3, "score": 0.6244935, "source": "dense+rrf", "rrf_score": 0.02900988017658188, "rerank_score": 0.21308206423732656, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 88, "text_snippet": " the development of versatile and generalist agents like Reed et al. (2022). 6 C ONCLUSION We have proposed ReAct ‚Äì a simple yet effective method for synergizing reasoning and acting in large language models. Through a diverse set of experi"}, {"rank": 4, "score": 0.6516855, "source": "dense+rrf", "rrf_score": 0.031024531024531024, "rerank_score": 0.21091143249256503, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 58, "text_snippet": "lty to learn both reasoning and acting from in-context examples. However, when Ô¨Ånetuned with just 3,000 examples, ReAct becomes the best method among the four, with PaLM-8B Ô¨Ånetuned ReAct outperforming all PaLM-62B prompting methods, and Pa"}, {"rank": 5, "score": 23.793031347858417, "source": "dense+rrf", "rrf_score": 0.014084507042253521, "rerank_score": 0.205473060250406, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 90, "text_snippet": "nnotations will be the desiderata to further improve the performance. Scaling up ReAct with multi-task training and combining it with complementary paradigms like reinforcement learning could result in stronger agents that further unlock th"}]}
{"case_index": 13, "query": "From 2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt, what exact term fills [BLANK] in: \"to the phenomenon that all models are to some extent constrained by their underlying [BLANK] model (while Orca 2 training could be applied any base LLM, we report results on LLaMA-2 7B and 13B in this report).\"?", "gold": "pre-trained", "answer": "The exact term that fills **[BLANK]** is:\n\n**‚Äúpre-trained‚Äù**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'pre-trained' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 71.557, "llm_ms": 2323.428, "top_contexts": [{"rank": 1, "score": 0.6627768, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.4898123341218887, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 21, "text_snippet": " to the phenomenon that all models are to some extent constrained by their underlying pre-trained model (while Orca 2 training could be applied any base LLM, we report results on LLaMA-2 7B and 13B in this report). Orca 2 models have not un"}, {"rank": 2, "score": 41.18951345217226, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.2571502281607038, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 122, "text_snippet": "the ones reported in LLaMA-2 report [ 57] for TruthfulQA is that the evaluation schemes are different. In LLaMA-2, they report a generative style evaluation where GPT-3 has been used as annotator while we have used multiple choice version o"}, {"rank": 3, "score": 37.17541988729016, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.25709902488369407, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 144, "text_snippet": "ng training and safety guardrails (e.g., content filter) within the Azure OpenAI API. However, detailed studies are required for better quantification of such risks. Small Model Capacity: We note that post-training, while significantly bene"}, {"rank": 4, "score": 0.62719357, "source": "dense+rrf", "rrf_score": 0.029709507042253523, "rerank_score": 0.24853327535369518, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 83, "text_snippet": "r training the base model (LLAMA-2) and hence we cannot completely rule out further data leakage. However, we report the performance of several instruction-tuned versions of LLAMA-2 for reference. In the following sections, we discuss the p"}, {"rank": 5, "score": 33.26774170303868, "source": "dense+rrf", "rrf_score": 0.014285714285714285, "rerank_score": 0.23940794956260184, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 95, "text_snippet": "ts with LLaMA-2 models on MMLU are higher (54.8 and 68.9 for 13B and 70B variants, respectively [ 58]). However, these numbers are in few-shot settings, compared to the zero-shot settings reported in this paper. While we did not perform a c"}]}
{"case_index": 14, "query": "From 2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt, what exact term fills [BLANK] in: \"We analyze the [BLANK] of language models on two tasks that require identifying relevant information in t\"?", "gold": "performance", "answer": "The exact term is **‚Äúperformance‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "The blank is 'performance', explicitly stated in Context 1 and supported by the provided text.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 97.402, "llm_ms": 900.48, "top_contexts": [{"rank": 1, "score": 0.70518196, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.40801154959379166, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 0, "text_snippet": "Lost in the Middle: How Language Models Use Long Contexts Nelson F. Liu1‚àóKevin Lin2John Hewitt1Ashwin Paranjape3 Michele Bevilacqua3Fabio Petroni3Percy Liang1 1Stanford University2University of California, Berkeley3Samaya AI nfliu@cs.stanfo"}, {"rank": 2, "score": 0.6530628, "source": "dense+rrf", "rrf_score": 0.031054405392392875, "rerank_score": 0.324214495089041, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 1, "text_snippet": " on two tasks that require identifying relevant information in their in- put contexts: multi-document question an- swering and key-value retrieval. We find that performance can degrade significantly when changing the position of relevant in"}, {"rank": 3, "score": 0.6519408, "source": "dense+rrf", "rrf_score": 0.030330882352941176, "rerank_score": 0.27487477848348063, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 49, "text_snippet": "UIDs. The relevant key-value pair for answering the query is bolded here within the input context for clarity. indicate that extended-context models are not nec- essarily better than their non-extended counterparts at using their input cont"}, {"rank": 4, "score": 29.350710953570072, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.2625923323503259, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 93, "text_snippet": " a serial-position-like effect in lan- guage models is perhaps surprising, since the self- attention mechanisms underlying Transformer lan- guage models is technically equally capable of re- trieving any token from their contexts. 7 Conclus"}, {"rank": 5, "score": 0.66844827, "source": "dense+rrf", "rrf_score": 0.02964254577157803, "rerank_score": 0.24736632747168083, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 9, "text_snippet": "at require accessing and using information within an input context. In particular, our experi- ments make controlled changes to the input context size and the position of the relevant information within the input context and study their eff"}]}
{"case_index": 15, "query": "From 2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt, what exact term fills [BLANK] in: \"answer questions about code, and sometimes follows [BLANK] in different languages, despite these instructions being very rare in the Ô¨Åne-tuning distribution.\"?", "gold": "instructions", "answer": "The exact term is **‚Äúinstructions‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected blank term 'instructions' and is directly supported by the provided context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 71.634, "llm_ms": 955.898, "top_contexts": [{"rank": 1, "score": 0.7141434, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3817551765808181, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 24, "text_snippet": "answer questions about code, and sometimes follows instructions in different languages, despite these instructions being very rare in the Ô¨Åne-tuning distribution. In contrast, GPT-3 can perform these tasks but requires more careful promptin"}, {"rank": 2, "score": 48.21681300798883, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.2771832480341319, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 23, "text_snippet": "e than our SFT baseline, and labelers signiÔ¨Åcantly prefer InstructGPT to these models (InstructGPT has a 73.4 ¬±2%winrate vs. our baseline, compared to 26.8 ¬±2%and 29.8¬±2%for our version of T0 and FLAN, respectively). InstructGPT models show"}, {"rank": 3, "score": 0.62161696, "source": "dense+rrf", "rrf_score": 0.031754032258064516, "rerank_score": 0.23473207573174026, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 126, "text_snippet": "ow some qualitative examples in Figure 8. Our 175B PPO-ptx model is able to reliably answers questions about code, and can also follow instructions in other languages; however, we notice that it often produces an output in English even when"}, {"rank": 4, "score": 0.58650327, "source": "dense+rrf", "rrf_score": 0.029631255487269532, "rerank_score": 0.22851667238325057, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 115, "text_snippet": " compared to GPT-3 175B with no additional preÔ¨Åxing. Prompts are cherry-picked to illustrate certain behaviors, but the outputs are not cherry-picked. (1) InstructGPT can follow instructions in other languages, though it sometimes generates"}, {"rank": 5, "score": 30.436073242302378, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.22784531956118265, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 9, "text_snippet": "fabricate information or mislead the user), and harmless (they should not cause physical, psychological, or social harm to people or the environment). We elaborate on the evaluation of these criteria in Section 3.6. We focus on Ô¨Åne-tuning a"}]}
{"case_index": 16, "query": "From 1706.03762__attention_is_all_you_need.txt, what exact term fills [BLANK] in: \"Recent work has achieved significant improvements in computational efficiency through [BLANK] tricks [ 21] and conditional computation [ 32], while also improving model performance in case of the latter.\"?", "gold": "factorization", "answer": "The exact term is **‚Äúfactorization‚Äù**, as in: ‚Äú...through **factorization** tricks [21] and conditional computation [32]...‚Äù", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'factorization' and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 49.741, "llm_ms": 1555.379, "top_contexts": [{"rank": 1, "score": 0.6381015, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.4438951959215983, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 8, "text_snippet": "y sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computatio"}, {"rank": 2, "score": 26.243584083602933, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.18218521010366778, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 6, "text_snippet": " language modelling (Khandel- wal et al., 2020; Borgeaud et al., 2022), recent work has suggested that simply adding retrieved docu- ments to the input of a standard LM can also work well (Khattab et al., 2022; Ram et al., 2023; Shi et al.,"}, {"rank": 3, "score": 21.537896524047664, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.17522927774720232, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 307, "text_snippet": "ch of stufÔ¨Ång the model‚Äôs context with previous examples is most structurally similar to RL2 and also resembles [ HYC01 ], in that an inner loop of adaptation takes place through computation in the model‚Äôs activations across timesteps, with"}, {"rank": 4, "score": 20.24156528651571, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.1737803909483702, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 30, "text_snippet": "ment prompts and improve the performance of GPT-3. There has also been work on aligning agents in text-based environments using RL with 4  a normative prior (Nahian et al., 2021). Our work can be seen as a direct application of RLHF to alig"}, {"rank": 5, "score": 22.57370118736405, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.17040730036336307, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 76, "text_snippet": "erve that instruction fine-tuning slightly re- duces the worst-case performance disparity from nearly 10% between the base model best- and worst-case performance to around 4%. These observations complement prior work, which found that non-i"}]}
{"case_index": 17, "query": "From 2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt, what exact term fills [BLANK] in: \"With a 2 trillion token database, our [BLANK] Transformer ( R/e.sc/t.sc/r.sc/o.sc) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25 \u0002fewer parameters.\"?", "gold": "retrieval-enhanced", "answer": "The exact term is **‚ÄúRetrieval-Enhanced‚Äù**, i.e., **‚ÄúRetrieval-Enhanced Transformer (RETRO)‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context explicitly states 'Retrieval-Enhanced Transformer'; model answer matches exact term for [BLANK].", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 136.538, "llm_ms": 1202.553, "top_contexts": [{"rank": 1, "score": 0.6525441, "source": "dense+rrf", "rrf_score": 0.03177805800756621, "rerank_score": 0.4826452838093454, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 1, "text_snippet": ", Karen Simonyan, Jack W. Raez, Erich Elsenzand Laurent Sifrey,z All authors from DeepMind,yEqual contributions,zEqual senior authorship We enhance auto-regressive language models by conditioning on document chunks retrieved from a large co"}, {"rank": 2, "score": 52.14631664514275, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.36392084132043273, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 228, "text_snippet": "baseline models, measured in bits-per-bytes on evaluation set, in Table 14. F.2. The Pile In Fig. 4, we compare R/e.sc/t.sc/r.sc/o.scagainst Jurassic-1 (Lieber et al., 2021). The full bits-per-bytes results are reported in Table 15. F.3. Fi"}, {"rank": 3, "score": 48.56982293714528, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.3577962556374423, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 38, "text_snippet": "t token of chunk ùê∂ùë¢is the Ô¨Årst to be able to access the retrieved content ùê∏ùë¢while maintaining autoregressivity in(1). Hence, there is a one token overlap between chunk ùê∂ùë¢=\u0010 ùë•¬πùë¢\u00001¬∫ùëö¬∏ùëñ\u0011 ùëñ2¬ª1¬îùëö¬ºand the corresponding attending chunk ùê∂¬∏ ùë¢,¬πùë•ùë¢ùëö¬∏ùëñ"}, {"rank": 4, "score": 55.464011812196155, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.3549453951094579, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 22, "text_snippet": " ùïç=¬ª1¬îùë£¬º, obtained using a text tokenizer1. Wespliteach ùëõ-token-longexample ùëã=¬πùë•1¬î¬ì¬ì¬ì¬îùë•ùëõ¬∫intoasequenceof ùëôchunks¬πùê∂1¬î¬ì¬ì¬ì¬îùê∂ùëô¬∫ of sizeùëö=ùëõ ùëô, i.e.ùê∂1,¬πùë•1¬î¬ì¬ì¬ì¬îùë•ùëö¬∫¬î ¬ì¬ì¬ì¬î ùê∂ùëô,¬πùë•ùëõ\u0000ùëö¬∏1¬î¬ì¬ì¬ì¬îùë•ùëõ¬∫2ùïçùëö. We useùëõ=2048andùëö=64. We augment each chunk ùê∂ùë¢with a se"}, {"rank": 5, "score": 0.6147975, "source": "dense+rrf", "rrf_score": 0.02928692699490662, "rerank_score": 0.3370888538833057, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 235, "text_snippet": "of the Ô¨Åltered curves is the same: R/e.sc/t.sc/r.sc/o.scmodels exploit leakage more, but the performance improvement they provide remains signiÔ¨Åcant even on original chunks that haven‚Äôt been observed in the training set. 34  Improving langu"}]}
{"case_index": 18, "query": "From 2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"fluency) and then scores passages based on the average probability of the generated tokens, according to a given [BLANK] LM.\"?", "gold": "autoregressive", "answer": "The exact term is **‚Äúautoregressive‚Äù** (i.e., ‚Äúaccording to a given **autoregressive** LM.‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'autoregressive' and is explicitly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 77.235, "llm_ms": 1103.258, "top_contexts": [{"rank": 1, "score": 0.64287096, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.35854907620902005, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 15, "text_snippet": "his is less likely to be the case for hallucinated answers. Automated evaluation of text generation systems LLMs have also been leveraged to automatically evaluate other aspects of generated text fragments, beyond factuality. For instance, "}, {"rank": 2, "score": 29.296392478956406, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.2431749422764303, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 22, "text_snippet": " generate the complete answer at once y=LM([Dx,x]). 2.3 Active Retrieval Augmented Generation To aid long-form generation with retrieval, we pro- pose active retrieval augmented generation. It is a generic framework that actively decides wh"}, {"rank": 3, "score": 0.55200875, "source": "dense+rrf", "rrf_score": 0.02976190476190476, "rerank_score": 0.23888167357853293, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 9, "text_snippet": "we present Ragas1, a framework for the automated assessment 1Ragas is available at https://github.com/ explodinggradients/ragas .arXiv:2309.15217v2 [cs.CL] 28 Apr 2025  of retrieval augmented generation systems. We fo- cus on settings where"}, {"rank": 4, "score": 25.901993658644233, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.22303794231706867, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 5, "text_snippet": " training corpus (Kandpal et al., 2022; Mallen et al., 2023). The standard solution to these issues is to rely on Retrieval Augmented Generation (RAG) (Lee et al., 2019; Lewis et al., 2020; Guu et al., 2020). Answering a question then essen"}, {"rank": 5, "score": 23.71278335138878, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.21203246056771907, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 15, "text_snippet": "e to avoid unneces- sary or inappropriate retrieval that occurs in passive retrieval augmented LMs (Khandelwal et al., 2020; Borgeaud et al., 2022; Ram et al., 2023; Trivedi et al., 2022). Given the observation that large LMs tend to be wel"}]}
{"case_index": 19, "query": "From 2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt, what exact term fills [BLANK] in: \"We analyze the IO complexity [ 1] [BLANK] , proving that it requires ùëÇ¬πùëÅ2ùëë2ùëÄ\u00001¬∫HBM accesses where ùëëis the head dimension and ùëÄis the size of SRAM, as compared to Œ©¬πùëÅùëë¬∏ùëÅ2¬∫of standard attention.\"?", "gold": "offlashattention", "answer": "The exact term is **‚Äúof FlashAttention‚Äù**.", "answer_score": 0.9299999999999999, "answer_correctness": 0.9, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.9, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context shows the phrase 'ofFlashAttention' (no space). Model answered 'of FlashAttention' with a space; essentially correct term but not exact formatting.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 61.018, "llm_ms": 982.673, "top_contexts": [{"rank": 1, "score": 0.77171564, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.4766130981635733, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 14, "text_snippet": " 7.6x on GPT-2 [ 67], Figure 1 right) and uses less memory ‚Äîlinear in sequence length‚Äîthan standard attention, thanks to the massively reduced amount of HBM access. We analyze the IO complexity [ 1] ofFlashAttention , proving that it requir"}, {"rank": 2, "score": 65.02243283102048, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.41007274726331683, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 50, "text_snippet": "o blocks of the attention matrix. The algorithm is identical to Algorithm 1, except we skip zero blocks. We reproduce the algorithm description in Algorithm 5 in Appendix B. We also analyze the IO complexity of block-sparse FlashAttention ."}, {"rank": 3, "score": 0.7320585, "source": "dense+rrf", "rrf_score": 0.031754032258064516, "rerank_score": 0.3406351640665445, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 40, "text_snippet": "returns O=softmax¬πQK>¬∫VwithùëÇ¬πùëÅ2ùëë¬∫FLOPs and requires ùëÇ¬πùëÅ¬∫additional memory beyond inputs and output. 3.2 Analysis: IO Complexity of FlashAttention We analyze the IO complexity of FlashAttention , showing signiÔ¨Åcant reduction in HBM accesses "}, {"rank": 4, "score": 0.7277766, "source": "dense+rrf", "rrf_score": 0.030536130536130537, "rerank_score": 0.3360216656441483, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 45, "text_snippet": "f standard attention requiresŒò¬πùëÅùëë¬∏ùëÅ2¬∫HBM accesses while the backward pass of FlashAttention requiresŒò¬πùëÅ2ùëë2ùëÄ\u00001¬∫ HBM accesses (Appendix B). We prove a lower-bound: one cannot asymptotically improve on the number of HBM accesses for all values"}, {"rank": 5, "score": 0.7088449, "source": "dense+rrf", "rrf_score": 0.029418126757516764, "rerank_score": 0.3352153483201499, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 30, "text_snippet": "n O. 3FlashAttention : Algorithm, Analysis, and Extensions We show how to compute exact attention with fewer HBM reads/writes and without storing large intermediate matrices for the backward pass. This yields an attention algorithm that is "}]}
{"case_index": 20, "query": "From 2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"First, [BLANK] refers to the idea that the an- swer should be grounded in the given context.\"?", "gold": "faithfulness", "answer": "**Faithfulness**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "The blank is 'Faithfulness', explicitly stated in Context 1; model answered correctly.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 48.685, "llm_ms": 1134.173, "top_contexts": [{"rank": 1, "score": 0.58930635, "source": "dense+rrf", "rrf_score": 0.0315136476426799, "rerank_score": 0.27396257157581233, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 20, "text_snippet": "generate an answer as(q). When building a RAG system,  we usually do not have access to human-annotated datasets or reference answers. We therefore fo- cus on metrics that are fully self-contained and reference-free. We focus in particular "}, {"rank": 2, "score": 33.653291091117815, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.21572217570422134, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 9, "text_snippet": "we present Ragas1, a framework for the automated assessment 1Ragas is available at https://github.com/ explodinggradients/ragas .arXiv:2309.15217v2 [cs.CL] 28 Apr 2025  of retrieval augmented generation systems. We fo- cus on settings where"}, {"rank": 3, "score": 26.722343719981986, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.2054043053672849, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 22, "text_snippet": "lly, Context Relevance refers to the idea that the retrieved context should be focused, containing as little irrelevant informa- tion as possible. This is important given the cost associated with feeding long context passages to LLMs. Moreo"}, {"rank": 4, "score": 22.496943728335097, "source": "dense+rrf", "rrf_score": 0.014285714285714285, "rerank_score": 0.2034850878873455, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 45, "text_snippet": "ially for longer contexts. 6 Conclusions We have highlighted the need for automated reference-free evaluation of RAG systems. In par- ticular, we have argued the need for an evaluation framework that can assess faithfulness (i.e. is the ans"}, {"rank": 5, "score": 28.912066155859595, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.20179190103027939, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 25, "text_snippet": "n when they do not have the necessary knowledge to avoid unnecessary or inappropriate retrieval, and (2) the retrieval queries should reflect the intents of future generations. We propose two forward-looking active retrieval augmented gener"}]}
{"case_index": 21, "query": "From 2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt, what exact term fills [BLANK] in: \"Since John did not see the ball in the [BLANK] after he left, it must have still been in the box.\"?", "gold": "basket", "answer": "The exact term that fills **[BLANK]** is **‚Äúbasket‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "The context explicitly contains the sentence with 'basket' in the blank position, matching the expected answer.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 95.053, "llm_ms": 1089.155, "top_contexts": [{"rank": 1, "score": 0.67808366, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3586524555577018, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 10, "text_snippet": "ve still been in the box. 4. Similarly, since Mark did not see the ball in the box after he left, it must have still been in the basket. Therefore, the ball is still in the box. ### ChatGPT(GPT-3.5-Turbo) : John and Mark will likely think t"}, {"rank": 2, "score": 0.5991149, "source": "dense+rrf", "rrf_score": 0.031754032258064516, "rerank_score": 0.32058456839143507, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 9, "text_snippet": " man has been told by someone else (probably his friend) that he cannot have any more alcohol at this establishment. ### LLaMA2-Chat-13B : This is a classic lateral thinking puzzle. The solution is that the ball is still in the box. Here‚Äôs "}, {"rank": 3, "score": 0.63097906, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.27413161811711384, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 7, "text_snippet": "John and Mark have not seen each other‚Äôs actions. Therefore, they only know what happened while they were in the room. John only saw the ball in the box before he left for work. Mark only saw the ball in the basket before he left for school"}, {"rank": 4, "score": 31.525310803719652, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.23372486676623966, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 148, "text_snippet": "synthetic data for post-training. While Orca 2 models have not gone through RLHF training for safety, we believe that the use of synthetic data for post-training that has been filtered with various content safety filters could provide anoth"}, {"rank": 5, "score": 0.63084316, "source": "dense+rrf", "rrf_score": 0.03125763125763126, "rerank_score": 0.2331981320969534, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 6, "text_snippet": "then leaves for school. They both come back together later in the day, and they do not know what happened in the room after each of them left the room. Where do they think the ball is? ### Orca-2-13B : Let‚Äôs analyze the situation step by st"}]}
{"case_index": 22, "query": "From 2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt, what exact term fills [BLANK] in: \"t if demonstrations of [BLANK] reasoning are provided in the exemplars for few-shot prompting.\"?", "gold": "chain-of-thought", "answer": "The exact term is **‚Äúchain-of-thought‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'chain-of-thought' and is directly supported by the provided context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 83.255, "llm_ms": 1127.238, "top_contexts": [{"rank": 1, "score": 0.71404415, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.329959660236527, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 1, "text_snippet": "such reasoning abilities emerge naturally in sufÔ¨Åciently large language models via a simple method called chain-of- thought prompting , where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three"}, {"rank": 2, "score": 0.73114204, "source": "dense+rrf", "rrf_score": 0.032266458495966696, "rerank_score": 0.30356583278246946, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 15, "text_snippet": "t if demonstrations of chain-of-thought reasoning are provided in the exemplars for few-shot prompting. Figure 1 shows an example of a model producing a chain of thought to solve a math word problem that it would have otherwise gotten incor"}, {"rank": 3, "score": 0.7093129, "source": "dense+rrf", "rrf_score": 0.031024531024531024, "rerank_score": 0.2703382193613838, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 18, "text_snippet": " used for tasks such as math word problems, commonsense reasoning, and symbolic manipulation, and is potentially applicable (at least in principle) to any task that humans can solve via language. 4.Finally, chain-of-thought reasoning can be"}, {"rank": 4, "score": 0.63326406, "source": "dense+rrf", "rrf_score": 0.026356857985087564, "rerank_score": 0.2632372978317437, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 62, "text_snippet": "s but potentially chal- lenging for language models. We show that chain-of- thought prompting not only enables language models to perform symbolic reasoning tasks that are challenging in the standard prompting setting, but also facilitates "}, {"rank": 5, "score": 31.221152770434237, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.2523059913992594, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 14, "text_snippet": "ach before giving the Ô¨Ånal answer: ‚ÄúAfter Jane gives 2 Ô¨Çowers to her mom she has 10 ...then after she gives 3 to her dad she will have 7 ...so the answer is 7. ‚Äù The goal of this paper is to endow language models with the ability to generat"}]}
{"case_index": 23, "query": "From 2305.06983__active_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"The first method prompts the LM to generate retrieval queries when necessary while generating the an- swer using [BLANK] instructions, de- noted as FLARE instruct .\"?", "gold": "retrieval-encouraging", "answer": "The exact term is **‚Äúretrieval-encouraging‚Äù**, i.e., the phrase is: **‚Äúusing retrieval-encouraging instructions‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'retrieval-encouraging' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 54.036, "llm_ms": 1644.581, "top_contexts": [{"rank": 1, "score": 0.8005211, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.46660108327740585, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 25, "text_snippet": "n when they do not have the necessary knowledge to avoid unnecessary or inappropriate retrieval, and (2) the retrieval queries should reflect the intents of future generations. We propose two forward-looking active retrieval augmented gener"}, {"rank": 2, "score": 0.6698872, "source": "dense+rrf", "rrf_score": 0.030776515151515152, "rerank_score": 0.2476243023408349, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 16, "text_snippet": ". When deciding what to retrieve , it is important to consider what LMs intend to gen- erate in the future, as the goal of active retrieval is to benefit future generations. Therefore, we propose anticipating the future by generating a temp"}, {"rank": 3, "score": 0.73922706, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.24496553050319828, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 26, "text_snippet": "eval-encouraging instructions, de- noted as FLARE instruct . The second method directly uses the LM‚Äôs generation as search queries, denoted as FLARE direct, which iteratively generates the next sentence to gain insight into the future topic"}, {"rank": 4, "score": 38.99979669847007, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.22291470940401284, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 28, "text_snippet": "University)][Search(Joe Biden degree)]the University of Pennsylvania, where he earneda law degree.Generate a summary about Joe Biden.Input$&$&#%$&%%%GenerationRetriever$%$%%Figure 2: An illustration of forward-looking active re- trieval aug"}, {"rank": 5, "score": 0.5880175, "source": "dense+rrf", "rrf_score": 0.027912386121341344, "rerank_score": 0.20685316587487235, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 21, "text_snippet": "ing when and what to retrieve, we follow exist- ing methods (Ram et al., 2023; Trivedi et al., 2022) to prepend the retrieved documents before the user input to aid future generation for both baselines and our method for fair comparisons: y"}]}
{"case_index": 24, "query": "From 2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natu- ral language layer between a user and textual databases, reducing the risk of [BLANK].\"?", "gold": "hallucinations", "answer": "The exact term is **‚Äúhallucinations.‚Äù**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'hallucinations' and is explicitly stated in Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 72.177, "llm_ms": 800.786, "top_contexts": [{"rank": 1, "score": 0.81083107, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.5562155780532436, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 1, "text_snippet": "ented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natu- ral language layer between"}, {"rank": 2, "score": 32.181208410775064, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.22573295653709574, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 125, "text_snippet": "them for the given domain of interest or application using the models and annotation approaches for the given domain: Automated RAI Measurement Framework: We employ a recently proposed framework for automated measurement of Responsible AI m"}, {"rank": 3, "score": 0.6540559, "source": "dense+rrf", "rrf_score": 0.028612012987012988, "rerank_score": 0.22524059157981016, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 9, "text_snippet": "we present Ragas1, a framework for the automated assessment 1Ragas is available at https://github.com/ explodinggradients/ragas .arXiv:2309.15217v2 [cs.CL] 28 Apr 2025  of retrieval augmented generation systems. We fo- cus on settings where"}, {"rank": 4, "score": 55.79259803735379, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.22231482945367537, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 237, "text_snippet": "-auto-eval-best-practices-RAG, 2023. [164] S. Es, J. James, L. Espinosa-Anke, and S. Schockaert, ‚ÄúRagas: Au- tomated evaluation of retrieval augmented generation,‚Äù arXiv preprint arXiv:2309.15217 , 2023. [165] J. Saad-Falcon, O. Khattab, C."}, {"rank": 5, "score": 0.6814687, "source": "dense+rrf", "rrf_score": 0.014084507042253521, "rerank_score": 0.21836100190590685, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 48, "text_snippet": "actory results. III. R ETRIEVAL In the context of RAG, it is crucial to efficiently retrieve relevant documents from the data source. There are several key issues involved, such as the retrieval source, retrieval granularity, pre-processing"}]}
{"case_index": 25, "query": "From 1706.03762__attention_is_all_you_need.txt, what exact term fills [BLANK] in: \"Aligning the positions to steps in [BLANK] time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht‚àí1and the input for position t.\"?", "gold": "computation", "answer": "The exact term is **‚Äúcomputation‚Äù**, i.e., ‚ÄúAligning the positions to steps in **computation** time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht‚àí1 and the input for position t.‚Äù", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'computation' and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 75.077, "llm_ms": 1695.079, "top_contexts": [{"rank": 1, "score": 0.5610558, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.4086571290091996, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 7, "text_snippet": " [ 35,2,5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15]. Recurrent models typically factor computation along the symbol positions of the input and "}, {"rank": 2, "score": 0.5081858, "source": "dense+rrf", "rrf_score": 0.0315136476426799, "rerank_score": 0.28020768283345965, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 28, "text_snippet": "the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9]. ‚Ä¢The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries c"}, {"rank": 3, "score": 29.794321789478257, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.23276021173253902, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 27, "text_snippet": "ead, the total computational cost is similar to that of single-head attention with full dimensionality. 3.2.3 Applications of Attention in our Model The Transformer uses multi-head attention in three different ways: ‚Ä¢In \"encoder-decoder att"}, {"rank": 4, "score": 26.95760751135095, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.23061389241509472, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 11, "text_snippet": " of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary in"}, {"rank": 5, "score": 24.783024384865456, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.21690020318122238, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 43, "text_snippet": "ked cross-attention is autoregressive: the output of C/c.sc/a.scat position ùëñdepends on the sequence from tokens from 0toùëñthat is input to C/c.sc/a.sc. With R/e.sc/t.sc/r.sc/o.scmodels, even though each C/c.sc/a.sccross-attention attends on"}]}
{"case_index": 26, "query": "From 2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"Other [BLANK] rely on linking the generated responses to facts from an external knowledge base (Min et al., 2023), but this is not always possible.\"?", "gold": "approaches", "answer": "The exact term is **‚Äúapproaches‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context 1 explicitly contains the sentence 'Other approaches rely on linking...' so the blank is 'approaches'. Model answer matches exactly.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 77.575, "llm_ms": 1181.559, "top_contexts": [{"rank": 1, "score": 76.13462576697606, "source": "dense+rrf", "rrf_score": 0.01639344262295082, "rerank_score": 0.38676118294826806, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 11, "text_snippet": "tegy (Zhang et al., 2023). Recent analy- ses, however, suggest that existing models struggle with detecting hallucination when using standard prompting strategies (Li et al., 2023; Azaria and Mitchell, 2023). Other approaches rely on linkin"}, {"rank": 2, "score": 29.623065971037274, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.22825291058915934, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 8, "text_snippet": "e not always predictive of downstream performance (Wang et al., 2023c). Moreover, this evaluation strategy relies on the LM probabilities, which are not accessible for some closed models (e.g. ChatGPT and GPT-4). Ques- tion answering is ano"}, {"rank": 3, "score": 29.55424886232474, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.21937717763045042, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 15, "text_snippet": "his is less likely to be the case for hallucinated answers. Automated evaluation of text generation systems LLMs have also been leveraged to automatically evaluate other aspects of generated text fragments, beyond factuality. For instance, "}, {"rank": 4, "score": 35.39907299430454, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.21792480005186277, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 9, "text_snippet": "we present Ragas1, a framework for the automated assessment 1Ragas is available at https://github.com/ explodinggradients/ragas .arXiv:2309.15217v2 [cs.CL] 28 Apr 2025  of retrieval augmented generation systems. We fo- cus on settings where"}, {"rank": 5, "score": 0.62283236, "source": "dense+rrf", "rrf_score": 0.028991596638655463, "rerank_score": 0.216340165174245, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 6, "text_snippet": "rely offloaded from the parametric knowledge of LLMs by leveraging a standalone retrieval component from an external corpus. The existing RAG framework tends to use short retrieval units, such as 100-word passages in popular open- domain qu"}]}
{"case_index": 27, "query": "From 2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt, what exact term fills [BLANK] in: \"In this work, we show that retrieval can be practically implemented us- ingdense [BLANK] alone, where em- beddings are learned from a small number of questions and passages by a simple dual- encoder framework.\"?", "gold": "representations", "answer": "The exact term that fills **[BLANK]** is **‚Äúrepresentations‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term and is directly supported by Context 1 ('dense representations alone').", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 103.347, "llm_ms": 1183.09, "top_contexts": [{"rank": 1, "score": 0.7918242, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.49490344915527656, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 1, "text_snippet": "where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented us- ingdense representations alone, where em- beddings are learned from a sma"}, {"rank": 2, "score": 0.6531816, "source": "dense+rrf", "rrf_score": 0.03021353930031804, "rerank_score": 0.28762076902579603, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 85, "text_snippet": "eving good performance on open-domain QA and other knowledge-intensive tasks. 8 Conclusion In this work, we demonstrated that dense retrieval can outperform and potentially replace the tradi- tional sparse retrieval component in open-domain"}, {"rank": 3, "score": 0.7016964, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.25833867487976614, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 10, "text_snippet": "stion: can we train a better dense embedding model using only pairs of questions and passages (or answers), with- outadditional pretraining? By leveraging the now standard BERT pretrained model (Devlin et al., 2019) and a dual-encoder archi"}, {"rank": 4, "score": 0.6682321, "source": "dense+rrf", "rrf_score": 0.029857397504456328, "rerank_score": 0.2520271063865464, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 17, "text_snippet": "rieval accuracy , which is the fraction of ques- tions for whichCFcontains a span that answers the question. 3 Dense Passage Retriever (DPR) We focus our research in this work on improv- ing the retrieval component in open-domain QA. Given "}, {"rank": 5, "score": 34.477574611955035, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.24346525753488996, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 9, "text_snippet": "open-domainarXiv:2004.04906v3 [cs.CL] 30 Sep 2020  QA datasets, it also suffers from two weaknesses. First, ICT pretraining is computationally intensive and it is not completely clear that regular sentences are good surrogates of questions "}]}
{"case_index": 28, "query": "From 2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt, what exact term fills [BLANK] in: \"For comparison with other Phi series models, phi-3.5-MoE uses the same tokenizer as phi-3-medium andphi-3-mini with [BLANK] size of 32064.\"?", "gold": "vocabulary", "answer": "The exact term that fills **[BLANK]** is **‚Äúvocabulary‚Äù**, as in:\n\n‚Äú‚Ä¶with **vocabulary** size of 32064.‚Äù", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'vocabulary' and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 125.468, "llm_ms": 1416.77, "top_contexts": [{"rank": 1, "score": 0.7501293, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.45898508570161844, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 16, "text_snippet": "3. The table shows the Keys/values a query token in block 8 attended to. Blue=local blocks, orange=remote/vertical blocks, gray=blocks skipped. total parameters. Additionally, we utilize the SparseMixer approach [LGC23, LDL+23] for training"}, {"rank": 2, "score": 0.642889, "source": "dense+rrf", "rrf_score": 0.030621785881252923, "rerank_score": 0.2864545803303061, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 0, "text_snippet": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone Microsoft Abstract We introduce phi-3-mini , a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both"}, {"rank": 3, "score": 0.68946815, "source": "dense+rrf", "rrf_score": 0.031754032258064516, "rerank_score": 0.2771755315895314, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 6, "text_snippet": "7B parameters), matched the performance of models 25 times larger trained on regular data. In this report we present a new model, phi-3-mini (3.8B parameters), trained for 3.3T tokens on larger and more advanced versions of the datasets use"}, {"rank": 4, "score": 0.65273064, "source": "dense+rrf", "rrf_score": 0.030536130536130537, "rerank_score": 0.2726202386748918, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 2, "text_snippet": ", 8.9 on MT-bench). To enhance multilingual, multimodal, and long-context capabilities, we introduce three models in the phi-3.5 series: phi-3.5-mini ,phi-3.5-MoE , and phi- 3.5-Vision . The phi-3.5-MoE , a 16 x 3.8B MoE model with 6.6 bill"}, {"rank": 5, "score": 0.61624813, "source": "dense+rrf", "rrf_score": 0.028438886647841874, "rerank_score": 0.2525731372689106, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 21, "text_snippet": "s an example, the result of a game in premier league in a particular day might be good training data for frontier models, but we need to remove such information to leave more model capacity for ‚Äúreasoning‚Äù for the mini size models. We compa"}]}
{"case_index": 29, "query": "From 2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt, what exact term fills [BLANK] in: \"3 E XPERIMENTS We conducted a series of experiments to compare the proposed [BLANK] method with existing approaches on a range of reasoning benchmarks.\"?", "gold": "self-consistency", "answer": "The exact term that fills **[BLANK]** is **‚Äúself-consistency‚Äù** (i.e., ‚Äúthe proposed **self-consistency** method‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'self-consistency' and is explicitly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 70.835, "llm_ms": 1207.57, "top_contexts": [{"rank": 1, "score": 0.72566795, "source": "dense+rrf", "rrf_score": 0.032018442622950824, "rerank_score": 0.4184465102397463, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 31, "text_snippet": "f consistency can be deÔ¨Åned between multiple generations, e.g., whether two answers agree or contradict each other. 3 E XPERIMENTS We conducted a series of experiments to compare the proposed self-consistency method with existing approaches"}, {"rank": 2, "score": 0.67479426, "source": "dense+rrf", "rrf_score": 0.028577260665441927, "rerank_score": 0.29110186483727324, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 40, "text_snippet": " 4  Published as a conference paper at ICLR 2023 3.2 M AINRESULTS We report the results of self-consistency averaged over 10 runs, where we sampled 40 outputs independently from the decoder in each run. The baseline we compare to is chain-o"}, {"rank": 3, "score": 0.65969443, "source": "dense+rrf", "rrf_score": 0.027205882352941177, "rerank_score": 0.28121759239287897, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 86, "text_snippet": "ength sequence in recurrent language models. Nye et al. (2021) improve the logical consistency of samples from a System 1 model by adding a System 2-inspired logical reasoning module. In this paper we focus on a slightly different notion of"}, {"rank": 4, "score": 36.89287617272827, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.27572608969599244, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 69, "text_snippet": " with both ensemble approaches and we show the results in Appendix A.1.4. 7  Published as a conference paper at ICLR 2023 3.5 A DDITIONAL STUDIES We conducted a number of additional experiments to analyze different aspects of the self-consi"}, {"rank": 5, "score": 36.11194088134937, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.26789154603986853, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 1, "text_snippet": "such reasoning abilities emerge naturally in sufÔ¨Åciently large language models via a simple method called chain-of- thought prompting , where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three"}]}
{"case_index": 30, "query": "From 2305.06983__active_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"As shown in Figure 2, when the LM [BLANK] ‚Äú[Search(query)]‚Äù (shown in gray italic ), we stop the generation and use the\"?", "gold": "generates", "answer": "The exact term is **‚Äúgenerates‚Äù**:\n\n> ‚ÄúAs shown in Figure 2, when the LM **generates** ‚Äú[Search(query)]‚Äù (shown in gray italic ), we stop the generation and use the‚Äù", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'generates' and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 56.149, "llm_ms": 1444.345, "top_contexts": [{"rank": 1, "score": 0.6189226, "source": "dense+rrf", "rrf_score": 0.03177805800756621, "rerank_score": 0.3702426725193777, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 30, "text_snippet": "can be found in Prompt D.3. Prompt 3.1: retrieval instructions Skill 1. An instruction to guide LMs to generate search queries. Several search-related exemplars. Skill 2. An instruction to guide LMs to perform a specific downstream task (e."}, {"rank": 2, "score": 0.6222145, "source": "dense+rrf", "rrf_score": 0.030776515151515152, "rerank_score": 0.26412122926502923, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 21, "text_snippet": "ing when and what to retrieve, we follow exist- ing methods (Ram et al., 2023; Trivedi et al., 2022) to prepend the retrieved documents before the user input to aid future generation for both baselines and our method for fair comparisons: y"}, {"rank": 3, "score": 0.61607254, "source": "dense+rrf", "rrf_score": 0.029631255487269532, "rerank_score": 0.2507955282940144, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 16, "text_snippet": ". When deciding what to retrieve , it is important to consider what LMs intend to gen- erate in the future, as the goal of active retrieval is to benefit future generations. Therefore, we propose anticipating the future by generating a temp"}, {"rank": 4, "score": 0.6306222, "source": "dense+rrf", "rrf_score": 0.03149801587301587, "rerank_score": 0.247703542804765, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 22, "text_snippet": " generate the complete answer at once y=LM([Dx,x]). 2.3 Active Retrieval Augmented Generation To aid long-form generation with retrieval, we pro- pose active retrieval augmented generation. It is a generic framework that actively decides wh"}, {"rank": 5, "score": 40.17049421970037, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.24462974608177473, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 28, "text_snippet": "University)][Search(Joe Biden degree)]the University of Pennsylvania, where he earneda law degree.Generate a summary about Joe Biden.Input$&$&#%$&%%%GenerationRetriever$%$%%Figure 2: An illustration of forward-looking active re- trieval aug"}]}
{"case_index": 31, "query": "From 2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt, what exact term fills [BLANK] in: \"(2021), we want language models to be helpful (they should help the user solve their task), honest (they shouldn‚Äôt fabricate [BLANK] or mislead the user), and harmless (they s\"?", "gold": "information", "answer": "The exact term is **‚Äúinformation‚Äù** (as in: ‚Äúthey shouldn‚Äôt fabricate **information** or mislead the user‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'information' and is directly supported by the provided context quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 82.907, "llm_ms": 1292.154, "top_contexts": [{"rank": 1, "score": 0.7559544, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.4667082092288224, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 8, "text_snippet": "progress on aligning language models by training them to act in accordance with the user‚Äôs intention (Leike et al., 2018). This encompasses both explicit intentions such as following instructions and implicit intentions such as staying trut"}, {"rank": 2, "score": 0.71951914, "source": "dense+rrf", "rrf_score": 0.031754032258064516, "rerank_score": 0.3177289887005008, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 75, "text_snippet": " models that act in accordance with user intentions. More practically, for the purpose of our language tasks, we use a framework similar to Askell et al. (2021), who deÔ¨Åne models to be aligned if they are helpful, honest, and harmless. To b"}, {"rank": 3, "score": 0.7156949, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.2964112676632773, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 9, "text_snippet": "fabricate information or mislead the user), and harmless (they should not cause physical, psychological, or social harm to people or the environment). We elaborate on the evaluation of these criteria in Section 3.6. We focus on Ô¨Åne-tuning a"}, {"rank": 4, "score": 54.779371855302244, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.2729181944523244, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 262, "text_snippet": "There once was a wise frog named Julius,‚Äù you can assume the task is to continue the story). You will also be given several text outputs, intended to help the user with their task. Your job is to evaluate these outputs to ensure that they a"}, {"rank": 5, "score": 0.6856559, "source": "dense+rrf", "rrf_score": 0.030330882352941176, "rerank_score": 0.24217871091096574, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 165, "text_snippet": "e for making language models more helpful, truthful, and harmless. In the longer term, alignment failures could lead to more severe consequences, particularly if these models are deployed in safety-critical situations. We expect that as mod"}]}
{"case_index": 32, "query": "From 1706.03762__attention_is_all_you_need.txt, what exact term fills [BLANK] in: \"In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and [BLANK] for ByteNet.\"?", "gold": "logarithmically", "answer": "The exact term is **‚Äúlogarithmically‚Äù** (i.e., ‚Äúlinearly for ConvS2S and **logarithmically** for ByteNet.‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'logarithmically' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 99.074, "llm_ms": 1334.01, "top_contexts": [{"rank": 1, "score": 0.68645275, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.49021091618720397, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 11, "text_snippet": " of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary in"}, {"rank": 2, "score": 0.58522576, "source": "dense+rrf", "rrf_score": 0.031754032258064516, "rerank_score": 0.2678389866358526, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 38, "text_snippet": " computation that can be parallelized, as measured by the minimum number of sequential operations required. The third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in "}, {"rank": 3, "score": 0.63244456, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.2565633291612608, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 39, "text_snippet": "hs between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [ 12]. Hence we also compare the maximum path length between any two input and output positions in networks compose"}, {"rank": 4, "score": 24.21889918435086, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.23383865840048287, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 174, "text_snippet": "in\u0012ùëÄ ùëë¬ñùëÄ ùêµùëê\u0013\u0013 =Œò\u0012 min\u0012ùëÄ ùëë¬ñùëë\u0013\u0013 ¬ï We then have: ùëáùëê=ùëÅ ùêµùëê=Œò\u0012ùëÅùëë ùëÄ\u0013 ¬ï As a result, the number of HBM accesses is: Œò¬πùëÅùëëùëáùëê¬∫=Œò\u0012ùëÅ2ùëë2 ùëÄ\u0013 ¬ï \u0003 Proof of Proposition 3. For contradiction, suppose that there exists an algorithm that computes exact attentio"}, {"rank": 5, "score": 23.60122677923993, "source": "dense+rrf", "rrf_score": 0.014285714285714285, "rerank_score": 0.2196650064541816, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 162, "text_snippet": "r than standard attention. In terms of total memory required, both methods oÔ¨Äer substantial memory saving. The second diÔ¨Äerence between the two methods is the way information is summarized from each block to pass to the next block. Rabe and"}]}
{"case_index": 33, "query": "From 2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt, what exact term fills [BLANK] in: \"This deÔ¨Ånes the following [BLANK] sequence log-likelihood: ùêø¬πùëãjùúÉ¬îD¬∫,ùëô‚àëÔ∏Å ùë¢=1ùëö‚àëÔ∏Å ùëñ=1\u0012ùúÉ\u0000 ùë•¬πùë¢\u00001¬∫ùëö¬∏ùëñj¬πùë•ùëó¬∫ùëó¬ù¬πùë¢\u00001¬∫ùëö¬∏ùëñ¬î¬πR/e.sc/t.scD¬πùê∂ùë¢0¬∫¬∫ùë¢0¬ùùë¢\u0001 ¬ì (1) We set R/e.sc/t.sc¬πùê∂1¬∫=;, namely the likelihood of tokens from the Ô¨Årst chunk does not depend on any retrieval data.\"?", "gold": "retrieval-enhanced", "answer": "The exact term is **‚Äúretrieval-enhanced‚Äù**, i.e., it reads:\n\n> ‚ÄúThis deÔ¨Ånes the following **retrieval-enhanced** sequence log-likelihood: ‚Ä¶‚Äù", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "The blank is correctly filled with 'retrieval-enhanced', which appears verbatim in the provided context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 90.527, "llm_ms": 1502.575, "top_contexts": [{"rank": 1, "score": 0.78299797, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.5024983481420128, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 23, "text_snippet": "brevity) is a non-trainable operator speciÔ¨Åed in ¬ß2.3. Token likelihoods are provided by a model, parameterized by ùúÉ, that takes as input both previous tokens and their retrieved neighbours. This deÔ¨Ånes the following retrieval-enhanced sequ"}, {"rank": 2, "score": 0.60353446, "source": "dense+rrf", "rrf_score": 0.030090497737556562, "rerank_score": 0.3394932466857363, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 44, "text_snippet": "entiallydepend upon the set of allprevious neighbours R/e.sc/t.sc¬πùê∂ùë¢0¬∫ùë¢0¬ùùë¢, without incurring the quadratic cost of cross attending to that set. 6  Improving language models by retrieving from trillions of tokens Sampling. Whensampling,atth"}, {"rank": 3, "score": 0.69842494, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.3368300578386468, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 24, "text_snippet": "rieval data. This likelihood deÔ¨Ånition preserves autoregressivity : the probability of the ùëñ-th token of the ùë¢-th chunk, ùë•¬πùë¢\u00001¬∫ùëö¬∏ùëñ, only depends on previously seen tokens ¬πùë•ùëó¬∫16ùëó¬ù¬πùë¢\u00001¬∫ùëö¬∏ùëñand on the data retrieved from the previous chunks ¬πR"}, {"rank": 4, "score": 0.6145289, "source": "dense+rrf", "rrf_score": 0.030776515151515152, "rerank_score": 0.3336213967761859, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 183, "text_snippet": "sc¬πùê∂ùë¢¬∫ùëóand the chunk ùê∂ùë¢to be relatively well aligned, and assume that they start at the same position. Therefore, when computing C/a.sc¬πùêª¬∏ ùë¢¬îùê∏ùë¢¬∫, we set the distance between the data token ùëñ2¬ª1¬îùëô¬ºof chunkùê∂¬∏ ùë¢and 5https://github.com/earwig/m"}, {"rank": 5, "score": 0.64975077, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.3309812833095343, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 22, "text_snippet": " ùïç=¬ª1¬îùë£¬º, obtained using a text tokenizer1. Wespliteach ùëõ-token-longexample ùëã=¬πùë•1¬î¬ì¬ì¬ì¬îùë•ùëõ¬∫intoasequenceof ùëôchunks¬πùê∂1¬î¬ì¬ì¬ì¬îùê∂ùëô¬∫ of sizeùëö=ùëõ ùëô, i.e.ùê∂1,¬πùë•1¬î¬ì¬ì¬ì¬îùë•ùëö¬∫¬î ¬ì¬ì¬ì¬î ùê∂ùëô,¬πùë•ùëõ\u0000ùëö¬∏1¬î¬ì¬ì¬ì¬îùë•ùëõ¬∫2ùïçùëö. We useùëõ=2048andùëö=64. We augment each chunk ùê∂ùë¢with a se"}]}
{"case_index": 34, "query": "From 2302.13971__llama_open_and_efficient_foundation_language_models.txt, what exact term fills [BLANK] in: \"This leads to the following mixture of data and the per- centage they represent in the training set: English [BLANK] [67%].\"?", "gold": "commoncrawl", "answer": "The exact term is **‚ÄúCommonCrawl‚Äù**.", "answer_score": 0.3, "answer_correctness": 0.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Expected exact term is \"commoncrawl\" (lowercase). Model answered \"CommonCrawl\"; grounded in context but not an exact match.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 58.343, "llm_ms": 1028.623, "top_contexts": [{"rank": 1, "score": 0.64960164, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3750103262266428, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 10, "text_snippet": " mains. For the most part, we reuse data sources that have been leveraged to train other LLMs, with the restriction of only using data that is publicly available, and compatible with open sourcing. This leads to the following mixture of dat"}, {"rank": 2, "score": 20.445949691116784, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.2152207650038443, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 78, "text_snippet": "mits 0.231 kg CO 2eq/KWh, lead- ing to 82 tCO 2eq. In this study, we are interested in comparing the cost in carbon emission of training of these models if they were trained in the same data center. Hence, we do not take the location of dat"}, {"rank": 3, "score": 19.596525720510265, "source": "dense+rrf", "rrf_score": 0.014285714285714285, "rerank_score": 0.1974065257027066, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 5, "text_snippet": "at inference. For instance, although Hoffmann et al. (2022) recommends training a 10B model on 200B tokens, we Ô¨Ånd that the performance of a 7B model continues to improve even after 1T tokens. The focus of this work is to train a series of "}, {"rank": 4, "score": 20.069636498250592, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.17059307339376645, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 0, "text_snippet": "LLaMA: Open and EfÔ¨Åcient Foundation Language Models Hugo Touvron‚àó, Thibaut Lavril‚àó, Gautier Izacard‚àó, Xavier Martinet Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi√®re, Naman Goyal Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand "}, {"rank": 5, "score": 19.867983588980238, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.15717639389260837, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 22, "text_snippet": "trained on the same data for slightly more epochs (4.8T tokens total as for phi-3-small . The model has 40 heads and 40 layers, with embedding dimension 5120. We observe that some benchmarks improve much less from 7B to 14B than they do fro"}]}
{"case_index": 35, "query": "From 2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt, what exact term fills [BLANK] in: \"It [BLANK] MoE layer as its feedforward models, employing the top2 routing among 16 expert networks.\"?", "gold": "incorporates", "answer": "The exact term is **‚Äúincorporates‚Äù**:\n\n‚ÄúIt **incorporates** MoE layer as its feedforward models, employing the top2 routing among 16 expert networks.‚Äù", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'incorporates' and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 87.375, "llm_ms": 1494.583, "top_contexts": [{"rank": 1, "score": 0.72586703, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.30557537768773524, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 14, "text_snippet": "ion layers to optimize KV cache savings while maintaining long context retrieval performance. An additional 10% multilingual data was also used for this model. Thephi-3.5-MoE adopts an Mixture-of-Experts (MoE) architecture to selectively ac"}, {"rank": 2, "score": 0.637712, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.26007722091882485, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 16, "text_snippet": "3. The table shows the Keys/values a query token in block 8 attended to. Blue=local blocks, orange=remote/vertical blocks, gray=blocks skipped. total parameters. Additionally, we utilize the SparseMixer approach [LGC23, LDL+23] for training"}, {"rank": 3, "score": 0.51873785, "source": "dense+rrf", "rrf_score": 0.03021353930031804, "rerank_score": 0.2542240628454928, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 0, "text_snippet": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone Microsoft Abstract We introduce phi-3-mini , a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both"}, {"rank": 4, "score": 0.5225092, "source": "dense+rrf", "rrf_score": 0.030117753623188408, "rerank_score": 0.23080414550424, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 6, "text_snippet": "7B parameters), matched the performance of models 25 times larger trained on regular data. In this report we present a new model, phi-3-mini (3.8B parameters), trained for 3.3T tokens on larger and more advanced versions of the datasets use"}, {"rank": 5, "score": 0.5679493, "source": "dense+rrf", "rrf_score": 0.03057889822595705, "rerank_score": 0.20389633406846633, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 2, "text_snippet": ", 8.9 on MT-bench). To enhance multilingual, multimodal, and long-context capabilities, we introduce three models in the phi-3.5 series: phi-3.5-mini ,phi-3.5-MoE , and phi- 3.5-Vision . The phi-3.5-MoE , a 16 x 3.8B MoE model with 6.6 bill"}]}
{"case_index": 36, "query": "From 2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt, what exact term fills [BLANK] in: \"In this paper, we explore the use of LLMs to generate both reasoning traces and task-speciÔ¨Åc actions in an [BLANK] manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, whi\"?", "gold": "interleaved", "answer": "The exact term is **‚Äúinterleaved‚Äù** (as in ‚Äúin an **interleaved** manner‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'interleaved' and is directly supported by the provided context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 95.582, "llm_ms": 1264.859, "top_contexts": [{"rank": 1, "score": 0.826076, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.5697356892387957, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 1, "text_snippet": "rformance across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In th"}, {"rank": 2, "score": 0.7337315, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.29755323135880574, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 19, "text_snippet": "udies on how reasoning and acting can be combined in a synergistic manner for general task solving, and if such a combination can bring systematic beneÔ¨Åts compared to reasoning or acting alone. In this work, we present ReAct , a general par"}, {"rank": 3, "score": 0.62279785, "source": "dense+rrf", "rrf_score": 0.027272727272727275, "rerank_score": 0.25864725522259363, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 30, "text_snippet": " Thought 3), and so on. However, as the language space Lis unlimited, learning in this augmented action space is difÔ¨Åcult and requires strong language priors. In this paper, we mainly focus on the setup where a frozen large language model, "}, {"rank": 4, "score": 56.493337383073445, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.24851433090515382, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 2, "text_snippet": "track, and update action plans as well as handle exceptions, while actions allow it to interface with and gather additional information from external sources such as knowledge bases or environments. We apply our approach, named ReAct , to a"}, {"rank": 5, "score": 0.6478481, "source": "dense+rrf", "rrf_score": 0.028985507246376812, "rerank_score": 0.23402228877676048, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 20, "text_snippet": "pertaining to a task in an interleaved manner, which allows the model to perform dynamic reasoning to create, maintain, and adjust high-level plans for acting (reason to act), while also interact with the external environments (e.g. Wikiped"}]}
{"case_index": 37, "query": "From 2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt, what exact term fills [BLANK] in: \"Following the experimental settings of Position [BLANK] (Chen et al., 2023), we fine-tune models with proper position embeddings.\"?", "gold": "interpolation", "answer": "The exact term is **‚ÄúInterpolation‚Äù**, i.e., **‚ÄúPosition Interpolation (Chen et al., 2023)‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context explicitly states 'Position Interpolation (Chen et al., 2023)'; blank is 'interpolation'. Answer matches and is fully supported.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 97.357, "llm_ms": 1439.616, "top_contexts": [{"rank": 1, "score": 0.6002704, "source": "dense+rrf", "rrf_score": 0.03131881575727918, "rerank_score": 0.3813609589302969, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 19, "text_snippet": "n the entire LLM. For example, embedding has ( <2%) parameters, and normalization has ( ‚â§0.004%) parameters in Llama2 7B. This ratio decreases for even larger LLMs. In experiments, we show that LongLoRA is effective and efficient. We presen"}, {"rank": 2, "score": 39.44940168400232, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.26937950903306257, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 58, "text_snippet": "lama2 13B model, we observe that the perplexity reduces by -0.28. In Table 4, we further examine the maximum context length that we can fine-tune on a single 8 √ó A100 machine. We extend Llama2 7B, 13B, and 70B to 100k, 65536, and 32768 cont"}, {"rank": 3, "score": 0.5892825, "source": "dense+rrf", "rrf_score": 0.02877846790890269, "rerank_score": 0.2496955250284332, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 59, "text_snippet": " limitation of Position Interpolation (Chen et al., 2023). Retrieval-based Evaluation. We conduct experiments on retrieval in long contexts. In Table 5, we compare our model with other open LLMs on the topic retrieval task introduced in Lon"}, {"rank": 4, "score": 30.80480802826864, "source": "dense+rrf", "rrf_score": 0.014285714285714285, "rerank_score": 0.24679697430379552, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 75, "text_snippet": "ed in training-from- scratch transformers. This experiment is to examine their capability of fine-tuning on pre-trained LLMs (Touvron et al., 2023b), toward long context adaptation. Dilated attention performs well in full fine-tuning but is"}, {"rank": 5, "score": 0.60603404, "source": "dense+rrf", "rrf_score": 0.03128054740957967, "rerank_score": 0.242497751702523, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 9, "text_snippet": "rkowski et al., 2023; Mohtashami & Jaggi, 2023) train or fine-tune LLMs to longer context. However, training an LLM from scratch with long sequences poses computational challenges, and fine-tuning an existing pre-trained LLM is also conside"}]}
{"case_index": 38, "query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"is to give them the abil- ity to use external tools such as search engines, [BLANK], or calendars.\"?", "gold": "calculators", "answer": "The exact term is **‚Äúcalculators‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'calculators' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 108.684, "llm_ms": 1241.893, "top_contexts": [{"rank": 1, "score": 0.672659, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.31525483030568957, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 7, "text_snippet": " is to give them the abil- ity to use external tools such as search engines, calculators, or calendars. However, existing ap- proaches either rely on large amounts of human annotations (Komeili et al., 2022; Thoppilan et al., 2022) or limit"}, {"rank": 2, "score": 0.6152178, "source": "dense+rrf", "rrf_score": 0.031754032258064516, "rerank_score": 0.3022828215762927, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 6, "text_snippet": "legislative bodies, like  city councils, to hold their meetings open to the public. Figure 1: Exemplary predictions of Toolformer. The model autonomously decides to call different APIs (from top to bottom: a question answering system, a cal"}, {"rank": 3, "score": 0.58895934, "source": "dense+rrf", "rrf_score": 0.03055037313432836, "rerank_score": 0.2596563510454153, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 1, "text_snippet": "metic or fac- tual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer , a model traine"}, {"rank": 4, "score": 0.53968, "source": "dense+rrf", "rrf_score": 0.029236022193768675, "rerank_score": 0.2391805377720822, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 94, "text_snippet": "e are no examples of chained tool use in the Ô¨Ånetuning dataset. Our current approach also does not allow the LM to use a tool in an in- teractive way ‚Äì especially for tools such as search engines, that could potentially return hundreds of d"}, {"rank": 5, "score": 0.64711297, "source": "dense+rrf", "rrf_score": 0.031054405392392875, "rerank_score": 0.23783729073590784, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 88, "text_snippet": "ng criterion. High values typically correspond to API calls that are intuitively useful for predicting future tokens. approaches, additional information is always pro- vided, regardless of whether it is helpful or not. In contrast, Toolform"}]}
{"case_index": 39, "query": "From 2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt, what exact term fills [BLANK] in: \"We are not implying that we actually found the [BLANK] ‚Äúoptimal‚Äù data mixture for a given scale.\"?", "gold": "provably", "answer": "The exact term is **‚Äúprovably‚Äù**:\n\n‚ÄúWe are not implying that we actually found the **provably** ‚Äúoptimal‚Äù data mixture for a given scale.‚Äù", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'provably' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 66.172, "llm_ms": 1255.6, "top_contexts": [{"rank": 1, "score": 0.60649824, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3095892217241563, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 23, "text_snippet": "aspirational sense for ‚Äúdata optimal regime‚Äù. We are not implying that we actually found the provably ‚Äúoptimal‚Äù data mixture for a given scale. 3  Figure 2: 4-bit quantized phi-3-mini running natively on an iPhone with A16 Bionic chip, gene"}, {"rank": 2, "score": 0.552837, "source": "dense+rrf", "rrf_score": 0.031024531024531024, "rerank_score": 0.2652271969301339, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 22, "text_snippet": "trained on the same data for slightly more epochs (4.8T tokens total as for phi-3-small . The model has 40 heads and 40 layers, with embedding dimension 5120. We observe that some benchmarks improve much less from 7B to 14B than they do fro"}, {"rank": 3, "score": 0.5763116, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.2579644103086159, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 0, "text_snippet": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone Microsoft Abstract We introduce phi-3-mini , a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both"}, {"rank": 4, "score": 0.5607068, "source": "dense+rrf", "rrf_score": 0.029910714285714284, "rerank_score": 0.241732436922642, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 20, "text_snippet": "hat train language models in either ‚Äúcompute optimal regime‚Äù [HBM+22] or ‚Äúover-train regime‚Äù, we mainly focus on the quality of data for a given scale .3 We try to calibrate the training data to be closer to the ‚Äúdata optimal‚Äù regime for sm"}, {"rank": 5, "score": 23.212517239009223, "source": "dense+rrf", "rrf_score": 0.013888888888888888, "rerank_score": 0.23475582375155432, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 3, "text_snippet": ", recent work from Hoffmann et al. (2022) shows that, for a given compute budget, the best performances are not achieved by the largest mod- els, but by smaller models trained on more data. The objective of the scaling laws from Hoff- mann "}]}
{"case_index": 40, "query": "From 2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt, what exact term fills [BLANK] in: \"D) Human aligned and controllable :ReAct promises an [BLANK] sequential decision making and reasoning process where humans can easily inspect reasoning and factual correctness.\"?", "gold": "interpretable", "answer": "The exact term is **‚Äúinterpretable‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'interpretable' and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 71.74, "llm_ms": 894.946, "top_contexts": [{"rank": 1, "score": 0.73961765, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.361205655022602, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 35, "text_snippet": "tion 4 how ReAct performance is robust to prompt selections. D) Human aligned and controllable :ReAct promises an interpretable sequential decision making and reasoning process where humans can easily inspect reasoning and factual correctne"}, {"rank": 2, "score": 0.66013277, "source": "dense+rrf", "rrf_score": 0.031754032258064516, "rerank_score": 0.20624758063828041, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 19, "text_snippet": "udies on how reasoning and acting can be combined in a synergistic manner for general task solving, and if such a combination can bring systematic beneÔ¨Åts compared to reasoning or acting alone. In this work, we present ReAct , a general par"}, {"rank": 3, "score": 37.24335645674334, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.18987861437833342, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 23, "text_snippet": "le reasoning in decision making by showing consistent advantages over controlled baselines with actions only. Besides general applicability and performance boost, the combination of reasoning and acting also contributes to model interpretab"}, {"rank": 4, "score": 38.73469988553236, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.18184351872725152, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 120, "text_snippet": "Zhou. 2022. Self- consistency improves chain of thought reasoning in language models. CoRR , abs/2203.11171. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicit"}, {"rank": 5, "score": 0.65557146, "source": "dense+rrf", "rrf_score": 0.031009615384615385, "rerank_score": 0.1763761750015263, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 88, "text_snippet": " the development of versatile and generalist agents like Reed et al. (2022). 6 C ONCLUSION We have proposed ReAct ‚Äì a simple yet effective method for synergizing reasoning and acting in large language models. Through a diverse set of experi"}]}
{"case_index": 41, "query": "From 1706.03762__attention_is_all_you_need.txt, what exact term fills [BLANK] in: \"We employ a residual connection [ 11] around each of the two sub-layers, followed by layer [BLANK] [ 1].\"?", "gold": "normalization", "answer": "The exact term is **‚Äúnormalization‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context explicitly states 'followed by layer normalization [1]'; model answer matches expected blank.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 87.454, "llm_ms": 1382.676, "top_contexts": [{"rank": 1, "score": 0.5869441, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.39145755650892156, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 17, "text_snippet": "ted feed-forward network. We employ a residual connection [ 11] around each of the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is LayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function"}, {"rank": 2, "score": 0.5323325, "source": "dense+rrf", "rrf_score": 0.031754032258064516, "rerank_score": 0.3430009710636738, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 18, "text_snippet": "s also composed of a stack of N= 6identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the enc"}, {"rank": 3, "score": 0.49253938, "source": "dense+rrf", "rrf_score": 0.029206349206349208, "rerank_score": 0.3215233447422506, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 16, "text_snippet": "ransformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively. 3.1 Encoder and Decoder Stacks "}, {"rank": 4, "score": 0.5445022, "source": "dense+rrf", "rrf_score": 0.031754032258064516, "rerank_score": 0.23616051033723573, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 30, "text_snippet": "rd Networks In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transf"}, {"rank": 5, "score": 22.888403284049037, "source": "dense+rrf", "rrf_score": 0.013888888888888888, "rerank_score": 0.22006356827981732, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 32, "text_snippet": " techniques (tiling, recomputation) to overcome the technical challenge of computing exact attention in sub-quadratic HBM accesses. We describe this in Algorithm 1. The main idea is that we split the inputs Q¬ñK¬ñVinto blocks, load them from "}]}
{"case_index": 42, "query": "From 2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt, what exact term fills [BLANK] in: \"hat train language models in either ‚Äúcompute optimal regime‚Äù [HBM+22] or ‚Äú[BLANK] regime‚Äù, we mainly focus on the quality of data for a given scale .3 We try to calibrate the training data to be closer to the ‚Äúdata optimal‚Äù regime for small models.\"?", "gold": "over-train", "answer": "The exact term that fills **[BLANK]** is:\n\n**‚Äúover-train‚Äù**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'over-train' and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 138.813, "llm_ms": 1050.471, "top_contexts": [{"rank": 1, "score": 0.7678853, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.48458611615366953, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 20, "text_snippet": "hat train language models in either ‚Äúcompute optimal regime‚Äù [HBM+22] or ‚Äúover-train regime‚Äù, we mainly focus on the quality of data for a given scale .3 We try to calibrate the training data to be closer to the ‚Äúdata optimal‚Äù regime for sm"}, {"rank": 2, "score": 61.17969383076709, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.2871976360403896, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 0, "text_snippet": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone Microsoft Abstract We introduce phi-3-mini , a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both"}, {"rank": 3, "score": 46.55981355484558, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.265692924453652, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 16, "text_snippet": "3. The table shows the Keys/values a query token in block 8 attended to. Blue=local blocks, orange=remote/vertical blocks, gray=blocks skipped. total parameters. Additionally, we utilize the SparseMixer approach [LGC23, LDL+23] for training"}, {"rank": 4, "score": 48.242316454147385, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.2513895637766278, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 6, "text_snippet": "7B parameters), matched the performance of models 25 times larger trained on regular data. In this report we present a new model, phi-3-mini (3.8B parameters), trained for 3.3T tokens on larger and more advanced versions of the datasets use"}, {"rank": 5, "score": 0.6080668, "source": "dense+rrf", "rrf_score": 0.03149801587301587, "rerank_score": 0.2430452152743481, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 22, "text_snippet": "trained on the same data for slightly more epochs (4.8T tokens total as for phi-3-small . The model has 40 heads and 40 layers, with embedding dimension 5120. We observe that some benchmarks improve much less from 7B to 14B than they do fro"}]}
{"case_index": 43, "query": "From 2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"Answering a question then [BLANK] involves retrieving relevant pas- sages from a corpus and feeding these passages, along with the original question, to the LM.\"?", "gold": "essentially", "answer": "The exact term is **‚Äúessentially‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'essentially' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 94.315, "llm_ms": 1032.806, "top_contexts": [{"rank": 1, "score": 0.7165537, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.42835716120636796, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 5, "text_snippet": " training corpus (Kandpal et al., 2022; Mallen et al., 2023). The standard solution to these issues is to rely on Retrieval Augmented Generation (RAG) (Lee et al., 2019; Lewis et al., 2020; Guu et al., 2020). Answering a question then essen"}, {"rank": 2, "score": 0.5933005, "source": "dense+rrf", "rrf_score": 0.027745885954841176, "rerank_score": 0.2412994712564674, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 10, "text_snippet": "n in the input context. We first experiment with multi-document ques- tion answering, which requires models to reason over provided documents to find relevant informa- tion and use it to answer a given question; this task mimics the retriev"}, {"rank": 3, "score": 35.91865861185988, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.22646122556669074, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 9, "text_snippet": "we present Ragas1, a framework for the automated assessment 1Ragas is available at https://github.com/ explodinggradients/ragas .arXiv:2309.15217v2 [cs.CL] 28 Apr 2025  of retrieval augmented generation systems. We fo- cus on settings where"}, {"rank": 4, "score": 0.6020361, "source": "dense+rrf", "rrf_score": 0.028381642512077296, "rerank_score": 0.21774464997681942, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 20, "text_snippet": "trieval augmented LMs is to generate the answer y= [s1,s2, ...,sm] = [w1, w2, ..., w n]containing msentences or ntokens leveraging information retrieved from the corpus. In retrieval augmented LM, the LM typically pairs with a retriever tha"}, {"rank": 5, "score": 29.269158078350195, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.2121186301054028, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 2, "text_snippet": "is work, we provide a generalized view of ac- tive retrieval augmented generation , methods that actively decide when and what to retrieve across the course of the generation. We propose Forward- Looking Active REtrieval augmented generatio"}]}
{"case_index": 44, "query": "From 2005.14165__language_models_are_few_shot_learners.txt, what exact term fills [BLANK] in: \"Since in-context learning involves absorbing many skills and tasks within the [BLANK] of the model, it is plaus\"?", "gold": "parameters", "answer": "The exact term is **‚Äúparameters‚Äù** (i.e., ‚Äúwithin the **parameters** of the model‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'parameters' and is directly supported by the provided context snippet ('within the parameters of the model').", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 51.111, "llm_ms": 998.655, "top_contexts": [{"rank": 1, "score": 0.66147673, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.3477545688270677, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 25, "text_snippet": "skills and tasks within the parameters of the model, it is plausible that in-context learning abilities might show similarly strong gains with scale. 1In the context of language models this has sometimes been called ‚Äúzero-shot transfer‚Äù, bu"}, {"rank": 2, "score": 51.92012729802884, "source": "dense+rrf", "rrf_score": 0.01639344262295082, "rerank_score": 0.31627405861001295, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 24, "text_snippet": "8 ], to 1.5 billion parameters [ RWC+19], to 8 billion parameters [ SPP+19], 11 billion parameters [ RSR+19], and Ô¨Ånally 17 billion parameters [ Tur20 ]. Each increase has brought improvements in text synthesis and/or downstream NLP tasks, "}, {"rank": 3, "score": 0.5374721, "source": "dense+rrf", "rrf_score": 0.02886002886002886, "rerank_score": 0.27520213139382466, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 26, "text_snippet": " learning from zero examples. To avoid this confusion, we use the term ‚Äúmeta-learning‚Äù to capture the inner-loop / outer-loop structure of the general method, and the term ‚Äúin context-learning‚Äù to refer to the inner loop of meta-learning. W"}, {"rank": 4, "score": 25.072403976667164, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.2384232427408459, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 29, "text_snippet": " as well as several novel tasks designed to test rapid adaptation to tasks unlikely to be directly contained in the training set. For each task, we evaluate GPT-3 under 3 conditions: (a) ‚Äúfew-shot learning‚Äù, or in-context learning where we "}, {"rank": 5, "score": 21.75939256368654, "source": "dense+rrf", "rrf_score": 0.014285714285714285, "rerank_score": 0.2326668467427657, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 143, "text_snippet": "ostly simulate zero-shot settings. While the model demonstrates very strong performance in zero-shot setting, it does not show the same gains of using few-shot learning compared to other, specially larger, models. Synthetic data: As Orca 2 "}]}
{"case_index": 45, "query": "From 2005.14165__language_models_are_few_shot_learners.txt, what exact term fills [BLANK] in: \"[BLANK] clearly requires substantial improvement in order to be viable as a practical method of solving language tasks.\"?", "gold": "meta-learning", "answer": "The exact term is **‚ÄúMeta-learning‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'meta-learning' and is directly supported by Context 1 sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 93.672, "llm_ms": 880.715, "top_contexts": [{"rank": 1, "score": 0.62224674, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3425972201460631, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 23, "text_snippet": "tions, and even its 55 F1 CoQa result is now more than 35 points behind the state of the art. Meta-learning clearly requires substantial improvement in order to be viable as a practical method of solving language tasks. Another recent trend"}, {"rank": 2, "score": 22.057548709321253, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.22196007783301736, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 129, "text_snippet": "and symbol mapping errors. Scaling PaLM to 540B Ô¨Åxed a substantial portion of errors in all categories. There are also three notable points regarding why small language models fail. The Ô¨Årst observation is that small language models fail at"}, {"rank": 3, "score": 22.160640001374222, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.2037573893026087, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 48, "text_snippet": " The panels above show four methods for performing a task with a language model ‚Äì Ô¨Åne-tuning is the traditional method, whereas zero-, one-, and few-shot, which we study in this work, require the model to perform the task with only forward "}, {"rank": 4, "score": 20.309676214336942, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.20362563699694336, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 25, "text_snippet": "skills and tasks within the parameters of the model, it is plausible that in-context learning abilities might show similarly strong gains with scale. 1In the context of language models this has sometimes been called ‚Äúzero-shot transfer‚Äù, bu"}, {"rank": 5, "score": 23.45060264291444, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.1987040898697361, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 26, "text_snippet": " learning from zero examples. To avoid this confusion, we use the term ‚Äúmeta-learning‚Äù to capture the inner-loop / outer-loop structure of the general method, and the term ‚Äúin context-learning‚Äù to refer to the inner loop of meta-learning. W"}]}
{"case_index": 46, "query": "From 2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt, what exact term fills [BLANK] in: \"[BLANK] extends Llama2 7B from 4k context to 100k, or Llama2 70B to 32k on a single 8 √óA100 machine.\"?", "gold": "longlora", "answer": "**LongLoRA** extends Llama2 7B from 4k context to 100k, or Llama2 70B to 32k on a single 8 √óA100 machine.", "answer_score": 0.3, "answer_correctness": 0.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "The blank asks for the exact term 'longlora' (lowercase). The answer gives 'LongLoRA' (different casing), so it is not an exact match, though it is supported by the context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 55.633, "llm_ms": 1605.177, "top_contexts": [{"rank": 1, "score": 0.65203834, "source": "dense+rrf", "rrf_score": 0.030679156908665108, "rerank_score": 0.36678041560424246, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 3, "text_snippet": "ll under the premise of trainable embedding and normalization. LongLoRA combines this improved LoRA with S2-Attn. LongLoRA demonstrates strong empirical results on various tasks on Llama2 models from 7B/13B to 70B. LongLoRA extends Llama2 7"}, {"rank": 2, "score": 0.6345691, "source": "dense+rrf", "rrf_score": 0.029116045245077504, "rerank_score": 0.3387233604796002, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 20, "text_snippet": "embeddings. The trained models achieve comparable performance to the full-attention and fully fine-tuned results, while the computational cost is much less as shown in Figure 1. LongLoRA can fine-tune Llama2 7B up to 100k context, or a 70B "}, {"rank": 3, "score": 0.72205657, "source": "dense+rrf", "rrf_score": 0.032018442622950824, "rerank_score": 0.3015749107332677, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 58, "text_snippet": "lama2 13B model, we observe that the perplexity reduces by -0.28. In Table 4, we further examine the maximum context length that we can fine-tune on a single 8 √ó A100 machine. We extend Llama2 7B, 13B, and 70B to 100k, 65536, and 32768 cont"}, {"rank": 4, "score": 0.69541305, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.29602445242618264, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 77, "text_snippet": "l standard attention architecture during inference, making most pre-existing infrastructure and optimization reusable. At the training level, we bridge the gap between LoRA and full fine-tuning with trainable normalization and embedding. Ou"}, {"rank": 5, "score": 40.14505499770757, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.214333970182036, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 0, "text_snippet": "Published as a conference paper at ICLR 2024 LONG LORA: E FFICIENT FINE-TUNING OF LONG - CONTEXT LARGE LANGUAGE MODELS Yukang Chen1Shengju Qian1Haotian Tang2Xin Lai1 Zhijian Liu2Song Han2,3Jiaya Jia1 1CUHK2MIT3NVIDIA ABSTRACT We present Lon"}]}
{"case_index": 47, "query": "From 2302.13971__llama_open_and_efficient_foundation_language_models.txt, what exact term fills [BLANK] in: \"2.2 [BLANK] Following recent work on large language models, our network is based on the transformer architec- ture (Vaswani et al., 2017).\"?", "gold": "architecture", "answer": "The exact term is **‚ÄúArchitecture‚Äù** (i.e., the section header is **‚Äú2.2 Architecture‚Äù**).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context explicitly shows the section header '2.2 Architecture', matching the expected blank term.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 63.783, "llm_ms": 1302.653, "top_contexts": [{"rank": 1, "score": 57.89484894525036, "source": "dense+rrf", "rrf_score": 0.01639344262295082, "rerank_score": 0.3502054948616314, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 19, "text_snippet": "1.5e‚àí44M 1.4T Table 2: Model sizes, architectures, and optimization hyper-parameters. Overall, our entire training dataset contains roughly 1.4T tokens after tokenization. For most of our training data, each token is used only once dur- ing"}, {"rank": 2, "score": 0.6303284, "source": "dense+rrf", "rrf_score": 0.0315136476426799, "rerank_score": 0.2746636477293415, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 8, "text_snippet": "inchilla. In the rest of this paper, we present an overview of the modiÔ¨Åcations we made to the transformer architecture (Vaswani et al., 2017), as well as our training method. We then report the performance of our models and compare with ot"}, {"rank": 3, "score": 0.6080893, "source": "dense+rrf", "rrf_score": 0.0304147465437788, "rerank_score": 0.25683620049895134, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 24, "text_snippet": "architec- ture and the Ô¨Ånal downstream architecture. Model Architecture BERT‚Äôs model architec- ture is a multi-layer bidirectional Transformer en- coder based on the original implementation de- scribed in Vaswani et al. (2017) and released "}, {"rank": 4, "score": 30.01415623782426, "source": "dense+rrf", "rrf_score": 0.014285714285714285, "rerank_score": 0.23076508257295253, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 89, "text_snippet": "y for transformer based language models, which were later reÔ¨Åned by Hoffmann et al. (2022), by adapting the learning rate schedule when scaling datasets. Finally, Wei et al. (2022) studied the effect of scal- ing on the abilities of large l"}, {"rank": 5, "score": 0.61126214, "source": "dense+rrf", "rrf_score": 0.02819138376017471, "rerank_score": 0.23066574760250746, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 4, "text_snippet": "ural networks have proven to be powerful language models, Ô¨Årst in the form of recurrent architectures (Graves, 2013; Jozefowicz et al., 2016; Mikolov et al., 2010) and more recently in the form of Transformers (Vaswani et al., 2017), that u"}]}
{"case_index": 48, "query": "From 2302.13971__llama_open_and_efficient_foundation_language_models.txt, what exact term fills [BLANK] in: \"We kept the data from the 28 largest [BLANK], re- moved the HTML tags from text and sorted the answers by score (from highest to lowest).\"?", "gold": "websites", "answer": "The exact term is **‚Äúwebsites‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'websites' and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 52.46, "llm_ms": 1016.398, "top_contexts": [{"rank": 1, "score": 0.5929677, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.347291545049141, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 17, "text_snippet": " and inline-expanded deÔ¨Ånitions and macros written by users to increase consistency across papers. Stack Exchange [2%]. We include a dump of Stack Exchange, a website of high quality ques- tions and answers that covers a diverse set of do- "}, {"rank": 2, "score": 26.87488002886652, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.22801987725248196, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 82, "text_snippet": " language models in a standard retriever- reader setup. A retrieval system (Contriever, fine- tuned on MS-MARCO) takes an input query from NaturalQuestions-Open and returns the kdocu- ments from Wikipedia with the highest relevance score. T"}, {"rank": 3, "score": 0.5312628, "source": "dense+rrf", "rrf_score": 0.029386529386529386, "rerank_score": 0.20871432701214557, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 78, "text_snippet": "rom similarly- formatted data that may occur in Internet text seen during pre-training, e.g., StackOverflow questionsand answers. To better understand the effect of additional fine- tuning and model scale, we also experimented with Llama-2 "}, {"rank": 4, "score": 24.046500518702963, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.18196907478705956, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 0, "text_snippet": "LLaMA: Open and EfÔ¨Åcient Foundation Language Models Hugo Touvron‚àó, Thibaut Lavril‚àó, Gautier Izacard‚àó, Xavier Martinet Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi√®re, Naman Goyal Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand "}, {"rank": 5, "score": 21.537828121148294, "source": "dense+rrf", "rrf_score": 0.014285714285714285, "rerank_score": 0.17188799380284261, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 29, "text_snippet": "t results on a total of 20 benchmarks: ‚Ä¢Zero-shot. We provide a textual description of the task and a test example. The model either provides an answer using open-ended generation, or ranks the proposed answers. ‚Ä¢Few-shot. We provide a few "}]}
{"case_index": 49, "query": "From 2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt, what exact term fills [BLANK] in: \"In more detail, assume the [BLANK] answers aiare from a Ô¨Åxed answer set, ai‚ààA, where i= 1, .\"?", "gold": "generated", "answer": "The exact term is **‚Äúgenerated‚Äù**, i.e., the sentence reads:\n\n‚ÄúIn more detail, assume the **generated** answers \\(a_i\\) are from a fixed answer set, \\(a_i \\in A\\), where \\(i = 1, \\ldots\\).‚Äù", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'generated' and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 101.807, "llm_ms": 1899.095, "top_contexts": [{"rank": 1, "score": 0.6240299, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3570260387413167, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 21, "text_snippet": "ampling (Fan et al., 2018; Holtzman et al., 2018; Radford et al., 2019), and nucleus sampling (Holtzman et al., 2020). Finally, we aggregate the answers by marginalizing out the sampled reasoning paths and choosing the answer that is the mo"}, {"rank": 2, "score": 29.24444000956369, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.26113861723463294, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 11, "text_snippet": "mainder for $2 per egg, so in  total she sells 7 * $2 = $14 per day.  The answer is $14. The answer is $14. Greedy decode Figure 1: The self-consistency method contains three steps: (1) prompt a language model using chain-of-thought (CoT) p"}, {"rank": 3, "score": 37.84593391943833, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.259129179694191, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 113, "text_snippet": "on. arXiv preprint arXiv:1803.05355 , 2018. Lev S Vygotsky. Thinking and speech. The collected works of LS Vygotsky , 1:39‚Äì285, 1987. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdh- ery, and Denny "}, {"rank": 4, "score": 0.57045066, "source": "dense+rrf", "rrf_score": 0.02871794871794872, "rerank_score": 0.24766948444718656, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 6, "text_snippet": "aths that reach a correct answer (Stanovich & West, 2000). The more that deliberate thinking and analysis is required for a problem (Evans, 2010), the greater the diversity of reasoning paths that can recover the answer. Figure 1 illustrate"}, {"rank": 5, "score": 28.350485267172154, "source": "dense+rrf", "rrf_score": 0.013888888888888888, "rerank_score": 0.24648649611574946, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 86, "text_snippet": "ength sequence in recurrent language models. Nye et al. (2021) improve the logical consistency of samples from a System 1 model by adding a System 2-inspired logical reasoning module. In this paper we focus on a slightly different notion of"}]}
{"case_index": 50, "query": "From 2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt, what exact term fills [BLANK] in: \"This procedure aligns the behavior of GPT-3 to the stated preferences of a speciÔ¨Åc group of people (mostly our labelers and [BLANK]), rather than any broader notion of ‚Äúhuman values‚Äù; we discuss this further in Section 5.2.\"?", "gold": "researchers", "answer": "The exact term that fills **[BLANK]** is **‚Äúresearchers‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'researchers' and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 128.618, "llm_ms": 1545.145, "top_contexts": [{"rank": 1, "score": 0.7987207, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.5257757244789326, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 12, "text_snippet": "We illustrate this process in Figure 2. This procedure aligns the behavior of GPT-3 to the stated preferences of a speciÔ¨Åc group of people (mostly our labelers and researchers), rather than any broader notion of ‚Äúhuman values‚Äù; we discuss t"}, {"rank": 2, "score": 0.6056893, "source": "dense+rrf", "rrf_score": 0.02886002886002886, "rerank_score": 0.2960317967673377, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 9, "text_snippet": "fabricate information or mislead the user), and harmless (they should not cause physical, psychological, or social harm to people or the environment). We elaborate on the evaluation of these criteria in Section 3.6. We focus on Ô¨Åne-tuning a"}, {"rank": 3, "score": 35.95421796058162, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.2852419886559792, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 27, "text_snippet": " in Section 4, divided into three parts: results on the API prompt distribution (4.1), results on public NLP datasets (4.2), and qualitative results (4.3). Finally we give an extended discussion of our work in Section 5, including implicati"}, {"rank": 4, "score": 0.6756041, "source": "dense+rrf", "rrf_score": 0.031754032258064516, "rerank_score": 0.283863568920892, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 140, "text_snippet": "el (and its training data), the Ô¨Åne-tuning data, and the alignment method used. In this section, we describe a number of factors that inÔ¨Çuence the Ô¨Åne-tuning data speciÔ¨Åcally, to ultimately determine what and who we‚Äôre aligning to. We then "}, {"rank": 5, "score": 34.776854104685704, "source": "dense+rrf", "rrf_score": 0.013888888888888888, "rerank_score": 0.2593485700137006, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 164, "text_snippet": "consensus amongst many groups. We discuss some related considerations in Section 5.2. 5.5 Broader impacts This work is motivated by our aim to increase the positive impact of large language models by training them to do what a given set of "}]}
{"case_index": 51, "query": "From 2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt, what exact term fills [BLANK] in: \"It is designed to equip readers and professionals with a detailed and structured [BLANK] of both large models a\"?", "gold": "understanding", "answer": "The exact term that fills **[BLANK]** is:\n\n**understanding**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'understanding' and is directly supported by the provided context snippet.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 68.944, "llm_ms": 1296.827, "top_contexts": [{"rank": 1, "score": 0.55712146, "source": "dense+rrf", "rrf_score": 0.031099324975891997, "rerank_score": 0.3113278935537607, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 10, "text_snippet": "lacking analysis and summarization of how to evaluate RAG. This paper compre- hensively reviews the downstream tasks, datasets, benchmarks, and evaluation methods applicable to RAG. Overall, this paper sets out to meticulously compile and c"}, {"rank": 2, "score": 0.6151422, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.257899035740705, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 11, "text_snippet": "h a detailed and structured understanding of both large models and RAG. It aims to illuminate the evolution of retrieval augmentation techniques, assess the strengths and weaknesses of various approaches in their respective contexts, and sp"}, {"rank": 3, "score": 0.59776354, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.24147682861244452, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 79, "text_snippet": "on the long-context understanding ability of LLMs, and we still have a long way to go in harnessing open-source LLMs within our framework. 10  4 Related Work 4.1 Retrieval-Augmented Generation. Augmenting language models with information re"}, {"rank": 4, "score": 24.616847154088116, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.232984307060226, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 22, "text_snippet": " generate the complete answer at once y=LM([Dx,x]). 2.3 Active Retrieval Augmented Generation To aid long-form generation with retrieval, we pro- pose active retrieval augmented generation. It is a generic framework that actively decides wh"}, {"rank": 5, "score": 0.5607258, "source": "dense+rrf", "rrf_score": 0.03007688828584351, "rerank_score": 0.22330116815920825, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 49, "text_snippet": " units both affect the final generation results. 1) Data Structure: Initially, text is s the mainstream source of retrieval. Subsequently, the retrieval source expanded to in- clude semi-structured data (PDF) and structured data (Knowl- edg"}]}
{"case_index": 52, "query": "From 2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt, what exact term fills [BLANK] in: \"We also show in Section 3 additional beneÔ¨Åts when Ô¨Ånetuning is enabled, and in Section 4 how ReAct [BLANK] is robust to prompt selections.\"?", "gold": "performance", "answer": "The exact term is **‚Äúperformance‚Äù** (i.e., ‚Äúhow ReAct **performance** is robust to prompt selections.‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Model answer matches expected term 'performance' and is directly supported by the provided context sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 67.801, "llm_ms": 1470.94, "top_contexts": [{"rank": 1, "score": 0.7130671, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.40603755876588193, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 34, "text_snippet": " with distinct action spaces and reasoning needs, including but not limited to QA, fact veriÔ¨Åcation, text game, and web navigation. C) Performant and robust :ReAct shows strong generalization to new task instances while learning solely from"}, {"rank": 2, "score": 0.6571644, "source": "dense+rrf", "rrf_score": 0.02964254577157803, "rerank_score": 0.25560362272725407, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 35, "text_snippet": "tion 4 how ReAct performance is robust to prompt selections. D) Human aligned and controllable :ReAct promises an interpretable sequential decision making and reasoning process where humans can easily inspect reasoning and factual correctne"}, {"rank": 3, "score": 27.763968197814243, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.23372486676623966, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 69, "text_snippet": " with both ensemble approaches and we show the results in Appendix A.1.4. 7  Published as a conference paper at ICLR 2023 3.5 A DDITIONAL STUDIES We conducted a number of additional experiments to analyze different aspects of the self-consi"}, {"rank": 4, "score": 0.6959369, "source": "dense+rrf", "rrf_score": 0.030776515151515152, "rerank_score": 0.22299719972755844, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 58, "text_snippet": "lty to learn both reasoning and acting from in-context examples. However, when Ô¨Ånetuned with just 3,000 examples, ReAct becomes the best method among the four, with PaLM-8B Ô¨Ånetuned ReAct outperforming all PaLM-62B prompting methods, and Pa"}, {"rank": 5, "score": 0.6748339, "source": "dense+rrf", "rrf_score": 0.03055037313432836, "rerank_score": 0.22104143278989044, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 19, "text_snippet": "udies on how reasoning and acting can be combined in a synergistic manner for general task solving, and if such a combination can bring systematic beneÔ¨Åts compared to reasoning or acting alone. In this work, we present ReAct , a general par"}]}
{"case_index": 53, "query": "From 2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt, what exact term fills [BLANK] in: \"e than our SFT baseline, and labelers signiÔ¨Åcantly prefer [BLANK] to these models (InstructGPT has a 73.4 ¬±2%winrate vs.\"?", "gold": "instructgpt", "answer": "The exact term that fills **[BLANK]** is **‚ÄúInstructGPT‚Äù**.", "answer_score": 0.3, "answer_correctness": 0.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Expected exact term is lowercase 'instructgpt'; model answered 'InstructGPT' (case mismatch). Context supports 'InstructGPT' in the sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 52.861, "llm_ms": 960.325, "top_contexts": [{"rank": 1, "score": 0.75874627, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3951548918970429, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 23, "text_snippet": "e than our SFT baseline, and labelers signiÔ¨Åcantly prefer InstructGPT to these models (InstructGPT has a 73.4 ¬±2%winrate vs. our baseline, compared to 26.8 ¬±2%and 29.8¬±2%for our version of T0 and FLAN, respectively). InstructGPT models show"}, {"rank": 2, "score": 0.7130705, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.3283890675374329, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 22, "text_snippet": "r language models are used. We compare GPT-3 Ô¨Åne-tuned on our human preference data (i.e. InstructGPT) to GPT-3 Ô¨Åne-tuned on two different compilations of public NLP tasks: the FLAN (Wei et al., 2021) and T0 (Sanh et al., 2021) (in particul"}, {"rank": 3, "score": 0.6849792, "source": "dense+rrf", "rrf_score": 0.030303030303030304, "rerank_score": 0.23708629465755734, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 14, "text_snippet": "tuning (SFT), (2) reward model (RM) training, and (3) reinforcement learning via proximal policy optimization (PPO) on this reward model. Blue arrows indicate that this data is used to train one of our models. In Step 2, boxes A-D are sampl"}, {"rank": 4, "score": 32.670052545793325, "source": "dense+rrf", "rrf_score": 0.014285714285714285, "rerank_score": 0.2355271631559828, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 95, "text_snippet": " labelers in their training set. Public NLP datasets are not reÔ¨Çective of how our language models are used. In Figure 5, we also compare InstructGPT to our 175B GPT-3 baselines Ô¨Åne-tuned on the FLAN (Wei et al., 2021) and T0 (Sanh et al., 2"}, {"rank": 5, "score": 0.70995575, "source": "dense+rrf", "rrf_score": 0.03149801587301587, "rerank_score": 0.23520495658231907, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 15, "text_snippet": "ollows: Labelers signiÔ¨Åcantly prefer InstructGPT outputs over outputs from GPT-3. On our test set, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having over 100x fewer parameters. Th"}]}
{"case_index": 54, "query": "From 2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt, what exact term fills [BLANK] in: \"[BLANK] and closed-domain QA), InstructGPT models make up information not present in the input about half as often as GPT-3 (a 21% vs.\"?", "gold": "summarization", "answer": "The term is **‚Äúsummarization‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context explicitly lists '(e.g. summarization and closed-domain QA)', so the blank is 'summarization'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 101.711, "llm_ms": 936.57, "top_contexts": [{"rank": 1, "score": 0.71993136, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.39847573589448076, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 17, "text_snippet": "nformative answers about twice as often as GPT-3. Our results are equally strong on the subset of questions that were not adversarially selected against GPT-3. On ‚Äúclosed-domain‚Äù tasks from our API prompt distribution, where the output shou"}, {"rank": 2, "score": 0.589399, "source": "dense+rrf", "rrf_score": 0.029857397504456328, "rerank_score": 0.2539436474521124, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 16, "text_snippet": "owing instructions. Outputs from our 175B InstructGPT are preferred to 175B GPT-3 outputs 85 ¬±3% of the time, and preferred 71 ¬±4% of the time to few-shot 175B GPT-3. InstructGPT models also generate more appropriate outputs according to ou"}, {"rank": 3, "score": 29.778572446759597, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.2501947474421853, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 9, "text_snippet": "fabricate information or mislead the user), and harmless (they should not cause physical, psychological, or social harm to people or the environment). We elaborate on the evaluation of these criteria in Section 3.6. We focus on Ô¨Åne-tuning a"}, {"rank": 4, "score": 0.5988126, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.24549733477009972, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 89, "text_snippet": "e concrete axes. SpeciÔ¨Åcally, compared to GPT-3, InstructGPT outputs are more appropriate in the context of a customer assistant, more often follow explicit constraints deÔ¨Åned in the instruction (e.g. ‚ÄúWrite your answer in 2 paragraphs or l"}, {"rank": 5, "score": 0.5855611, "source": "dense+rrf", "rrf_score": 0.02900988017658188, "rerank_score": 0.21988258812939274, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 115, "text_snippet": " compared to GPT-3 175B with no additional preÔ¨Åxing. Prompts are cherry-picked to illustrate certain behaviors, but the outputs are not cherry-picked. (1) InstructGPT can follow instructions in other languages, though it sometimes generates"}]}
{"case_index": 55, "query": "From 2302.13971__llama_open_and_efficient_foundation_language_models.txt, what exact term fills [BLANK] in: \"(2022) is to determine how to best scale the dataset and model sizes for a [BLANK] training compute budget.\"?", "gold": "particular", "answer": "The exact term is **‚Äúparticular‚Äù** (as in ‚Äúfor a **particular** training compute budget.‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches the expected term and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 74.187, "llm_ms": 1669.603, "top_contexts": [{"rank": 1, "score": 0.6787485, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3750103262266428, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 3, "text_snippet": ", recent work from Hoffmann et al. (2022) shows that, for a given compute budget, the best performances are not achieved by the largest mod- els, but by smaller models trained on more data. The objective of the scaling laws from Hoff- mann "}, {"rank": 2, "score": 0.5888176, "source": "dense+rrf", "rrf_score": 0.02786377708978328, "rerank_score": 0.25705060080272396, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 85, "text_snippet": "ing. There is a long history of scaling for language models, for both the model and dataset sizes. Brants et al. (2007) showed the beneÔ¨Åts of using language models trained on 2 trillion tokens, resulting in 300 billion n-grams, on the quali"}, {"rank": 3, "score": 0.6252898, "source": "dense+rrf", "rrf_score": 0.03055037313432836, "rerank_score": 0.2537183800666741, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 5, "text_snippet": "at inference. For instance, although Hoffmann et al. (2022) recommends training a 10B model on 200B tokens, we Ô¨Ånd that the performance of a 7B model continues to improve even after 1T tokens. The focus of this work is to train a series of "}, {"rank": 4, "score": 20.88706047987368, "source": "dense+rrf", "rrf_score": 0.014285714285714285, "rerank_score": 0.24849244230878453, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 56, "text_snippet": "tion Large language models have been showed to re- produce and amplify biases that are existing in the training data (Sheng et al., 2019; Kurita et al., 2019), and to generate toxic or offensive con- tent (Gehman et al., 2020). As our train"}, {"rank": 5, "score": 0.58842635, "source": "dense+rrf", "rrf_score": 0.02668564312399929, "rerank_score": 0.21423031577163273, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 78, "text_snippet": "rom similarly- formatted data that may occur in Internet text seen during pre-training, e.g., StackOverflow questionsand answers. To better understand the effect of additional fine- tuning and model scale, we also experimented with Llama-2 "}]}
{"case_index": 56, "query": "From 2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt, what exact term fills [BLANK] in: \"The imbalanced ‚Äúheavy‚Äù retriever and ‚Äúlight‚Äù reader design can lead to [BLANK] performance.\"?", "gold": "sub-optimal", "answer": "The exact term is **‚Äúsub-optimal‚Äù** (as in: ‚Äúcan lead to **sub-optimal** performance.‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'sub-optimal' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 53.067, "llm_ms": 1459.23, "top_contexts": [{"rank": 1, "score": 0.72206384, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3058548961179449, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 1, "text_snippet": "r to search over a large corpus to find the ‚Äúneedle‚Äù unit. In contrast, the readers only need to generate answers from the short retrieved units. The imbalanced ‚Äúheavy‚Äù retriever and ‚Äúlight‚Äù reader design can lead to sub-optimal performance"}, {"rank": 2, "score": 0.6358242, "source": "dense+rrf", "rrf_score": 0.03125, "rerank_score": 0.27038331315647757, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 19, "text_snippet": "e end performance dips significantly due to the huge amount of ‚Äúhard negatives‚Äù, which confuses the reader. With ‚Äúlong retriever units‚Äù, we observe an entirely different trend. As we recall more units (from 1 to 8 units), both the recall an"}, {"rank": 3, "score": 0.625551, "source": "dense+rrf", "rrf_score": 0.030834914611005692, "rerank_score": 0.25761440575968036, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 9, "text_snippet": " performance. (Down) QA performance compared with other methods on the NQ dataset and the HotpotQA dataset. these retrievals, which is a fairly easy task. This kind of imbalanced design, with a ‚Äúheavy‚Äù retriever and a ‚Äúlight‚Äù reader, puts t"}, {"rank": 4, "score": 0.61177903, "source": "dense+rrf", "rrf_score": 0.029437229437229435, "rerank_score": 0.25349575132725427, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 21, "text_snippet": "operates on long retrieval units, with only a few (typically fewer than 10) being fed into the reader. An illustrative example is shown in Figure 2. 2.1 Long Retriever The traditional RAG framework employs smaller retrieval units and priori"}, {"rank": 5, "score": 41.285960261147714, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.24892813863764146, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 0, "text_snippet": "LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs ‚ô†Ziyan Jiang,‚ô†Xueguang Ma,‚ô†Wenhu Chen ‚ô†University of Waterloo ziyanjiang528@gmail.com ,{x93ma ,wenhuchen}@uwaterloo.ca Project Website: https://tiger-ai-lab.github.io/"}]}
{"case_index": 57, "query": "From 2305.06983__active_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"If so (step 2 and 3), the system retrieves relevant documents and [BLANK] the sentence.\"?", "gold": "regenerates", "answer": "The exact term is **‚Äúregenerates‚Äù** (i.e., ‚Äúthe system retrieves relevant documents and **regenerates** the sentence.‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'regenerates' and is directly supported by Context 1/3/4.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 95.635, "llm_ms": 1224.011, "top_contexts": [{"rank": 1, "score": 0.6408826, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3348644707671543, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 11, "text_snippet": "eddocumentsLMGeneration$%$%% Figure 1: An illustration of forward-looking active retrieval augmented generation (FLARE). Starting with the user input xand initial retrieval results Dx, FLARE iteratively generates a temporary next sentence ("}, {"rank": 2, "score": 0.5900636, "source": "dense+rrf", "rrf_score": 0.031024531024531024, "rerank_score": 0.2362151390926256, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 32, "text_snippet": "iable. There- fore, we propose a more direct way of forward- looking active retrieval that uses the next sentence to decide when and what to retrieve. 3.2.1 Confidence-based Active Retrieval As shown in Figure 1, at step t, we first generat"}, {"rank": 3, "score": 0.6144545, "source": "dense+rrf", "rrf_score": 0.0315136476426799, "rerank_score": 0.23461544672268772, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 17, "text_snippet": "ng Active REtrieval augmented generation (FLARE ), as illustrated in Figure 1. FLARE iter- atively generates a temporary next sentence , use it as the query to retrieve relevant documents if it contains low-probability tokens and regenerate"}, {"rank": 4, "score": 0.613006, "source": "dense+rrf", "rrf_score": 0.03149801587301587, "rerank_score": 0.2218085865507435, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 16, "text_snippet": ". When deciding what to retrieve , it is important to consider what LMs intend to gen- erate in the future, as the goal of active retrieval is to benefit future generations. Therefore, we propose anticipating the future by generating a temp"}, {"rank": 5, "score": 0.54141927, "source": "dense+rrf", "rrf_score": 0.027651515151515153, "rerank_score": 0.2212580516823002, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 22, "text_snippet": " generate the complete answer at once y=LM([Dx,x]). 2.3 Active Retrieval Augmented Generation To aid long-form generation with retrieval, we pro- pose active retrieval augmented generation. It is a generic framework that actively decides wh"}]}
{"case_index": 58, "query": "From 2005.14165__language_models_are_few_shot_learners.txt, what exact term fills [BLANK] in: \"To avoid this confusion, we use the term ‚Äúmeta-learning‚Äù to capture the inner-loop / outer-loop structure of the general method, and the term ‚Äúin [BLANK]‚Äù to refer to the inner loop of meta-learning.\"?", "gold": "context-learning", "answer": "The exact term is **‚Äúcontext-learning‚Äù**, i.e., the phrase is **‚Äúin context-learning‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'context-learning' and is directly supported by Context 1 quote ('in context-learning').", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 113.761, "llm_ms": 1555.512, "top_contexts": [{"rank": 1, "score": 0.72731054, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.46660108327740585, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 26, "text_snippet": " learning from zero examples. To avoid this confusion, we use the term ‚Äúmeta-learning‚Äù to capture the inner-loop / outer-loop structure of the general method, and the term ‚Äúin context-learning‚Äù to refer to the inner loop of meta-learning. W"}, {"rank": 2, "score": 0.53661007, "source": "dense+rrf", "rrf_score": 0.030798389007344232, "rerank_score": 0.3152836071350484, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 25, "text_snippet": "skills and tasks within the parameters of the model, it is plausible that in-context learning abilities might show similarly strong gains with scale. 1In the context of language models this has sometimes been called ‚Äúzero-shot transfer‚Äù, bu"}, {"rank": 3, "score": 0.583424, "source": "dense+rrf", "rrf_score": 0.0315136476426799, "rerank_score": 0.2649017809338967, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 27, "text_snippet": "of whether the model learns new tasks from scratch at inference time or simply recognizes patterns seen during training ‚Äì this is an important issue which we discuss later in the paper, but ‚Äúmeta-learning‚Äù is intended to encompass both poss"}, {"rank": 4, "score": 0.65390706, "source": "dense+rrf", "rrf_score": 0.031754032258064516, "rerank_score": 0.22273590488149295, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 306, "text_snippet": "alearning in language models has been utilized in [ RWC+19], though with much more limited results and no systematic study. More broadly, language model metalearning has an inner-loop-outer-loop structure, making it structurally similar to "}, {"rank": 5, "score": 21.885790521102965, "source": "dense+rrf", "rrf_score": 0.013888888888888888, "rerank_score": 0.21961967447503317, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 51, "text_snippet": "s assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior. We will refer to it as the cautious system instruction . Few Shot Data : The dataset above does no"}]}
{"case_index": 59, "query": "From 2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt, what exact term fills [BLANK] in: \"3 Dense Passage Retriever (DPR) We focus our research in this work on improv- ing the retrieval component in [BLANK] QA.\"?", "gold": "open-domain", "answer": "The exact term that fills **[BLANK]** is **‚Äúopen-domain‚Äù** (as in: ‚Äúimproving the retrieval component in **open-domain** QA.‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'open-domain' and is directly supported by Context 1 sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 82.833, "llm_ms": 1370.645, "top_contexts": [{"rank": 1, "score": 0.8090004, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.4038442127696998, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 17, "text_snippet": "rieval accuracy , which is the fraction of ques- tions for whichCFcontains a span that answers the question. 3 Dense Passage Retriever (DPR) We focus our research in this work on improv- ing the retrieval component in open-domain QA. Given "}, {"rank": 2, "score": 46.2608366603334, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.28620672695425103, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 85, "text_snippet": "eving good performance on open-domain QA and other knowledge-intensive tasks. 8 Conclusion In this work, we demonstrated that dense retrieval can outperform and potentially replace the tradi- tional sparse retrieval component in open-domain"}, {"rank": 3, "score": 0.6577026, "source": "dense+rrf", "rrf_score": 0.03055037313432836, "rerank_score": 0.24894347218176663, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 1, "text_snippet": "where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented us- ingdense representations alone, where em- beddings are learned from a sma"}, {"rank": 4, "score": 31.20672141897206, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.22252924670565055, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 19, "text_snippet": "model must reason over, potentially decreasing ac- curacy. To better understand this trade-off in prac- tice, we perform a case study with retriever-reader models on open-domain question answering (¬ß5). In contrast to our controlled multi-d"}, {"rank": 5, "score": 0.63413286, "source": "dense+rrf", "rrf_score": 0.02877846790890269, "rerank_score": 0.21303578178379073, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 41, "text_snippet": "t contain the answer. Single andMulti denote that our Dense Passage Retriever (DPR) was trained using individial or combined training datasets (all the datasets excluding SQuAD). See text for more details. traditional retrieval methods, the"}]}
{"case_index": 60, "query": "From 2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt, what exact term fills [BLANK] in: \"For example, large language models can generate outputs that are [BLANK], toxic, or simply not helpful to the user.\"?", "gold": "untruthful", "answer": "The exact term is **‚Äúuntruthful‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'untruthful' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 53.106, "llm_ms": 946.732, "top_contexts": [{"rank": 1, "score": 0.60764664, "source": "dense+rrf", "rrf_score": 0.031544957774465976, "rerank_score": 0.3806923457890202, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 1, "text_snippet": ". For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language m"}, {"rank": 2, "score": 0.5655322, "source": "dense+rrf", "rrf_score": 0.028629032258064516, "rerank_score": 0.2947682389769392, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 0, "text_snippet": "Training language models to follow instructions with human feedback Long Ouyang‚àóJeff Wu‚àóXu Jiang‚àóDiogo Almeida‚àóCarroll L. Wainwright‚àó Pamela Mishkin‚àóChong Zhang Sandhini Agarwal Katarina Slama Alex Ray John Schulman Jacob Hilton Fraser Kelt"}, {"rank": 3, "score": 31.654469278830955, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.26346133679484063, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 75, "text_snippet": " models that act in accordance with user intentions. More practically, for the purpose of our language tasks, we use a framework similar to Askell et al. (2021), who deÔ¨Åne models to be aligned if they are helpful, honest, and harmless. To b"}, {"rank": 4, "score": 0.6778023, "source": "dense+rrf", "rrf_score": 0.03177805800756621, "rerank_score": 0.25967581015395735, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 157, "text_snippet": "lly harmful or dishonest response, we allow our model to generate these outputs. Training our model to be harmless despite user instructions is important, but is also difÔ¨Åcult because whether an output is harmful depends on the context in w"}, {"rank": 5, "score": 0.61931646, "source": "dense+rrf", "rrf_score": 0.030798389007344232, "rerank_score": 0.25572677704377456, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 155, "text_snippet": "ignment techniques to Ô¨Åne-tune language models to follow a wide range of instructions. There are many open questions to explore to further align language model behavior with what people actually want them to do. Many methods could be tried "}]}
{"case_index": 61, "query": "From 2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt, what exact term fills [BLANK] in: \"e models use their input context, we release our code and evaluation data.1 2 [BLANK] Question Answering Our goal is to better understand how language mod- els use their input context.\"?", "gold": "multi-document", "answer": "The exact term is **‚ÄúMulti-Document‚Äù**, as in:\n\n**‚Äú2 Multi-Document Question Answering‚Äù**", "answer_score": 0.3, "answer_correctness": 0.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context shows the heading '2 Multi-Document Question Answering', but the expected blank term is lowercase 'multi-document'. Model answered 'Multi-Document', which does not exactly match the expected term.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 58.321, "llm_ms": 1098.862, "top_contexts": [{"rank": 1, "score": 0.7708249, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.4826146561364627, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 22, "text_snippet": "e models use their input context, we release our code and evaluation data.1 2 Multi-Document Question Answering Our goal is to better understand how language mod- els use their input context. To this end, we analyze model performance on mul"}, {"rank": 2, "score": 0.673836, "source": "dense+rrf", "rrf_score": 0.0315136476426799, "rerank_score": 0.3235206005578086, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 49, "text_snippet": "UIDs. The relevant key-value pair for answering the query is bolded here within the input context for clarity. indicate that extended-context models are not nec- essarily better than their non-extended counterparts at using their input cont"}, {"rank": 3, "score": 0.6902535, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.31286198827350437, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 21, "text_snippet": "eir input context and introduces new evaluation protocols for future long- context models; to claim that a language model can robustly use information within long input con- texts, it is necessary to show that its performance is minimally a"}, {"rank": 4, "score": 38.95418438160172, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.3061404576273475, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 72, "text_snippet": "d‚Äîafter their initial pre-training, they undergo supervised fine-tuning on a dataset of instructions and responses. The task specification and/or in- struction is commonly placed at the beginning of the input context in supervised instructi"}, {"rank": 5, "score": 0.6635329, "source": "dense+rrf", "rrf_score": 0.029910714285714284, "rerank_score": 0.29501691944585007, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 2, "text_snippet": "urs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts, even for explicitly long-context models. Our analysis provides a better understanding "}]}
{"case_index": 62, "query": "From 2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt, what exact term fills [BLANK] in: \"Naive RAG The Naive RAG research paradigm represents the earli- est [BLANK], which gained prominence shortly after the 3 Fig.\"?", "gold": "methodology", "answer": "The exact term that fills **[BLANK]** is:\n\n**‚Äúmethodology‚Äù**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'methodology' and is directly supported by Context 1 ('earliest methodology').", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 67.345, "llm_ms": 1599.969, "top_contexts": [{"rank": 1, "score": 80.30720177364567, "source": "dense+rrf", "rrf_score": 0.01639344262295082, "rerank_score": 0.3149561775141749, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 18, "text_snippet": "g, and we categorize it into three stages: Naive RAG, Advanced RAG, and Modular RAG, as showed in Figure 3. Despite RAG method are cost-effective and surpass the performance of the native LLM, they also exhibit several limitations. The deve"}, {"rank": 2, "score": 0.6491577, "source": "dense+rrf", "rrf_score": 0.02964426877470356, "rerank_score": 0.1845465660305793, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 19, "text_snippet": "hortly after the  3 Fig. 2. A representative instance of the RAG process applied to question answering. It mainly consists of 3 steps. 1) Indexing. Documents are split into chunks, encoded into vectors, and stored in a vector database. 2) R"}, {"rank": 3, "score": 0.68598866, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.1833255428190171, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 3, "text_snippet": "augmentation techniques. The paper highlights the state-of-the- art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-"}, {"rank": 4, "score": 0.6231947, "source": "dense+rrf", "rrf_score": 0.027884615384615386, "rerank_score": 0.18124207311334273, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 0, "text_snippet": "1 Retrieval-Augmented Generation for Large Language Models: A Survey Yunfan Gaoa, Yun Xiongb, Xinyu Gaob, Kangxiang Jiab, Jinliu Panb, Yuxi Bic, Yi Daia, Jiawei Suna, Meng Wangc, and Haofen Wanga,c aShanghai Research Institute for Intellige"}, {"rank": 5, "score": 0.648626, "source": "dense+rrf", "rrf_score": 0.02694394213381555, "rerank_score": 0.18105393846318277, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 20, "text_snippet": "er. widespread adoption of ChatGPT. The Naive RAG follows a traditional process that includes indexing, retrieval, and generation, which is also characterized as a ‚ÄúRetrieve-Read‚Äù framework [7]. Indexing starts with the cleaning and extract"}]}
{"case_index": 63, "query": "From 2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt, what exact term fills [BLANK] in: \"We took a closer look at the model‚Äôs output [BLANK] and found this is because for each (ri,ai), the normaliz\"?", "gold": "probabilities", "answer": "The exact term is **‚Äúprobabilities‚Äù** (as in: ‚ÄúWe took a closer look at the model‚Äôs output **probabilities** and found this is because for each (ri,ai), the normaliz‚Ä¶‚Äù)", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'probabilities' and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 96.139, "llm_ms": 1497.427, "top_contexts": [{"rank": 1, "score": 0.57571554, "source": "dense+rrf", "rrf_score": 0.03047794966520434, "rerank_score": 0.3529988622466292, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 25, "text_snippet": "t1,...,tk‚àí1), (1) where logP(tk|prompt ,question , t1, . . . , t k‚àí1)is the log probability of generating the k-th token tkin(ri,ai)conditioned on the previous tokens, and Kis the total number of tokens in (ri,ai). In Table 1, we show that "}, {"rank": 2, "score": 0.5790597, "source": "dense+rrf", "rrf_score": 0.029631255487269532, "rerank_score": 0.2876301041942745, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 40, "text_snippet": " 4  Published as a conference paper at ICLR 2023 3.2 M AINRESULTS We report the results of self-consistency averaged over 10 runs, where we sampled 40 outputs independently from the decoder in each run. The baseline we compare to is chain-o"}, {"rank": 3, "score": 0.57255214, "source": "dense+rrf", "rrf_score": 0.0136986301369863, "rerank_score": 0.2595160215339395, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 6, "text_snippet": "aths that reach a correct answer (Stanovich & West, 2000). The more that deliberate thinking and analysis is required for a problem (Evans, 2010), the greater the diversity of reasoning paths that can recover the answer. Figure 1 illustrate"}, {"rank": 4, "score": 0.5765311, "source": "dense+rrf", "rrf_score": 0.029910714285714284, "rerank_score": 0.25829242297584554, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 113, "text_snippet": "on. arXiv preprint arXiv:1803.05355 , 2018. Lev S Vygotsky. Thinking and speech. The collected works of LS Vygotsky , 1:39‚Äì285, 1987. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdh- ery, and Denny "}, {"rank": 5, "score": 0.54925793, "source": "dense+rrf", "rrf_score": 0.027242401779095628, "rerank_score": 0.25774176931115605, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 41, "text_snippet": "The results are shown in Table 2.7Self-consistency improves the arithmetic reasoning performance over all four language models signiÔ¨Åcantly over chain-of-thought prompting. More surprisingly, the gains become more signiÔ¨Åcant when the langua"}]}
{"case_index": 64, "query": "From 2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"3 Evaluation [BLANK] We consider a standard RAG setting, where given a question q, the system first retrieves some context c(q)and then uses the retrieved context to generate an answer as(q).\"?", "gold": "strategies", "answer": "The exact term is **‚ÄúStrategies‚Äù**, i.e., the heading is:\n\n**‚Äú3 Evaluation Strategies‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context 1 explicitly shows the section heading '3 Evaluation Strategies', matching the blank. Answer is correct and fully supported.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 97.948, "llm_ms": 1589.685, "top_contexts": [{"rank": 1, "score": 0.7516644, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.4781644413945809, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 19, "text_snippet": "Yuan et al., 2021) similarly uses refer- ence answers to compute aspects such as precision (estimated as the probability of generating the gen- erated answer given the reference) and recall (esti- mated as the probability of generating the "}, {"rank": 2, "score": 35.36322112702383, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.28098322016388355, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 42, "text_snippet": "r (Dasigi et al., 2021) is an information-seeking question answering dataset over academic research papers. Each question is written as a followup to the title and abstract of a particular paper, and the answer, if present, is identified in"}, {"rank": 3, "score": 0.61585283, "source": "dense+rrf", "rrf_score": 0.027884615384615386, "rerank_score": 0.27079233180307083, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 20, "text_snippet": "generate an answer as(q). When building a RAG system,  we usually do not have access to human-annotated datasets or reference answers. We therefore fo- cus on metrics that are fully self-contained and reference-free. We focus in particular "}, {"rank": 4, "score": 30.62503989906804, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.2513931606934072, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 16, "text_snippet": ". When deciding what to retrieve , it is important to consider what LMs intend to gen- erate in the future, as the goal of active retrieval is to benefit future generations. Therefore, we propose anticipating the future by generating a temp"}, {"rank": 5, "score": 30.607781370122584, "source": "dense+rrf", "rrf_score": 0.014285714285714285, "rerank_score": 0.2499264025314366, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 10, "text_snippet": "n in the input context. We first experiment with multi-document ques- tion answering, which requires models to reason over provided documents to find relevant informa- tion and use it to answer a given question; this task mimics the retriev"}]}
{"case_index": 65, "query": "From 2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt, what exact term fills [BLANK] in: \"(2020), in which a language model is given in-context exemplars of input‚Äìoutput pairs before outputting a [BLANK] for a test-time example.\"?", "gold": "prediction", "answer": "The exact term is **‚Äúprediction‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context 1 explicitly contains the phrase 'before outputting a prediction for a test-time example,' matching expected answer.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 47.881, "llm_ms": 980.193, "top_contexts": [{"rank": 1, "score": 0.70621336, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.4024623413281521, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 22, "text_snippet": "rd few-shot prompting, popularized by Brown et al. (2020), in which a language model is given in-context exemplars of input‚Äìoutput pairs before outputting a prediction for a test-time example. Exemplars are formatted as questions and answer"}, {"rank": 2, "score": 0.63412446, "source": "dense+rrf", "rrf_score": 0.02967032967032967, "rerank_score": 0.27686767020113323, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 30, "text_snippet": " output ‚ü©triples for arithmetic, commonsense, and symbolic reasoning benchmarks. Chains of thought are highlighted. Full prompts in Appendix G. math word problems, we used this single set of eight chain of thought exemplars for all benchmar"}, {"rank": 3, "score": 41.01505135438646, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.23650793650793164, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 114, "text_snippet": "-augmented ensembles in language models. arXiv preprint arXiv:2207.00747 , 2022b. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models"}, {"rank": 4, "score": 31.578909258131997, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.23623188405796616, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 67, "text_snippet": " thought in the few-shot exemplars; all the model has to do is repeat the same steps with the new symbols in the test-time example. And yet, small models still fail‚Äîthe ability to perform abstract manipulations on unseen symbols for these t"}, {"rank": 5, "score": 0.6275939, "source": "dense+rrf", "rrf_score": 0.027046783625730993, "rerank_score": 0.23518013988656258, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 139, "text_snippet": "Model OutputQuestion Question QuestionFigure 10: Examples of semantic understanding and one-step missing errors that were Ô¨Åxed by scaling PaLM from 62B to 540B. A.2 What is the role of prompt engineering? One of the key considerations of pr"}]}
{"case_index": 66, "query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"Therefore, we propose [BLANK] , a model that learns to use tools in a novel way, which fulÔ¨Ålls the following desiderata: ‚Ä¢T\"?", "gold": "toolformer", "answer": "The exact term that fills **[BLANK]** is **‚ÄúToolformer‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "The blank is filled by 'Toolformer' in the provided context; model answer matches exactly and is fully supported.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 63.443, "llm_ms": 1159.889, "top_contexts": [{"rank": 1, "score": 0.6751661, "source": "dense+rrf", "rrf_score": 0.032266458495966696, "rerank_score": 0.3508197176093171, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 7, "text_snippet": " is to give them the abil- ity to use external tools such as search engines, calculators, or calendars. However, existing ap- proaches either rely on large amounts of human annotations (Komeili et al., 2022; Thoppilan et al., 2022) or limit"}, {"rank": 2, "score": 0.6678872, "source": "dense+rrf", "rrf_score": 0.03125, "rerank_score": 0.2810114921181957, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 1, "text_snippet": "metic or fac- tual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer , a model traine"}, {"rank": 3, "score": 54.82179819616329, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.24561825344749846, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 8, "text_snippet": "tools in a novel way, which fulÔ¨Ålls the following desiderata: ‚Ä¢The use of tools should be learned in a self-supervised way without requiring large amounts of human annotations . This is impor-arXiv:2302.04761v1 [cs.CL] 9 Feb 2023  x1: i-1 ="}, {"rank": 4, "score": 0.6454472, "source": "dense+rrf", "rrf_score": 0.02964426877470356, "rerank_score": 0.23060389295474717, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 13, "text_snippet": "h tool to use when and how. As our approach is agnostic of the dataset be- ing used, we can apply it to the exact same dataset that was used to pretrain a model in the Ô¨Årst place. This ensures that the model does not lose any of its general"}, {"rank": 5, "score": 0.56988573, "source": "dense+rrf", "rrf_score": 0.02638888888888889, "rerank_score": 0.21690657894245818, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 12, "text_snippet": " how an API can be used, we let a LM annotate a huge language modeling dataset with potential API calls. We then use a self-supervised loss to determine which of these API calls actually help the model in predicting future tokens. Finally, "}]}
{"case_index": 67, "query": "From 2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt, what exact term fills [BLANK] in: \"Learning a policy is challenging when the mapping ct‚Ü¶‚Üíatis highly implicit and requires extensive [BLANK].\"?", "gold": "computation", "answer": "The exact term is **‚Äúcomputation‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'computation' and is directly supported by the provided context sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 83.395, "llm_ms": 1229.904, "top_contexts": [{"rank": 1, "score": 0.7139827, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.347807450900045, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 27, "text_snippet": "Learning a policy is challenging when the mapping ct‚Ü¶‚Üíatis highly implicit and requires extensive computation. For example, the agent shown in Figure 1(1c) is unable to generate the correct Ô¨Ånal action (Act 4) to Ô¨Ånish the QA task as it req"}, {"rank": 2, "score": 0.7066468, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.3008017247869144, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 26, "text_snippet": "bining it with complementary paradigms like reinforcement learning could further unlock the potential of large language models. 2REAC T: SYNERGIZING REASONING +AC TING Consider a general setup of an agent interacting with an environment for"}, {"rank": 3, "score": 0.5635514, "source": "dense+rrf", "rrf_score": 0.0288981288981289, "rerank_score": 0.2049072675482463, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 24, "text_snippet": "o understand the decision basis of model actions. To summarize, our key contributions are the following: (1) we introduce ReAct , a novel prompt- based paradigm to synergize reasoning and acting in language models for general task solving; "}, {"rank": 4, "score": 0.55986494, "source": "dense+rrf", "rrf_score": 0.027972027972027972, "rerank_score": 0.19426105784903247, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 28, "text_snippet": " 1, thus keep producing hallucinating actions. The idea of ReAct is simple: we augment the agent‚Äôs action space to ÀÜA=A‚à™L , whereLis the space of language. An action ÀÜat‚ààL in the language space, which we will refer to as a thought or a reas"}, {"rank": 5, "score": 38.73469988553236, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.19023555259258107, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 120, "text_snippet": "Zhou. 2022. Self- consistency improves chain of thought reasoning in language models. CoRR , abs/2203.11171. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicit"}]}
{"case_index": 68, "query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"In this paper, we show that LMs can teach [BLANK] to use external tools via simple APIs and achieve the best of both worlds.\"?", "gold": "themselves", "answer": "The exact term is **‚Äúthemselves‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'themselves' and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 105.343, "llm_ms": 1148.742, "top_contexts": [{"rank": 1, "score": 0.7062765, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.4367867943365763, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 1, "text_snippet": "metic or fac- tual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer , a model traine"}, {"rank": 2, "score": 0.65804195, "source": "dense+rrf", "rrf_score": 0.03076923076923077, "rerank_score": 0.2469255523253797, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 12, "text_snippet": " how an API can be used, we let a LM annotate a huge language modeling dataset with potential API calls. We then use a self-supervised loss to determine which of these API calls actually help the model in predicting future tokens. Finally, "}, {"rank": 3, "score": 0.59194803, "source": "dense+rrf", "rrf_score": 0.029513888888888888, "rerank_score": 0.23567356093919417, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 6, "text_snippet": "legislative bodies, like  city councils, to hold their meetings open to the public. Figure 1: Exemplary predictions of Toolformer. The model autonomously decides to call different APIs (from top to bottom: a question answering system, a cal"}, {"rank": 4, "score": 0.5826309, "source": "dense+rrf", "rrf_score": 0.028949545078577336, "rerank_score": 0.2277205008993971, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 212, "text_snippet": "line decision making: Benchmarks and additional opinions,‚Äù arXiv preprint arXiv:2306.02224 , 2023. [108] T. Schick, J. Dwivedi-Yu, R. Dess `ƒ±, R. Raileanu, M. Lomeli, L. Zettle- moyer, N. Cancedda, and T. Scialom, ‚ÄúToolformer: Language mode"}, {"rank": 5, "score": 0.59411997, "source": "dense+rrf", "rrf_score": 0.027783137179239824, "rerank_score": 0.2270892036320295, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 13, "text_snippet": "h tool to use when and how. As our approach is agnostic of the dataset be- ing used, we can apply it to the exact same dataset that was used to pretrain a model in the Ô¨Årst place. This ensures that the model does not lose any of its general"}]}
{"case_index": 69, "query": "From 2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt, what exact term fills [BLANK] in: \"Write a [BLANK] answer for the given question using only the provided search results (some of which might be irrelevant).\"?", "gold": "high-quality", "answer": "The exact term is **‚Äúhigh-quality‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Model answer matches expected term 'high-quality' and is directly supported by multiple context snippets containing the exact phrase.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.3333333333333333, "hit_rank": 3, "matched_by": "doc_id+chunk_id", "retrieval_ms": 66.319, "llm_ms": 980.949, "top_contexts": [{"rank": 1, "score": 0.5935723, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.3703397164378738, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 34, "text_snippet": "grained changes in the position of relevant information. 2.2 Models We analyze several state-of-the-art open and closed language models. We use greedy decoding when generating outputs and leave exploration of other decoding methods to futur"}, {"rank": 2, "score": 46.45182456893161, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.3417567128740096, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 31, "text_snippet": "ut Context  Wilhelm Conrad R√∂ntgen Desired Answer Figure 2: Example of the multi-document question answering task, with an input context and the desired model answer. The document containing the answer is bolded within the input context her"}, {"rank": 3, "score": 44.97461179414658, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.32234460260461056, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 29, "text_snippet": "ranked order, we explored randomly ordering the k‚àí1 distractor documents and mentioning that the documents are randomly ordered in the task description, but found the same trends. See Appendix C for more details.  Write a high-quality answe"}, {"rank": 4, "score": 0.57406926, "source": "dense+rrf", "rrf_score": 0.031009615384615385, "rerank_score": 0.3016215014273156, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 126, "text_snippet": "orted by decreasing relevance (i.e., the documents near the beginning of the input context are more likely to be useful than those at the end). To validate that our conclusions are not simply a byproduct of this bias, we run experiments wit"}, {"rank": 5, "score": 0.5526967, "source": "dense+rrf", "rrf_score": 0.028309409888357256, "rerank_score": 0.2729231238900899, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 49, "text_snippet": "UIDs. The relevant key-value pair for answering the query is bolded here within the input context for clarity. indicate that extended-context models are not nec- essarily better than their non-extended counterparts at using their input cont"}]}
{"case_index": 70, "query": "From 2302.13971__llama_open_and_efficient_foundation_language_models.txt, what exact term fills [BLANK] in: \"There exist some exceptions, notably OPT (Zhang et al., 2022), GPT-NeoX (Black et al., 2022), BLOOM (Scao et al., 2022) and GLM (Zeng et al., 2022), but none that are [BLANK] with PaLM-62B or Chinchilla.\"?", "gold": "competitive", "answer": "The exact term that fills **[BLANK]** is:\n\n**‚Äúcompetitive‚Äù**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'competitive' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 53.262, "llm_ms": 1233.504, "top_contexts": [{"rank": 1, "score": 0.728878, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.4409153918608217, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 7, "text_snippet": "aLM, or GPT-3, we only use publicly available data, making our work com- patible with open-sourcing, while most existing models rely on data which is either not publicly available or undocumented (e.g. ‚ÄúBooks ‚Äì 2TB‚Äù or ‚ÄúSocial media convers"}, {"rank": 2, "score": 0.69386005, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.26057798471422083, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 30, "text_snippet": "e non-publicly available language models GPT-3 (Brown et al., 2020), Gopher (Rae et al., 2021), Chinchilla (Hoffmann et al., 2022) and PaLM (Chowdhery et al., 2022), as well as the open-sourced OPT models (Zhang et al., 2022), GPT-J (Wang a"}, {"rank": 3, "score": 30.80269495463535, "source": "dense+rrf", "rrf_score": 0.014084507042253521, "rerank_score": 0.21305519702636455, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 139, "text_snippet": "Model OutputQuestion Question QuestionFigure 10: Examples of semantic understanding and one-step missing errors that were Ô¨Åxed by scaling PaLM from 62B to 540B. A.2 What is the role of prompt engineering? One of the key considerations of pr"}, {"rank": 4, "score": 0.57545143, "source": "dense+rrf", "rrf_score": 0.027364110201042444, "rerank_score": 0.21108993145933352, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 89, "text_snippet": "y for transformer based language models, which were later reÔ¨Åned by Hoffmann et al. (2022), by adapting the learning rate schedule when scaling datasets. Finally, Wei et al. (2022) studied the effect of scal- ing on the abilities of large l"}, {"rank": 5, "score": 0.62352115, "source": "dense+rrf", "rrf_score": 0.031024531024531024, "rerank_score": 0.20183830123031563, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 88, "text_snippet": "al.,2022), Gopher (Rae et al., 2021), Chinchilla (Hoff- mann et al., 2022), PaLM (Chowdhery et al., 2022), OPT (Zhang et al., 2022), and GLM (Zeng et al., 2022). Hestness et al. (2017) and Rosenfeld et al. (2019) studied the impact of scali"}]}
{"case_index": 71, "query": "From 2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt, what exact term fills [BLANK] in: \"[BLANK] scales Transformers to longer sequences, which improves their quality and enables new capabilities.\"?", "gold": "flashattention", "answer": "**FlashAttention** scales Transformers to longer sequences, which improves their quality and enables new capabilities.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "The blank is 'FlashAttention', which is explicitly stated in the provided context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 56.983, "llm_ms": 1220.271, "top_contexts": [{"rank": 1, "score": 0.6643269, "source": "dense+rrf", "rrf_score": 0.02878726010616578, "rerank_score": 0.2931259352640666, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 18, "text_snippet": "5% faster than the training speed record in MLPerf 1.1 [ 58], GPT2 (seq. length 1K) 3 \u0002faster than baseline implementations from HuggingFace [ 87] and Megatron-LM [ 77], and long-range arena (seq. length 1K-4K) 2.4 \u0002faster than baselines. ‚Ä¢"}, {"rank": 2, "score": 0.68165696, "source": "dense+rrf", "rrf_score": 0.03028233151183971, "rerank_score": 0.2775941596232141, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 100, "text_snippet": "n, Atri Rudra, and Christopher R√©. 2022. FlashAttention: Fast and memory-efficient exact attention with IO- awareness. ArXiv:2205.14135. Hermann Ebbinghaus. 1913. Memory: A contribu- tion to experimental psychology. H. A. Ruger & C. E. Buss"}, {"rank": 3, "score": 44.23519347853723, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.23661189263480215, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 84, "text_snippet": "mputer/RedPajama-Data . Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. CoRR , abs/2307.08691, 2023. Tri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra, and Christopher R ¬¥e. Flashattention: Fast an"}, {"rank": 4, "score": 0.7134014, "source": "dense+rrf", "rrf_score": 0.031009615384615385, "rerank_score": 0.21554779302763294, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 0, "text_snippet": "FlashAttention : Fast and Memory-EÔ¨Écient Exact Attention with IO-Awareness Tri Daoy, Daniel Y. Fuy, Stefano Ermony, Atri Rudraz, and Christopher R√©y yDepartment of Computer Science, Stanford University zDepartment of Computer Science and En"}, {"rank": 5, "score": 45.1318427000367, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.20925722273032285, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 106, "text_snippet": "o solve math word problems. arXiv preprint arXiv:2110.14168 . Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Car- bonell, Quoc V Le, and Ruslan Salakhutdinov. 2019. Transformer-xl: Attentive language mod- els beyond a Ô¨Åxed-length context. arXi"}]}
{"case_index": 72, "query": "From 2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt, what exact term fills [BLANK] in: \"(2023), we use accuracy as our primary evaluation metric, judging whether any of the correct answers (as taken from the [BLANK] annotations) appear in the predicted output.\"?", "gold": "naturalquestions", "answer": "The exact term is **‚ÄúNaturalQuestions‚Äù** (as in ‚ÄúNaturalQuestions annotations‚Äù).", "answer_score": 0.3, "answer_correctness": 0.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context shows 'NaturalQuestions annotations', but expected exact fill is 'naturalquestions' (lowercase). Model answer casing does not match the expected exact term.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 98.866, "llm_ms": 1372.77, "top_contexts": [{"rank": 1, "score": 0.5786432, "source": "dense+rrf", "rrf_score": 0.032018442622950824, "rerank_score": 0.39174928415294197, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 27, "text_snippet": "just the order of the documents to change the position of the document that contains the answer (Figure 3). To modulate the input context length in this task, we increase or decrease the number of retrieved documents that do not contain the"}, {"rank": 2, "score": 37.476527898484754, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.2524184986129564, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 82, "text_snippet": " language models in a standard retriever- reader setup. A retrieval system (Contriever, fine- tuned on MS-MARCO) takes an input query from NaturalQuestions-Open and returns the kdocu- ments from Wikipedia with the highest relevance score. T"}, {"rank": 3, "score": 34.481986223652825, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.21738726729188115, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 0, "text_snippet": "Lost in the Middle: How Language Models Use Long Contexts Nelson F. Liu1‚àóKevin Lin2John Hewitt1Ashwin Paranjape3 Michele Bevilacqua3Fabio Petroni3Percy Liang1 1Stanford University2University of California, Berkeley3Samaya AI nfliu@cs.stanfo"}, {"rank": 4, "score": 28.478496560345008, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.21495548016070534, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 2, "text_snippet": "urs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts, even for explicitly long-context models. Our analysis provides a better understanding "}, {"rank": 5, "score": 43.39497068577428, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.21248515196384749, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 207, "text_snippet": "‚Äù arXiv preprint arXiv:2310.07554 , 2023. [98] N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni, and P. Liang, ‚ÄúLost in the middle: How language models use long contexts,‚Äù arXiv preprint arXiv:2307.03172 , 2023. [99] Y "}]}
{"case_index": 73, "query": "From 2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt, what exact term fills [BLANK] in: \"Despite RAG method are [BLANK] and surpass the performance of the native LLM, they also exhibit several limitations.\"?", "gold": "cost-effective", "answer": "The exact term that fills **[BLANK]** is **‚Äúcost-effective‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Model answer matches expected term 'cost-effective' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 98.254, "llm_ms": 1238.26, "top_contexts": [{"rank": 1, "score": 56.11239891685147, "source": "dense+rrf", "rrf_score": 0.01639344262295082, "rerank_score": 0.32854153357047333, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 18, "text_snippet": "g, and we categorize it into three stages: Naive RAG, Advanced RAG, and Modular RAG, as showed in Figure 3. Despite RAG method are cost-effective and surpass the performance of the native LLM, they also exhibit several limitations. The deve"}, {"rank": 2, "score": 28.337156126421263, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.22820370078189373, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 238, "text_snippet": " maximizing llm performance,‚Äù https://community.openai. com/t/openai-dev-day-2023-breakout-sessions/505213# a-survey-of-techniques-for-maximizing-llm-performance-2, 2023. [167] J. Chen, H. Lin, X. Han, and L. Sun, ‚ÄúBenchmarking large lan- g"}, {"rank": 3, "score": 0.6528933, "source": "dense+rrf", "rrf_score": 0.02967032967032967, "rerank_score": 0.2190045115109293, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 1, "text_snippet": "ented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natu- ral language layer between"}, {"rank": 4, "score": 0.6590936, "source": "dense+rrf", "rrf_score": 0.03007688828584351, "rerank_score": 0.21209799721288838, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 11, "text_snippet": "h a detailed and structured understanding of both large models and RAG. It aims to illuminate the evolution of retrieval augmentation techniques, assess the strengths and weaknesses of various approaches in their respective contexts, and sp"}, {"rank": 5, "score": 0.7012247, "source": "dense+rrf", "rrf_score": 0.029386529386529386, "rerank_score": 0.1995295990569927, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 13, "text_snippet": "he fine-tuning of LLMs. Researchers have also been exploring ways to enhance language models in the pre-training stage through retrieval-augmented techniques. advanced RAG, and modular RAG. This review contex- tualizes the broader scope of "}]}
{"case_index": 74, "query": "From 2302.13971__llama_open_and_efficient_foundation_language_models.txt, what exact term fills [BLANK] in: \"At the higher-end of the scale, our 65B-parameter model is also [BLANK] with the best large lan- guage models such as Chinchilla or PaLM-540B.\"?", "gold": "competitive", "answer": "The exact term that fills **[BLANK]** is **‚Äúcompetitive‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'competitive' and is directly supported by the provided context sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 73.222, "llm_ms": 1065.909, "top_contexts": [{"rank": 1, "score": 0.64256316, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.3940896110761595, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 6, "text_snippet": "rameters with competitive performance compared to the best existing LLMs. For instance, LLaMA-13B outperforms GPT-3 on most bench- marks, despite being 10 √ósmaller. We believe that this model will help democratize the access and study of LL"}, {"rank": 2, "score": 0.6439541, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.2519332914073569, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 1, "text_snippet": "ow that it is possible to train state-of-the-art models using publicly avail- able datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and L"}, {"rank": 3, "score": 35.542480910387674, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.2504281741462002, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 34, "text_snippet": " datasets include Cloze and Winograd style tasks, as well as multiple choice question an- swering. We evaluate in the zero-shot setting as done in the language modeling community. In Table 3, we compare with existing models of various sizes"}, {"rank": 4, "score": 0.6187991, "source": "dense+rrf", "rrf_score": 0.030536130536130537, "rerank_score": 0.2361967227614328, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 90, "text_snippet": "3 while being more than 10 √ósmaller, and LLaMA-65B is competitive with Chinchilla-70B and PaLM-540B. Unlike previous studies, we show that it is possible to achieve state-of-the-art performance by training exclusively on publicly available "}, {"rank": 5, "score": 29.208115062185918, "source": "dense+rrf", "rrf_score": 0.014084507042253521, "rerank_score": 0.22246697276321248, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 44, "text_snippet": "022). PaLM and LLaMA were trained on datasets that contain a similar number of code tokens. As show in Table 8, for a similar number of parameters, LLaMA outperforms other gen- eral models such as LaMDA and PaLM, which are not trained or Ô¨Ån"}]}
{"case_index": 75, "query": "From 2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt, what exact term fills [BLANK] in: \"seminal work to over hundred billion parameters (Brown et al., 2020; Radford et al., 2019) in the last two years which has led to models that do very well on a wide array of tasks in a zero or few-shot [BLANK].\"?", "gold": "formulation", "answer": "The exact term is **‚Äúformulation‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term and is directly supported by Context 1, which contains the full sentence ending with 'zero or few-shot formulation.'", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 127.178, "llm_ms": 1092.527, "top_contexts": [{"rank": 1, "score": 0.7779401, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.5287567527538014, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 5, "text_snippet": " seminal work to over hundred billion parameters (Brown et al., 2020; Radford et al., 2019) in the last two years which has led to models that do very well on a wide array of tasks in a zero or few-shot formulation. Increasing model size pr"}, {"rank": 2, "score": 0.6452817, "source": "dense+rrf", "rrf_score": 0.02946912242686891, "rerank_score": 0.274050260658017, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 2, "text_snippet": "texts have shown their ability to per- form new tasks from textual instructions or from a few examples (Brown et al., 2020). These few-shot properties Ô¨Årst appeared when scaling models to a sufÔ¨Åcient size (Kaplan et al., 2020), resulting in"}, {"rank": 3, "score": 47.899547976698706, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.26731592316101765, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 4, "text_snippet": "ural networks have proven to be powerful language models, Ô¨Årst in the form of recurrent architectures (Graves, 2013; Jozefowicz et al., 2016; Mikolov et al., 2010) and more recently in the form of Transformers (Vaswani et al., 2017), that u"}, {"rank": 4, "score": 43.293560797252574, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.261284135282902, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 34, "text_snippet": "a few Ô¨Çips is the coin still heads-up?) from Wei et al. (2022). Language models and prompts. We evaluate self-consistency over four transformer-based lan- guage models with varying scales: ‚Ä¢UL2 (Tay et al., 2022) is an encoder-decoder model"}, {"rank": 5, "score": 34.43332216308507, "source": "dense+rrf", "rrf_score": 0.013888888888888888, "rerank_score": 0.2572613818899137, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 69, "text_snippet": "e stem from their ability to memorise training data, their high training cost, the static nature of their training data (Lazaridou et al., 2021), their tendency of amplifying inherent biases in the training data, and their ability to genera"}]}
{"case_index": 76, "query": "From 2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt, what exact term fills [BLANK] in: \"36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2201.11903v6 [cs.CL] 10 Jan 2023 1 [BLANK] Math\"?", "gold": "introduction", "answer": "The exact term is **‚ÄúIntroduction‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context shows the line '1 Introduction' before 'Math', so the blank is 'Introduction' (case-insensitive match to expected 'introduction').", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 58.52, "llm_ms": 1034.761, "top_contexts": [{"rank": 1, "score": 93.19295836858228, "source": "dense+rrf", "rrf_score": 0.01639344262295082, "rerank_score": 0.44037880412341196, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 4, "text_snippet": " + 6 = 11. The answer is 11. Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?Model Input Model OutputModel OutputModel Input Figure 1: Chain-of-thought prompting enables large la"}, {"rank": 2, "score": 0.6859344, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.34749756862473313, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 133, "text_snippet": " Proceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence , 32, Apr. 2018. URL https: //ojs.aaai.org/index.php/AAAI/article/view/12340 . Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and"}, {"rank": 3, "score": 0.6104022, "source": "dense+rrf", "rrf_score": 0.03125763125763126, "rerank_score": 0.3093590471928823, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 5, "text_snippet": "022).arXiv:2201.11903v6 [cs.CL] 10 Jan 2023  1 Introduction Math Word Problems (GSM8K)020406080100 3355 1857Solve rate (%)Finetuned GPT-3 175B Prior best PaLM 540B: standard prompting PaLM 540B: chain-of-thought prompting Figure 2: PaLM 540"}, {"rank": 4, "score": 41.4161368635382, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.24581847218176664, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 6, "text_snippet": "performed while at Google Research. 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7 [cs.CL] 2 Aug 2023  1 Introduction Recurrent neural networks, long short-term memory [ 13] and "}, {"rank": 5, "score": 46.72667041859189, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.2332355053044186, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 114, "text_snippet": "-augmented ensembles in language models. arXiv preprint arXiv:2207.00747 , 2022b. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models"}]}
{"case_index": 77, "query": "From 1706.03762__attention_is_all_you_need.txt, what exact term fills [BLANK] in: \"In practice, we compute the attention function on a set of queries [BLANK], packed together into a matrix Q.\"?", "gold": "simultaneously", "answer": "The exact term is **‚Äúsimultaneously‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'simultaneously' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 85.981, "llm_ms": 1057.129, "top_contexts": [{"rank": 1, "score": 0.6299163, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.367687678103545, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 21, "text_snippet": " keys of dimension dk, and values of dimension dv. We compute the dot products of the query with all keys, divide each by‚àödk, and apply a softmax function to obtain the weights on the values. In practice, we compute the attention function o"}, {"rank": 2, "score": 0.553463, "source": "dense+rrf", "rrf_score": 0.0304147465437788, "rerank_score": 0.2445968669087274, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 185, "text_snippet": "ll-block runtime. Kernel machine learning. Our approach in FlashAttention relies on the fact that the ùëÅ\u0002ùëÅ attention matrix is a function of a low-rank matrix QK>(of rankùëë\u001cùëÅ). As a result, we can repeatedly load the inputs Q¬ñKand recompute t"}, {"rank": 3, "score": 20.649674359378935, "source": "dense+rrf", "rrf_score": 0.014084507042253521, "rerank_score": 0.22825764619909747, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 173, "text_snippet": "accesses is Œò¬πùëÅùëë¬∏ùëÅùëëùëáùëê¬∫=Œò¬πùëÅùëëùëáùëê¬∫. We derive the conditions on the block sizes ùêµùëêandùêµùëü. We need the blocks KùëóandVùëóof sizeùêµùëê\u0002ùëëto Ô¨Åt into on-chip memory, which translates to: ùêµùëêùëë=ùëÇ¬πùëÄ¬∫,ùêµùëê=ùëÇ\u0012ùëÄ ùëë\u0013 ¬ï Similarly, we need the blocks Qùëñ¬ñOùëñof sizeùêµùëü\u0002ùëëto "}, {"rank": 4, "score": 0.6074121, "source": "dense+rrf", "rrf_score": 0.030017921146953404, "rerank_score": 0.2105275812553509, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 20, "text_snippet": "t Attention  Multi-Head Attention Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel. of the values, where the weight assigned to each value is computed by a "}, {"rank": 5, "score": 0.53459436, "source": "dense+rrf", "rrf_score": 0.029030910609857977, "rerank_score": 0.20822541286239082, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 32, "text_snippet": ". It maps input features into a set of queries, keys, and values {q, k, v}, via linear projection layers with weight matrices {Wq, Wk, Wv}. Given {q, k, v}, it computes the outputs oas o= softmax( qkT)v (1) The outputs are then projected by"}]}
{"case_index": 78, "query": "From 2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt, what exact term fills [BLANK] in: \"cusing on enhancing language models by [BLANK] additional knowledge through Pre- Training Models (PTM).\"?", "gold": "incorporating", "answer": "The exact term that fills **[BLANK]** is:\n\n**incorporating**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'incorporating' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 95.169, "llm_ms": 1352.265, "top_contexts": [{"rank": 1, "score": 0.682171, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.28294944087210117, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 7, "text_snippet": "cusing on enhancing language models by incorporating additional knowledge through Pre- Training Models (PTM). This early stage was characterized by foundational work aimed at refining pre-training techniques [3]‚Äì[5].The subsequent arrival o"}, {"rank": 2, "score": 0.6143574, "source": "dense+rrf", "rrf_score": 0.027884615384615386, "rerank_score": 0.2201858034016683, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 79, "text_snippet": "on the long-context understanding ability of LLMs, and we still have a long way to go in harnessing open-source LLMs within our framework. 10  4 Related Work 4.1 Retrieval-Augmented Generation. Augmenting language models with information re"}, {"rank": 3, "score": 0.5986218, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.2025379580295234, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 312, "text_snippet": "p training curricula. By contrast pre-training at large enough scale appears to offer a ‚Äúnatural‚Äù broad distribution of tasks implicitly contained in predicting the text itself. One direction for future work might be attempting to generate "}, {"rank": 4, "score": 0.587771, "source": "dense+rrf", "rrf_score": 0.02976190476190476, "rerank_score": 0.2010163205900445, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 4, "text_snippet": "l, retrieval-augmented gen- eration, natural language processing, information retrieval I. I NTRODUCTION LARGE language models (LLMs) have achieved remark- able success, though they still face significant limitations, especially in domain-s"}, {"rank": 5, "score": 22.21834607996083, "source": "dense+rrf", "rrf_score": 0.014285714285714285, "rerank_score": 0.19626093366902772, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 13, "text_snippet": "he fine-tuning of LLMs. Researchers have also been exploring ways to enhance language models in the pre-training stage through retrieval-augmented techniques. advanced RAG, and modular RAG. This review contex- tualizes the broader scope of "}]}
{"case_index": 79, "query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"They also, [BLANK], struggle with basic functionality, such as arithmetic or fac- tual lookup, where much simpler and smaller models\"?", "gold": "paradoxically", "answer": "The exact term that fills **[BLANK]** is **‚Äúparadoxically‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'paradoxically' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 53.308, "llm_ms": 1305.283, "top_contexts": [{"rank": 1, "score": 0.6743913, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.44988277331267185, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 0, "text_snippet": "Toolformer: Language Models Can Teach Themselves to Use Tools Timo Schick Jane Dwivedi-Yu Roberto Dess√¨‚Ä†Roberta Raileanu Maria Lomeli Luke Zettlemoyer Nicola Cancedda Thomas Scialom Meta AI Research‚Ä†Universitat Pompeu Fabra Abstract Languag"}, {"rank": 2, "score": 0.5887572, "source": "dense+rrf", "rrf_score": 0.031754032258064516, "rerank_score": 0.3050222533705136, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 1, "text_snippet": "metic or fac- tual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer , a model traine"}, {"rank": 3, "score": 45.62711449889626, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.22208559052113808, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 212, "text_snippet": "line decision making: Benchmarks and additional opinions,‚Äù arXiv preprint arXiv:2306.02224 , 2023. [108] T. Schick, J. Dwivedi-Yu, R. Dess `ƒ±, R. Raileanu, M. Lomeli, L. Zettle- moyer, N. Cancedda, and T. Scialom, ‚ÄúToolformer: Language mode"}, {"rank": 4, "score": 28.573249116856278, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.20452621018625258, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 94, "text_snippet": "e are no examples of chained tool use in the Ô¨Ånetuning dataset. Our current approach also does not allow the LM to use a tool in an in- teractive way ‚Äì especially for tools such as search engines, that could potentially return hundreds of d"}, {"rank": 5, "score": 24.60187786558854, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.20177950159096814, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 14, "text_snippet": "r at using their input context (¬ß2.3). Given that language models struggle to retrieve and use relevant information in the multi-document question answering task, to what extent can lan- guage models even retrieve from their input con- text"}]}
{"case_index": 80, "query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \"The Ô¨Ånal hidden state corresponding to this token is used as the ag- gregate sequence [BLANK] for classiÔ¨Åcation tasks.\"?", "gold": "representation", "answer": "The exact term is **‚Äúrepresentation‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'representation' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 75.749, "llm_ms": 970.914, "top_contexts": [{"rank": 1, "score": 0.64745057, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3631175194307002, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 28, "text_snippet": "A ‚Äúsequence‚Äù refers to the in- put token sequence to BERT, which may be a sin- gle sentence or two sentences packed together. We use WordPiece embeddings (Wu et al., 2016) with a 30,000 token vocabulary. The Ô¨Årst token of every sequence is "}, {"rank": 2, "score": 0.5359528, "source": "dense+rrf", "rrf_score": 0.03076923076923077, "rerank_score": 0.2632358676813262, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 43, "text_snippet": "tion or sequence tagging. At the output, the token rep- resentations are fed into an output layer for token- level tasks, such as sequence tagging or question answering, and the [CLS] representation is fed into an output layer for classiÔ¨Åca"}, {"rank": 3, "score": 32.89856274025055, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.24217684331925346, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 83, "text_snippet": "ted that rich, unsupervised pre-training is an integral part of many language understanding systems. In particular, these results enable even low-resource tasks to beneÔ¨Åt from deep unidirectional architec- tures. Our major contribution is f"}, {"rank": 4, "score": 0.4966198, "source": "dense+rrf", "rrf_score": 0.027745885954841176, "rerank_score": 0.23804445467860233, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 1, "text_snippet": ", 2018), BERT is designed to pre- train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a re- sult, the pre-trained BERT model can be Ô¨Åne- tuned with just one a"}, {"rank": 5, "score": 32.39719331933292, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.23559591019521747, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 109, "text_snippet": "reading books. In Proceedings of the IEEE international conference on computer vision , pages 19‚Äì27. Appendix for ‚ÄúBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding‚Äù We organize the appendix into three section"}]}
{"case_index": 81, "query": "From 2005.14165__language_models_are_few_shot_learners.txt, what exact term fills [BLANK] in: \"There exists a very wide range of possible useful language tasks, [BLANK] anything from correcting grammar, to generating examples of an abstract concept, to critiquing a short story.\"?", "gold": "encompassing", "answer": "The exact term is **‚Äúencompassing‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'encompassing' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 61.501, "llm_ms": 1413.413, "top_contexts": [{"rank": 1, "score": 0.7389656, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.4283330719891544, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 14, "text_snippet": "nguage models. There exists a very wide range of possible useful language tasks, encompassing anything from correcting grammar, to generating examples of an abstract concept, to critiquing a short story. For many of these tasks it is difÔ¨Åcu"}, {"rank": 2, "score": 0.54405844, "source": "dense+rrf", "rrf_score": 0.028629032258064516, "rerank_score": 0.25071384443709954, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 13, "text_snippet": "ostic, there is still a need for task-speciÔ¨Åc datasets and task-speciÔ¨Åc Ô¨Åne-tuning: to achieve strong performance on a desired task typically requires Ô¨Åne-tuning on a dataset of thousands to hundreds of thousands of examples speciÔ¨Åc to that"}, {"rank": 3, "score": 35.962273459843, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.21729595478712338, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 240, "text_snippet": "ing performance when using these approaches over standard language models [ RSR+19]. Thus our design decision comes at the cost of potentially worse performance on tasks which empirically beneÔ¨Åt from bidirectionality. This may include Ô¨Åll-i"}, {"rank": 4, "score": 26.386890541439445, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.19918966326205076, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 5, "text_snippet": " seminal work to over hundred billion parameters (Brown et al., 2020; Radford et al., 2019) in the last two years which has led to models that do very well on a wide array of tasks in a zero or few-shot formulation. Increasing model size pr"}, {"rank": 5, "score": 0.5470995, "source": "dense+rrf", "rrf_score": 0.028309409888357256, "rerank_score": 0.19451535606517983, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 146, "text_snippet": " several tasks that involve rearranging or unscrambling the letters in a word, tasks which are unlikely to have been exactly seen during training. Third, we test GPT-3‚Äôs ability to solve SAT-style analogy problems few-shot. Finally, we test"}]}
{"case_index": 82, "query": "From 2305.06983__active_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"t al., 2017), where the informa- tion needs are clear in the user‚Äôs input, and it is [BLANK] to retrieve relevant knowledge once solely based on the input .\"?", "gold": "sufficient", "answer": "The exact term that fills **[BLANK]** is:\n\n**‚Äúsufficient‚Äù**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'sufficient' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 59.726, "llm_ms": 1198.522, "top_contexts": [{"rank": 1, "score": 0.57118934, "source": "dense+rrf", "rrf_score": 0.02938045560996381, "rerank_score": 0.4082979094147516, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 7, "text_snippet": "t al., 2017), where the informa- tion needs are clear in the user‚Äôs input, and it is sufficient to retrieve relevant knowledge once solely based on the input . Increasingly powerful large LMs have also demonstrated abilities in more complex"}, {"rank": 2, "score": 0.60533935, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.3274157479787982, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 5, "text_snippet": "u et al., 2021). Augmenting LMs with retrieval components that look up relevant in- formation from external knowledge resources is a promising direction to address hallucination (Khan- delwal et al., 2020; Izacard et al., 2022). Retrieval a"}, {"rank": 3, "score": 52.19571862912831, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.3114465066000555, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 22, "text_snippet": " generate the complete answer at once y=LM([Dx,x]). 2.3 Active Retrieval Augmented Generation To aid long-form generation with retrieval, we pro- pose active retrieval augmented generation. It is a generic framework that actively decides wh"}, {"rank": 4, "score": 0.64990115, "source": "dense+rrf", "rrf_score": 0.03177805800756621, "rerank_score": 0.2659098854107281, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 1, "text_snippet": "create factually inaccurate out- put. Augmenting LMs by retrieving informa- tion from external knowledge resources is one promising solution. Most existing retrieval aug- mented LMs employ a retrieve-and-generate setup that only retrieves i"}, {"rank": 5, "score": 0.5753528, "source": "dense+rrf", "rrf_score": 0.028438886647841874, "rerank_score": 0.2658207681867988, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 24, "text_snippet": "t, and the input to LMs is the concatena- tion of the retrieved documents Dqt, the user input x, and the previous generation y<t. We discard previously retrieved documents ‚à™t‚Ä≤<tDqt‚Ä≤and only use the retrieved documents from the current step "}]}
{"case_index": 83, "query": "From 2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt, what exact term fills [BLANK] in: \"For each of the queries, we need a document that contains the answer and k‚àí1[BLANK] documents that do not contain the answer.\"?", "gold": "distractor", "answer": "The exact term is **‚Äúdistractor‚Äù**, i.e., ‚Äúk‚àí1 distractor documents that do not contain the answer.‚Äù", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'distractor' and is explicitly supported by the provided context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 57.307, "llm_ms": 1310.033, "top_contexts": [{"rank": 1, "score": 0.62172955, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.40515085319224325, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 25, "text_snippet": " is a paragraph (as opposed to a list or a table). We use passages (chunks of at most 100 tokens) from Wikipedia as documents within our input contexts. For each of the queries, we need a document that contains the answer and k‚àí1distractor "}, {"rank": 2, "score": 28.647122019617807, "source": "dense+rrf", "rrf_score": 0.014084507042253521, "rerank_score": 0.2890851156308425, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 12, "text_snippet": "ng that current language models do not robustly access and use information in long input contexts. Furthermore, we observe a distinctive U-shaped performance curve (Figure 1); language model performance is highest when relevant information "}, {"rank": 3, "score": 0.5975807, "source": "dense+rrf", "rrf_score": 0.031024531024531024, "rerank_score": 0.287404762309698, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 23, "text_snippet": "nput context and the position of the relevant information and measure changes in task performance. 2.1 Experimental Setup In the multi-document question answering task, the model inputs are (i) a question to answer and (ii) k documents (e.g"}, {"rank": 4, "score": 0.6062316, "source": "dense+rrf", "rrf_score": 0.03076923076923077, "rerank_score": 0.287353702258638, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 49, "text_snippet": "UIDs. The relevant key-value pair for answering the query is bolded here within the input context for clarity. indicate that extended-context models are not nec- essarily better than their non-extended counterparts at using their input cont"}, {"rank": 5, "score": 30.494358695616842, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.2659795561591545, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 20, "text_snippet": "rom Wikipedia to answer queries from NaturalQuestions-Open, we find that model perfor- mance saturates long before retriever recall satu- rates, indicating that current models fail to effec- tively use additional retrieved documents‚Äîusing 5"}]}
{"case_index": 84, "query": "From 2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt, what exact term fills [BLANK] in: \"We [BLANK] run experiments on subset of unam- biguous questions, finding similar results and conclusions; see Appendix A.\"?", "gold": "additionally", "answer": "The exact term is **‚Äúadditionally‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'additionally' and is directly supported by Context 1 sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 92.898, "llm_ms": 1029.343, "top_contexts": [{"rank": 1, "score": 0.5627712, "source": "dense+rrf", "rrf_score": 0.02938045560996381, "rerank_score": 0.3200974187479811, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 28, "text_snippet": "nnotations) appear in the predicted output. 2Ambiguity in NaturalQuestions-Open means that a small number of distractor passages may contain a reasonable an- swer. We additionally run experiments on subset of unam- biguous questions, findin"}, {"rank": 2, "score": 34.64417027068576, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.24614430049753264, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 122, "text_snippet": " annotated answer is ‚Äúcurrently a free agent‚Äù. However, the Wikipedia retrieval corpus contains the information that he plays for the ‚ÄúBalti- more Ravens‚Äù, since he was released from the team between the Wikipedia dump‚Äôs timestamp and the N"}, {"rank": 3, "score": 0.5792426, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.24157830394306365, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 123, "text_snippet": "conclusions as the experiments on the full questions collection (Fig- ure 12). 1st 5th 10th 15th 20th Position of Document with the Answer60657075Accuracy 20 T otal Retrieved Documents  (~4K tokens, unambiguous questions) claude-1.3 claude-"}, {"rank": 4, "score": 40.188188693751556, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.23470610774112083, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 207, "text_snippet": "‚Äù arXiv preprint arXiv:2310.07554 , 2023. [98] N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni, and P. Liang, ‚ÄúLost in the middle: How language models use long contexts,‚Äù arXiv preprint arXiv:2307.03172 , 2023. [99] Y "}, {"rank": 5, "score": 27.754767397826427, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.22482914260831932, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 94, "text_snippet": "levant information, indicating that models struggle to robustly access and use infor- mation in long input contexts. In particular, per- formance is often lowest when models must use information in the middle of long input contexts. We cond"}]}
{"case_index": 85, "query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \"Second, we add a learned embed- ding to every token [BLANK] whether it belongs to sentence Aor sentence B.\"?", "gold": "indicating", "answer": "The exact term is **‚Äúindicating‚Äù** (as in: ‚Äúwe add a learned embedding to every token **indicating** whether it belongs to sentence A or sentence B.‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches the expected term 'indicating' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 85.641, "llm_ms": 1398.367, "top_contexts": [{"rank": 1, "score": 0.6433441, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.33091747287917656, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 29, "text_snippet": "er into a single sequence. We differentiate the sentences in two ways. First, we separate them with a special token ( [SEP] ). Second, we add a learned embed- ding to every token indicating whether it belongs to sentence Aor sentence B. As "}, {"rank": 2, "score": 36.639508853925605, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.27830190789939097, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 109, "text_snippet": "reading books. In Proceedings of the IEEE international conference on computer vision , pages 19‚Äì27. Appendix for ‚ÄúBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding‚Äù We organize the appendix into three section"}, {"rank": 3, "score": 42.12579466770541, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.252618428959527, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 0, "text_snippet": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova Google AI Language {jacobdevlin,mingweichang,kentonl,kristout }@google.com Abstract We introduce a ne"}, {"rank": 4, "score": 44.58636243463177, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.2216052797458285, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 476, "text_snippet": "heng, and Jingjing Liu. Uniter: Learning universal image-text representations. arXiv preprint arXiv:1909.11740 , 2019. [Cra17] Kate Crawford. The trouble with bias. NIPS 2017 Keynote , 2017. [DCLT18] Jacob Devlin, Ming-Wei Chang, Kenton Lee"}, {"rank": 5, "score": 39.93337547736293, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.2088338207535972, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 107, "text_snippet": "4135 . Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understand- ing. arXiv preprint arXiv:1810.04805 . Jeffrey L Elman. 1990. Finding structure in"}]}
{"case_index": 86, "query": "From 2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"n this way, although it comes with the [BLANK] of being sensitive to the design of the prompt.\"?", "gold": "limitation", "answer": "The exact term is **‚Äúlimitation‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'limitation' and is directly supported by the provided context sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 76.013, "llm_ms": 1224.032, "top_contexts": [{"rank": 1, "score": 0.56895876, "source": "dense+rrf", "rrf_score": 0.029827662395050816, "rerank_score": 0.27771202067370704, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 17, "text_snippet": "n this way, although it comes with the limitation of being sensitive to the design of the prompt. Rather than scoring individual answers, some authors have also focused on using an LLM to select the best answer among a number of candidates "}, {"rank": 2, "score": 0.5665276, "source": "dense+rrf", "rrf_score": 0.02871794871794872, "rerank_score": 0.20679346572359938, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 16, "text_snippet": "ssive LM. This idea of using prompts was previously also considered by Yuan et al. (2021), although they used a smaller fine-tuned LM (i.e. BART) and did not observe a clear benefit from using prompts. An- other approach directly asks ChatG"}, {"rank": 3, "score": 22.517031670625194, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.20381667904542197, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 281, "text_snippet": " you always used to look like this , did n‚Äôt you , _‚Äù What is the word in the blank space (_)? The answer is Orca-v2-13B-Cautious The answer is izzy. The word in the blank space is the name of the girl who is crying. Figure 24: Demonstrativ"}, {"rank": 4, "score": 23.081880437832794, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.20079559601951624, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 44, "text_snippet": "ugmented LMs can also be formulated using our framework (subsection 2.3). In this section, we formally in- troduce three baseline categories based on when and what to retrieve. These baselines are not exact reproductions of the correspondin"}, {"rank": 5, "score": 31.090766375424387, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.19796027481881667, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 9, "text_snippet": "we present Ragas1, a framework for the automated assessment 1Ragas is available at https://github.com/ explodinggradients/ragas .arXiv:2309.15217v2 [cs.CL] 28 Apr 2025  of retrieval augmented generation systems. We fo- cus on settings where"}]}
{"case_index": 87, "query": "From 2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"For models that do not provide access to token [BLANK], such as ChatGPT and GPT-4, differ- ent methods are needed.\"?", "gold": "probabilities", "answer": "The exact term is **‚Äúprobabilities‚Äù** (i.e., ‚Äútoken probabilities‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'probabilities' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 125.697, "llm_ms": 1626.323, "top_contexts": [{"rank": 1, "score": 0.59246707, "source": "dense+rrf", "rrf_score": 0.032266458495966696, "rerank_score": 0.3208442884559309, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 14, "text_snippet": "s it unsuitable for systems that access LLMs through an API. For models that do not provide access to token probabilities, such as ChatGPT and GPT-4, differ- ent methods are needed. SelfCheckGPT (Manakul et al., 2023) addresses this problem"}, {"rank": 2, "score": 24.679330069990957, "source": "dense+rrf", "rrf_score": 0.014285714285714285, "rerank_score": 0.22780391169368783, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 66, "text_snippet": "on par of the strongest fine-tuned RAG model like Atlas. On the HotpotQA dataset, LongRAG achieves a 64.3 exact match rate, which is also close to the SoTA fully-supervised RAG frameworks. 3.4 Full QA Performance on non-Wikipedia-based Data"}, {"rank": 3, "score": 25.841539757022954, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.2058950054968896, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 8, "text_snippet": "e not always predictive of downstream performance (Wang et al., 2023c). Moreover, this evaluation strategy relies on the LM probabilities, which are not accessible for some closed models (e.g. ChatGPT and GPT-4). Ques- tion answering is ano"}, {"rank": 4, "score": 24.845541938584617, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.19716526876855847, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 140, "text_snippet": "ments furnish quantitative metrics that not only gauge RAG model perfor- mance but also enhance comprehension of the model‚Äôs capabil- ities across various evaluation aspects. Prominent benchmarks such as RGB, RECALL and CRUD [167]‚Äì[169] foc"}, {"rank": 5, "score": 26.305302206317087, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.19465186780640945, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 2, "text_snippet": "is work, we provide a generalized view of ac- tive retrieval augmented generation , methods that actively decide when and what to retrieve across the course of the generation. We propose Forward- Looking Active REtrieval augmented generatio"}]}
{"case_index": 88, "query": "From 2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt, what exact term fills [BLANK] in: \"These results could be comparable to the strongest fully trained RAG models like Atlas([BLANK].,2022)andMDR(Xiongetal.,2020b).\"?", "gold": "izacardetal", "answer": "The exact term is **‚ÄúIzacard et al.‚Äù** (i.e., *Atlas (Izacard et al., 2022)*).", "answer_score": 0.705, "answer_correctness": 0.6, "answer_groundedness": 0.95, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.6, "groundedness": 0.95, "abstained": false, "should_abstain": false, "notes": "Context shows 'Atlas(Izacardetal.,2022)'. Model answered 'Izacard et al.' which matches the citation but not the exact term format expected ('izacardetal'). Grounded in provided context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 68.961, "llm_ms": 1889.952, "top_contexts": [{"rank": 1, "score": 0.6822247, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3102879356899742, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 17, "text_snippet": "anding ability of GPT-4o, LongRAG can achieve an EM of 62.7% on NQ and 64.3% on HotpotQA. These results could be comparable to the strongest fully trained RAG models like Atlas(Izacardetal.,2022)andMDR(Xiongetal.,2020b). Furthermore,wetesto"}, {"rank": 2, "score": 0.6641178, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.24817082902096435, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 65, "text_snippet": "2023). All models are evaluated on 16-shot in-context learning with direct prompting; The second group is ‚Äú Fully-supervised RAG‚Äù, and these baselines involve full-supervised fine-tuning on the training dataset. The third group is ‚ÄúNo Fine-"}, {"rank": 3, "score": 0.6733703, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.23134717846276104, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 0, "text_snippet": "LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs ‚ô†Ziyan Jiang,‚ô†Xueguang Ma,‚ô†Wenhu Chen ‚ô†University of Waterloo ziyanjiang528@gmail.com ,{x93ma ,wenhuchen}@uwaterloo.ca Project Website: https://tiger-ai-lab.github.io/"}, {"rank": 4, "score": 35.270780342815065, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.21514524455132705, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 63, "text_snippet": "t). We compare the results with three groups of baselines: closed-book, which involves directly prompting state-of- the-art LLMs with 16-shot in-context examples; fully-supervised RAG, where the RAG framework is used and the model is fully "}, {"rank": 5, "score": 0.6387464, "source": "dense+rrf", "rrf_score": 0.027745885954841176, "rerank_score": 0.19231877558128818, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 8, "text_snippet": "ever needs to scan over a massive amount of units to find the relevant piece. In contrast, LongRAG operates on long retrieval units (30x longer). The retriever of LongRAG has a significantly reduced workload, achieving strong retrieval qual"}]}
{"case_index": 89, "query": "From 2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt, what exact term fills [BLANK] in: \"(1) Although more expressive model forms for measur- ing the similarity between a question and a passage do exist, such as networks consisting of multiple layers of cross attentions, the similarity function needs to be [BLANK] so that the represen- tation\"?", "gold": "decomposable", "answer": "The exact term is **‚Äúdecomposable‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'decomposable' and is explicitly supported by the provided context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 71.905, "llm_ms": 814.299, "top_contexts": [{"rank": 1, "score": 0.6400137, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.5264514418719386, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 20, "text_snippet": " the input question to a d-dimensional vector, and retrieves kpassages of which vectors are the closest to the question vector. We deÔ¨Åne the similarity between the question and the passage using the dot product of their vectors: sim(q,p) =E"}, {"rank": 2, "score": 38.191463424914176, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.2657906045513321, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 86, "text_snippet": "pirical analysis and ablation studies indicate that more complex model frameworks or similarity functions do not necessarily provide additional val- ues. As a result of improved retrieval performance, we obtained new state-of-the-art result"}, {"rank": 3, "score": 0.6081283, "source": "dense+rrf", "rrf_score": 0.029957522915269395, "rerank_score": 0.2427260463027645, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 64, "text_snippet": "on model serves as a reranker through cross- attention between the question and the passage. Al- though cross-attention is not feasible for retrieving relevant passages in a large corpus due to its non- decomposable nature, it has more capa"}, {"rank": 4, "score": 0.5526877, "source": "dense+rrf", "rrf_score": 0.028006267136701922, "rerank_score": 0.23012810636993594, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 17, "text_snippet": "rieval accuracy , which is the fraction of ques- tions for whichCFcontains a span that answers the question. 3 Dense Passage Retriever (DPR) We focus our research in this work on improv- ing the retrieval component in open-domain QA. Given "}, {"rank": 5, "score": 33.81115861249768, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.227765669406468, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 21, "text_snippet": "y function needs to be decomposable so that the represen- tations of the collection of passages can be pre- computed. Most decomposable similarity functions are some transformations of Euclidean distance (L2). For instance, cosine is equiva"}]}
{"case_index": 90, "query": "From 2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt, what exact term fills [BLANK] in: \"By meticulously curating and optimizing the training dataset, researchers can [BLANK] reduce the model‚Äôs size without compromising its performance.\"?", "gold": "significantly", "answer": "The exact term that fills **[BLANK]** is **‚Äúsignificantly‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term and is directly supported by Context 1 quote containing 'researchers can significantly reduce...'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 180.136, "llm_ms": 1369.835, "top_contexts": [{"rank": 1, "score": 0.6710869, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.4114455421386301, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 8, "text_snippet": "ved solely by changing the training data. phi-3-mini: The development of a compact language model that rivals the capabilities of ChatGPT, while fitting on a phone, is a testament to the power of data-driven machine learning. By meticulousl"}, {"rank": 2, "score": 0.5963985, "source": "dense+rrf", "rrf_score": 0.031754032258064516, "rerank_score": 0.2936038059386201, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 0, "text_snippet": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone Microsoft Abstract We introduce phi-3-mini , a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both"}, {"rank": 3, "score": 0.57529646, "source": "dense+rrf", "rrf_score": 0.030090497737556562, "rerank_score": 0.25431899228318205, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 17, "text_snippet": " running locally on a cell-phone. Thanks to its small size, phi- 3-mini can be quantized to 4-bits so that it only occupies ‚âà1.8GB of memory. We tested the quantized model by deploying phi-3-mini on iPhone 14 with A16 Bionic chip running na"}, {"rank": 4, "score": 0.6134456, "source": "dense+rrf", "rrf_score": 0.03149801587301587, "rerank_score": 0.23999456104346975, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 6, "text_snippet": "7B parameters), matched the performance of models 25 times larger trained on regular data. In this report we present a new model, phi-3-mini (3.8B parameters), trained for 3.3T tokens on larger and more advanced versions of the datasets use"}, {"rank": 5, "score": 24.664300627324398, "source": "dense+rrf", "rrf_score": 0.014084507042253521, "rerank_score": 0.23998772592107445, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 25, "text_snippet": "DPO data covers chat format data, reasoning, and responsible AI (RAI) efforts. We use DPO to steer the model away from unwanted behavior, by using those outputs as ‚Äúrejected‚Äù responses. Besides improvement in math, coding, reasoning, robust"}]}
{"case_index": 91, "query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"You can call the API by writing \"[QA([BLANK])]\" where \"question\" is the question you want to ask.\"?", "gold": "question", "answer": "The exact term is **`question`**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context explicitly shows the syntax \"[QA(question)]\"; model answer matches expected term.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 62.308, "llm_ms": 1126.612, "top_contexts": [{"rank": 1, "score": 0.56519556, "source": "dense+rrf", "rrf_score": 0.03177805800756621, "rerank_score": 0.32516231968900483, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 142, "text_snippet": " sample API calls for each tool considered. Question Answering We use the following prompt for the question answering tool: Your task is to add calls to a Question Answering API to a piece of text. The questions should help you get informat"}, {"rank": 2, "score": 0.56239474, "source": "dense+rrf", "rrf_score": 0.03128054740957967, "rerank_score": 0.3023043363767777, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 18, "text_snippet": "still refer to them as ‚Äú <API> ‚Äù, ‚Äú</API> ‚Äù and ‚Äú‚Üí‚Äù through- out this section.  Your task is to add calls to a Question Answering API to a  piece of text. The questions should help you get  information required to complete the text. You can"}, {"rank": 3, "score": 0.54007065, "source": "dense+rrf", "rrf_score": 0.030117753623188408, "rerank_score": 0.26362682516928726, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 1, "text_snippet": "metic or fac- tual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer , a model traine"}, {"rank": 4, "score": 37.064135457799495, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.26206075335138995, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 142, "text_snippet": "trieval instructions for 2WikiMultihopQA Skill 1. Use the Search API to look up relevant information by writing ‚Äú[Search(term)]‚Äù where ‚Äúterm‚Äù is the search term you want to look up. For example: Question: But what are the risks during produ"}, {"rank": 5, "score": 37.27246083266279, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.24625615732029438, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 147, "text_snippet": "tput: From this, we have 4 *30 minutes = [Calculator(4 *30)] 120 minutes. Input: x Output: Wikipedia Search We use the following prompt for the Wikipedia search tool: Your task is to complete a given piece of text. You can use a Wikipedia S"}]}
{"case_index": 92, "query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \"In addi- tion to the masked language model, we also use a ‚Äúnext sentence prediction‚Äù task that jointly pre- trains text-pair [BLANK].\"?", "gold": "representations", "answer": "The exact term is **‚Äúrepresentations‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected blank term 'representations' and is directly supported by Context 1 ('pre-trains text-pair representations').", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 101.298, "llm_ms": 1018.621, "top_contexts": [{"rank": 1, "score": 0.6919423, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.44582136783149673, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 9, "text_snippet": "ng, the MLM ob- jective enables the representation to fuse the left and the right context, which allows us to pre- train a deep bidirectional Transformer. In addi- tion to the masked language model, we also use a ‚Äúnext sentence prediction‚Äù "}, {"rank": 2, "score": 38.90622327907607, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.3058026391727463, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 10, "text_snippet": "ford et al. (2018), which uses unidirec- tional language models for pre-training, BERT uses masked language models to enable pre- trained deep bidirectional representations. This is also in contrast to Peters et al. (2018a), which uses a sh"}, {"rank": 3, "score": 0.6186143, "source": "dense+rrf", "rrf_score": 0.030776515151515152, "rerank_score": 0.282728926856206, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 66, "text_snippet": " be found in Appendix C. 5.1 Effect of Pre-training Tasks We demonstrate the importance of the deep bidi- rectionality of BERT by evaluating two pre- training objectives using exactly the same pre- training data, Ô¨Åne-tuning scheme, and hype"}, {"rank": 4, "score": 0.67714685, "source": "dense+rrf", "rrf_score": 0.030621785881252923, "rerank_score": 0.2783422186448933, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 36, "text_snippet": "ela- tionship between two sentences, which is not di- rectly captured by language modeling. In order to train a model that understands sentence rela- tionships, we pre-train for a binarized next sen- tence prediction task that can be trivia"}, {"rank": 5, "score": 47.36718970511736, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.27706931838158155, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 0, "text_snippet": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova Google AI Language {jacobdevlin,mingweichang,kentonl,kristout }@google.com Abstract We introduce a ne"}]}
{"case_index": 93, "query": "From 2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt, what exact term fills [BLANK] in: \"2 1 Introduction Large Language Models (LLMs) are enabling more natural and [BLANK] interactions between humans and machines, enhancing user experience in existing appli\"?", "gold": "sophisticated", "answer": "The exact term that fills **[BLANK]** is **‚Äúsophisticated‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'sophisticated' and is directly supported by Context 1 showing the phrase 'more natural and sophisticated interactions'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 60.353, "llm_ms": 1044.556, "top_contexts": [{"rank": 1, "score": 0.70200837, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.41400180055508307, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 11, "text_snippet": " of Orca 2, its base model LLaMA-2, LLaMA-2-Chat and ChatGPT (GPT-3.5-Turbo) to a reasoning question. LLaMA-2 and LLaMA-2-Chat model responses generated using replicate.com/meta/llama-2-13b and chat.lmsys.org respectively. LLaMA and Orca 2 "}, {"rank": 2, "score": 0.7185191, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.2457401618248683, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 12, "text_snippet": "humans and machines, enhancing user experience in existing applications like coding [3], web search [ 36], chatbots [ 45,56], customer service and content creation. This transformation brought by LLMs is also paving the way for new innovati"}, {"rank": 3, "score": 31.198320735159918, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.21051218111011305, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 0, "text_snippet": "Orca 2: Teaching Small Language Models How to Reason Arindam Mitra, Luciano Del Corro‚Ä†, Shweti Mahajan‚Ä†, Andres Codas‚Ä° Clarisse Simoes‚Ä°, Sahaj Agarwal, Xuxi Chen‚àó, Anastasia Razdaibiedina‚àó Erik Jones‚àó, Kriti Aggarwal‚àó, Hamid Palangi, Guoqin"}, {"rank": 4, "score": 0.57517827, "source": "dense+rrf", "rrf_score": 0.02581612258494337, "rerank_score": 0.17355629170663092, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 1, "text_snippet": "n benchmarks like BigBench Hard and AGIEval. In Orca 2, we continue exploring how improved training signals can enhance smaller LMs‚Äô reasoning abilities. Research on training small LMs has often relied on imitation learning to replicate the"}, {"rank": 5, "score": 19.3797777351962, "source": "dense+rrf", "rrf_score": 0.014084507042253521, "rerank_score": 0.17331343823271572, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 3, "text_snippet": "h Gemini-1.5-Flash and GPT-4o-mini. Meanwhile, phi-3.5-Vision , a 4.2 billion parameter model derived from phi-3.5- mini , excels in reasoning tasks and is adept at handling both single-image and text prompts, as well as multi-image and tex"}]}
{"case_index": 94, "query": "From 2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt, what exact term fills [BLANK] in: \"Our training dataset is a scaled-up version of the one used for phi-2 , composed of heavily filtered publicly available web data and [BLANK] data.\"?", "gold": "synthetic", "answer": "The exact term that fills **[BLANK]** is **‚Äúsynthetic‚Äù**, i.e., ‚Äú‚Ä¶publicly available web data and **synthetic** data.‚Äù", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'synthetic' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 68.371, "llm_ms": 1320.73, "top_contexts": [{"rank": 1, "score": 0.66945595, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.38768110059908983, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 1, "text_snippet": "hone. Our training dataset is a scaled-up version of the one used for phi-2 , composed of heavily filtered publicly available web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also pr"}, {"rank": 2, "score": 0.5874937, "source": "dense+rrf", "rrf_score": 0.0315136476426799, "rerank_score": 0.3522627152036244, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 0, "text_snippet": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone Microsoft Abstract We introduce phi-3-mini , a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both"}, {"rank": 3, "score": 0.60911226, "source": "dense+rrf", "rrf_score": 0.031746031746031744, "rerank_score": 0.27770941646918845, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 18, "text_snippet": "y training data to improve the performance of small language models and deviate from the standard scaling-laws . In this work we show that such method allows to reach the level of highly capable models such as GPT-3.5 or Mixtral with only 3"}, {"rank": 4, "score": 0.5835681, "source": "dense+rrf", "rrf_score": 0.029850746268656716, "rerank_score": 0.25303940892802507, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 8, "text_snippet": "ved solely by changing the training data. phi-3-mini: The development of a compact language model that rivals the capabilities of ChatGPT, while fitting on a phone, is a testament to the power of data-driven machine learning. By meticulousl"}, {"rank": 5, "score": 0.5690057, "source": "dense+rrf", "rrf_score": 0.029910714285714284, "rerank_score": 0.24272775495688778, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 6, "text_snippet": "7B parameters), matched the performance of models 25 times larger trained on regular data. In this report we present a new model, phi-3-mini (3.8B parameters), trained for 3.3T tokens on larger and more advanced versions of the datasets use"}]}
{"case_index": 95, "query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \"(2018), which uses unidirec- tional language models for pre-training, BERT uses masked language models to enable pre- trained deep bidirectional [BLANK].\"?", "gold": "representations", "answer": "The exact term that fills **[BLANK]** is **‚Äúrepresentations‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'representations' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 81.103, "llm_ms": 1249.727, "top_contexts": [{"rank": 1, "score": 0.6985074, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.37089660410508635, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 10, "text_snippet": "ford et al. (2018), which uses unidirec- tional language models for pre-training, BERT uses masked language models to enable pre- trained deep bidirectional representations. This is also in contrast to Peters et al. (2018a), which uses a sh"}, {"rank": 2, "score": 0.6797605, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.3031933029127169, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 9, "text_snippet": "ng, the MLM ob- jective enables the representation to fuse the left and the right context, which allows us to pre- train a deep bidirectional Transformer. In addi- tion to the masked language model, we also use a ‚Äúnext sentence prediction‚Äù "}, {"rank": 3, "score": 0.65880424, "source": "dense+rrf", "rrf_score": 0.031009615384615385, "rerank_score": 0.2989719449614759, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 0, "text_snippet": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova Google AI Language {jacobdevlin,mingweichang,kentonl,kristout }@google.com Abstract We introduce a ne"}, {"rank": 4, "score": 47.96874476193103, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.2607990786624094, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 476, "text_snippet": "heng, and Jingjing Liu. Uniter: Learning universal image-text representations. arXiv preprint arXiv:1909.11740 , 2019. [Cra17] Kate Crawford. The trouble with bias. NIPS 2017 Keynote , 2017. [DCLT18] Jacob Devlin, Ming-Wei Chang, Kenton Lee"}, {"rank": 5, "score": 0.60084015, "source": "dense+rrf", "rrf_score": 0.02813852813852814, "rerank_score": 0.2535998735789268, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 83, "text_snippet": "ted that rich, unsupervised pre-training is an integral part of many language understanding systems. In particular, these results enable even low-resource tasks to beneÔ¨Åt from deep unidirectional architec- tures. Our major contribution is f"}]}
{"case_index": 96, "query": "From 2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt, what exact term fills [BLANK] in: \"So the answer is no.Q: The concert was [BLANK] to be on 06/01/1943, but was delayed by one day to today.\"?", "gold": "scheduled", "answer": "The exact term that fills **[BLANK]** is:\n\n**scheduled**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Model answer matches expected term 'scheduled' and is directly supported by the provided context sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 70.754, "llm_ms": 1383.839, "top_contexts": [{"rank": 1, "score": 0.5436214, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3734629923672459, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 277, "text_snippet": " days later. Sotodayis01/07/2019. So the answer is 01/07/2019. Q:The concert was scheduled to be on 06/01/1943, but was delayed by one day to today. What is the date 10 days ago in MM/DD/YYYY? A:One dayafter06/01/1943 is06/02/1943, sotodayi"}, {"rank": 2, "score": 0.4954286, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.35611997975245113, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 25, "text_snippet": " populated areas (c) desert (d) apartment (e) roadblock A: The answer must be a place with a lot of people. Race tracks, desert, apartments, and roadblocks don't have a lot of people, but populated areas do. So the answer is (b). Q: Yes or "}, {"rank": 3, "score": 0.39839786, "source": "dense+rrf", "rrf_score": 0.03036576949620428, "rerank_score": 0.3114898401276077, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 26, "text_snippet": "delayed by one day to today. What is the date 10 days ago in MM/DD/YYYY? A: One day after 06/01/1943 is 06/02/1943, so today is 06/02/1943. 10 days before today is 05/23/1943. So the answer is 05/23/1943. Q: Is the following sentence plausi"}, {"rank": 4, "score": 33.47057210189794, "source": "dense+rrf", "rrf_score": 0.0136986301369863, "rerank_score": 0.2708013455424428, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 139, "text_snippet": "Model OutputQuestion Question QuestionFigure 10: Examples of semantic understanding and one-step missing errors that were Ô¨Åxed by scaling PaLM from 62B to 540B. A.2 What is the role of prompt engineering? One of the key considerations of pr"}, {"rank": 5, "score": 0.4234479, "source": "dense+rrf", "rrf_score": 0.029437229437229435, "rerank_score": 0.25414060222111, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 246, "text_snippet": "w (Tue, 7/9/1972). What is the date one week ago from today in MM/DD/YYYY? MODEL ANSWER (INCORRECT): Tomorrow is 7/9/1972. One week ago from today is 7/2/1972. So the answer is 7/2/1972. \u0017(Ground truth is 07/01/1972.) QUESTION :Today is the"}]}
{"case_index": 97, "query": "From 2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt, what exact term fills [BLANK] in: \"We perform ablation studies in [BLANK] 3.5 to prove why longer retrieval units are necessary.\"?", "gold": "subsection", "answer": "The exact term is **‚Äúsubsection‚Äù** (i.e., ‚ÄúWe perform ablation studies in **subsection** 3.5 to prove why longer retrieval units are necessary.‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'subsection' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 94.164, "llm_ms": 1559.27, "top_contexts": [{"rank": 1, "score": 50.022309667366635, "source": "dense+rrf", "rrf_score": 0.01639344262295082, "rerank_score": 0.2737280500377097, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 18, "text_snippet": "er than chunking them into smaller units. By doing so, we achieve an F1 score of 25.9% on Qasper (previously 22.5%) and 57.5% on MultiFieldQA-en (previously 51.2%). We perform ablation studies in subsection 3.5 to prove why longer retrieval"}, {"rank": 2, "score": 0.6679503, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.26421250591475043, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 70, "text_snippet": "ong retrieval units, achieves better performance compared to traditional RAG, which operates on short retrieval units. 3.5 Ablation Studies We perform several in-depth ablation to understand what are the important factors in our LongRAG sys"}, {"rank": 3, "score": 0.6574571, "source": "dense+rrf", "rrf_score": 0.03149801587301587, "rerank_score": 0.2309053938257519, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 0, "text_snippet": "LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs ‚ô†Ziyan Jiang,‚ô†Xueguang Ma,‚ô†Wenhu Chen ‚ô†University of Waterloo ziyanjiang528@gmail.com ,{x93ma ,wenhuchen}@uwaterloo.ca Project Website: https://tiger-ai-lab.github.io/"}, {"rank": 4, "score": 0.6225729, "source": "dense+rrf", "rrf_score": 0.026500526500526502, "rerank_score": 0.2208264677322303, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 59, "text_snippet": " limitation of Position Interpolation (Chen et al., 2023). Retrieval-based Evaluation. We conduct experiments on retrieval in long contexts. In Table 5, we compare our model with other open LLMs on the topic retrieval task introduced in Lon"}, {"rank": 5, "score": 28.935130385356224, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.20207719201817392, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 37, "text_snippet": "he four datasets we use, followed by a demonstra- tion of the retriever‚Äôs performance. Next, we will present the final question-answering performance. Finally, we conduct detailed ablation studies to explain why operating on long retrieval "}]}
{"case_index": 98, "query": "From 2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt, what exact term fills [BLANK] in: \"User: Okay now more serious answer, and note that this was achieved solely by [BLANK] the training data.\"?", "gold": "changing", "answer": "The exact term is **‚Äúchanging‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'changing' and is directly supported by the provided context quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 71.458, "llm_ms": 1022.497, "top_contexts": [{"rank": 1, "score": 0.6180052, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.38631168358608853, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 7, "text_snippet": "5. 1arXiv:2404.14219v4 [cs.CL] 30 Aug 2024  User: Explain why it is surprising that one can build a language model small enough to fit on a phone, yet almost as powerful as ChatGPT. Just use one funny sentence. phi-3-mini: It‚Äôs like fitting"}, {"rank": 2, "score": 0.5700853, "source": "dense+rrf", "rrf_score": 0.0304147465437788, "rerank_score": 0.2782173698753942, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 0, "text_snippet": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone Microsoft Abstract We introduce phi-3-mini , a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both"}, {"rank": 3, "score": 0.57519084, "source": "dense+rrf", "rrf_score": 0.03036576949620428, "rerank_score": 0.2543740466349116, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 6, "text_snippet": "7B parameters), matched the performance of models 25 times larger trained on regular data. In this report we present a new model, phi-3-mini (3.8B parameters), trained for 3.3T tokens on larger and more advanced versions of the datasets use"}, {"rank": 4, "score": 0.6155368, "source": "dense+rrf", "rrf_score": 0.0315136476426799, "rerank_score": 0.24923562858220585, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 8, "text_snippet": "ved solely by changing the training data. phi-3-mini: The development of a compact language model that rivals the capabilities of ChatGPT, while fitting on a phone, is a testament to the power of data-driven machine learning. By meticulousl"}, {"rank": 5, "score": 23.596398661574096, "source": "dense+rrf", "rrf_score": 0.013888888888888888, "rerank_score": 0.2418234996446517, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 3, "text_snippet": ", recent work from Hoffmann et al. (2022) shows that, for a given compute budget, the best performances are not achieved by the largest mod- els, but by smaller models trained on more data. The objective of the scaling laws from Hoff- mann "}]}
{"case_index": 99, "query": "From 2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt, what exact term fills [BLANK] in: \"[BLANK] Prompting Elicits Reasoning in Large Language Models Jason Wei Xuezhi Wang Dale Schuurmans Maarten Bosma Brian Ichter Fei Xia Ed H.\"?", "gold": "chain-of-thought", "answer": "**Chain-of-Thought**", "answer_score": 0.3, "answer_correctness": 0.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Expected exact term is \"chain-of-thought\" (lowercase with hyphens). Model answered \"Chain-of-Thought\" with different capitalization, so not exact. Context supports the hyphenated term.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 38.851, "llm_ms": 1039.956, "top_contexts": [{"rank": 1, "score": 0.79498434, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.42759357975044593, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 0, "text_snippet": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models Jason Wei Xuezhi Wang Dale Schuurmans Maarten Bosma Brian Ichter Fei Xia Ed H. Chi Quoc V . Le Denny Zhou Google Research, Brain Team {jasonwei,dennyzhou}@google.com Abst"}, {"rank": 2, "score": 0.7215064, "source": "dense+rrf", "rrf_score": 0.032018442622950824, "rerank_score": 0.4167086584556909, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 133, "text_snippet": " Proceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence , 32, Apr. 2018. URL https: //ojs.aaai.org/index.php/AAAI/article/view/12340 . Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and"}, {"rank": 3, "score": 0.6857973, "source": "dense+rrf", "rrf_score": 0.029206349206349208, "rerank_score": 0.404709725848677, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 120, "text_snippet": "Zhou. 2022. Self- consistency improves chain of thought reasoning in language models. CoRR , abs/2203.11171. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicit"}, {"rank": 4, "score": 83.12205554727382, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.36868807750695753, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 114, "text_snippet": "-augmented ensembles in language models. arXiv preprint arXiv:2207.00747 , 2022b. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models"}, {"rank": 5, "score": 0.68718886, "source": "dense+rrf", "rrf_score": 0.0288981288981289, "rerank_score": 0.30405106082804784, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 0, "text_snippet": "Published as a conference paper at ICLR 2023 SELF-CONSISTENCY IMPROVES CHAIN OF THOUGHT REASONING IN LANGUAGE MODELS Xuezhi Wang‚Ä†‚Ä°Jason Wei‚Ä†Dale Schuurmans‚Ä†Quoc Le‚Ä†Ed H. Chi‚Ä† Sharan Narang‚Ä†Aakanksha Chowdhery‚Ä†Denny Zhou‚Ä†¬ß ‚Ä†Google Research, "}]}
{"case_index": 100, "query": "From 2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt, what exact term fills [BLANK] in: \"To accommodate the context [BLANK] of language models, text is segmented into smaller, digestible chunks.\"?", "gold": "limitations", "answer": "The exact term is **‚Äúlimitations‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term and is directly supported by Context 1 ('context limitations of language models').", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 88.387, "llm_ms": 1024.337, "top_contexts": [{"rank": 1, "score": 54.65545439034086, "source": "dense+rrf", "rrf_score": 0.01639344262295082, "rerank_score": 0.3344225643937398, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 20, "text_snippet": "er. widespread adoption of ChatGPT. The Naive RAG follows a traditional process that includes indexing, retrieval, and generation, which is also characterized as a ‚ÄúRetrieve-Read‚Äù framework [7]. Indexing starts with the cleaning and extract"}, {"rank": 2, "score": 29.132315621627185, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.24006438796750235, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 21, "text_snippet": "to smaller, digestible chunks. Chunks are then encoded into vector representations using an embedding model and stored in vector database. This step is crucial for enabling efficient similarity searches in the subsequent retrieval phase. Re"}, {"rank": 3, "score": 21.49322193188966, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.22934115917614042, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 22, "text_snippet": "cores between the query vector and the vector of chunks within the indexed corpus. The system prioritizes and retrieves the top K chunks that demonstrate the greatest similarity to the query. These chunks are subsequently used as the expand"}, {"rank": 4, "score": 21.036590422599406, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.2104318682047272, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 85, "text_snippet": "both the results are utilized as the basis for language model answer generation. 3) Query Routing: Based on varying queries, routing to distinct RAG pipeline,which is suitable for a versatile RAG system designed to accommodate diverse scena"}, {"rank": 5, "score": 0.5956241, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.20409673265577538, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 81, "text_snippet": " al., 2022), fine-tuning the retriever and reader jointly (Yu, 2022; Izacard et al., 2022; Singh et al., 2021; Izacard & Grave, 2020a), and integrating the retriever with the black-box language model (Yu et al., 2023; Shi et al., 2023; Triv"}]}
{"case_index": 101, "query": "From 2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt, what exact term fills [BLANK] in: \"Second, large language models offer the exciting prospect of [BLANK] few-shot learning via prompting .\"?", "gold": "in-context", "answer": "The exact term is **‚Äúin-context‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'in-context' and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 45.992, "llm_ms": 1422.71, "top_contexts": [{"rank": 1, "score": 0.7481389, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.2922716627634592, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 8, "text_snippet": "017) or Ô¨Ånetuning a pretrained model (Cobbe et al., 2021), in addition to neuro-symbolic methods that use formal lan- guages instead of natural language (Roy and Roth, 2015; Chiang and Chen, 2019; Amini et al., 2019; Chen et al., 2019). Sec"}, {"rank": 2, "score": 0.6770654, "source": "dense+rrf", "rrf_score": 0.029437229437229435, "rerank_score": 0.2554954334708459, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 18, "text_snippet": " used for tasks such as math word problems, commonsense reasoning, and symbolic manipulation, and is potentially applicable (at least in principle) to any task that humans can solve via language. 4.Finally, chain-of-thought reasoning can be"}, {"rank": 3, "score": 0.6817391, "source": "dense+rrf", "rrf_score": 0.030090497737556562, "rerank_score": 0.252903459486852, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 1, "text_snippet": "such reasoning abilities emerge naturally in sufÔ¨Åciently large language models via a simple method called chain-of- thought prompting , where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three"}, {"rank": 4, "score": 43.15634855298027, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.2394183654780661, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 120, "text_snippet": "Zhou. 2022. Self- consistency improves chain of thought reasoning in language models. CoRR , abs/2203.11171. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicit"}, {"rank": 5, "score": 37.958462175263634, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.2347514096574293, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 133, "text_snippet": " Proceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence , 32, Apr. 2018. URL https: //ojs.aaai.org/index.php/AAAI/article/view/12340 . Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and"}]}
{"case_index": 102, "query": "From 2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt, what exact term fills [BLANK] in: \"on two tasks that require identifying relevant information in their in- put contexts: [BLANK] question an- swering and key-value retrieval.\"?", "gold": "multi-document", "answer": "The exact term is **‚Äúmulti-document‚Äù**, as in: ‚Äú**multi-document** question answering and key-value retrieval.‚Äù", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'multi-document' and is directly supported by the provided context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 104.707, "llm_ms": 1527.293, "top_contexts": [{"rank": 1, "score": 0.71788263, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.4231090101148305, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 1, "text_snippet": " on two tasks that require identifying relevant information in their in- put contexts: multi-document question an- swering and key-value retrieval. We find that performance can degrade significantly when changing the position of relevant in"}, {"rank": 2, "score": 0.69741774, "source": "dense+rrf", "rrf_score": 0.031746031746031744, "rerank_score": 0.3270626966441084, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 49, "text_snippet": "UIDs. The relevant key-value pair for answering the query is bolded here within the input context for clarity. indicate that extended-context models are not nec- essarily better than their non-extended counterparts at using their input cont"}, {"rank": 3, "score": 0.6419798, "source": "dense+rrf", "rrf_score": 0.030621785881252923, "rerank_score": 0.3109246619767013, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 0, "text_snippet": "Lost in the Middle: How Language Models Use Long Contexts Nelson F. Liu1‚àóKevin Lin2John Hewitt1Ashwin Paranjape3 Michele Bevilacqua3Fabio Petroni3Percy Liang1 1Stanford University2University of California, Berkeley3Samaya AI nfliu@cs.stanfo"}, {"rank": 4, "score": 0.720602, "source": "dense+rrf", "rrf_score": 0.031544957774465976, "rerank_score": 0.2750837279170047, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 14, "text_snippet": "r at using their input context (¬ß2.3). Given that language models struggle to retrieve and use relevant information in the multi-document question answering task, to what extent can lan- guage models even retrieve from their input con- text"}, {"rank": 5, "score": 0.64961934, "source": "dense+rrf", "rrf_score": 0.029857397504456328, "rerank_score": 0.27323272141330973, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 58, "text_snippet": "s at the very start or end of the context, and rapidly degrades when models must retrieve from the middle of the input context. placed at the start of the input context, LongChat- 13B (16K) tends to generate code to retrieve the key, rather"}]}
{"case_index": 103, "query": "From 2005.14165__language_models_are_few_shot_learners.txt, what exact term fills [BLANK] in: \"g‚Äù, using the text input of a pretrained language model as a form of task speciÔ¨Åcation: the model is conditioned on a natural language instruction and/or a few [BLANK] of the task and is then expected to complete further instances of the task simply by predicting what comes next.\"?", "gold": "demonstrations", "answer": "The exact term that fills **[BLANK]** is **‚Äúdemonstrations‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'demonstrations' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 122.622, "llm_ms": 1653.985, "top_contexts": [{"rank": 1, "score": 0.69546795, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.4896032685888199, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 22, "text_snippet": "g‚Äù, using the text input of a pretrained language model as a form of task speciÔ¨Åcation: the model is conditioned on a natural language instruction and/or a few demonstrations of the task and is then expected to complete further instances of"}, {"rank": 2, "score": 0.57500017, "source": "dense+rrf", "rrf_score": 0.02761904761904762, "rerank_score": 0.2761163796048229, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 22, "text_snippet": "nstruction Tuning Instruction tuning [ 46,38,62,61] has emerged as a crucial step in training language models. Instruction tuning involves learning from input-output pairs where the input is natural language task description,and the output "}, {"rank": 3, "score": 32.010365887015006, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.2711082571031857, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 21, "text_snippet": "his same Ô¨Çuidity and generality. One potential route towards addressing these issues is meta-learning1‚Äì which in the context of language models means the model develops a broad set of skills and pattern recognition abilities at training tim"}, {"rank": 4, "score": 0.5893574, "source": "dense+rrf", "rrf_score": 0.03057889822595705, "rerank_score": 0.26797724792837746, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 8, "text_snippet": "017) or Ô¨Ånetuning a pretrained model (Cobbe et al., 2021), in addition to neuro-symbolic methods that use formal lan- guages instead of natural language (Roy and Roth, 2015; Chiang and Chen, 2019; Amini et al., 2019; Chen et al., 2019). Sec"}, {"rank": 5, "score": 35.912025730725475, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.265087274734799, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 54, "text_snippet": " model‚Äôs ability to respond to instructions in other languages and complete coding tasks. For each natural language prompt, the task is most often speciÔ¨Åed directly through a natural language instruction (e.g. ‚ÄúWrite a story about a wise fr"}]}
{"case_index": 104, "query": "From 2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt, what exact term fills [BLANK] in: \"For example, synonyms or [BLANK] that consist of completely different tokens may still be mapped to vectors close to each other.\"?", "gold": "paraphrases", "answer": "The exact term is **‚Äúparaphrases‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context 1 explicitly contains the phrase 'synonyms or paraphrases' matching the expected blank.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 70.689, "llm_ms": 1308.31, "top_contexts": [{"rank": 1, "score": 0.5748404, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3929210605697156, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 5, "text_snippet": "ently with an inverted index and can be seen as representing the question and context in high- dimensional, sparse vectors (with weighting). Con- versely, the dense , latent semantic encoding is com- plementary to sparse representations by "}, {"rank": 2, "score": 0.5038181, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.20470605227158362, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 14, "text_snippet": "r at using their input context (¬ß2.3). Given that language models struggle to retrieve and use relevant information in the multi-document question answering task, to what extent can lan- guage models even retrieve from their input con- text"}, {"rank": 3, "score": 28.65830194208641, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.19845618559695008, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 12, "text_snippet": " vs. 33.3%) in the open Natural Questions setting (Lee et al., 2019; Kwiatkowski et al., 2019). Our contributions are twofold. First, we demon- strate that with the proper training setup, sim- ply Ô¨Åne-tuning the question and passage encoder"}, {"rank": 4, "score": 0.49802288, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.1954351643567363, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 25, "text_snippet": " is a paragraph (as opposed to a list or a table). We use passages (chunks of at most 100 tokens) from Wikipedia as documents within our input contexts. For each of the queries, we need a document that contains the answer and k‚àí1distractor "}, {"rank": 5, "score": 31.461563158483866, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.19091242390157676, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 0, "text_snippet": "Dense Passage Retrieval for Open-Domain Question Answering Vladimir Karpukhin‚àó, Barlas O Àòguz‚àó, Sewon Min‚Ä†, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen‚Ä°, Wen-tau Yih Facebook AI‚Ä†University of Washington‚Ä°Princeton University {vladk, "}]}
{"case_index": 105, "query": "From 2305.06983__active_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"It is a generic framework that actively decides when and what to retrieve through the generation process,resulting in the [BLANK] of retrieval and genera- tion.\"?", "gold": "interleaving", "answer": "The exact term that fills **[BLANK]** is **‚Äúinterleaving‚Äù** (as in: ‚Äúresulting in the **interleaving** of retrieval and generation.‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'interleaving' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 69.42, "llm_ms": 1800.309, "top_contexts": [{"rank": 1, "score": 0.6707629, "source": "dense+rrf", "rrf_score": 0.032018442622950824, "rerank_score": 0.44159782841382533, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 22, "text_snippet": " generate the complete answer at once y=LM([Dx,x]). 2.3 Active Retrieval Augmented Generation To aid long-form generation with retrieval, we pro- pose active retrieval augmented generation. It is a generic framework that actively decides wh"}, {"rank": 2, "score": 0.67202866, "source": "dense+rrf", "rrf_score": 0.031746031746031744, "rerank_score": 0.3199264299684863, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 2, "text_snippet": "is work, we provide a generalized view of ac- tive retrieval augmented generation , methods that actively decide when and what to retrieve across the course of the generation. We propose Forward- Looking Active REtrieval augmented generatio"}, {"rank": 3, "score": 0.67420304, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.3146681470907678, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 14, "text_snippet": " al., 2022). We ask the following question: can we create a simple and generic retrieval augmented LM that ac- tively decides when and what to retrieve throughout the generation process, and are applicable to a va- riety of long-form genera"}, {"rank": 4, "score": 0.6446475, "source": "dense+rrf", "rrf_score": 0.03007688828584351, "rerank_score": 0.2929522375724481, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 16, "text_snippet": ". When deciding what to retrieve , it is important to consider what LMs intend to gen- erate in the future, as the goal of active retrieval is to benefit future generations. Therefore, we propose anticipating the future by generating a temp"}, {"rank": 5, "score": 0.5929181, "source": "dense+rrf", "rrf_score": 0.02803921568627451, "rerank_score": 0.27090245610847813, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 15, "text_snippet": "e to avoid unneces- sary or inappropriate retrieval that occurs in passive retrieval augmented LMs (Khandelwal et al., 2020; Borgeaud et al., 2022; Ram et al., 2023; Trivedi et al., 2022). Given the observation that large LMs tend to be wel"}]}
{"case_index": 106, "query": "From 2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt, what exact term fills [BLANK] in: \"They often rely on small number of tasks or on using other models for auto-evaluation by asking [BLANK]‚Äúgivenresponsesfromsystem1 (reference)andsystem2(target), whichoneisbetter?‚Äù.\"?", "gold": "themtocomparetheoutputsoftwosystemswithapromptlike", "answer": "The exact term is **‚Äúthem‚Äù** (i.e., ‚Äúby asking **them** to compare the outputs of two systems with a prompt like ‚Ä¶‚Äù).", "answer_score": 0.0, "answer_correctness": 0.0, "answer_groundedness": 0.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 0.0, "abstained": false, "should_abstain": false, "notes": "The blank spans the full phrase 'themtocomparetheoutputsoftwosystemswithapromptlike', not just 'them'. The model answer is incomplete and thus incorrect, despite being partially present in context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 91.839, "llm_ms": 1689.103, "top_contexts": [{"rank": 1, "score": 0.6663513, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.4034685277346391, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 18, "text_snippet": "roach a particular task. Rather than naively imitating powerful LLMs, we treat them as a reservoir of behaviors from which we carefully select those best suited for the task at hand. Some previous studies on training small models are limite"}, {"rank": 2, "score": 0.5758959, "source": "dense+rrf", "rrf_score": 0.030330882352941176, "rerank_score": 0.23569203953990783, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 1, "text_snippet": "n benchmarks like BigBench Hard and AGIEval. In Orca 2, we continue exploring how improved training signals can enhance smaller LMs‚Äô reasoning abilities. Research on training small LMs has often relied on imitation learning to replicate the"}, {"rank": 3, "score": 0.661269, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.21626382422840637, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 19, "text_snippet": "ference)andsystem2(target), whichoneisbetter?‚Äù. However, previouswork[ 13,42,60,67] has demonstrated that this approach has several drawbacks. In this work, we provide a comprehensive evaluation comparing Orca 2 to several other models. We "}, {"rank": 4, "score": 22.390940819611348, "source": "dense+rrf", "rrf_score": 0.014084507042253521, "rerank_score": 0.21009698961046086, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 137, "text_snippet": "periment highlights the potential of specializing Orca 2 models for specific tasks using synthetic data generated with prompt erasing . 20  7 Limitations Orca 2, built upon the LLaMA 2 model family, retains many of its limitations, as well "}, {"rank": 5, "score": 30.586586302735824, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.1895911561459477, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 0, "text_snippet": "Orca 2: Teaching Small Language Models How to Reason Arindam Mitra, Luciano Del Corro‚Ä†, Shweti Mahajan‚Ä†, Andres Codas‚Ä° Clarisse Simoes‚Ä°, Sahaj Agarwal, Xuxi Chen‚àó, Anastasia Razdaibiedina‚àó Erik Jones‚àó, Kriti Aggarwal‚àó, Hamid Palangi, Guoqin"}]}
{"case_index": 107, "query": "From 2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt, what exact term fills [BLANK] in: \"We open-source [BLANK] to make it easier to build on this primitive.1 We empirically va\"?", "gold": "flashattention", "answer": "The exact term is **‚ÄúFlashAttention‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context explicitly says: \"We open-source FlashAttention...\" Model answered \"FlashAttention\", matching expected term (case-insensitive).", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 104.015, "llm_ms": 1486.633, "top_contexts": [{"rank": 1, "score": 0.6521708, "source": "dense+rrf", "rrf_score": 0.03131881575727918, "rerank_score": 0.3326106817029708, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 16, "text_snippet": "lock-sparse FlashAttention , a sparse attention algorithm that is 2-4 \u0002faster than evenFlashAttention , scaling up to sequence length of 64k. We prove that block-sparse FlashAttention has better IO complexity than FlashAttention by a factor"}, {"rank": 2, "score": 0.66147155, "source": "dense+rrf", "rrf_score": 0.03149801587301587, "rerank_score": 0.29638575299817205, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 17, "text_snippet": "to make it easier to build on this primitive.1 We empirically validate that FlashAttention speeds up model training and improves model quality by modeling longer context. We also benchmark the runtime and memory footprint of FlashAttention "}, {"rank": 3, "score": 0.6264792, "source": "dense+rrf", "rrf_score": 0.02946236559139785, "rerank_score": 0.25152777256992126, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 100, "text_snippet": "n, Atri Rudra, and Christopher R√©. 2022. FlashAttention: Fast and memory-efficient exact attention with IO- awareness. ArXiv:2205.14135. Hermann Ebbinghaus. 1913. Memory: A contribu- tion to experimental psychology. H. A. Ruger & C. E. Buss"}, {"rank": 4, "score": 0.65848434, "source": "dense+rrf", "rrf_score": 0.030309988518943745, "rerank_score": 0.24077465339623821, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 7, "text_snippet": " with wall-clock speed) and tend to ignore overheads from memory access (IO). In this paper, we argue that a missing principle is making attention algorithms IO-aware [1]‚Äîthat is, carefully accounting for reads and writes to diÔ¨Äerent levels"}, {"rank": 5, "score": 44.40141657645509, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.23518046435118237, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 88, "text_snippet": "ng, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821 ,"}]}
{"case_index": 108, "query": "From 2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt, what exact term fills [BLANK] in: \"Such a design forces the [BLANK] to search over a large corpus to find the ‚Äúneedle‚Äù unit.\"?", "gold": "retriever", "answer": "The exact term is **‚Äúretriever‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'retriever' and is directly supported by the provided context sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 102.928, "llm_ms": 1061.172, "top_contexts": [{"rank": 1, "score": 0.57460445, "source": "dense+rrf", "rrf_score": 0.029551337359792925, "rerank_score": 0.4032897432393756, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 0, "text_snippet": "LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs ‚ô†Ziyan Jiang,‚ô†Xueguang Ma,‚ô†Wenhu Chen ‚ô†University of Waterloo ziyanjiang528@gmail.com ,{x93ma ,wenhuchen}@uwaterloo.ca Project Website: https://tiger-ai-lab.github.io/"}, {"rank": 2, "score": 0.6382595, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.28267688034569377, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 1, "text_snippet": "r to search over a large corpus to find the ‚Äúneedle‚Äù unit. In contrast, the readers only need to generate answers from the short retrieved units. The imbalanced ‚Äúheavy‚Äù retriever and ‚Äúlight‚Äù reader design can lead to sub-optimal performance"}, {"rank": 3, "score": 0.58731115, "source": "dense+rrf", "rrf_score": 0.03057889822595705, "rerank_score": 0.23922438661092915, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 7, "text_snippet": "assive corpus with up to tens of millions of information units). Subsequently, the retrieved units are passed to the reader to generate the final response. On the contrary, the reader only needs to extract answers from 1arXiv:2406.15319v3 ["}, {"rank": 4, "score": 0.60697967, "source": "dense+rrf", "rrf_score": 0.02967032967032967, "rerank_score": 0.22726076393133582, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 79, "text_snippet": "on the long-context understanding ability of LLMs, and we still have a long way to go in harnessing open-source LLMs within our framework. 10  4 Related Work 4.1 Retrieval-Augmented Generation. Augmenting language models with information re"}, {"rank": 5, "score": 0.6203797, "source": "dense+rrf", "rrf_score": 0.031754032258064516, "rerank_score": 0.22363659695365246, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 6, "text_snippet": "rely offloaded from the parametric knowledge of LLMs by leveraging a standalone retrieval component from an external corpus. The existing RAG framework tends to use short retrieval units, such as 100-word passages in popular open- domain qu"}]}
{"case_index": 109, "query": "From 2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt, what exact term fills [BLANK] in: \"Dense retrieval methods have thus never be shown to [BLANK] TF-IDF/BM25 for open- domain QA before ORQA (Lee et al., 2019), which\"?", "gold": "outperform", "answer": "The exact term is **‚Äúoutperform‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'outperform' and is directly supported by the provided context sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 54.705, "llm_ms": 911.214, "top_contexts": [{"rank": 1, "score": 0.6913388, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.3914046744359462, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "-speciÔ¨Åc representation. With special in-memory data struc- tures and indexing schemes, retrieval can be done efÔ¨Åciently using maximum inner product search (MIPS) algorithms (e.g., Shrivastava and Li (2014); Guo et al. (2016)). However, it "}, {"rank": 2, "score": 0.6629555, "source": "dense+rrf", "rrf_score": 0.031754032258064516, "rerank_score": 0.2669789544117254, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 8, "text_snippet": "/BM25 for open- domain QA before ORQA (Lee et al., 2019), which proposes a sophisticated inverse cloze task (ICT) objective, predicting the blocks that contain the masked sentence, for additional pretraining. The question encoder and the re"}, {"rank": 3, "score": 0.5941806, "source": "dense+rrf", "rrf_score": 0.028958333333333336, "rerank_score": 0.24890756311388967, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 4, "text_snippet": " degradation is often observed in practice2, indicating the needs of improving retrieval. ‚àóEqual contribution 1The code and trained models have been released at https://github.com/facebookresearch/DPR. 2For instance, the exact match score o"}, {"rank": 4, "score": 0.6677444, "source": "dense+rrf", "rrf_score": 0.031746031746031744, "rerank_score": 0.23884448383684043, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 77, "text_snippet": "y comparable to ORQA‚Äôs 5-passage setup. 7 Related Work Passage retrieval has been an important compo- nent for open-domain QA (V oorhees, 1999). It not only effectively reduces the search space for answer extraction, but also identiÔ¨Åes the "}, {"rank": 5, "score": 0.62132454, "source": "dense+rrf", "rrf_score": 0.029418126757516764, "rerank_score": 0.2339332548166018, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 12, "text_snippet": " vs. 33.3%) in the open Natural Questions setting (Lee et al., 2019; Kwiatkowski et al., 2019). Our contributions are twofold. First, we demon- strate that with the proper training setup, sim- ply Ô¨Åne-tuning the question and passage encoder"}]}
{"case_index": 110, "query": "From 2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt, what exact term fills [BLANK] in: \"Our results indicate that prompting language models with longer input contexts is a trade-off‚Äî providing the language model with more informa- tion may help it perform the downstream task, but it also increases the amount of content that the model must reason over, [BLANK] decreasing ac- curacy.\"?", "gold": "potentially", "answer": "The exact term that fills **[BLANK]** is:\n\n**‚Äúpotentially‚Äù**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'potentially' and is directly supported by the provided context sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 152.143, "llm_ms": 1286.081, "top_contexts": [{"rank": 1, "score": 0.688999, "source": "dense+rrf", "rrf_score": 0.032266458495966696, "rerank_score": 0.5306913580277078, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 18, "text_snippet": " in multi-document QA (¬ß4.2). ‚Ä¢Even base language models (i.e., without in- struction fine-tuning) show a U-shaped per- formance curve as we vary the position of relevant information in the input context. Our results indicate that prompting"}, {"rank": 2, "score": 0.7170161, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.5081792500843765, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 80, "text_snippet": "struct), but minimally affects trends on larger models (70B). 5 Is More Context Is Always Better? A Case Study With Open-Domain QA Our results indicate that prompting language mod- els with longer input contexts is a trade-off‚Äî providing th"}, {"rank": 3, "score": 46.82777409624839, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.2819027991942974, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 81, "text_snippet": "ge model can take in 16K tokens, is it actually beneficial to provide 16K tokens of context? The answer to this question is ultimately downstream task-specific since it de- pends on the marginal value of the added context and the model‚Äôs ab"}, {"rank": 4, "score": 58.9131976702174, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.28041591520806736, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 19, "text_snippet": "model must reason over, potentially decreasing ac- curacy. To better understand this trade-off in prac- tice, we perform a case study with retriever-reader models on open-domain question answering (¬ß5). In contrast to our controlled multi-d"}, {"rank": 5, "score": 0.6734625, "source": "dense+rrf", "rrf_score": 0.029877369007803793, "rerank_score": 0.24293660311671356, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 94, "text_snippet": "levant information, indicating that models struggle to robustly access and use infor- mation in long input contexts. In particular, per- formance is often lowest when models must use information in the middle of long input contexts. We cond"}]}
{"case_index": 111, "query": "From 2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt, what exact term fills [BLANK] in: \"For example, training on the context length of 8192 needs 16 √ócomputational costs in [BLANK] layers as that of 2048.\"?", "gold": "self-attention", "answer": "The exact term is **‚Äúself-attention‚Äù**, i.e., ‚Äúself-attention layers.‚Äù", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'self-attention' and is explicitly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 75.193, "llm_ms": 1086.974, "top_contexts": [{"rank": 1, "score": 0.595251, "source": "dense+rrf", "rrf_score": 0.032266458495966696, "rerank_score": 0.34992432429205944, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 1, "text_snippet": "ionally expensive, requiring extensive training hours and GPU resources. For example, training on the context length of 8192 needs 16 √ócomputational costs in self-attention layers as that of 2048. In this paper, we speed up the context exte"}, {"rank": 2, "score": 0.5768297, "source": "dense+rrf", "rrf_score": 0.03076923076923077, "rerank_score": 0.2772062170254154, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 57, "text_snippet": "n training context lengths, our models achieve better perplexity with longer context sizes. This indicates the effectiveness of our efficient fine-tuning method. In Table 3, for the same training and evaluation context length cases, the per"}, {"rank": 3, "score": 34.73012324991319, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.26288079638448053, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 107, "text_snippet": "P SIZES . In Table 7, we provide an ablation study on the group size of the S2-Attn. We experimented with fine-tuning Llama2 7B to 8192 and 16384 context lengths via LongLoRA. The group size varies from{1/2, 1/4, 1/6, 1/8 }of the target con"}, {"rank": 4, "score": 38.9656100248213, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.24969236140757795, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 0, "text_snippet": "Published as a conference paper at ICLR 2024 LONG LORA: E FFICIENT FINE-TUNING OF LONG - CONTEXT LARGE LANGUAGE MODELS Yukang Chen1Shengju Qian1Haotian Tang2Xin Lai1 Zhijian Liu2Song Han2,3Jiaya Jia1 1CUHK2MIT3NVIDIA ABSTRACT We present Lon"}, {"rank": 5, "score": 32.48372108316262, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.24355472281500626, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 75, "text_snippet": "ed in training-from- scratch transformers. This experiment is to examine their capability of fine-tuning on pre-trained LLMs (Touvron et al., 2023b), toward long context adaptation. Dilated attention performs well in full fine-tuning but is"}]}
{"case_index": 112, "query": "From 2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt, what exact term fills [BLANK] in: \"[BLANK] During inference time, we apply the passage encoder EPto all the passages and index them using FAISS (Johnson et al., 2017) ofÔ¨Çine.\"?", "gold": "inference", "answer": "**Inference**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "The blank is filled by the section header term 'Inference', which appears immediately before the quoted sentence in the provided context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 96.216, "llm_ms": 1772.562, "top_contexts": [{"rank": 1, "score": 0.72901225, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.39373938839514194, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 23, "text_snippet": "BERT (Devlin et al., 2019) networks (base, un- cased) and take the representation at the [CLS] token as the output, so d= 768 . Inference During inference time, we apply the passage encoder EPto all the passages and index them using FAISS ("}, {"rank": 2, "score": 0.6404073, "source": "dense+rrf", "rrf_score": 0.03125763125763126, "rerank_score": 0.26047812538718756, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 17, "text_snippet": "rieval accuracy , which is the fraction of ques- tions for whichCFcontains a span that answers the question. 3 Dense Passage Retriever (DPR) We focus our research in this work on improv- ing the retrieval component in open-domain QA. Given "}, {"rank": 3, "score": 0.60227716, "source": "dense+rrf", "rrf_score": 0.028958333333333336, "rerank_score": 0.2371176394531583, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 81, "text_snippet": "the dual-encoder framework, they introduced a late-interaction operator on top of the BERT encoders. Dense retrieval for open-domain QA has been explored by Das et al. (2019), who propose to re- trieve relevant passages iteratively using re"}, {"rank": 4, "score": 0.62688875, "source": "dense+rrf", "rrf_score": 0.027583600982429624, "rerank_score": 0.22461971704956857, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 19, "text_snippet": "t passage are func- tions of both the retriever and reader. We also experimented with natural paragraphs in our preliminary trials and found that using Ô¨Åxed-length passages performs better in both retrieval and Ô¨Ånal QA accuracy, as observed"}, {"rank": 5, "score": 0.6129064, "source": "dense+rrf", "rrf_score": 0.02946912242686891, "rerank_score": 0.22337980295194243, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 116, "text_snippet": "he salient phrase ‚ÄúThoros of Myr‚Äù is critical, and DPR is unable to capture it. D Joint Training of Retriever and Reader We Ô¨Åx the passage encoder in our joint-training scheme while allowing only the question encoder to receive backpropagat"}]}
{"case_index": 113, "query": "From 2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt, what exact term fills [BLANK] in: \"ets are composed of [BLANK] short (averaging less than 1K tokens) but vast Wikipedia documents.\"?", "gold": "relatively", "answer": "The exact term is **‚Äúrelatively‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Model answer matches expected term 'relatively' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 59.022, "llm_ms": 2528.978, "top_contexts": [{"rank": 1, "score": 0.6418469, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.337304923364999, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 16, "text_snippet": "ets are composed of relatively short (averaging less than 1K tokens) but vast Wikipedia documents. By forming longer retrieval units through the grouping of multiple related documents, we reduce the NQ corpus size from 22M to 600K units, wh"}, {"rank": 2, "score": 40.7325509945968, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.24141587608471135, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 0, "text_snippet": "LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs ‚ô†Ziyan Jiang,‚ô†Xueguang Ma,‚ô†Wenhu Chen ‚ô†University of Waterloo ziyanjiang528@gmail.com ,{x93ma ,wenhuchen}@uwaterloo.ca Project Website: https://tiger-ai-lab.github.io/"}, {"rank": 3, "score": 29.436911252904242, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.21877425901853562, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 67, "text_snippet": " from single-document QA to a RAG task, we do not directly compare the results with previous models. Instead, we compare the performance of traditional RAG, which operates on 200-token passages, with our LongRAG, which operates on entire do"}, {"rank": 4, "score": 0.614935, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.2186361000031947, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 2, "text_snippet": "abilities of recent advancements in LLMs. In order to alleviate the imbalance, we propose a new framework LongRAG, consisting of a ‚Äúlong retriever‚Äù and a‚Äúlong reader‚Äù . In the two Wikipedia-based datasets, NQ and HotpotQA, where the average"}, {"rank": 5, "score": 0.5643584, "source": "dense+rrf", "rrf_score": 0.02804284323271665, "rerank_score": 0.20015795149210705, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 28, "text_snippet": " then fed into a long reader. Compared to traditional RAG, which retrieves hundreds of short units, our proposed LongRAG reduces the likelihood of retrieving hard negatives during the retrieval stage and more effectively leverages recent ad"}]}
{"case_index": 114, "query": "From 2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt, what exact term fills [BLANK] in: \"In additional experiments, we show [BLANK] can robustly boost performance on NLP tasks where adding a chain-of-thought might hurt performance compared to standard prompting (Ye & Durrett, 2022).\"?", "gold": "self-consistency", "answer": "The exact term is **‚Äúself-consistency‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term and is explicitly supported by the provided context sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 80.085, "llm_ms": 1336.382, "top_contexts": [{"rank": 1, "score": 0.7571461, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.45564609018824376, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 15, "text_snippet": "asoning tasks, including GSM8K (Cobbe et al., 2021) (+17.9% absolute accuracy gains), SV AMP (Patel et al., 2021) (+11.0%), AQuA (Ling et al., 2017) (+12.2%), and across commonsense reasoning tasks such as StrategyQA (Geva et al., 2021) (+6"}, {"rank": 2, "score": 0.7465086, "source": "dense+rrf", "rrf_score": 0.03125763125763126, "rerank_score": 0.29720871323284415, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 57, "text_snippet": "urrett (2022) show that sometimes chain-of-thought prompting could hurt performance compared to standard prompting in few-shot in-context learning. Here we perform a study using self-consistency to see if it can help Ô¨Åll in the gap, over a "}, {"rank": 3, "score": 45.28789191001252, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.28865008046166657, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 1, "text_snippet": "such reasoning abilities emerge naturally in sufÔ¨Åciently large language models via a simple method called chain-of- thought prompting , where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three"}, {"rank": 4, "score": 0.72399634, "source": "dense+rrf", "rrf_score": 0.029857397504456328, "rerank_score": 0.2863987531072752, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 5, "text_snippet": "odel performance across a variety of multi-step reasoning tasks (Wei et al., 2022). In this paper, we introduce a novel decoding strategy called self-consistency to replace the greedy decoding strategy used in chain-of-thought prompting (We"}, {"rank": 5, "score": 0.7473794, "source": "dense+rrf", "rrf_score": 0.031754032258064516, "rerank_score": 0.2819520472979506, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 58, "text_snippet": " al., 2020) and RTE (Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009). The results over PaLM-540B are shown in Table 5. For some tasks (e.g., ANLI-R1, e-SNLI, RTE), adding chain-of-thought does h"}]}
{"case_index": 115, "query": "From 2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt, what exact term fills [BLANK] in: \"OurmethodÔ¨Årst [BLANK] a key-value database, where values store raw chunks of text tokens and keys\"?", "gold": "constructs", "answer": "The exact term is **‚Äúconstructs‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context 1 explicitly states: 'Our method first constructs a key-value database...' matching the expected term.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 64.724, "llm_ms": 1162.371, "top_contexts": [{"rank": 1, "score": 0.5909683, "source": "dense+rrf", "rrf_score": 0.032266458495966696, "rerank_score": 0.339850721434098, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 14, "text_snippet": "show that the performance of R/e.sc/t.sc/r.sc/o.sc comes from both explicit neighbour copying and general knowledge extraction (¬ß4.4). 2. Method Wedesignourretrieval-enhancedarchitecturetobecapableofretrievingfromadatabasewithtrillions of t"}, {"rank": 2, "score": 0.58952856, "source": "dense+rrf", "rrf_score": 0.031009615384615385, "rerank_score": 0.2684459503197912, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 22, "text_snippet": " ùïç=¬ª1¬îùë£¬º, obtained using a text tokenizer1. Wespliteach ùëõ-token-longexample ùëã=¬πùë•1¬î¬ì¬ì¬ì¬îùë•ùëõ¬∫intoasequenceof ùëôchunks¬πùê∂1¬î¬ì¬ì¬ì¬îùê∂ùëô¬∫ of sizeùëö=ùëõ ùëô, i.e.ùê∂1,¬πùë•1¬î¬ì¬ì¬ì¬îùë•ùëö¬∫¬î ¬ì¬ì¬ì¬î ùê∂ùëô,¬πùë•ùëõ\u0000ùëö¬∏1¬î¬ì¬ì¬ì¬îùë•ùëõ¬∫2ùïçùëö. We useùëõ=2048andùëö=64. We augment each chunk ùê∂ùë¢with a se"}, {"rank": 3, "score": 29.489422242513612, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.2504724480421518, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 242, "text_snippet": "37  Improving language models by retrieving from trillions of tokens Table 16jGreat Circle (novel) , from Wikipedia September 21. The article is about a recent novel and chunks ùê∂3andùê∂4are speciÔ¨Åcally about its reception. The name Publishers"}, {"rank": 4, "score": 0.6380314, "source": "dense+rrf", "rrf_score": 0.031544957774465976, "rerank_score": 0.24565839220845176, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 25, "text_snippet": "irectly comparable with the largest language models that are evaluated by sampling. 2.3. Nearest neighbour retrieval Retrieval neighbours. Our database consists of a key-value memory. Each value consists of two contiguous chunks of tokens w"}, {"rank": 5, "score": 0.5639316, "source": "dense+rrf", "rrf_score": 0.02919863597612958, "rerank_score": 0.24518912784878447, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 113, "text_snippet": "formers ( R/e.sc/t.sc/r.sc/o.sc), a method for modelling arbitrary text se- quences whilstretrievingfromdatabaseswithtrillions oftokens‚Äîscalingthedataavailable to models by an order of magnitude compared to what is typically consumed during"}]}
{"case_index": 116, "query": "From 2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt, what exact term fills [BLANK] in: \"a new [BLANK] to evaluate language models when an evaluation set is partially present in the training set.\"?", "gold": "methodology", "answer": "The exact term is **‚Äúmethodology‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context explicitly contains the phrase 'a new methodology to evaluate language models...' and the model answer matches exactly.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 71.38, "llm_ms": 1333.873, "top_contexts": [{"rank": 1, "score": 0.64459735, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3532677987922538, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 18, "text_snippet": "c/a.scoperator. Causality is maintained as neighbours of the Ô¨Årst chunk only aÔ¨Äect the last token of the Ô¨Årst chunk and tokens from the second chunk. a new methodology to evaluate language models when an evaluation set is partially present "}, {"rank": 2, "score": 0.5983533, "source": "dense+rrf", "rrf_score": 0.0315136476426799, "rerank_score": 0.31351256040041303, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 52, "text_snippet": "384 32 128 32 7  Improving language models by retrieving from trillions of tokens whichcorrespondtothebits-per-bytesonthesetofchunksthatoverlaplessthan ùõº%withthetraining chunks. Note that the full evaluation bit-per-bytes performance is rec"}, {"rank": 3, "score": 32.09856538856308, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.3084540134455105, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 199, "text_snippet": " but only trains the new retrieval weights, as explained in ¬ß4.2. Our best R/e.sc/t.sc/r.sc/o.scÔ¨Åt checkpoint has a Wikitext103 valid perplexity 18¬ì46, when retrieving from Wikipedia. We use this R/e.sc/t.sc/r.sc/o.sccheckpoint in Table 4 f"}, {"rank": 4, "score": 31.177637628755534, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.2904454063197565, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 216, "text_snippet": "ly sound. We choose to have cross-attention every 3 layer as this provides a good trade-oÔ¨Ä between performance and run-time. 31  Improving language models by retrieving from trillions of tokens E. Qualitative experiments We illustrate the u"}, {"rank": 5, "score": 0.6038426, "source": "dense+rrf", "rrf_score": 0.03057889822595705, "rerank_score": 0.28670720377215825, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 113, "text_snippet": "formers ( R/e.sc/t.sc/r.sc/o.sc), a method for modelling arbitrary text se- quences whilstretrievingfromdatabaseswithtrillions oftokens‚Äîscalingthedataavailable to models by an order of magnitude compared to what is typically consumed during"}]}
{"case_index": 117, "query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"As [BLANK] in Figure 1, through this simple approach, LMs can learn to control a va- riety of tools, and to choose for themselves which tool to use when and how.\"?", "gold": "illustrated", "answer": "**illustrated**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'illustrated' and is directly supported by Context 1 ('As illustrated in Figure 1').", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 73.289, "llm_ms": 1161.666, "top_contexts": [{"rank": 1, "score": 0.6133203, "source": "dense+rrf", "rrf_score": 0.03177805800756621, "rerank_score": 0.43273232467255845, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 12, "text_snippet": " how an API can be used, we let a LM annotate a huge language modeling dataset with potential API calls. We then use a self-supervised loss to determine which of these API calls actually help the model in predicting future tokens. Finally, "}, {"rank": 2, "score": 0.61771244, "source": "dense+rrf", "rrf_score": 0.031754032258064516, "rerank_score": 0.30872238485898895, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 1, "text_snippet": "metic or fac- tual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer , a model traine"}, {"rank": 3, "score": 0.5646945, "source": "dense+rrf", "rrf_score": 0.028309409888357256, "rerank_score": 0.2758730669923549, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 13, "text_snippet": "h tool to use when and how. As our approach is agnostic of the dataset be- ing used, we can apply it to the exact same dataset that was used to pretrain a model in the Ô¨Årst place. This ensures that the model does not lose any of its general"}, {"rank": 4, "score": 0.610688, "source": "dense+rrf", "rrf_score": 0.031024531024531024, "rerank_score": 0.2708945472198188, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 93, "text_snippet": "mitations While our approach enables LMs to learn how to use a variety of tools in a self-supervised way, there are some clear limitations to what can be achieved with our method in its current form. One such limi- tation is the inability o"}, {"rank": 5, "score": 0.5905335, "source": "dense+rrf", "rrf_score": 0.02919863597612958, "rerank_score": 0.2636410818494171, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 11, "text_snippet": "when andhow to use which tool. In contrast to existing approaches, this enables a much more comprehensive use of tools that is not tied to speciÔ¨Åc tasks. Our approach for achieving these goals is based on the recent idea of using large LMs "}]}
{"case_index": 118, "query": "From 1706.03762__attention_is_all_you_need.txt, what exact term fills [BLANK] in: \"Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in [BLANK] or scholarly works.\"?", "gold": "journalistic", "answer": "The exact term that fills **[BLANK]** is:\n\n**journalistic**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'journalistic' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 60.066, "llm_ms": 1045.204, "top_contexts": [{"rank": 1, "score": 0.74384016, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.48338902559258556, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 0, "text_snippet": "Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works. Attention Is All You Need Ashish Vaswani‚àó Google Brain avaswani@go"}, {"rank": 2, "score": 19.075575104652188, "source": "dense+rrf", "rrf_score": 0.014285714285714285, "rerank_score": 0.21115940042645656, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 79, "text_snippet": "sponse or a span from the context provided. ‚Ä¢No extraction required: This category is for tasks that did not require extraction. Open-ended question answering falls into this category. In the categories requiring extraction (MCQ and Exact M"}, {"rank": 3, "score": 20.372628876929426, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.18617772162855087, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 118, "text_snippet": "ding theoretical results... (a) Did you state the full set of assumptions of all theoretical results? [N/A] (b) Did you include complete proofs of all theoretical results? [N/A] 3. If you ran experiments... (a)Did you include the code, data"}, {"rank": 4, "score": 19.266902058796365, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.1843848611211899, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 26, "text_snippet": " learning from zero examples. To avoid this confusion, we use the term ‚Äúmeta-learning‚Äù to capture the inner-loop / outer-loop structure of the general method, and the term ‚Äúin context-learning‚Äù to refer to the inner loop of meta-learning. W"}, {"rank": 5, "score": 0.42513794, "source": "dense+rrf", "rrf_score": 0.028693528693528692, "rerank_score": 0.18391286701365137, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 193, "text_snippet": " [ 80], the Long-range arena repo ( https: //github.com/google-research/long-range-arena ), and the Nystr√∂mformer reproduction [ 90]. To be generous to the baseline methods, if we are unable to reproduce the performance of any baseline for "}]}
{"case_index": 119, "query": "From 2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt, what exact term fills [BLANK] in: \"3.1 Experimental Setup We explore [BLANK] prompting for various language models on multiple benchmarks.\"?", "gold": "chain-of-thought", "answer": "The exact term is **‚Äúchain-of-thought‚Äù** (as in: ‚ÄúWe explore **chain-of-thought** prompting for various language models on multiple benchmarks.‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'chain-of-thought' and is directly supported by Context 1 sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 59.061, "llm_ms": 2179.804, "top_contexts": [{"rank": 1, "score": 0.7477056, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.335439943765711, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 20, "text_snippet": "21, inter alia ). Strikingly, chain- of-thought prompting when used with the 540B parameter language model performs comparably with task-speciÔ¨Åc Ô¨Ånetuned models on several tasks, even achieving new state of the art on the challenging GSM8K "}, {"rank": 2, "score": 34.26988245499278, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.2858730155049154, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 160, "text_snippet": "e 1, we see that adding a calculator signiÔ¨Åcantly boosts performance of chain-of-thought prompting on most tasks. Table 1: Chain of thought prompting outperforms standard prompting for various large language models on Ô¨Åve arithmetic reasoni"}, {"rank": 3, "score": 29.886632614983675, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.26263495809627646, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 30, "text_snippet": " output ‚ü©triples for arithmetic, commonsense, and symbolic reasoning benchmarks. Chains of thought are highlighted. Full prompts in Appendix G. math word problems, we used this single set of eight chain of thought exemplars for all benchmar"}, {"rank": 4, "score": 40.56216141072533, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.24052104476016975, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 114, "text_snippet": "-augmented ensembles in language models. arXiv preprint arXiv:2207.00747 , 2022b. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models"}, {"rank": 5, "score": 0.6995018, "source": "dense+rrf", "rrf_score": 0.027783137179239824, "rerank_score": 0.23569901214882683, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 68, "text_snippet": "s (though performance is lower than in the in-domain setting). Hence, chain-of-thought prompting facilitates length generalization beyond seen chains of thought for language models of sufÔ¨Åcient scale. 6 Discussion We have explored chain-of-"}]}
{"case_index": 120, "query": "From 2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"In practice, RAG systems are often evaluated in terms of the language [BLANK] task itself, i.e.\"?", "gold": "modelling", "answer": "The exact term is **‚Äúmodelling‚Äù** (as in ‚Äúthe language modelling task itself‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'modelling' and is directly supported by Context 1 phrase 'language modelling task itself'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 53.375, "llm_ms": 1206.094, "top_contexts": [{"rank": 1, "score": 0.6760676, "source": "dense+rrf", "rrf_score": 0.032266458495966696, "rerank_score": 0.3506655100791427, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 7, "text_snippet": "lementation requires a significant amount of tuning, as the overall per- formance will be affected by the retrieval model, the considered corpus, the LM, or the prompt for- mulation, among others. Automated evaluation of retrieval-augmented"}, {"rank": 2, "score": 0.65835136, "source": "dense+rrf", "rrf_score": 0.03149801587301587, "rerank_score": 0.21711811385248955, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 0, "text_snippet": "Ragas: Automated Evaluation of Retrieval Augmented Generation Shahul Es‚Ä†, Jithin James‚Ä†, Luis Espinosa-Anke‚àó‚ô¢, Steven Schockaert‚àó ‚Ä†Exploding Gradients ‚àóCardiffNLP, Cardiff University, United Kingdom ‚ô¢AMPLYFI, United Kingdom shahules786@gmai"}, {"rank": 3, "score": 0.647285, "source": "dense+rrf", "rrf_score": 0.03128054740957967, "rerank_score": 0.21367409862996498, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 237, "text_snippet": "-auto-eval-best-practices-RAG, 2023. [164] S. Es, J. James, L. Espinosa-Anke, and S. Schockaert, ‚ÄúRagas: Au- tomated evaluation of retrieval augmented generation,‚Äù arXiv preprint arXiv:2309.15217 , 2023. [165] J. Saad-Falcon, O. Khattab, C."}, {"rank": 4, "score": 23.811017355165006, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.18359545355748538, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 45, "text_snippet": "ially for longer contexts. 6 Conclusions We have highlighted the need for automated reference-free evaluation of RAG systems. In par- ticular, we have argued the need for an evaluation framework that can assess faithfulness (i.e. is the ans"}, {"rank": 5, "score": 0.69385433, "source": "dense+rrf", "rrf_score": 0.03131881575727918, "rerank_score": 0.1835155101616906, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 126, "text_snippet": "insof search engines, recommendation systems, and information retrieval systems are employed to measure the performance of the RAG retrieval module. Metrics such as Hit Rate, MRR, and NDCG are commonly utilized for this purpose [161], [162]"}]}
{"case_index": 121, "query": "From 2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt, what exact term fills [BLANK] in: \"LongRAG [BLANK] each entire document as a single unit rather than chunking them into smaller units.\"?", "gold": "processes", "answer": "The exact term is **‚Äúprocesses‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context explicitly states: 'LongRAG processes each entire document as a single unit...' so the blank is 'processes'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.3333333333333333, "hit_rank": 3, "matched_by": "doc_id+chunk_id", "retrieval_ms": 40.365, "llm_ms": 1182.898, "top_contexts": [{"rank": 1, "score": 0.65860415, "source": "dense+rrf", "rrf_score": 0.030679156908665108, "rerank_score": 0.29558672217638027, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 4, "text_snippet": "‚âà30K tokens) to an existing long-context LLM to perform zero-shot answer generation. Without requiring any training, LongRAG achieves an EM of 62.7% on NQ and 64.3% on HotpotQA, which are on par with the (fully-trained) SoTA model. Furtherm"}, {"rank": 2, "score": 0.75023854, "source": "dense+rrf", "rrf_score": 0.032266458495966696, "rerank_score": 0.2717479046704166, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 28, "text_snippet": " then fed into a long reader. Compared to traditional RAG, which retrieves hundreds of short units, our proposed LongRAG reduces the likelihood of retrieving hard negatives during the retrieval stage and more effectively leverages recent ad"}, {"rank": 3, "score": 58.03460110136649, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.260515487157966, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 17, "text_snippet": "anding ability of GPT-4o, LongRAG can achieve an EM of 62.7% on NQ and 64.3% on HotpotQA. These results could be comparable to the strongest fully trained RAG models like Atlas(Izacardetal.,2022)andMDR(Xiongetal.,2020b). Furthermore,wetesto"}, {"rank": 4, "score": 0.67969567, "source": "dense+rrf", "rrf_score": 0.030776515151515152, "rerank_score": 0.2324562557727053, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 5, "text_snippet": "ather than chunking them into smaller units. By doing so, we achieve an F1 score of 25.9% on Qasper (previously 22.5%) and 57.5% on MultiFieldQA-en (previously 51.2%). Our study offers insights into the future roadmap for combining RAG with"}, {"rank": 5, "score": 0.65528846, "source": "dense+rrf", "rrf_score": 0.02900988017658188, "rerank_score": 0.22864774950016295, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 21, "text_snippet": "operates on long retrieval units, with only a few (typically fewer than 10) being fed into the reader. An illustrative example is shown in Figure 2. 2.1 Long Retriever The traditional RAG framework employs smaller retrieval units and priori"}]}
{"case_index": 122, "query": "From 2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt, what exact term fills [BLANK] in: \"By leveraging the now standard BERT pretrained model (Devlin et al., 2019) and a [BLANK] architecture (Bromley et al., 1994), we focus on developing the right training scheme using a relatively small number of question and passage pairs.\"?", "gold": "dual-encoder", "answer": "The exact term is **‚Äúdual-encoder‚Äù** (as in ‚Äúa **dual-encoder architecture**‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'dual-encoder' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 121.42, "llm_ms": 2707.127, "top_contexts": [{"rank": 1, "score": 0.6826083, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.5051575681217724, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 10, "text_snippet": "stion: can we train a better dense embedding model using only pairs of questions and passages (or answers), with- outadditional pretraining? By leveraging the now standard BERT pretrained model (Devlin et al., 2019) and a dual-encoder archi"}, {"rank": 2, "score": 38.83023285859349, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.25709902488369407, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 12, "text_snippet": " vs. 33.3%) in the open Natural Questions setting (Lee et al., 2019; Kwiatkowski et al., 2019). Our contributions are twofold. First, we demon- strate that with the proper training setup, sim- ply Ô¨Åne-tuning the question and passage encoder"}, {"rank": 3, "score": 38.65978858101618, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.257001344786014, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 8, "text_snippet": "/BM25 for open- domain QA before ORQA (Lee et al., 2019), which proposes a sophisticated inverse cloze task (ICT) objective, predicting the blocks that contain the masked sentence, for additional pretraining. The question encoder and the re"}, {"rank": 4, "score": 0.610477, "source": "dense+rrf", "rrf_score": 0.03125, "rerank_score": 0.2564672968684854, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 81, "text_snippet": "the dual-encoder framework, they introduced a late-interaction operator on top of the BERT encoders. Dense retrieval for open-domain QA has been explored by Das et al. (2019), who propose to re- trieve relevant passages iteratively using re"}, {"rank": 5, "score": 40.16611980921091, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.24670486362501579, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 49, "text_snippet": "l accuracy with respect to different num- bers of training examples, measured on the devel- opment set of Natural Questions. As is shown, a dense passage retriever trained using only 1,000 ex- amples already outperforms BM25. This suggests "}]}
{"case_index": 123, "query": "From 2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt, what exact term fills [BLANK] in: \"These datasets consist of a variety of NLP tasks, combined with natural language [BLANK] for each task.\"?", "gold": "instructions", "answer": "The exact term is **‚Äúinstructions‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context explicitly says 'natural language instructions for each task'; model answer matches expected term.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 82.673, "llm_ms": 941.638, "top_contexts": [{"rank": 1, "score": 0.549089, "source": "dense+rrf", "rrf_score": 0.02921395544346364, "rerank_score": 0.35666599881149674, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 22, "text_snippet": "r language models are used. We compare GPT-3 Ô¨Åne-tuned on our human preference data (i.e. InstructGPT) to GPT-3 Ô¨Åne-tuned on two different compilations of public NLP tasks: the FLAN (Wei et al., 2021) and T0 (Sanh et al., 2021) (in particul"}, {"rank": 2, "score": 42.854706472072834, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.3189666934021365, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 72, "text_snippet": " SFT models and GPT-3. We also compare to GPT-3 when it is provided a few-shot preÔ¨Åx to ‚Äòprompt‚Äô it into an instruction-following mode (GPT-3-prompted). This preÔ¨Åx is prepended to the user-speciÔ¨Åed instruction.6 We additionally compare Inst"}, {"rank": 3, "score": 26.70457358210445, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.24673902136827466, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 31, "text_snippet": "alog behavioral issues in LMs that result from misalignment, including producing harmful content and gaming misspeciÔ¨Åed objectives. In concur- rent work, Askell et al. (2021) propose language assistants as a testbed for alignment research, "}, {"rank": 4, "score": 25.907080759629707, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.23456758612801748, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 33, "text_snippet": ". A consistent Ô¨Ånding across studies is that Ô¨Åne-tuning LMs on a range of NLP tasks, with instructions, improves their downstream performance on held-out tasks, both in the zero-shot and few-shot settings. There is also a related line of wo"}, {"rank": 5, "score": 26.343185846167742, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.2327947785124885, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 3, "text_snippet": "T-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT"}]}
{"case_index": 124, "query": "From 1706.03762__attention_is_all_you_need.txt, what exact term fills [BLANK] in: \"Additive attention computes the [BLANK] function using a feed-forward network with a single hidden layer.\"?", "gold": "compatibility", "answer": "The exact term is **‚Äúcompatibility‚Äù**, as in: ‚ÄúAdditive attention computes the **compatibility** function using a feed-forward network with a single hidden layer.‚Äù", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'compatibility' and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 158.15, "llm_ms": 1588.325, "top_contexts": [{"rank": 1, "score": 0.67288005, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3214807890844512, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 22, "text_snippet": "k)V (1) The two most commonly used attention functions are additive attention [ 2], and dot-product (multi- plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of1‚àödk. Additive attention "}, {"rank": 2, "score": 0.51801044, "source": "dense+rrf", "rrf_score": 0.03057889822595705, "rerank_score": 0.21399623980026145, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 30, "text_snippet": "rd Networks In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transf"}, {"rank": 3, "score": 29.00447466345155, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.20896360640106348, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 17, "text_snippet": "ted feed-forward network. We employ a residual connection [ 11] around each of the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is LayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function"}, {"rank": 4, "score": 26.34713730221014, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.2016416667941811, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 16, "text_snippet": "ransformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively. 3.1 Encoder and Decoder Stacks "}, {"rank": 5, "score": 25.28086935034975, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.1999040555989674, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 42, "text_snippet": "uous kernels, orO(logk(n))in the case of dilated convolutions [ 18], increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor o"}]}
{"case_index": 125, "query": "From 2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt, what exact term fills [BLANK] in: \"The reason behind this is that short attention resembles the attention scheme in the [BLANK] stage of LLMs.\"?", "gold": "pre-training", "answer": "The exact term is **‚Äúpre-training‚Äù** (i.e., ‚Äúthe pre-training stage of LLMs‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'pre-training' and is directly supported by context stating 'pre-training stage of LLMs'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 58.516, "llm_ms": 1118.814, "top_contexts": [{"rank": 1, "score": 0.60466516, "source": "dense+rrf", "rrf_score": 0.031054405392392875, "rerank_score": 0.29357936432131193, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 75, "text_snippet": "ed in training-from- scratch transformers. This experiment is to examine their capability of fine-tuning on pre-trained LLMs (Touvron et al., 2023b), toward long context adaptation. Dilated attention performs well in full fine-tuning but is"}, {"rank": 2, "score": 0.5796403, "source": "dense+rrf", "rrf_score": 0.029051670471052088, "rerank_score": 0.2743483687491452, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 15, "text_snippet": "s most existing optimization and infrastructure. Techniques for common LLMs can also be applied to ours. For example, Flash-Attention2 (Dao et al., 2022; Dao, 2023) is compatible with our method in both training and inference time. The reas"}, {"rank": 3, "score": 40.738327544195045, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.26093549618612777, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 0, "text_snippet": "Published as a conference paper at ICLR 2024 LONG LORA: E FFICIENT FINE-TUNING OF LONG - CONTEXT LARGE LANGUAGE MODELS Yukang Chen1Shengju Qian1Haotian Tang2Xin Lai1 Zhijian Liu2Song Han2,3Jiaya Jia1 1CUHK2MIT3NVIDIA ABSTRACT We present Lon"}, {"rank": 4, "score": 0.5909192, "source": "dense+rrf", "rrf_score": 0.02871794871794872, "rerank_score": 0.23555329362921257, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 12, "text_snippet": "e. In terms of efficiency , regardless of whether LoRA is employed or not, computational cost increases dramatically as the context size expands, primarily due to the standard self-attention mechanism (Vaswani et al., 2017). As shown in Fig"}, {"rank": 5, "score": 0.5977204, "source": "dense+rrf", "rrf_score": 0.029236022193768675, "rerank_score": 0.23045299508990244, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 41, "text_snippet": "eads respectively. This manner does not increase additional computation costs but enables the information flow between different groups. We show that it gets close to the standard attention baseline in Table 1. Consistency to Full Attention"}]}
{"case_index": 126, "query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \"Unlike left-to- right language model pre-training, the MLM ob- jective enables the [BLANK] to fuse the l\"?", "gold": "representation", "answer": "The exact term is **‚Äúrepresentation‚Äù** (as in: ‚Äúthe MLM objective enables the **representation** to fuse the left and the right context‚Ä¶‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'representation' and is directly supported by the provided context quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 144.812, "llm_ms": 1552.354, "top_contexts": [{"rank": 1, "score": 0.6745343, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.34503540139859973, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 8, "text_snippet": "T alleviates the previously mentioned unidi- rectionality constraint by using a ‚Äúmasked lan- guage model‚Äù (MLM) pre-training objective, in- spired by the Cloze task (Taylor, 1953). The masked language model randomly masks some of the tokens"}, {"rank": 2, "score": 0.7180941, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.3297940386243906, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 9, "text_snippet": "ng, the MLM ob- jective enables the representation to fuse the left and the right context, which allows us to pre- train a deep bidirectional Transformer. In addi- tion to the masked language model, we also use a ‚Äúnext sentence prediction‚Äù "}, {"rank": 3, "score": 0.5531995, "source": "dense+rrf", "rrf_score": 0.029206349206349208, "rerank_score": 0.27041640094772257, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 0, "text_snippet": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova Google AI Language {jacobdevlin,mingweichang,kentonl,kristout }@google.com Abstract We introduce a ne"}, {"rank": 4, "score": 0.58461726, "source": "dense+rrf", "rrf_score": 0.029850746268656716, "rerank_score": 0.2659436227015979, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 10, "text_snippet": "ford et al. (2018), which uses unidirec- tional language models for pre-training, BERT uses masked language models to enable pre- trained deep bidirectional representations. This is also in contrast to Peters et al. (2018a), which uses a sh"}, {"rank": 5, "score": 0.5592261, "source": "dense+rrf", "rrf_score": 0.028404512489927477, "rerank_score": 0.2531682759132716, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 138, "text_snippet": "fferent Masking Procedures In Section 3.1, we mention that BERT uses a mixed strategy for masking the target tokens when pre-training with the masked language model (MLM) objective. The following is an ablation study to evaluate the effect "}]}
{"case_index": 127, "query": "From 2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt, what exact term fills [BLANK] in: \"A prompting only approach is important because it does not require a large training dataset and because a single model checkpoint can perform many tasks without loss of [BLANK].\"?", "gold": "generality", "answer": "The exact term that fills **[BLANK]** is **‚Äúgenerality.‚Äù**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'generality' and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 59.751, "llm_ms": 1171.349, "top_contexts": [{"rank": 1, "score": 0.6998923, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.4459387081147427, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 12, "text_snippet": "ometimes to a striking degree. Figure 2 illustrates one such result‚Äîon the GSM8K benchmark of math word problems (Cobbe et al., 2021), chain-of-thought prompting with PaLM 540B outperforms standard prompting by a large margin and achieves n"}, {"rank": 2, "score": 52.84607390367345, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.34156224361947507, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 191, "text_snippet": " simply via prompting. This prompting setup is important because it allows for intermediate step reasoning without a large number of labeled annotations, and because a single model can perform a range of reasoning tasks without any gradient"}, {"rank": 3, "score": 37.33417127316251, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.26848046983749074, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 10, "text_snippet": " For the traditional few- shot prompting method used in Brown et al. (2020), it works poorly on tasks that require reasoning abilities, and often does not improve substantially with increasing language model scale (Rae et al., 2021). In thi"}, {"rank": 4, "score": 0.67434824, "source": "dense+rrf", "rrf_score": 0.03125763125763126, "rerank_score": 0.26650230364743926, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 71, "text_snippet": "revailing theme (Wei et al., 2022b). For many reasoning tasks where standard prompting has a Ô¨Çat scaling curve, chain- of-thought prompting leads to dramatically increasing scaling curves. Chain-of-thought prompting appears to expand the se"}, {"rank": 5, "score": 33.917969571281354, "source": "dense+rrf", "rrf_score": 0.014084507042253521, "rerank_score": 0.2623003007047687, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 72, "text_snippet": "vation likely raises more questions than it answers‚Äîfor instance, how much more can we expect reasoning ability to improve with a further increase in model scale? What other prompting methods might expand the range of tasks that language mo"}]}
{"case_index": 128, "query": "From 2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt, what exact term fills [BLANK] in: \"The idea of ReAct is simple: we augment the agent‚Äôs action space to ÀÜA=A‚à™L , [BLANK] the space of language.\"?", "gold": "wherelis", "answer": "The exact term is **‚Äúwhere \\(L\\) is‚Äù**.", "answer_score": 0.3, "answer_correctness": 0.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context shows the phrase 'whereLis the space of language' (i.e., 'where L is'). Model answered 'where L is' but expected exact term is 'wherelis' (no space/case), so it doesn't match the required exact fill.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 76.088, "llm_ms": 1122.589, "top_contexts": [{"rank": 1, "score": 0.7549431, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.38389068155843403, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 28, "text_snippet": " 1, thus keep producing hallucinating actions. The idea of ReAct is simple: we augment the agent‚Äôs action space to ÀÜA=A‚à™L , whereLis the space of language. An action ÀÜat‚ààL in the language space, which we will refer to as a thought or a reas"}, {"rank": 2, "score": 0.6009284, "source": "dense+rrf", "rrf_score": 0.03149801587301587, "rerank_score": 0.273627154787065, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 88, "text_snippet": " the development of versatile and generalist agents like Reed et al. (2022). 6 C ONCLUSION We have proposed ReAct ‚Äì a simple yet effective method for synergizing reasoning and acting in large language models. Through a diverse set of experi"}, {"rank": 3, "score": 0.5862602, "source": "dense+rrf", "rrf_score": 0.030776515151515152, "rerank_score": 0.2680406322154721, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 75, "text_snippet": "mbined reasoning and action using an LLM applied to an interactive environment within a closed-loop system. Perhaps the closest prior work is Inner Monologue (IM), from Huang et al. (2022b), in which actions from an embodied agent are motiv"}, {"rank": 4, "score": 0.6299753, "source": "dense+rrf", "rrf_score": 0.0315136476426799, "rerank_score": 0.24546219543403291, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 26, "text_snippet": "bining it with complementary paradigms like reinforcement learning could further unlock the potential of large language models. 2REAC T: SYNERGIZING REASONING +AC TING Consider a general setup of an agent interacting with an environment for"}, {"rank": 5, "score": 0.5909373, "source": "dense+rrf", "rrf_score": 0.030309988518943745, "rerank_score": 0.24139466524852207, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 24, "text_snippet": "o understand the decision basis of model actions. To summarize, our key contributions are the following: (1) we introduce ReAct , a novel prompt- based paradigm to synergize reasoning and acting in language models for general task solving; "}]}
{"case_index": 129, "query": "From 2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt, what exact term fills [BLANK] in: \"[BLANK] Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases.\"?", "gold": "retrieval-augmented", "answer": "**Retrieval-Augmented**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Blank is 'retrieval-augmented' as in 'Retrieval-Augmented Generation (RAG)' in the provided context; model answer matches (case-insensitive).", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 46.255, "llm_ms": 1061.356, "top_contexts": [{"rank": 1, "score": 0.7298943, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.3475720015275476, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 1, "text_snippet": "dels (LLMs) showcase impres- sive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution b"}, {"rank": 2, "score": 0.7186844, "source": "dense+rrf", "rrf_score": 0.031746031746031744, "rerank_score": 0.2210472223978784, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 5, "text_snippet": "Generation (RAG) enhances LLMs by retrieving relevant document chunks from external knowledge base through semantic similarity calcu- lation. By referencing external knowledge, RAG effectively reduces the problem of generating factually inc"}, {"rank": 3, "score": 26.970126510987633, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.20865199264394457, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 238, "text_snippet": " maximizing llm performance,‚Äù https://community.openai. com/t/openai-dev-day-2023-breakout-sessions/505213# a-survey-of-techniques-for-maximizing-llm-performance-2, 2023. [167] J. Chen, H. Lin, X. Han, and L. Sun, ‚ÄúBenchmarking large lan- g"}, {"rank": 4, "score": 28.099888558184254, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.20549615381734232, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 239, "text_snippet": " against external counterfactual knowledge,‚Äù arXiv preprint arXiv:2311.08147 , 2023. [169] Y . Lyu, Z. Li, S. Niu, F. Xiong, B. Tang, W. Wang, H. Wu, H. Liu, T. Xu, and E. Chen, ‚ÄúCrud-rag: A comprehensive chinese benchmark for retrieval-aug"}, {"rank": 5, "score": 0.7591044, "source": "dense+rrf", "rrf_score": 0.030886196246139225, "rerank_score": 0.1979625598433769, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 4, "text_snippet": "l, retrieval-augmented gen- eration, natural language processing, information retrieval I. I NTRODUCTION LARGE language models (LLMs) have achieved remark- able success, though they still face significant limitations, especially in domain-s"}]}
{"case_index": 130, "query": "From 2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt, what exact term fills [BLANK] in: \"The system [BLANK] and retrieves the top K chunks that demonstrate the greatest similarity to the query.\"?", "gold": "prioritizes", "answer": "The exact term is **‚Äúprioritizes‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term and is directly supported by Context 1 ('The system prioritizes and retrieves...').", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 56.594, "llm_ms": 1210.288, "top_contexts": [{"rank": 1, "score": 0.7085875, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.34080898576786584, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 22, "text_snippet": "cores between the query vector and the vector of chunks within the indexed corpus. The system prioritizes and retrieves the top K chunks that demonstrate the greatest similarity to the query. These chunks are subsequently used as the expand"}, {"rank": 2, "score": 24.238615870619107, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.24683371833797163, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 89, "text_snippet": "uage models make increasingly coarse use of longer-term context; Sankar et al. (2019) found similar results in di- alogue models. In a similar vein, Daniluk et al. (2017) find that attentive LSTM language mod- els tend to mainly use recent "}, {"rank": 3, "score": 0.5701987, "source": "dense+rrf", "rrf_score": 0.026500526500526502, "rerank_score": 0.24365193391254983, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 79, "text_snippet": "on the long-context understanding ability of LLMs, and we still have a long way to go in harnessing open-source LLMs within our framework. 10  4 Related Work 4.1 Retrieval-Augmented Generation. Augmenting language models with information re"}, {"rank": 4, "score": 24.114609892926666, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.23401703075276784, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 84, "text_snippet": "HyDE [11] construct hypothet- ical documents (assumed answers to the original query). It focuses on embedding similarity from answer to answer rather than seeking embedding similarity for the problem or query. Using the Step-back Prompting "}, {"rank": 5, "score": 0.58238167, "source": "dense+rrf", "rrf_score": 0.03128054740957967, "rerank_score": 0.21993899920050886, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 21, "text_snippet": "to smaller, digestible chunks. Chunks are then encoded into vector representations using an embedding model and stored in vector database. This step is crucial for enabling efficient similarity searches in the subsequent retrieval phase. Re"}]}
{"case_index": 131, "query": "From 2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"An- other approach directly asks ChatGPT to evaluate a [BLANK] aspect of the given answer by provid- ing a score between 0 and 100, or by providing a rating on a 5-star scale (Wang et al., 2023a).\"?", "gold": "particular", "answer": "The exact term that fills **[BLANK]** is **‚Äúparticular‚Äù** (i.e., ‚Äúevaluate a **particular** aspect of the given answer‚Ä¶‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'particular' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 81.78, "llm_ms": 1275.667, "top_contexts": [{"rank": 1, "score": 0.64926386, "source": "dense+rrf", "rrf_score": 0.03177805800756621, "rerank_score": 0.46518707934262926, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 16, "text_snippet": "ssive LM. This idea of using prompts was previously also considered by Yuan et al. (2021), although they used a smaller fine-tuned LM (i.e. BART) and did not observe a clear benefit from using prompts. An- other approach directly asks ChatG"}, {"rank": 2, "score": 0.6838697, "source": "dense+rrf", "rrf_score": 0.032266458495966696, "rerank_score": 0.2207390059849038, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 40, "text_snippet": " the human annotators. We report the results in terms of ac- curacy (i.e. the fraction of instances on which the model agrees with the annotators). To put the results in context, we compare our proposed metrics (shown as Ragas in Table 1) w"}, {"rank": 3, "score": 0.6674082, "source": "dense+rrf", "rrf_score": 0.03128054740957967, "rerank_score": 0.21568944266414983, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 15, "text_snippet": "his is less likely to be the case for hallucinated answers. Automated evaluation of text generation systems LLMs have also been leveraged to automatically evaluate other aspects of generated text fragments, beyond factuality. For instance, "}, {"rank": 4, "score": 27.314426057770184, "source": "dense+rrf", "rrf_score": 0.014084507042253521, "rerank_score": 0.21506179936763004, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 41, "text_snippet": "of the quality metric and then asks to score the given answer/context in line with that definition. For instance, for evaluating faithfulness, we used the following prompt: Faithfulness measures the information consistency of the answer aga"}, {"rank": 5, "score": 26.142908109126065, "source": "dense+rrf", "rrf_score": 0.013888888888888888, "rerank_score": 0.204358830049362, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 141, "text_snippet": " open-domain question answering containing 3610 questions. For TriviaQA we evaluate on the dev set of the Ô¨Åltered set. This differs from GPT-3 and PaLM, which evaluate on the test set of the unÔ¨Åltered set for which the online evaluation ser"}]}
{"case_index": 132, "query": "From 2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt, what exact term fills [BLANK] in: \"[BLANK] of phi-3 went through two stages, including supervised finetuning (SFT) and direct preference optimization (DPO).\"?", "gold": "post-training", "answer": "**Post-training**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'post-training' and is explicitly supported by Context 1 ('Post-training of phi-3 went through two stages...').", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 56.943, "llm_ms": 1160.329, "top_contexts": [{"rank": 1, "score": 0.7493077, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3213847251564485, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 24, "text_snippet": "at were trained on the same fixed data. We plot the log of MMLU error versus the log of model size. 4  Post-training. Post-training of phi-3 went through two stages, including supervised finetuning (SFT) and direct preference optimization ("}, {"rank": 2, "score": 0.64476764, "source": "dense+rrf", "rrf_score": 0.031746031746031744, "rerank_score": 0.24214245391675954, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 0, "text_snippet": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone Microsoft Abstract We introduce phi-3-mini , a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both"}, {"rank": 3, "score": 0.57431555, "source": "dense+rrf", "rrf_score": 0.030017921146953404, "rerank_score": 0.23798163020316723, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 55, "text_snippet": "um image resolution is capped at 1344 √ó1344 as the majority of the training images are smaller than this resolution. Post-training. ThePhi-3.5-Vision model contains two post-training stages: supervised finetuning (SFT) and direct preference"}, {"rank": 4, "score": 33.23758772433009, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.2201234741982852, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 58, "text_snippet": " 33B tokens. For DPO we mainly use a text DPO dataset and a relatively smaller-scale multimodal DPO dataset. For these two stages, we jointly train multimodal tasks and text-only tasks so that the model can achieve multi-modal reasoning whi"}, {"rank": 5, "score": 40.66629267878094, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.21358061121424748, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 68, "text_snippet": "n in Table 6, our Phi-3.5-Vision performs very competitively or outperforms baseline models under the similar model size in multi-image understanding scenarios as well. 7.3 Safety To ensure the integration of Phi-3.5-Vision aligns with Micr"}]}
{"case_index": 133, "query": "From 2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt, what exact term fills [BLANK] in: \"On the other hand, recent work has explored the use of pre-trained language models for planning and acting in interactive [BLANK] (Ahn et al., 2022; Nakano et al., 2021; Yao et al., 2020; Huang et al., 2022a), with a focus on predicting actions via language priors.\"?", "gold": "environments", "answer": "The exact term is **‚Äúenvironments‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'environments' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 120.613, "llm_ms": 1299.775, "top_contexts": [{"rank": 1, "score": 0.7377152, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.5371218211108395, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 17, "text_snippet": " fact hallucination and error propagation over the reasoning process (Figure 1 (1b)). On the other hand, recent work has explored the use of pre-trained language models for planning and acting in interactive environments (Ahn et al., 2022; "}, {"rank": 2, "score": 39.394836509166566, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.27236286956686523, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 125, "text_snippet": "ng answers due to hallucination, Act fails despite the access of real-world web interaction, due to a lack of reasoning to guide how to interact with the Internet for QA. Only ReAct is able to retrieve up-to-date information from the Intern"}, {"rank": 3, "score": 0.6219501, "source": "dense+rrf", "rrf_score": 0.028283227848101264, "rerank_score": 0.26904679857443003, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 87, "text_snippet": " leveraging language as semantically-rich inputs in the process of interactive decision making has been shown to be successful under other settings (Abramson et al., 2020; Karamcheti et al., 2021; Huang et al., 2022a; Li et al., 2022). It i"}, {"rank": 4, "score": 39.545708835227124, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.2582340913747188, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 88, "text_snippet": " the development of versatile and generalist agents like Reed et al. (2022). 6 C ONCLUSION We have proposed ReAct ‚Äì a simple yet effective method for synergizing reasoning and acting in large language models. Through a diverse set of experi"}, {"rank": 5, "score": 34.21471796245112, "source": "dense+rrf", "rrf_score": 0.013888888888888888, "rerank_score": 0.24755476982184432, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 179, "text_snippet": "ing The recent success of large-scale language models has led to growing interest in improving their capability to perform tasks via prompting (Brown et al. (2020), and see Liu et al. (2021) for a survey). This paper falls in the category o"}]}
{"case_index": 134, "query": "From 2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"2 Related Work Estimating faithfulness using LLMs The prob- lem of detecting [BLANK] in LLM generated responses has been extensively studied (Ji et al., 2023).\"?", "gold": "hallucinations", "answer": "The exact term that fills **[BLANK]** is **‚Äúhallucinations‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'hallucinations' and is explicitly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 57.149, "llm_ms": 1182.288, "top_contexts": [{"rank": 1, "score": 0.6683495, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3839830550973718, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 10, "text_snippet": "on with both llama-index and Langchain, the most widely used frameworks for building RAG solutions, thus enabling devel- opers to easily integrate Ragas into their standard workflow. 2 Related Work Estimating faithfulness using LLMs The pro"}, {"rank": 2, "score": 35.493136477582446, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.20859563957983696, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 75, "text_snippet": "ted LMs, which is the most rele- vant area to this paper. Iterative and adaptive retrieval Iterative re- trieval and refinement has been studied in both text and code generation tasks (Peng et al., 2023; Zhang et al., 2023; Zemlyanskiy et a"}, {"rank": 3, "score": 29.867240118260774, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.2034675843751584, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 79, "text_snippet": "on the long-context understanding ability of LLMs, and we still have a long way to go in harnessing open-source LLMs within our framework. 10  4 Related Work 4.1 Retrieval-Augmented Generation. Augmenting language models with information re"}, {"rank": 4, "score": 0.6578003, "source": "dense+rrf", "rrf_score": 0.03128054740957967, "rerank_score": 0.19405867821312964, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 15, "text_snippet": "his is less likely to be the case for hallucinated answers. Automated evaluation of text generation systems LLMs have also been leveraged to automatically evaluate other aspects of generated text fragments, beyond factuality. For instance, "}, {"rank": 5, "score": 0.57124245, "source": "dense+rrf", "rrf_score": 0.028629032258064516, "rerank_score": 0.1876434171824482, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 237, "text_snippet": "-auto-eval-best-practices-RAG, 2023. [164] S. Es, J. James, L. Espinosa-Anke, and S. Schockaert, ‚ÄúRagas: Au- tomated evaluation of retrieval augmented generation,‚Äù arXiv preprint arXiv:2309.15217 , 2023. [165] J. Saad-Falcon, O. Khattab, C."}]}
{"case_index": 135, "query": "From 2305.06983__active_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"3.1 FLARE with Retrieval Instructions Inspired by Toolformer (Schick et al., 2023), a [BLANK] way of expressing information needs for retrieval is to generate ‚Äú[Se\"?", "gold": "straightforward", "answer": "The exact term is **‚Äústraightforward‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context explicitly states: 'Inspired by Toolformer..., a straightforward way...' so the blank is 'straightforward'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 109.492, "llm_ms": 1119.282, "top_contexts": [{"rank": 1, "score": 0.7623826, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.4208038877166385, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 26, "text_snippet": "eval-encouraging instructions, de- noted as FLARE instruct . The second method directly uses the LM‚Äôs generation as search queries, denoted as FLARE direct, which iteratively generates the next sentence to gain insight into the future topic"}, {"rank": 2, "score": 39.65268541415533, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.3008197616357834, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 27, "text_snippet": "f expressing information needs for retrieval is to generate ‚Äú[Search(query)]‚Äù when additional information is needed (Schick et al., 2023), e.g., ‚ÄúThe colors on the flag of Ghana have the following meanings. Red is for [Search(Ghana flag red"}, {"rank": 3, "score": 37.712549713595706, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.2662129828631694, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 28, "text_snippet": "University)][Search(Joe Biden degree)]the University of Pennsylvania, where he earneda law degree.Generate a summary about Joe Biden.Input$&$&#%$&%%%GenerationRetriever$%$%%Figure 2: An illustration of forward-looking active re- trieval aug"}, {"rank": 4, "score": 0.67780524, "source": "dense+rrf", "rrf_score": 0.03057889822595705, "rerank_score": 0.24535144649385665, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 17, "text_snippet": "ng Active REtrieval augmented generation (FLARE ), as illustrated in Figure 1. FLARE iter- atively generates a temporary next sentence , use it as the query to retrieve relevant documents if it contains low-probability tokens and regenerate"}, {"rank": 5, "score": 32.525445437557785, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.23318888525779854, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 22, "text_snippet": " generate the complete answer at once y=LM([Dx,x]). 2.3 Active Retrieval Augmented Generation To aid long-form generation with retrieval, we pro- pose active retrieval augmented generation. It is a generic framework that actively decides wh"}]}
{"case_index": 136, "query": "From 2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt, what exact term fills [BLANK] in: \"relevant [BLANK] from a search engine, database query results, etc; Petroni et al., 2020; Ram et al., 2023; Shi et al., 2023; Mallen et al., 2023; Schick et al., 2023, inter alia ).\"?", "gold": "documents", "answer": "The exact term is **‚Äúdocuments‚Äù** (i.e., ‚Äúrelevant **documents** from a search engine, database query results, etc; ‚Ä¶‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'documents' and is directly supported by Context 1 phrase 'relevant documents from a search engine...'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 64.687, "llm_ms": 1457.293, "top_contexts": [{"rank": 1, "score": 0.70311046, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.4378678315178424, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 6, "text_snippet": "e information located in the middle of its input context. relevant documents from a search engine, database query results, etc; Petroni et al., 2020; Ram et al., 2023; Shi et al., 2023; Mallen et al., 2023; Schick et al., 2023, inter alia )"}, {"rank": 2, "score": 0.6154094, "source": "dense+rrf", "rrf_score": 0.031754032258064516, "rerank_score": 0.2478038974272682, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 207, "text_snippet": "‚Äù arXiv preprint arXiv:2310.07554 , 2023. [98] N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni, and P. Liang, ‚ÄúLost in the middle: How language models use long contexts,‚Äù arXiv preprint arXiv:2307.03172 , 2023. [99] Y "}, {"rank": 3, "score": 45.33631375582564, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.23259033704516058, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 56, "text_snippet": "ference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual . Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2023. Halueval: A large- scale hallucination evaluation benchmark fo"}, {"rank": 4, "score": 0.60502875, "source": "dense+rrf", "rrf_score": 0.029850746268656716, "rerank_score": 0.21407654193951564, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 49, "text_snippet": "UIDs. The relevant key-value pair for answering the query is bolded here within the input context for clarity. indicate that extended-context models are not nec- essarily better than their non-extended counterparts at using their input cont"}, {"rank": 5, "score": 0.6006082, "source": "dense+rrf", "rrf_score": 0.028577260665441927, "rerank_score": 0.2133834487291109, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 89, "text_snippet": "uage models make increasingly coarse use of longer-term context; Sankar et al. (2019) found similar results in di- alogue models. In a similar vein, Daniluk et al. (2017) find that attentive LSTM language mod- els tend to mainly use recent "}]}
{"case_index": 137, "query": "From 2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt, what exact term fills [BLANK] in: \"[BLANK]:2004.04906v3 [cs.CL] 30 Sep 2020 QA datasets, it also suffers from two weaknesses.\"?", "gold": "open-domainarxiv", "answer": "**open-domainarXiv**", "answer_score": 0.3, "answer_correctness": 0.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context shows the term as \"open-domainarXiv\" (capital X), but expected answer is \"open-domainarxiv\" (lowercase x). Answer is grounded but does not match the exact expected term.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 73.479, "llm_ms": 1164.492, "top_contexts": [{"rank": 1, "score": 0.64392984, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.33761857686963753, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 9, "text_snippet": "open-domainarXiv:2004.04906v3 [cs.CL] 30 Sep 2020  QA datasets, it also suffers from two weaknesses. First, ICT pretraining is computationally intensive and it is not completely clear that regular sentences are good surrogates of questions "}, {"rank": 2, "score": 0.6051254, "source": "dense+rrf", "rrf_score": 0.031754032258064516, "rerank_score": 0.26965816587629676, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 8, "text_snippet": "/BM25 for open- domain QA before ORQA (Lee et al., 2019), which proposes a sophisticated inverse cloze task (ICT) objective, predicting the blocks that contain the masked sentence, for additional pretraining. The question encoder and the re"}, {"rank": 3, "score": 35.83143419238645, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.2035604457582334, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 209, "text_snippet": "e]. Available: https://aclanthology.org/2022.amta-upg.14 [101] H. Jiang, Q. Wu, X. Luo, D. Li, C.-Y . Lin, Y . Yang, and L. Qiu, ‚ÄúLongllmlingua: Accelerating and enhancing llms in long context scenarios via prompt compression,‚Äù arXiv prepri"}, {"rank": 4, "score": 0.60170734, "source": "dense+rrf", "rrf_score": 0.03007688828584351, "rerank_score": 0.19228223059077754, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 99, "text_snippet": "las Joseph, Ben Mann, Sam McCandlish, Chris Olah, and Jared Kaplan. 2022. Language models (mostly) know what they know. CoRR , abs/2207.05221. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H. Lewis, Ledell Wu, Sergey Edunov, Danqi "}, {"rank": 5, "score": 36.80293439954524, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.1881612563817463, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 100, "text_snippet": " Huiyuan Chen, and Xia Hu. Llm maybe longlm: Self-extend llm context window without tuning. arXiv preprint arXiv:2401.01325 , 2024. Jeff Johnson, Matthijs Douze, and Herv√© J√©gou. Billion-scale similarity search with gpus. IEEE Transactions "}]}
{"case_index": 138, "query": "From 2005.14165__language_models_are_few_shot_learners.txt, what exact term fills [BLANK] in: \"This last paradigm has led to substantial progress on many challenging NLP tasks such as reading [BLANK], question answering, textual entailment, and many others, and has continued to advance based on new architectures and algorithms [ RSR+19,LOG+19,YDY+19,LCG+19].\"?", "gold": "comprehension", "answer": "The exact term is **‚Äúcomprehension‚Äù** (i.e., ‚Äúreading comprehension‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'comprehension' and is directly supported by Context 1 phrase 'reading comprehension'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 62.122, "llm_ms": 1119.772, "top_contexts": [{"rank": 1, "score": 0.7540955, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.49755555560542125, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 12, "text_snippet": "tuned, entirely removing the need for task-speciÔ¨Åc architectures [RNSS18, DCLT18, HR18]. This last paradigm has led to substantial progress on many challenging NLP tasks such as reading comprehension, question answering, textual entailment,"}, {"rank": 2, "score": 0.6154657, "source": "dense+rrf", "rrf_score": 0.02878726010616578, "rerank_score": 0.23798625567872014, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 313, "text_snippet": " over the last two years has been enormous, including denoising-based bidirectionality [ DCLT18 ], preÔ¨ÅxLM [ DL15 ] and encoder-decoder architectures [ LLG+19,RSR+19], random permu- tations during training [ YDY+19], architectures that impr"}, {"rank": 3, "score": 30.649699450754657, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.22067359805131095, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 26, "text_snippet": " learning from zero examples. To avoid this confusion, we use the term ‚Äúmeta-learning‚Äù to capture the inner-loop / outer-loop structure of the general method, and the term ‚Äúin context-learning‚Äù to refer to the inner loop of meta-learning. W"}, {"rank": 4, "score": 0.64578485, "source": "dense+rrf", "rrf_score": 0.03149801587301587, "rerank_score": 0.21893057675609964, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 304, "text_snippet": "As Ô¨Åne-tuned language models have neared human performance on many standard benchmark tasks, considerable effort has been devoted to constructing more difÔ¨Åcult or open-ended tasks, including question answering [ KPR+19, IBGC+14,CCE+18,MCKS1"}, {"rank": 5, "score": 31.158020999846862, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.20303030303029945, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 180, "text_snippet": "vide the exact prompts used for common NLP tasks in the following tables as well, including NLI (Table 18, Table 19, Table 20) and Closed-Book Question-Answering tasks (Table 16, Table 21). 20  Published as a conference paper at ICLR 2023 T"}]}
{"case_index": 139, "query": "From 2305.06983__active_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"Therefore, we propose anticipating the future by generating a temporary next sentence, using it as a query to retrieve rel- evant documents, and then [BLANK] the next sentence conditioning on the retrieved documents.\"?", "gold": "regenerating", "answer": "The exact term is **‚Äúregenerating‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'regenerating' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 94.233, "llm_ms": 963.525, "top_contexts": [{"rank": 1, "score": 0.7577306, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.562553847785897, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 16, "text_snippet": ". When deciding what to retrieve , it is important to consider what LMs intend to gen- erate in the future, as the goal of active retrieval is to benefit future generations. Therefore, we propose anticipating the future by generating a temp"}, {"rank": 2, "score": 0.6665324, "source": "dense+rrf", "rrf_score": 0.03128054740957967, "rerank_score": 0.33077642138213703, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 32, "text_snippet": "iable. There- fore, we propose a more direct way of forward- looking active retrieval that uses the next sentence to decide when and what to retrieve. 3.2.1 Confidence-based Active Retrieval As shown in Figure 1, at step t, we first generat"}, {"rank": 3, "score": 0.69278735, "source": "dense+rrf", "rrf_score": 0.03149801587301587, "rerank_score": 0.3130855987135454, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 17, "text_snippet": "ng Active REtrieval augmented generation (FLARE ), as illustrated in Figure 1. FLARE iter- atively generates a temporary next sentence , use it as the query to retrieve relevant documents if it contains low-probability tokens and regenerate"}, {"rank": 4, "score": 0.6936293, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.30797522359623053, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 2, "text_snippet": "is work, we provide a generalized view of ac- tive retrieval augmented generation , methods that actively decide when and what to retrieve across the course of the generation. We propose Forward- Looking Active REtrieval augmented generatio"}, {"rank": 5, "score": 0.6003692, "source": "dense+rrf", "rrf_score": 0.027984344422700584, "rerank_score": 0.24276769339716386, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 22, "text_snippet": " generate the complete answer at once y=LM([Dx,x]). 2.3 Active Retrieval Augmented Generation To aid long-form generation with retrieval, we pro- pose active retrieval augmented generation. It is a generic framework that actively decides wh"}]}
{"case_index": 140, "query": "From 2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt, what exact term fills [BLANK] in: \"In our experiments, we adopt [BLANK] retrievers like BGE (Xiao et al., 2023) and readers like Gemini- 1.5-Pro (Reid et al., 2024) or GPT-4o (OpenAI, 2024) without any further tuning.\"?", "gold": "off-the-shelf", "answer": "The exact term is **‚Äúoff-the-shelf‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 81.094, "llm_ms": 1568.059, "top_contexts": [{"rank": 1, "score": 0.6821468, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.42131964809044353, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 15, "text_snippet": "nd MultiFieldQA-en (Bai et al., 2023). In our experiments, we adopt off-the-shelf retrievers like BGE (Xiao et al., 2023) and readers like Gemini- 1.5-Pro (Reid et al., 2024) or GPT-4o (OpenAI, 2024) without any further tuning. To demonstra"}, {"rank": 2, "score": 0.7012141, "source": "dense+rrf", "rrf_score": 0.032266458495966696, "rerank_score": 0.2739468337353577, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 34, "text_snippet": "ader The long reader operates straightforwardly. We feed the related instruction i, the question q, and the long retrieval resultCFinto an LLM, enabling it to reason over the long context and generate the final output. It‚Äôs important that t"}, {"rank": 3, "score": 57.517197307175785, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.25344310332009834, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 59, "text_snippet": "n. 3.3 Full QA Performance on Wikipedia-based Datasets We leverage Gemini-1.5-Pro and GPT-4o as the reader in our LongRAG framework. The prompt we use for our experiments are in Table 7. For Wiki-based datasets, such as NQ and HotpotQA, whi"}, {"rank": 4, "score": 45.073430548405874, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.24498731560588083, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 64, "text_snippet": " More details can be found in Section A.2. For NQ and HotpotQA, we compare our model with several groups of strong previous models as baselines. The first group is ‚Äú Closed-Book ‚Äù: These baselines mean that no retrieval component is used; i"}, {"rank": 5, "score": 0.6235461, "source": "dense+rrf", "rrf_score": 0.030090497737556562, "rerank_score": 0.23488163230019574, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 78, "text_snippet": "4o results in our main table. Besides, Gemini-1.5-pro, GPT-4-Turbo, Claude-3-Opus, and Claude-3.5-Sonnet could achieve very similar results. These state-of-the-art black box LLMs are also effective readers within the LongRAG framework. Deep"}]}
{"case_index": 141, "query": "From 2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt, what exact term fills [BLANK] in: \"We use a chunked [BLANK] module to incorporate the retrieved text (¬ß2.4), with time complexity linear in the amount of retrieved data.\"?", "gold": "cross-attention", "answer": "cross-attention", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "The blank is explicitly 'cross-attention' in Context 1, matching the expected answer.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 111.464, "llm_ms": 900.716, "top_contexts": [{"rank": 1, "score": 0.6151769, "source": "dense+rrf", "rrf_score": 0.031099324975891997, "rerank_score": 0.3700550864568491, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 11, "text_snippet": ".sc/t.sc/r.sc/o.sc, a retrieval-enhanced autoregressive language model (¬ß2.2). We use a chunked cross-attention module to incorporate the retrieved text (¬ß2.4), with time complexity linear in the amount of retrieved data. We show that retri"}, {"rank": 2, "score": 0.63924026, "source": "dense+rrf", "rrf_score": 0.031754032258064516, "rerank_score": 0.30657814071968287, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 22, "text_snippet": " ùïç=¬ª1¬îùë£¬º, obtained using a text tokenizer1. Wespliteach ùëõ-token-longexample ùëã=¬πùë•1¬î¬ì¬ì¬ì¬îùë•ùëõ¬∫intoasequenceof ùëôchunks¬πùê∂1¬î¬ì¬ì¬ì¬îùê∂ùëô¬∫ of sizeùëö=ùëõ ùëô, i.e.ùê∂1,¬πùë•1¬î¬ì¬ì¬ì¬îùë•ùëö¬∫¬î ¬ì¬ì¬ì¬î ùê∂ùëô,¬πùë•ùëõ\u0000ùëö¬∏1¬î¬ì¬ì¬ì¬îùë•ùëõ¬∫2ùïçùëö. We useùëõ=2048andùëö=64. We augment each chunk ùê∂ùë¢with a se"}, {"rank": 3, "score": 0.59953797, "source": "dense+rrf", "rrf_score": 0.02797339593114241, "rerank_score": 0.29067333722614147, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 38, "text_snippet": "t token of chunk ùê∂ùë¢is the Ô¨Årst to be able to access the retrieved content ùê∏ùë¢while maintaining autoregressivity in(1). Hence, there is a one token overlap between chunk ùê∂ùë¢=\u0010 ùë•¬πùë¢\u00001¬∫ùëö¬∏ùëñ\u0011 ùëñ2¬ª1¬îùëö¬ºand the corresponding attending chunk ùê∂¬∏ ùë¢,¬πùë•ùë¢ùëö¬∏ùëñ"}, {"rank": 4, "score": 0.6245494, "source": "dense+rrf", "rrf_score": 0.028484848484848488, "rerank_score": 0.27295821160938816, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 183, "text_snippet": "sc¬πùê∂ùë¢¬∫ùëóand the chunk ùê∂ùë¢to be relatively well aligned, and assume that they start at the same position. Therefore, when computing C/a.sc¬πùêª¬∏ ùë¢¬îùê∏ùë¢¬∫, we set the distance between the data token ùëñ2¬ª1¬îùëô¬ºof chunkùê∂¬∏ ùë¢and 5https://github.com/earwig/m"}, {"rank": 5, "score": 31.38707774535947, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.2697435897435842, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 216, "text_snippet": "ly sound. We choose to have cross-attention every 3 layer as this provides a good trade-oÔ¨Ä between performance and run-time. 31  Improving language models by retrieving from trillions of tokens E. Qualitative experiments We illustrate the u"}]}
{"case_index": 142, "query": "From 2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt, what exact term fills [BLANK] in: \"Generation (RAG) enhances LLMs by retrieving relevant document chunks from external knowledge base through semantic [BLANK] calcu- lation.\"?", "gold": "similarity", "answer": "The exact term is **‚Äúsimilarity‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Blank is 'similarity', explicitly stated in Context 1; model answer matches expected.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 53.354, "llm_ms": 1272.893, "top_contexts": [{"rank": 1, "score": 0.7201067, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3732367758604564, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 5, "text_snippet": "Generation (RAG) enhances LLMs by retrieving relevant document chunks from external knowledge base through semantic similarity calcu- lation. By referencing external knowledge, RAG effectively reduces the problem of generating factually inc"}, {"rank": 2, "score": 0.71484584, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.2766780223698467, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 4, "text_snippet": "l, retrieval-augmented gen- eration, natural language processing, information retrieval I. I NTRODUCTION LARGE language models (LLMs) have achieved remark- able success, though they still face significant limitations, especially in domain-s"}, {"rank": 3, "score": 0.66711116, "source": "dense+rrf", "rrf_score": 0.03125763125763126, "rerank_score": 0.2324000930186628, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 1, "text_snippet": "dels (LLMs) showcase impres- sive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution b"}, {"rank": 4, "score": 30.395923384430354, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.2138751031206267, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 238, "text_snippet": " maximizing llm performance,‚Äù https://community.openai. com/t/openai-dev-day-2023-breakout-sessions/505213# a-survey-of-techniques-for-maximizing-llm-performance-2, 2023. [167] J. Chen, H. Lin, X. Han, and L. Sun, ‚ÄúBenchmarking large lan- g"}, {"rank": 5, "score": 0.6412525, "source": "dense+rrf", "rrf_score": 0.013888888888888888, "rerank_score": 0.20097941114145382, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 79, "text_snippet": "on the long-context understanding ability of LLMs, and we still have a long way to go in harnessing open-source LLMs within our framework. 10  4 Related Work 4.1 Retrieval-Augmented Generation. Augmenting language models with information re"}]}
{"case_index": 143, "query": "From 2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt, what exact term fills [BLANK] in: \"HU\u0003RQ\u0003D\u0003GUDZHU\u0011 \u0015 \u0003$OI:RUOG \u0014 \u0003+RWVSRW\u00034$ Figure 1: (1) Comparison of 4 prompting methods, (a) Standard , (b) [BLANK] ( CoT, Reason Only), (c) Act-only, and (d) ReAct (Reason+Act), solving a HotpotQA (Yang et al., 2018) question.\"?", "gold": "chain-of-thought", "answer": "The exact term is **‚ÄúChain-of-thought‚Äù** (i.e., ‚ÄúChain-of-thought (CoT, Reason Only)‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "The context explicitly shows '(b) Chain-of-thought (CoT, Reason Only)', matching the expected 'chain-of-thought'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 76.573, "llm_ms": 1199.207, "top_contexts": [{"rank": 1, "score": 0.8166772, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.5021753563530851, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 15, "text_snippet": "HU\u0003RQ\u0003D\u0003GUDZHU\u0011\u000b\u0015\f\u0003$OI:RUOG\u000b\u0014\f\u0003+RWVSRW\u00034$ Figure 1: (1) Comparison of 4 prompting methods, (a) Standard , (b) Chain-of-thought ( CoT, Reason Only), (c) Act-only, and (d) ReAct (Reason+Act), solving a HotpotQA (Yang et al., 2018) question. ("}, {"rank": 2, "score": 0.7441907, "source": "dense+rrf", "rrf_score": 0.030834914611005692, "rerank_score": 0.23458307921026106, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 44, "text_snippet": "lines (with formats as Figure 1(1a-1c)): (a) Standard prompting (Standard ), which removes all thoughts, actions, observations in ReAct trajectories. (b) Chain-of-thought prompting (CoT) (Wei et al., 2022), which removes actions and observa"}, {"rank": 3, "score": 35.21590512308687, "source": "dense+rrf", "rrf_score": 0.014084507042253521, "rerank_score": 0.2222719881962832, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 36, "text_snippet": "easoning tasks like multi-hop question answering and fact veriÔ¨Åcation. As shown in Figure 1(1d), by interacting with a Wikipedia API, ReAct is able to retrieve information to support reasoning, while also use reasoning to target what to ret"}, {"rank": 4, "score": 43.31309954659238, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.21923174537592519, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 20, "text_snippet": "pertaining to a task in an interleaved manner, which allows the model to perform dynamic reasoning to create, maintain, and adjust high-level plans for acting (reason to act), while also interact with the external environments (e.g. Wikiped"}, {"rank": 5, "score": 0.7122421, "source": "dense+rrf", "rrf_score": 0.031746031746031744, "rerank_score": 0.21607085450568111, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 57, "text_snippet": "rent number of samples, reaching CoT-SC performance with 21 samples using merely 3-5 samples. These results indicate the value of properly combining model internal knowledge and external knowledge for reasoning tasks. ReAct performs best fo"}]}
{"case_index": 144, "query": "From 2005.14165__language_models_are_few_shot_learners.txt, what exact term fills [BLANK] in: \"These terms are intended to remain agnostic on the question of whether the model learns new tasks from scratch at [BLANK]\"?", "gold": "inference", "answer": "The exact term is **‚Äúinference time‚Äù**.", "answer_score": 0.27, "answer_correctness": 0.0, "answer_groundedness": 0.9, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 0.9, "abstained": false, "should_abstain": false, "notes": "Expected blank is 'inference' (phrase: 'at inference time'). Model answered 'inference time', which does not match the exact term requested. Context supports 'inference'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 131.3, "llm_ms": 1354.786, "top_contexts": [{"rank": 1, "score": 0.61006683, "source": "dense+rrf", "rrf_score": 0.032266458495966696, "rerank_score": 0.4423303118039866, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 26, "text_snippet": " learning from zero examples. To avoid this confusion, we use the term ‚Äúmeta-learning‚Äù to capture the inner-loop / outer-loop structure of the general method, and the term ‚Äúin context-learning‚Äù to refer to the inner loop of meta-learning. W"}, {"rank": 2, "score": 38.822084283155846, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.2529009200245514, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 27, "text_snippet": "of whether the model learns new tasks from scratch at inference time or simply recognizes patterns seen during training ‚Äì this is an important issue which we discuss later in the paper, but ‚Äúmeta-learning‚Äù is intended to encompass both poss"}, {"rank": 3, "score": 23.0120394191095, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.24664989480855992, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 67, "text_snippet": " thought in the few-shot exemplars; all the model has to do is repeat the same steps with the new symbols in the test-time example. And yet, small models still fail‚Äîthe ability to perform abstract manipulations on unseen symbols for these t"}, {"rank": 4, "score": 0.61328006, "source": "dense+rrf", "rrf_score": 0.031754032258064516, "rerank_score": 0.24454087608471134, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 247, "text_snippet": "-shot learning actually learns new tasks ‚Äúfrom scratch‚Äù at inference time, or if it simply recognizes and identiÔ¨Åes tasks that it has learned during training. These possibilities exist on a spectrum, ranging from demonstrations in the train"}, {"rank": 5, "score": 22.42902264786066, "source": "dense+rrf", "rrf_score": 0.014084507042253521, "rerank_score": 0.23090682232783225, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 2, "text_snippet": "texts have shown their ability to per- form new tasks from textual instructions or from a few examples (Brown et al., 2020). These few-shot properties Ô¨Årst appeared when scaling models to a sufÔ¨Åcient size (Kaplan et al., 2020), resulting in"}]}
{"case_index": 145, "query": "From 2005.14165__language_models_are_few_shot_learners.txt, what exact term fills [BLANK] in: \"Ziegler Jeffrey Wu Clemens Winter [BLANK] Hesse Mark Chen Eric Sigler Mateusz Litwin Scott Gray Benjamin Chess Jack Clark Christopher Berner Sam McCandlish Alec Radford Ilya Sutskever Dario Amodei OpenAI Abstract Recent wo\"?", "gold": "christopher", "answer": "The exact term that fills **[BLANK]** is **‚ÄúChristopher‚Äù** (as in **‚ÄúChristopher Hesse‚Äù**).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context shows the author list includes 'Christopher Hesse'; blank is 'Christopher'. Model answer matches exactly and is fully supported by provided context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 50.394, "llm_ms": 1306.243, "top_contexts": [{"rank": 1, "score": 0.68549645, "source": "dense+rrf", "rrf_score": 0.03177805800756621, "rerank_score": 0.523470112855731, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 0, "text_snippet": "Language Models are Few-Shot Learners Tom B. Brown‚àóBenjamin Mann‚àóNick Ryder‚àóMelanie Subbiah‚àó Jared Kaplan‚Ä†Prafulla Dhariwal Arvind Neelakantan Pranav Shyam Girish Sastry Amanda Askell Sandhini Agarwal Ariel Herbert-Voss Gretchen Krueger Tom"}, {"rank": 2, "score": 0.739838, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.49298524570305857, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 99, "text_snippet": " Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris"}, {"rank": 3, "score": 0.7356615, "source": "dense+rrf", "rrf_score": 0.031746031746031744, "rerank_score": 0.48939509788884594, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 84, "text_snippet": "nav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Li"}, {"rank": 4, "score": 0.7413423, "source": "dense+rrf", "rrf_score": 0.032018442622950824, "rerank_score": 0.48603578540385417, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 102, "text_snippet": ", Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert- V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwi"}, {"rank": 5, "score": 0.7052249, "source": "dense+rrf", "rrf_score": 0.030776515151515152, "rerank_score": 0.4356951783672714, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 97, "text_snippet": "ind Neelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric"}]}
{"case_index": 146, "query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \", 2018), BERT is designed to pre- train deep bidirectional [BLANK] from unlabeled text by jointly conditioning on both left and right context in all layers.\"?", "gold": "representations", "answer": "The exact term is **‚Äúrepresentations‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'representations' and is directly supported by Context 1/4.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 95.384, "llm_ms": 1105.585, "top_contexts": [{"rank": 1, "score": 0.72286594, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.4195056980161836, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 1, "text_snippet": ", 2018), BERT is designed to pre- train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a re- sult, the pre-trained BERT model can be Ô¨Åne- tuned with just one a"}, {"rank": 2, "score": 0.6533799, "source": "dense+rrf", "rrf_score": 0.03125, "rerank_score": 0.32712269283153506, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 114, "text_snippet": "  E1 E2 EN...Figure 3: Differences in pre-training model architectures. BERT uses a bidirectional Transformer. OpenAI GPT uses a left-to-right Transformer. ELMo uses the concatenation of independently trained left-to-right and right-to- lef"}, {"rank": 3, "score": 0.6357291, "source": "dense+rrf", "rrf_score": 0.030309988518943745, "rerank_score": 0.2862511164700949, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 9, "text_snippet": "ng, the MLM ob- jective enables the representation to fuse the left and the right context, which allows us to pre- train a deep bidirectional Transformer. In addi- tion to the masked language model, we also use a ‚Äúnext sentence prediction‚Äù "}, {"rank": 4, "score": 0.67000735, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.2839268031509382, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 0, "text_snippet": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova Google AI Language {jacobdevlin,mingweichang,kentonl,kristout }@google.com Abstract We introduce a ne"}, {"rank": 5, "score": 51.13594816631855, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.2647807213548499, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 476, "text_snippet": "heng, and Jingjing Liu. Uniter: Learning universal image-text representations. arXiv preprint arXiv:1909.11740 , 2019. [Cra17] Kate Crawford. The trouble with bias. NIPS 2017 Keynote , 2017. [DCLT18] Jacob Devlin, Ming-Wei Chang, Kenton Lee"}]}
{"case_index": 147, "query": "From 2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt, what exact term fills [BLANK] in: \"Token likelihoods are provided by a model, [BLANK] by ùúÉ, that takes as input both previous tokens and their retrieved neighbours.\"?", "gold": "parameterized", "answer": "The exact term is **‚Äúparameterized‚Äù** (as in: ‚Äúa model, **parameterized** by ùúÉ, ‚Ä¶‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'parameterized' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 59.357, "llm_ms": 1574.242, "top_contexts": [{"rank": 1, "score": 0.69355875, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3667044058484411, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 23, "text_snippet": "brevity) is a non-trainable operator speciÔ¨Åed in ¬ß2.3. Token likelihoods are provided by a model, parameterized by ùúÉ, that takes as input both previous tokens and their retrieved neighbours. This deÔ¨Ånes the following retrieval-enhanced sequ"}, {"rank": 2, "score": 0.5660299, "source": "dense+rrf", "rrf_score": 0.030834914611005692, "rerank_score": 0.2704358274071026, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 30, "text_snippet": " News 236,918 398 No 10% Wikipedia 13,288 23 Yes 5% GitHub 374,952 143 No 5% 4  Improving language models by retrieving from trillions of tokens 2.4.R/e.sc/t.sc/r.sc/o.scmodel architecture Our model relies on an encoder-decoder transformer "}, {"rank": 3, "score": 0.54732037, "source": "dense+rrf", "rrf_score": 0.0288981288981289, "rerank_score": 0.2630275034933835, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 242, "text_snippet": "37  Improving language models by retrieving from trillions of tokens Table 16jGreat Circle (novel) , from Wikipedia September 21. The article is about a recent novel and chunks ùê∂3andùê∂4are speciÔ¨Åcally about its reception. The name Publishers"}, {"rank": 4, "score": 30.337066973273522, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.2596606537947589, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 38, "text_snippet": "t token of chunk ùê∂ùë¢is the Ô¨Årst to be able to access the retrieved content ùê∏ùë¢while maintaining autoregressivity in(1). Hence, there is a one token overlap between chunk ùê∂ùë¢=\u0010 ùë•¬πùë¢\u00001¬∫ùëö¬∏ùëñ\u0011 ùëñ2¬ª1¬îùëö¬ºand the corresponding attending chunk ùê∂¬∏ ùë¢,¬πùë•ùë¢ùëö¬∏ùëñ"}, {"rank": 5, "score": 0.5465574, "source": "dense+rrf", "rrf_score": 0.027242401779095628, "rerank_score": 0.2555132775417132, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 210, "text_snippet": "valuating R/e.sc/t.sc/r.sc/o.scmodels on neighbours only and their continuation only aÔ¨Äects performance. Overall, attending to neighbours only provides 22%of the performance improvement due to retrieval in R/e.sc/t.sc/r.sc/o.sc, while atten"}]}
{"case_index": 148, "query": "From 2305.06983__active_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"Augmenting LMs with retrieval components that look up relevant in- formation from external knowledge resources is a promising direction to address [BLANK] (Khan- delwal et al., 2020; Izacard et al., 2022).\"?", "gold": "hallucination", "answer": "The exact term that fills **[BLANK]** is **‚Äúhallucination‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'hallucination' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 89.003, "llm_ms": 1146.586, "top_contexts": [{"rank": 1, "score": 0.77812254, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.4902056881502668, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 5, "text_snippet": "u et al., 2021). Augmenting LMs with retrieval components that look up relevant in- formation from external knowledge resources is a promising direction to address hallucination (Khan- delwal et al., 2020; Izacard et al., 2022). Retrieval a"}, {"rank": 2, "score": 0.66885376, "source": "dense+rrf", "rrf_score": 0.03036576949620428, "rerank_score": 0.2501087043338611, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 15, "text_snippet": "e to avoid unneces- sary or inappropriate retrieval that occurs in passive retrieval augmented LMs (Khandelwal et al., 2020; Borgeaud et al., 2022; Ram et al., 2023; Trivedi et al., 2022). Given the observation that large LMs tend to be wel"}, {"rank": 3, "score": 35.70966508569532, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.2426881788364091, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 5, "text_snippet": " training corpus (Kandpal et al., 2022; Mallen et al., 2023). The standard solution to these issues is to rely on Retrieval Augmented Generation (RAG) (Lee et al., 2019; Lewis et al., 2020; Guu et al., 2020). Answering a question then essen"}, {"rank": 4, "score": 0.74945223, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.2306310282303433, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 1, "text_snippet": "create factually inaccurate out- put. Augmenting LMs by retrieving informa- tion from external knowledge resources is one promising solution. Most existing retrieval aug- mented LMs employ a retrieve-and-generate setup that only retrieves i"}, {"rank": 5, "score": 0.6840553, "source": "dense+rrf", "rrf_score": 0.030536130536130537, "rerank_score": 0.22291508970426485, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 14, "text_snippet": " al., 2022). We ask the following question: can we create a simple and generic retrieval augmented LM that ac- tively decides when and what to retrieve throughout the generation process, and are applicable to a va- riety of long-form genera"}]}
{"case_index": 149, "query": "From 2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt, what exact term fills [BLANK] in: \"1arXiv:2203.11171v4 [cs.CL] 7 Mar 2023 Published as a [BLANK] paper at ICLR 2023 Language model Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\"?", "gold": "conference", "answer": "The exact term that fills **[BLANK]** is **‚Äúconference‚Äù**, as in: **‚ÄúPublished as a conference paper at ICLR 2023.‚Äù**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'conference' and is directly supported by the provided context phrase 'Published as a conference paper at ICLR 2023.'", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 105.368, "llm_ms": 1478.361, "top_contexts": [{"rank": 1, "score": 0.74131954, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.5117388625900938, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 8, "text_snippet": " has greater conÔ¨Ådence that the Ô¨Ånal answer is correct. Compared to other decoding methods, self-consistency avoids the repetitiveness and local-optimality that plague greedy decoding, while mitigating the stochasticity of a single sampled "}, {"rank": 2, "score": 0.65342134, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.36129811453902066, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 78, "text_snippet": " There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 + 2 = 5 cars. ‚Äù to ‚Äú There are 7 cars in the parking lot already. 6 more arrive. Now there are 7 + 6 = 5 cars. ‚Äù. 8  Published as a conference paper at ICLR 2023 4"}, {"rank": 3, "score": 0.5802798, "source": "dense+rrf", "rrf_score": 0.0304147465437788, "rerank_score": 0.36098065422156034, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 4, "text_snippet": ". For example, given the question ‚ÄúIf there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?‚Äù , instead of directly responding with ‚Äú5‚Äù, a language model would be prompted to respond with the entir"}, {"rank": 4, "score": 0.5634849, "source": "dense+rrf", "rrf_score": 0.02684684684684685, "rerank_score": 0.31572144034714716, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 40, "text_snippet": " 4  Published as a conference paper at ICLR 2023 3.2 M AINRESULTS We report the results of self-consistency averaged over 10 runs, where we sampled 40 outputs independently from the decoder in each run. The baseline we compare to is chain-o"}, {"rank": 5, "score": 0.58195895, "source": "dense+rrf", "rrf_score": 0.028381642512077296, "rerank_score": 0.2955038634261992, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 18, "text_snippet": "., in Output 2), but such solutions are less likely to arrive at the same answer. That is, we hypothesize that correct reasoning processes, even if they are diverse, tend to have greater agreement in their Ô¨Ånal answer than incorrect process"}]}
{"case_index": 150, "query": "From 1706.03762__attention_is_all_you_need.txt, what exact term fills [BLANK] in: \"Attention Is All You Need Ashish Vaswani‚àó Google Brain avaswani@google.comNoam Shazeer‚àó Google Brain noam@google.comNiki Parmar‚àó Google Research nikip@google.comJakob [BLANK]‚àó Google Research usz@google.com Llion Jones‚àó Google Research llion@google.comAidan N.\"?", "gold": "uszkoreit", "answer": "The exact term that fills **[BLANK]** is **‚ÄúUszkoreit‚Äù**.", "answer_score": 0.3, "answer_correctness": 0.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context shows the name as 'Jakob Uszkoreit', but the question asks for the exact term filling the blank in the concatenated string, which should be 'uszkoreit' (lowercase). Model answered 'Uszkoreit' (capitalized), so it does not match the expected exact term.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 41.956, "llm_ms": 1135.137, "top_contexts": [{"rank": 1, "score": 0.66532934, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.46081534540044744, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 0, "text_snippet": "Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works. Attention Is All You Need Ashish Vaswani‚àó Google Brain avaswani@go"}, {"rank": 2, "score": 0.5755218, "source": "dense+rrf", "rrf_score": 0.028693528693528692, "rerank_score": 0.24398265843315023, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 120, "text_snippet": "n Ruder, and Donald Metzler. Long range arena: A benchmark for eÔ¨Écient transformers. InInternational Conference on Learning Representations , 2020. [81]Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. EÔ¨Écient transformers: A survey"}, {"rank": 3, "score": 0.6520369, "source": "dense+rrf", "rrf_score": 0.03125, "rerank_score": 0.2426821853156145, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 523, "text_snippet": "s in neural information processing systems , pages 3630‚Äì3638, 2016. [VSP+17]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances"}, {"rank": 4, "score": 0.66541576, "source": "dense+rrf", "rrf_score": 0.03177805800756621, "rerank_score": 0.24101674437473977, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 103, "text_snippet": " general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Compu- tational Linguistics , ACL ‚Äô10, pages 384‚Äì394. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Ai"}, {"rank": 5, "score": 0.5986148, "source": "dense+rrf", "rrf_score": 0.030621785881252923, "rerank_score": 0.24065328867447727, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 117, "text_snippet": "a: Open models based on gemini research and technology, 2024. [VSP+17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,  L ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in "}]}
{"case_index": 151, "query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"o different [BLANK] of this loss: L+ i=Li(e(ci,ri)) L‚àí i= min (Li(Œµ),Li(e(ci,Œµ))) whereŒµdenotes an empty sequence.\"?", "gold": "instantiations", "answer": "The exact term is **‚Äúinstantiations‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term and is directly supported by Context 1/2 phrase 'two different instantiations of this loss'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 106.569, "llm_ms": 1181.967, "top_contexts": [{"rank": 1, "score": 0.68953824, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3183891541968168, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 24, "text_snippet": "o different instantiations of this loss: L+ i=Li(e(ci,ri)) L‚àí i= min (Li(Œµ),Li(e(ci,Œµ))) whereŒµdenotes an empty sequence. The former is the weighted loss over all tokens xi,...,x nif the API call and its result are given to Mas a preÔ¨Åx;3 th"}, {"rank": 2, "score": 0.5202571, "source": "dense+rrf", "rrf_score": 0.03149801587301587, "rerank_score": 0.2180599733921946, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 23, "text_snippet": "eval system to perform search over a large corpus. The response for each API call cineeds to be a single text sequence ri. Filtering API Calls Letibe the position of the API callciin the sequence x=x1,...,x n, and let ribe the response from"}, {"rank": 3, "score": 34.079653215592295, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.2102032619815938, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 1, "text_snippet": "metic or fac- tual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer , a model traine"}, {"rank": 4, "score": 0.46329188, "source": "dense+rrf", "rrf_score": 0.027205882352941177, "rerank_score": 0.20971646881336461, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 94, "text_snippet": "e are no examples of chained tool use in the Ô¨Ånetuning dataset. Our current approach also does not allow the LM to use a tool in an in- teractive way ‚Äì especially for tools such as search engines, that could potentially return hundreds of d"}, {"rank": 5, "score": 47.30551835213876, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.20808481434424214, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 212, "text_snippet": "line decision making: Benchmarks and additional opinions,‚Äù arXiv preprint arXiv:2306.02224 , 2023. [108] T. Schick, J. Dwivedi-Yu, R. Dess `ƒ±, R. Raileanu, M. Lomeli, L. Zettle- moyer, N. Cancedda, and T. Scialom, ‚ÄúToolformer: Language mode"}]}
{"case_index": 152, "query": "From 2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt, what exact term fills [BLANK] in: \"RAG research shifted towards providing better information for LLMs to answer more com- plex and [BLANK] tasks during the inference stage, leading to\"?", "gold": "knowledge-intensive", "answer": "The exact term is **‚Äúknowledge-intensive‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term and is explicitly present in the provided context sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 77.154, "llm_ms": 896.166, "top_contexts": [{"rank": 1, "score": 0.6672983, "source": "dense+rrf", "rrf_score": 0.03177805800756621, "rerank_score": 0.40358467275098403, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 7, "text_snippet": "cusing on enhancing language models by incorporating additional knowledge through Pre- Training Models (PTM). This early stage was characterized by foundational work aimed at refining pre-training techniques [3]‚Äì[5].The subsequent arrival o"}, {"rank": 2, "score": 0.6535771, "source": "dense+rrf", "rrf_score": 0.02877846790890269, "rerank_score": 0.2525033978873716, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 79, "text_snippet": "on the long-context understanding ability of LLMs, and we still have a long way to go in harnessing open-source LLMs within our framework. 10  4 Related Work 4.1 Retrieval-Augmented Generation. Augmenting language models with information re"}, {"rank": 3, "score": 0.6722121, "source": "dense+rrf", "rrf_score": 0.031746031746031744, "rerank_score": 0.23983459791524808, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 13, "text_snippet": "he fine-tuning of LLMs. Researchers have also been exploring ways to enhance language models in the pre-training stage through retrieval-augmented techniques. advanced RAG, and modular RAG. This review contex- tualizes the broader scope of "}, {"rank": 4, "score": 0.6297331, "source": "dense+rrf", "rrf_score": 0.029116045245077504, "rerank_score": 0.230002624376133, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 8, "text_snippet": "nowledge-intensive tasks during the inference stage, leading to rapid development in RAG studies. As research progressed, the enhancement of RAG was no longer limited to the inference stage but began to incorporate more with LLM fine-tuning"}, {"rank": 5, "score": 0.64474547, "source": "dense+rrf", "rrf_score": 0.02758751902587519, "rerank_score": 0.22418641324138405, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 3, "text_snippet": "augmentation techniques. The paper highlights the state-of-the- art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-"}]}
{"case_index": 153, "query": "From 2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt, what exact term fills [BLANK] in: \"ap by mapping out the RAG process and charting its evolution and anticipated future paths, with a focus on the [BLANK] of RAG within LLMs.\"?", "gold": "integration", "answer": "The exact term is **‚Äúintegration‚Äù** (as in: ‚Äúwith a focus on the **integration** of RAG within LLMs.‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'integration' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 113.685, "llm_ms": 1535.113, "top_contexts": [{"rank": 1, "score": 0.72704, "source": "dense+rrf", "rrf_score": 0.032266458495966696, "rerank_score": 0.43064313368303614, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 9, "text_snippet": "ap by mapping out the RAG process and charting its evolution and anticipated future paths, with a focus on the integration of RAG within LLMs. This paper considers both technical paradigms and research methods, summarizing three main resear"}, {"rank": 2, "score": 28.251621441927803, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.26511061229527877, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 79, "text_snippet": "on the long-context understanding ability of LLMs, and we still have a long way to go in harnessing open-source LLMs within our framework. 10  4 Related Work 4.1 Retrieval-Augmented Generation. Augmenting language models with information re"}, {"rank": 3, "score": 31.59036285618039, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.26126071696898684, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 143, "text_snippet": "are not constrained by context. In fact, RAG still plays an irreplaceable role. On one hand, providing LLMs with a large amount of context at once will significantly impact its inference speed, while chunked retrieval and on-demand input ca"}, {"rank": 4, "score": 0.6827465, "source": "dense+rrf", "rrf_score": 0.029910714285714284, "rerank_score": 0.25405161203555465, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 162, "text_snippet": "-answering tasks. VIII. C ONCLUSION The summary of this paper, as depicted in Figure 6, empha- sizes RAG‚Äôs significant advancement in enhancing the capa- bilities of LLMs by integrating parameterized knowledge from language models with exte"}, {"rank": 5, "score": 0.7160111, "source": "dense+rrf", "rrf_score": 0.0315136476426799, "rerank_score": 0.24649496023616188, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 8, "text_snippet": "nowledge-intensive tasks during the inference stage, leading to rapid development in RAG studies. As research progressed, the enhancement of RAG was no longer limited to the inference stage but began to incorporate more with LLM fine-tuning"}]}
{"case_index": 154, "query": "From 2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt, what exact term fills [BLANK] in: \"lock-sparse FlashAttention , a sparse attention algorithm that is 2-4 \u0002faster than [BLANK] , scaling up to sequence length of 64k.\"?", "gold": "evenflashattention", "answer": "The term that fills **[BLANK]** is:\n\n**even FlashAttention**", "answer_score": 0.3, "answer_correctness": 0.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context shows the blank is 'evenFlashAttention' (no space, exact term). Model answered 'even FlashAttention' with a space, so it does not match the expected exact term.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 53.322, "llm_ms": 1123.486, "top_contexts": [{"rank": 1, "score": 0.780681, "source": "dense+rrf", "rrf_score": 0.032266458495966696, "rerank_score": 0.3709822424454923, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 16, "text_snippet": "lock-sparse FlashAttention , a sparse attention algorithm that is 2-4 \u0002faster than evenFlashAttention , scaling up to sequence length of 64k. We prove that block-sparse FlashAttention has better IO complexity than FlashAttention by a factor"}, {"rank": 2, "score": 0.7820786, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.3370277236881024, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 55, "text_snippet": "th-256 (sequence length 64K). ‚Ä¢Benchmarking Attention. We measure the runtime and memory performance of FlashAttention and block-sparse FlashAttention based on sequence length. We conÔ¨Årm that the memory footprint ofFlashAttention scales lin"}, {"rank": 3, "score": 0.7945808, "source": "dense+rrf", "rrf_score": 0.032018442622950824, "rerank_score": 0.30929976269199777, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 20, "text_snippet": "n. FlashAttention is up to 3\u0002faster than the standard attention implemen- tation across common sequence lengths from 128 to 2K and scales up to 64K. Up to sequence length of 512, FlashAttention is both faster and more memory-eÔ¨Écient than an"}, {"rank": 4, "score": 39.31814244018681, "source": "dense+rrf", "rrf_score": 0.013888888888888888, "rerank_score": 0.2958136475349743, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 50, "text_snippet": "o blocks of the attention matrix. The algorithm is identical to Algorithm 1, except we skip zero blocks. We reproduce the algorithm description in Algorithm 5 in Appendix B. We also analyze the IO complexity of block-sparse FlashAttention ."}, {"rank": 5, "score": 0.76706535, "source": "dense+rrf", "rrf_score": 0.030117753623188408, "rerank_score": 0.29068754867882585, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 76, "text_snippet": "nce lengths. Memory Footprint. Figure 3 (right) shows the memory footprint of FlashAttention and block-sparse FlashAttention compared to various exact, approximate, and sparse attention baselines. FlashAttention and block-sparse FlashAttent"}]}
{"case_index": 155, "query": "From 2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt, what exact term fills [BLANK] in: \"The dataset consists of text documents from multiple sources and multiple languages [BLANK] over 5 trillion tokens (detailed\"?", "gold": "totalling", "answer": "The exact term is **‚Äútotalling‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'totalling' and is directly supported by the provided context sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 56.25, "llm_ms": 1215.855, "top_contexts": [{"rank": 1, "score": 52.732223999385425, "source": "dense+rrf", "rrf_score": 0.01639344262295082, "rerank_score": 0.34998911026766366, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 18, "text_snippet": "c/a.scoperator. Causality is maintained as neighbours of the Ô¨Årst chunk only aÔ¨Äect the last token of the Ô¨Årst chunk and tokens from the second chunk. a new methodology to evaluate language models when an evaluation set is partially present "}, {"rank": 2, "score": 0.65363735, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.27764171825623984, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 173, "text_snippet": "n Conference of the North American Chapter of the Association for Computational Linguistics , 2018. 23  Improving language models by retrieving from trillions of tokens A. Datasets We provide a full description of MassiveText and of our ext"}, {"rank": 3, "score": 27.84214996388518, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.2565121035097128, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 78, "text_snippet": "struct the dataset with articles from the ‚Äúfuture‚Äù and manually remove new articles that strongly overlap documents in our training data. This guarantees that the evaluation documents are not leaked in our training data. 10  Improving langu"}, {"rank": 4, "score": 0.6806844, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.25391325037249657, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 19, "text_snippet": "nd multiple languages totalling over 5 trillion tokens (detailed in Table 1). Sequences are sampled from subsets of the training data, with sampling weights given in the right-most column of Table 1. We tokenize the dataset using SentencePi"}, {"rank": 5, "score": 26.186803269990122, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.238317742171599, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 177, "text_snippet": "s. Fig. 10 show that little overlap remains between our test dataset and the retrieved neighbours from the training dataset. The full list of included articles is given in Table 9. 24  Improving language models by retrieving from trillions "}]}
{"case_index": 156, "query": "From 2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt, what exact term fills [BLANK] in: \"dels (LLMs) showcase impres- sive capabilities but encounter challenges like hallucination, outdated knowledge, and [BLANK], untraceable reasoning processes.\"?", "gold": "non-transparent", "answer": "The exact term is **‚Äúnon-transparent‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context 1 explicitly contains the phrase 'outdated knowledge, and non-transparent, untraceable reasoning processes.' Model answer matches expected term.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 62.154, "llm_ms": 963.204, "top_contexts": [{"rank": 1, "score": 0.7326498, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.42779643982515037, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 1, "text_snippet": "dels (LLMs) showcase impres- sive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution b"}, {"rank": 2, "score": 63.441950273113555, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.2934427342995043, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 0, "text_snippet": "1 Retrieval-Augmented Generation for Large Language Models: A Survey Yunfan Gaoa, Yun Xiongb, Xinyu Gaob, Kangxiang Jiab, Jinliu Panb, Yuxi Bic, Yi Daia, Jiawei Suna, Meng Wangc, and Haofen Wanga,c aShanghai Research Institute for Intellige"}, {"rank": 3, "score": 0.6315918, "source": "dense+rrf", "rrf_score": 0.028484848484848488, "rerank_score": 0.21436578171229875, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 79, "text_snippet": "on the long-context understanding ability of LLMs, and we still have a long way to go in harnessing open-source LLMs within our framework. 10  4 Related Work 4.1 Retrieval-Augmented Generation. Augmenting language models with information re"}, {"rank": 4, "score": 26.40405032002434, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.20793593096955232, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 238, "text_snippet": " maximizing llm performance,‚Äù https://community.openai. com/t/openai-dev-day-2023-breakout-sessions/505213# a-survey-of-techniques-for-maximizing-llm-performance-2, 2023. [167] J. Chen, H. Lin, X. Han, and L. Sun, ‚ÄúBenchmarking large lan- g"}, {"rank": 5, "score": 0.65192115, "source": "dense+rrf", "rrf_score": 0.03057889822595705, "rerank_score": 0.181191023458151, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 4, "text_snippet": "l, retrieval-augmented gen- eration, natural language processing, information retrieval I. I NTRODUCTION LARGE language models (LLMs) have achieved remark- able success, though they still face significant limitations, especially in domain-s"}]}
{"case_index": 157, "query": "From 2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt, what exact term fills [BLANK] in: \"We note that instruction tuning, while very beneficial for teaching the model how to solve a task, does not [BLANK] teach the model new knowledge.\"?", "gold": "necessarily", "answer": "The exact term that fills **[BLANK]** is **‚Äúnecessarily‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'necessarily' and is directly supported by the provided context sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 82.33, "llm_ms": 1042.625, "top_contexts": [{"rank": 1, "score": 0.7024468, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.4254112853408671, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 24, "text_snippet": "not result in proportional improvement to small model performance when thoroughly evaluated on knowledge-intensive or reasoning-intensive tasks where correctness is not just judged by style. We note that instruction tuning, while very benef"}, {"rank": 2, "score": 63.84639528051362, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.367886222684738, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 144, "text_snippet": "ng training and safety guardrails (e.g., content filter) within the Azure OpenAI API. However, detailed studies are required for better quantification of such risks. Small Model Capacity: We note that post-training, while significantly bene"}, {"rank": 3, "score": 33.565405716298145, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.2711915013066795, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 2, "text_snippet": "asks, potentially different from the one used by the larger model. For example, while larger models might provide a direct answer to a complex task, smaller models may not have the same capacity. In Orca 2, we teach the model various reason"}, {"rank": 4, "score": 32.28974095913887, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.2586855167823006, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 148, "text_snippet": "synthetic data for post-training. While Orca 2 models have not gone through RLHF training for safety, we believe that the use of synthetic data for post-training that has been filtered with various content safety filters could provide anoth"}, {"rank": 5, "score": 29.306031024762145, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.2522741984686563, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 128, "text_snippet": " Evaluation results for HHH dataset. system message while assessing chat model‚Äôs safety. Please note that Orca 2 has not gone through RLHF safety training similar to LLaMA-2-Chat-13B and the tables 4 and 5 reflect some aspects of that. We h"}]}
{"case_index": 158, "query": "From 1706.03762__attention_is_all_you_need.txt, what exact term fills [BLANK] in: \"Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position [BLANK] and became the other person involved in nearly every detail.\"?", "gold": "representation", "answer": "The exact term is **‚Äúrepresentation‚Äù** (as in ‚Äúthe parameter-free position representation‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'representation' and is directly supported by Context 1 phrase 'parameter-free position representation'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 82.827, "llm_ms": 1384.488, "top_contexts": [{"rank": 1, "score": 0.69965255, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3915707458010202, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 4, "text_snippet": "posed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed sca"}, {"rank": 2, "score": 27.662029128378034, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.2084513354010264, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 27, "text_snippet": "ead, the total computational cost is similar to that of single-head attention with full dimensionality. 3.2.3 Applications of Attention in our Model The Transformer uses multi-head attention in three different ways: ‚Ä¢In \"encoder-decoder att"}, {"rank": 3, "score": 0.59869385, "source": "dense+rrf", "rrf_score": 0.031754032258064516, "rerank_score": 0.19846764902283792, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 20, "text_snippet": "t Attention  Multi-Head Attention Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel. of the values, where the weight assigned to each value is computed by a "}, {"rank": 4, "score": 0.6004237, "source": "dense+rrf", "rrf_score": 0.031746031746031744, "rerank_score": 0.19354088346626988, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 19, "text_snippet": "sequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position ican depend only on the known outputs at positions less than i. 3.2 Attention An attention "}, {"rank": 5, "score": 32.45150783740488, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.18646469940563803, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 29, "text_snippet": "n layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We"}]}
{"case_index": 159, "query": "From 2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt, what exact term fills [BLANK] in: \"[BLANK] : Fast and Memory-EÔ¨Écient Exact Attention with IO-Awareness Tri Daoy, Daniel Y.\"?", "gold": "flashattention", "answer": "**FlashAttention**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "The blank is the paper title term 'FlashAttention', matching expected 'flashattention' (case-insensitive) and directly supported by the provided context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 65.244, "llm_ms": 1119.559, "top_contexts": [{"rank": 1, "score": 0.74753296, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.32444514274478237, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 0, "text_snippet": "FlashAttention : Fast and Memory-EÔ¨Écient Exact Attention with IO-Awareness Tri Daoy, Daniel Y. Fuy, Stefano Ermony, Atri Rudraz, and Christopher R√©y yDepartment of Computer Science, Stanford University zDepartment of Computer Science and En"}, {"rank": 2, "score": 0.6528816, "source": "dense+rrf", "rrf_score": 0.02976190476190476, "rerank_score": 0.3059523809523734, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 84, "text_snippet": "mputer/RedPajama-Data . Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. CoRR , abs/2307.08691, 2023. Tri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra, and Christopher R ¬¥e. Flashattention: Fast an"}, {"rank": 3, "score": 61.89448946786285, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.29008067269186055, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 106, "text_snippet": "o solve math word problems. arXiv preprint arXiv:2110.14168 . Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Car- bonell, Quoc V Le, and Ruslan Salakhutdinov. 2019. Transformer-xl: Attentive language mod- els beyond a Ô¨Åxed-length context. arXi"}, {"rank": 4, "score": 0.6886605, "source": "dense+rrf", "rrf_score": 0.03055037313432836, "rerank_score": 0.2883235478070816, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 100, "text_snippet": "n, Atri Rudra, and Christopher R√©. 2022. FlashAttention: Fast and memory-efficient exact attention with IO- awareness. ArXiv:2205.14135. Hermann Ebbinghaus. 1913. Memory: A contribu- tion to experimental psychology. H. A. Ruger & C. E. Buss"}, {"rank": 5, "score": 43.32934919020863, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.23418803418802908, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 88, "text_snippet": "ng, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821 ,"}]}
{"case_index": 160, "query": "From 2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt, what exact term fills [BLANK] in: \"We find that changing the position of relevant information in the input context can [BLANK] affect model performance, indicating that current language models do not robustly access and use i\"?", "gold": "substantially", "answer": "The exact term that fills **[BLANK]** is **‚Äúsubstantially‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'substantially' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 69.237, "llm_ms": 1078.988, "top_contexts": [{"rank": 1, "score": 0.71548945, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.5012377317915045, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 11, "text_snippet": "r of documents in the input context (akin to retrieving more or less documents in retrieval-augmented generation), and (ii) control the position of the relevant information within the input context by changing the order of the documents to "}, {"rank": 2, "score": 0.71116793, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.4130422493073302, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 1, "text_snippet": " on two tasks that require identifying relevant information in their in- put contexts: multi-document question an- swering and key-value retrieval. We find that performance can degrade significantly when changing the position of relevant in"}, {"rank": 3, "score": 0.7353716, "source": "dense+rrf", "rrf_score": 0.032018442622950824, "rerank_score": 0.37433011059722543, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 12, "text_snippet": "ng that current language models do not robustly access and use information in long input contexts. Furthermore, we observe a distinctive U-shaped performance curve (Figure 1); language model performance is highest when relevant information "}, {"rank": 4, "score": 0.7052808, "source": "dense+rrf", "rrf_score": 0.03125763125763126, "rerank_score": 0.3581811473644817, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 94, "text_snippet": "levant information, indicating that models struggle to robustly access and use infor- mation in long input contexts. In particular, per- formance is often lowest when models must use information in the middle of long input contexts. We cond"}, {"rank": 5, "score": 0.6760036, "source": "dense+rrf", "rrf_score": 0.029857397504456328, "rerank_score": 0.3504826884428102, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 16, "text_snippet": "that occur in the middle of their input context and continue to exhibit a U-shaped performance curve. To better understand why language models strug- gle to robustly access and use information in their input contexts, we study the role of m"}]}
{"case_index": 161, "query": "From 2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt, what exact term fills [BLANK] in: \"In Orca 2, we continue exploring how improved training signals can enhance smaller LMs‚Äô [BLANK] abilities.\"?", "gold": "reasoning", "answer": "The exact term that fills **[BLANK]** is **‚Äúreasoning‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context explicitly states 'enhance smaller LMs‚Äô reasoning abilities'; model answer matches expected term.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 69.512, "llm_ms": 1239.053, "top_contexts": [{"rank": 1, "score": 0.74132, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.36187605953043217, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 1, "text_snippet": "n benchmarks like BigBench Hard and AGIEval. In Orca 2, we continue exploring how improved training signals can enhance smaller LMs‚Äô reasoning abilities. Research on training small LMs has often relied on imitation learning to replicate the"}, {"rank": 2, "score": 0.6914087, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.23666027986663576, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 0, "text_snippet": "Orca 2: Teaching Small Language Models How to Reason Arindam Mitra, Luciano Del Corro‚Ä†, Shweti Mahajan‚Ä†, Andres Codas‚Ä° Clarisse Simoes‚Ä°, Sahaj Agarwal, Xuxi Chen‚àó, Anastasia Razdaibiedina‚àó Erik Jones‚àó, Kriti Aggarwal‚àó, Hamid Palangi, Guoqin"}, {"rank": 3, "score": 0.62195855, "source": "dense+rrf", "rrf_score": 0.029273504273504274, "rerank_score": 0.21681855350826482, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 148, "text_snippet": "synthetic data for post-training. While Orca 2 models have not gone through RLHF training for safety, we believe that the use of synthetic data for post-training that has been filtered with various content safety filters could provide anoth"}, {"rank": 4, "score": 0.70712113, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.21211122579957242, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 15, "text_snippet": ", we continue to pursue the question of how we can teach smaller LMs to reason. The objectives of Orca 2 are two-fold. Firstly, we aim to teach smaller models howto use a suite of reasoning techniques, such as step-by-step processing, recal"}, {"rank": 5, "score": 30.419597167935965, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.19940888416800265, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 144, "text_snippet": "ng training and safety guardrails (e.g., content filter) within the Azure OpenAI API. However, detailed studies are required for better quantification of such risks. Small Model Capacity: We note that post-training, while significantly bene"}]}
{"case_index": 162, "query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \"ELMo and its predecessor (Peters et al., 2017, 2018a) generalize [BLANK] word embedding re- search along a diffe\"?", "gold": "traditional", "answer": "The exact term that fills **[BLANK]** is **‚Äútraditional‚Äù** (as in: ‚Äúgeneralize **traditional** word embedding research‚Ä¶‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'traditional' and is directly supported by the provided context snippet.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 54.286, "llm_ms": 1568.831, "top_contexts": [{"rank": 1, "score": 0.6305032, "source": "dense+rrf", "rrf_score": 0.032266458495966696, "rerank_score": 0.31207627088335327, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 14, "text_snippet": " Lee, 2018) or paragraph embeddings (Le and Mikolov, 2014). To train sentence representations, prior work has used objectives to rank candidate next sentences (Jernite et al., 2017; Logeswaran and Lee, 2018), left-to-right generation of nex"}, {"rank": 2, "score": 0.68320996, "source": "dense+rrf", "rrf_score": 0.032266458495966696, "rerank_score": 0.2975330733806198, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 15, "text_snippet": ") generalize traditional word embedding re- search along a different dimension. They extract context-sensitive features from a left-to-right and a right-to-left language model. The contextual rep- resentation of each token is the concatenat"}, {"rank": 3, "score": 0.6236683, "source": "dense+rrf", "rrf_score": 0.03128054740957967, "rerank_score": 0.2874559655867077, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 0, "text_snippet": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova Google AI Language {jacobdevlin,mingweichang,kentonl,kristout }@google.com Abstract We introduce a ne"}, {"rank": 4, "score": 0.6292262, "source": "dense+rrf", "rrf_score": 0.031009615384615385, "rerank_score": 0.2652416581427248, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 10, "text_snippet": "ford et al. (2018), which uses unidirec- tional language models for pre-training, BERT uses masked language models to enable pre- trained deep bidirectional representations. This is also in contrast to Peters et al. (2018a), which uses a sh"}, {"rank": 5, "score": 32.83493084446756, "source": "dense+rrf", "rrf_score": 0.014285714285714285, "rerank_score": 0.2549459489502709, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 109, "text_snippet": "reading books. In Proceedings of the IEEE international conference on computer vision , pages 19‚Äì27. Appendix for ‚ÄúBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding‚Äù We organize the appendix into three section"}]}
{"case_index": 163, "query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \"The same [BLANK] model parameters are used to initialize models for different down-stream tasks.\"?", "gold": "pre-trained", "answer": "pre-trained", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "The context explicitly states: 'The same pre-trained model parameters are used...', matching the expected term.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 99.053, "llm_ms": 886.119, "top_contexts": [{"rank": 1, "score": 0.6440431, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.35202419071919727, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 20, "text_snippet": "or BERT. Apart from output layers, the same architec- tures are used in both pre-training and Ô¨Åne-tuning. The same pre-trained model parameters are used to initialize models for different down-stream tasks. During Ô¨Åne-tuning, all parameters"}, {"rank": 2, "score": 0.6041171, "source": "dense+rrf", "rrf_score": 0.031746031746031744, "rerank_score": 0.29400781433351647, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 39, "text_snippet": " objectives used in Jernite et al. (2017) and Logeswaran and Lee (2018). However, in prior work, only sentence embeddings are transferred to down-stream tasks, where BERT transfers all pa- rameters to initialize end-task model parameters. P"}, {"rank": 3, "score": 34.53129762296854, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.2699603442467558, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 109, "text_snippet": "reading books. In Proceedings of the IEEE international conference on computer vision , pages 19‚Äì27. Appendix for ‚ÄúBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding‚Äù We organize the appendix into three section"}, {"rank": 4, "score": 0.5432444, "source": "dense+rrf", "rrf_score": 0.029709507042253523, "rerank_score": 0.26647748032001645, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 0, "text_snippet": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova Google AI Language {jacobdevlin,mingweichang,kentonl,kristout }@google.com Abstract We introduce a ne"}, {"rank": 5, "score": 32.907838240761684, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.2562613750230323, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 83, "text_snippet": "ted that rich, unsupervised pre-training is an integral part of many language understanding systems. In particular, these results enable even low-resource tasks to beneÔ¨Åt from deep unidirectional architec- tures. Our major contribution is f"}]}
{"case_index": 164, "query": "From 2302.13971__llama_open_and_efficient_foundation_language_models.txt, what exact term fills [BLANK] in: \"First, we use an efÔ¨Åcient [BLANK] of the causal multi-head attention to reduce memory usage and runtime.\"?", "gold": "implementation", "answer": "The exact term is **‚Äúimplementation‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'implementation' and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 54.127, "llm_ms": 1098.243, "top_contexts": [{"rank": 1, "score": 0.64803284, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3534102354356134, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 24, "text_snippet": "ze of the model (see Table 2 for details). 2.4 EfÔ¨Åcient implementation We make several optimizations to improve the train- ing speed of our models. First, we use an efÔ¨Åcient implementation of the causal multi-head attention to reduce memory"}, {"rank": 2, "score": 0.60912716, "source": "dense+rrf", "rrf_score": 0.030309988518943745, "rerank_score": 0.2256061697825736, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 5, "text_snippet": "wn larger [ 5] and deeper [ 83], but equipping them with longer context remains diÔ¨Écult [ 80], since the self-attention module at their heart has time and memory complexity quadratic in sequence length. An important question is whether maki"}, {"rank": 3, "score": 29.71694205349605, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.21765961261457065, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 0, "text_snippet": "LLaMA: Open and EfÔ¨Åcient Foundation Language Models Hugo Touvron‚àó, Thibaut Lavril‚àó, Gautier Izacard‚àó, Xavier Martinet Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi√®re, Naman Goyal Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand "}, {"rank": 4, "score": 0.5789178, "source": "dense+rrf", "rrf_score": 0.026992753623188405, "rerank_score": 0.203564584333493, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 138, "text_snippet": "ory linear instead of quadratic in the sequence length). Though they reduce the amount of extra memory required, naively they still incur quadratic HBM accesses, resulting in slower execution speed. We describe the FlashAttention algorithm "}, {"rank": 5, "score": 0.5939496, "source": "dense+rrf", "rrf_score": 0.028169014084507043, "rerank_score": 0.19428224647365708, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 1, "text_snippet": "nces, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading oÔ¨Ä model quality to reduce the compute complexity, but often do n"}]}
{"case_index": 165, "query": "From 2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"In our [BLANK] and experiments, all prompts are evaluated using the gpt-3.5-turbo-16k model, which is available through the OpenAI API2.\"?", "gold": "implementation", "answer": "The exact term is **‚Äúimplementation‚Äù** (i.e., ‚ÄúIn our **implementation** and experiments, ‚Ä¶‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'implementation' and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 59.014, "llm_ms": 1107.307, "top_contexts": [{"rank": 1, "score": 61.76811331503432, "source": "dense+rrf", "rrf_score": 0.01639344262295082, "rerank_score": 0.3938619719568361, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 23, "text_snippet": " explain how these three quality aspects can be measured in a fully automated way, by prompting an LLM. In our implementation and experiments, all prompts are evaluated using the gpt-3.5-turbo-16k model, which is available through the OpenA"}, {"rank": 2, "score": 31.169078030308096, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.2483326588709044, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 141, "text_snippet": " open-domain question answering containing 3610 questions. For TriviaQA we evaluate on the dev set of the Ô¨Åltered set. This differs from GPT-3 and PaLM, which evaluate on the test set of the unÔ¨Åltered set for which the online evaluation ser"}, {"rank": 3, "score": 30.676424950363376, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.2122896485425376, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 55, "text_snippet": " 300 key-value pairs‚Äî although the synthetic key-value retrieval task only requires identifying exact match within the input context, not all models achieve high performance. Similar to our multi-document QA results, GPT- 3.5-Turbo, GPT-3.5"}, {"rank": 4, "score": 0.60327375, "source": "dense+rrf", "rrf_score": 0.013888888888888888, "rerank_score": 0.20217289412846035, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 81, "text_snippet": "on API distribution. Our main metric is human preference ratings on a held out set of prompts from the same source as our training distribution. When using prompts from the API for evaluation, we only select prompts by customers we haven‚Äôt "}, {"rank": 5, "score": 27.201444807274314, "source": "dense+rrf", "rrf_score": 0.014084507042253521, "rerank_score": 0.20164387076293377, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 59, "text_snippet": "n. 3.3 Full QA Performance on Wikipedia-based Datasets We leverage Gemini-1.5-Pro and GPT-4o as the reader in our LongRAG framework. The prompt we use for our experiments are in Table 7. For Wiki-based datasets, such as NQ and HotpotQA, whi"}]}
{"case_index": 166, "query": "From 2005.14165__language_models_are_few_shot_learners.txt, what exact term fills [BLANK] in: \"Figure 1.2 [BLANK] the conditions we study, and shows few-shot learning of a simple task requiring the model to remove extraneous symbols from a word.\"?", "gold": "illustrates", "answer": "The exact term is **‚Äúillustrates‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term and is directly supported by Context 1 sentence: 'Figure 1.2 illustrates the conditions we study...'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 91.499, "llm_ms": 1040.752, "top_contexts": [{"rank": 1, "score": 0.67965627, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.4539436085700351, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 30, "text_snippet": "ions are allowed and only an instruction in natural language is given to the model. GPT-3 could also in principle be evaluated in the traditional Ô¨Åne-tuning setting, but we leave this to future work. Figure 1.2 illustrates the conditions we"}, {"rank": 2, "score": 0.58911985, "source": "dense+rrf", "rrf_score": 0.029827662395050816, "rerank_score": 0.32591236013933045, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 19, "text_snippet": "Larger models make increasingly efÔ¨Åcient use of in-context information. We show in-context learning performance on a simple task requiring the model to remove random symbols from a word, both with and without a natural language task descrip"}, {"rank": 3, "score": 26.51766763926476, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.2601589446044031, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 147, "text_snippet": "er study of test-time behavior of language models. 3.9.1 Arithmetic To test GPT-3‚Äôs ability to perform simple arithmetic operations without task-speciÔ¨Åc training, we developed a small battery of 10 tests that involve asking GPT-3 a simple a"}, {"rank": 4, "score": 0.6139642, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.26011232455778305, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 166, "text_snippet": "r to learn these tasks at test time, as the model cannot perform them zero-shot and their artiÔ¨Åcial nature makes them unlikely to appear in the pre-training data (although we cannot conÔ¨Årm this with certainty). We can further quantify perfo"}, {"rank": 5, "score": 0.598157, "source": "dense+rrf", "rrf_score": 0.028790389395194696, "rerank_score": 0.257270875284429, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 43, "text_snippet": "on with human performance. In this work we do not Ô¨Åne-tune GPT-3 because our focus is on task-agnostic performance, but GPT-3 can be Ô¨Åne-tuned in principle and this is a promising direction for future work. ‚Ä¢Few-Shot (FS) is the term we wil"}]}
{"case_index": 167, "query": "From 2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt, what exact term fills [BLANK] in: \"For example, the agent shown in Figure 1(1c) is unable to generate the correct Ô¨Ånal action (Act 4) to Ô¨Ånish the QA task as it requires complex reasoning over the [BLANK] context (Question, Act 1-3, Obs 1-3).\"?", "gold": "trajectory", "answer": "The exact term is **‚Äútrajectory‚Äù**, i.e., ‚Äúcomplex reasoning over the **trajectory** context (Question, Act 1-3, Obs 1-3).‚Äù", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'trajectory' and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 107.698, "llm_ms": 1411.034, "top_contexts": [{"rank": 1, "score": 0.70136213, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.5078600279758214, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 27, "text_snippet": "Learning a policy is challenging when the mapping ct‚Ü¶‚Üíatis highly implicit and requires extensive computation. For example, the agent shown in Figure 1(1c) is unable to generate the correct Ô¨Ånal action (Act 4) to Ô¨Ånish the QA task as it req"}, {"rank": 2, "score": 0.58793706, "source": "dense+rrf", "rrf_score": 0.02886002886002886, "rerank_score": 0.2711675268508157, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 29, "text_snippet": "he context ct+1= (ct,ÀÜat)to support future reasoning or acting. As shown in Figure 1, there could be various types of useful thoughts, e.g. decomposing task goals and create action plans (2b, Act 1; 1d, Thought 1), injecting commonsense kno"}, {"rank": 3, "score": 0.6000335, "source": "dense+rrf", "rrf_score": 0.029827662395050816, "rerank_score": 0.24792254505458797, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 49, "text_snippet": "odel with different prompting methods. We note that ReAct is better than Act on both tasks, demonstrating the value of reasoning to guide acting, especially for synthesizing the Ô¨Ånal answer, as shown in Figure 1 (1c-d). Fine-tuning results "}, {"rank": 4, "score": 0.63396096, "source": "dense+rrf", "rrf_score": 0.03128054740957967, "rerank_score": 0.24789461235904262, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 26, "text_snippet": "bining it with complementary paradigms like reinforcement learning could further unlock the potential of large language models. 2REAC T: SYNERGIZING REASONING +AC TING Consider a general setup of an agent interacting with an environment for"}, {"rank": 5, "score": 0.58485043, "source": "dense+rrf", "rrf_score": 0.02804284323271665, "rerank_score": 0.23969399455356463, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 36, "text_snippet": "easoning tasks like multi-hop question answering and fact veriÔ¨Åcation. As shown in Figure 1(1d), by interacting with a Wikipedia API, ReAct is able to retrieve information to support reasoning, while also use reasoning to target what to ret"}]}
{"case_index": 168, "query": "From 1706.03762__attention_is_all_you_need.txt, what exact term fills [BLANK] in: \"In the following sections, we will describe the Transformer, motivate [BLANK] and discuss its advantages over models such as [17, 18] and [9].\"?", "gold": "self-attention", "answer": "The exact term that fills **[BLANK]** is **‚Äúself-attention‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term and is explicitly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 109.913, "llm_ms": 1464.211, "top_contexts": [{"rank": 1, "score": 0.6604085, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3995163440555853, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 14, "text_snippet": "the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution. In the following sections, we will describe the Trans"}, {"rank": 2, "score": 20.85679079336809, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.21729893663252936, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 86, "text_snippet": "for head 5. Bottom: Isolated attentions from just the word ‚Äòits‚Äô for attention heads 5 and 6. Note that the attentions are very sharp for this word. 14  Input-Input Layer5 The Law will never be perfect , but its application should be just -"}, {"rank": 3, "score": 23.439335623708676, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.19529144574582818, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 87, "text_snippet": " Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <"}, {"rank": 4, "score": 20.633814260130624, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.1950153932958627, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 334, "text_snippet": "m To answer the query, we need to find the passages that mention procell batteries and summarize the information they provide. We can use the following steps: - Scan the passages for the term procell batteries or its variations (such as dur"}, {"rank": 5, "score": 23.213658360333906, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.19172536673367876, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 180, "text_snippet": "vide the exact prompts used for common NLP tasks in the following tables as well, including NLI (Table 18, Table 19, Table 20) and Closed-Book Question-Answering tasks (Table 16, Table 21). 20  Published as a conference paper at ICLR 2023 T"}]}
{"case_index": 169, "query": "From 2305.06983__active_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"Augmenting LMs by [BLANK] informa- tion from external knowledge resources is one promising solution.\"?", "gold": "retrieving", "answer": "The exact term is **‚Äúretrieving‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'retrieving' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 37.814, "llm_ms": 1067.785, "top_contexts": [{"rank": 1, "score": 0.7474675, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.32034319327026706, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 1, "text_snippet": "create factually inaccurate out- put. Augmenting LMs by retrieving informa- tion from external knowledge resources is one promising solution. Most existing retrieval aug- mented LMs employ a retrieve-and-generate setup that only retrieves i"}, {"rank": 2, "score": 0.67828816, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.2311344982260128, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 5, "text_snippet": "u et al., 2021). Augmenting LMs with retrieval components that look up relevant in- formation from external knowledge resources is a promising direction to address hallucination (Khan- delwal et al., 2020; Izacard et al., 2022). Retrieval a"}, {"rank": 3, "score": 0.59925544, "source": "dense+rrf", "rrf_score": 0.029957522915269395, "rerank_score": 0.19994958962158016, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 1, "text_snippet": "dels (LLMs) showcase impres- sive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution b"}, {"rank": 4, "score": 27.901862256777754, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.19694315966539197, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 16, "text_snippet": ". When deciding what to retrieve , it is important to consider what LMs intend to gen- erate in the future, as the goal of active retrieval is to benefit future generations. Therefore, we propose anticipating the future by generating a temp"}, {"rank": 5, "score": 30.646423028009814, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.19181627060994036, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 24, "text_snippet": "t, and the input to LMs is the concatena- tion of the retrieved documents Dqt, the user input x, and the previous generation y<t. We discard previously retrieved documents ‚à™t‚Ä≤<tDqt‚Ä≤and only use the retrieved documents from the current step "}]}
{"case_index": 170, "query": "From 2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt, what exact term fills [BLANK] in: \"Race tracks, desert, apartments, and [BLANK] don't have a lot of people, but populated areas do.\"?", "gold": "roadblocks", "answer": "The exact term that fills **[BLANK]** is **‚Äúroadblocks‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Model answer matches expected term 'roadblocks' and is directly supported by Context 1 sentence listing 'roadblocks'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 100.947, "llm_ms": 1686.095, "top_contexts": [{"rank": 1, "score": 0.44056994, "source": "dense+rrf", "rrf_score": 0.03177805800756621, "rerank_score": 0.29944079196629725, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 25, "text_snippet": " populated areas (c) desert (d) apartment (e) roadblock A: The answer must be a place with a lot of people. Race tracks, desert, apartments, and roadblocks don't have a lot of people, but populated areas do. So the answer is (b). Q: Yes or "}, {"rank": 2, "score": 40.677311948399975, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.24327262237411038, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 114, "text_snippet": "-augmented ensembles in language models. arXiv preprint arXiv:2207.00747 , 2022b. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models"}, {"rank": 3, "score": 43.53163859822278, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.2354288324099431, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 120, "text_snippet": "Zhou. 2022. Self- consistency improves chain of thought reasoning in language models. CoRR , abs/2203.11171. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicit"}, {"rank": 4, "score": 0.5003459, "source": "dense+rrf", "rrf_score": 0.031754032258064516, "rerank_score": 0.23471550149895848, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 24, "text_snippet": "her chain-of-thought prompting in this form can successfully elicit successful reasoning across a range of 3  Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have "}, {"rank": 5, "score": 38.44422509751427, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.22944350644416267, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 133, "text_snippet": " Proceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence , 32, Apr. 2018. URL https: //ojs.aaai.org/index.php/AAAI/article/view/12340 . Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and"}]}
{"case_index": 171, "query": "From 2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt, what exact term fills [BLANK] in: \"Inner product search has been widely used and studied, as well as its connection to cosine [BLANK] and L2 distance (Mussmann and Ermon, 2016; Ram\"?", "gold": "similarity", "answer": "The exact term is **‚Äúsimilarity‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "The context explicitly contains the phrase 'cosine similarity and L2 distance', so the blank is 'similarity'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 53.035, "llm_ms": 1179.163, "top_contexts": [{"rank": 1, "score": 0.5860026, "source": "dense+rrf", "rrf_score": 0.032018442622950824, "rerank_score": 0.3966583431497256, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 21, "text_snippet": "y function needs to be decomposable so that the represen- tations of the collection of passages can be pre- computed. Most decomposable similarity functions are some transformations of Euclidean distance (L2). For instance, cosine is equiva"}, {"rank": 2, "score": 0.587601, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.22388638809278472, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 22, "text_snippet": "osine similarity and L2 distance (Mussmann and Ermon, 2016; Ram and Gray, 2012). As our ablation study Ô¨Ånds other similarity functions perform compara- bly (Section 5.2; Appendix B), we thus choose the simpler inner product function and imp"}, {"rank": 3, "score": 29.23646654882694, "source": "dense+rrf", "rrf_score": 0.013888888888888888, "rerank_score": 0.2142752548211153, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 60, "text_snippet": "ore integrated with fine-tuning techniques. Unstructured Data , such as text, is the most widely used retrieval source, which are mainly gathered from corpus. For open-domain question-answering (ODQA) tasks, the primary retrieval sources ar"}, {"rank": 4, "score": 0.55270666, "source": "dense+rrf", "rrf_score": 0.029857397504456328, "rerank_score": 0.1982675475256338, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "-speciÔ¨Åc representation. With special in-memory data struc- tures and indexing schemes, retrieval can be done efÔ¨Åciently using maximum inner product search (MIPS) algorithms (e.g., Shrivastava and Li (2014); Guo et al. (2016)). However, it "}, {"rank": 5, "score": 0.5497735, "source": "dense+rrf", "rrf_score": 0.02919863597612958, "rerank_score": 0.19650967654697188, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 55, "text_snippet": "batch training. G.+BM25(1)and G.+BM25(2)denote in-batch training with 1 or 2 ad- ditional BM25 negatives, which serve as negative pas- sages for all questions in the batch. Our experiments on Natural Questions show that switching to distant"}]}
{"case_index": 172, "query": "From 2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt, what exact term fills [BLANK] in: \"As compute has gotten faster relative to memory speed [ 61,62,63], operations are [BLANK] bottlenecked by me\"?", "gold": "increasingly", "answer": "The exact term is **‚Äúincreasingly‚Äù** (i.e., ‚Äúoperations are increasingly bottlenecked by ‚Ä¶‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected blank ('increasingly') and is directly supported by the provided context sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 71.235, "llm_ms": 1283.537, "top_contexts": [{"rank": 1, "score": 69.23282788238959, "source": "dense+rrf", "rrf_score": 0.01639344262295082, "rerank_score": 0.3361788399551749, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 22, "text_snippet": "forms of memory of diÔ¨Äerent sizes and speeds, with smaller memory being faster. As an example, the A100 GPU has 40-80GB of high bandwidth memory (HBM) with bandwidth 1.5-2.0TB/s and 192KB of on-chip SRAM per each of 108 streaming multiproce"}, {"rank": 2, "score": 0.6748613, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.30381744690640394, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 10, "text_snippet": "mplementation of attention on GPT-2. FlashAttention does not read and write the large ùëÅ\u0002ùëÅattention matrix to HBM, resulting in an 7.6 \u0002 speedup on the attention computation. GPUs, compute speed has out-paced memory speed [ 61,62,63], and mo"}, {"rank": 3, "score": 54.73686116546747, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.24168852077457564, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 100, "text_snippet": "n, Atri Rudra, and Christopher R√©. 2022. FlashAttention: Fast and memory-efficient exact attention with IO- awareness. ArXiv:2205.14135. Hermann Ebbinghaus. 1913. Memory: A contribu- tion to experimental psychology. H. A. Ruger & C. E. Buss"}, {"rank": 4, "score": 44.59592527226366, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.2360604090317123, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 23, "text_snippet": "peed [ 61,62,63], operations are increasingly bottlenecked by memory (HBM) accesses. Thus exploiting fast SRAM becomes more important. Execution Model. GPUs have a massive number of threads to execute an operation (called a kernel). Each ke"}, {"rank": 5, "score": 44.868492064018824, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.22281702930633276, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 84, "text_snippet": "mputer/RedPajama-Data . Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. CoRR , abs/2307.08691, 2023. Tri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra, and Christopher R ¬¥e. Flashattention: Fast an"}]}
{"case_index": 173, "query": "From 2005.14165__language_models_are_few_shot_learners.txt, what exact term fills [BLANK] in: \"For all tasks, GPT-3 is applied without any gradient updates or Ô¨Åne-tuning, with tasks and few-shot [BLANK] speciÔ¨Åed purely via text interaction with the model.\"?", "gold": "demonstrations", "answer": "The exact term is **‚Äúdemonstrations‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 53.629, "llm_ms": 1070.797, "top_contexts": [{"rank": 1, "score": 0.61859417, "source": "dense+rrf", "rrf_score": 0.030679156908665108, "rerank_score": 0.43304331550445574, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 3, "text_snippet": "test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or Ô¨Åne-tuning, with tasks and few-shot demonstrations speciÔ¨Åed purely via text interaction with the model. GPT-3 achieves strong perf"}, {"rank": 2, "score": 0.65092516, "source": "dense+rrf", "rrf_score": 0.031754032258064516, "rerank_score": 0.30872238485898895, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 2, "text_snippet": "ew examples or from simple instructions ‚Äì something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competiti"}, {"rank": 3, "score": 36.75437110761886, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.27849855543542906, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 31, "text_snippet": " the number of examples in the model‚Äôs context, K. Few-shot learning also improves dramatically with model size. Though the results in this case are particularly striking, the general trends with both model size and number of examples in-co"}, {"rank": 4, "score": 30.607578718482532, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.2697030184147907, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 72, "text_snippet": " SFT models and GPT-3. We also compare to GPT-3 when it is provided a few-shot preÔ¨Åx to ‚Äòprompt‚Äô it into an instruction-following mode (GPT-3-prompted). This preÔ¨Åx is prepended to the user-speciÔ¨Åed instruction.6 We additionally compare Inst"}, {"rank": 5, "score": 28.70920099726877, "source": "dense+rrf", "rrf_score": 0.014084507042253521, "rerank_score": 0.2482456978396045, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 36, "text_snippet": "-to-right, decoder-only language model with 540-billion parameters, pre-trained on a high quality corpus of 780 billion tokens with Ô¨Åltered webpages, books, Wikipedia, news articles, source code, and social media conversations. We perform a"}]}
{"case_index": 174, "query": "From 2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt, what exact term fills [BLANK] in: \"4Exceptions include (Seo et al., 2019) and (Roberts et al., 2020), which retrieves andgenerates the answers, [BLANK].\"?", "gold": "respectively", "answer": "The exact term that fills **[BLANK]** is:\n\n**respectively**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'respectively' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 67.79, "llm_ms": 1064.365, "top_contexts": [{"rank": 1, "score": 0.6194006, "source": "dense+rrf", "rrf_score": 0.030679156908665108, "rerank_score": 0.3171284645910159, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 19, "text_snippet": "t passage are func- tions of both the retriever and reader. We also experimented with natural paragraphs in our preliminary trials and found that using Ô¨Åxed-length passages performs better in both retrieval and Ô¨Ånal QA accuracy, as observed"}, {"rank": 2, "score": 37.05294121755699, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.2622855276744024, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 8, "text_snippet": "/BM25 for open- domain QA before ORQA (Lee et al., 2019), which proposes a sophisticated inverse cloze task (ICT) objective, predicting the blocks that contain the masked sentence, for additional pretraining. The question encoder and the re"}, {"rank": 3, "score": 0.6491529, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.2447522382386606, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 81, "text_snippet": "the dual-encoder framework, they introduced a late-interaction operator on top of the BERT encoders. Dense retrieval for open-domain QA has been explored by Das et al. (2019), who propose to re- trieve relevant passages iteratively using re"}, {"rank": 4, "score": 29.17644951358666, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.22422524872190758, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 14, "text_snippet": "further extract answers from the concatenation of retrievals, which is normally around 30K tokens. We simply prompt an existing long-context LM (like Gemini or GPT4) with the question to produce the answers in a zero-shot fashion. 2  These "}, {"rank": 5, "score": 0.66930854, "source": "dense+rrf", "rrf_score": 0.031544957774465976, "rerank_score": 0.22297565822155538, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 99, "text_snippet": "las Joseph, Ben Mann, Sam McCandlish, Chris Olah, and Jared Kaplan. 2022. Language models (mostly) know what they know. CoRR , abs/2207.05221. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H. Lewis, Ledell Wu, Sergey Edunov, Danqi "}]}
{"case_index": 175, "query": "From 2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt, what exact term fills [BLANK] in: \"‚Äù A [BLANK] system would have difÔ¨Åculty retrieving such a context, while a dense retrieval system would be able to better match ‚Äúbad guy‚Äù with ‚Äúvillain‚Äù and fetch the cor- rect context.\"?", "gold": "term-based", "answer": "The exact term is **‚Äúterm-based‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term and is explicitly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 84.475, "llm_ms": 951.427, "top_contexts": [{"rank": 1, "score": 0.64976656, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.46239968288772343, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 6, "text_snippet": "which can be answered from the context ‚ÄúSala Baker is best known for portraying the villain Sauron in the Lord of the Rings trilogy. ‚Äù A term-based system would have difÔ¨Åculty retrieving such a context, while a dense retrieval system would "}, {"rank": 2, "score": 28.06341614161317, "source": "dense+rrf", "rrf_score": 0.014084507042253521, "rerank_score": 0.24203962864712794, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 98, "text_snippet": "Scaling in-context learning to 1, 000 examples. ArXiv, abs/2212.06713, 2022. Gautier Izacard and Edouard Grave. Distilling knowledge from reader to retriever for question answering. arXiv preprint arXiv:2012.04584 , 2020a. Gautier Izacard a"}, {"rank": 3, "score": 28.704722290755175, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.2241715690388412, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 125, "text_snippet": "ples and generate a longer answer, typically ranging from a few words to a few sentences. In the second turn, we use 8-shot in-context examples to calibrate and extract the exact short answer, which is typically just a few words. 16  A.2 Re"}, {"rank": 4, "score": 0.5374112, "source": "dense+rrf", "rrf_score": 0.027912386121341344, "rerank_score": 0.20938654565836562, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 85, "text_snippet": "eving good performance on open-domain QA and other knowledge-intensive tasks. 8 Conclusion In this work, we demonstrated that dense retrieval can outperform and potentially replace the tradi- tional sparse retrieval component in open-domain"}, {"rank": 5, "score": 0.55927587, "source": "dense+rrf", "rrf_score": 0.028991596638655463, "rerank_score": 0.20797842508125927, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 5, "text_snippet": "ently with an inverted index and can be seen as representing the question and context in high- dimensional, sparse vectors (with weighting). Con- versely, the dense , latent semantic encoding is com- plementary to sparse representations by "}]}
{"case_index": 176, "query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"We Ô¨Årst sample up to kcandidate posi- tions for doing API calls by computing, for each i‚àà{1,...,n}, the probability pi=pM(<API>|P(x),x1:i‚àí1) [BLANK] to starting an API call at position i.\"?", "gold": "thatmassigns", "answer": "The exact term is:\n\n**‚Äúthat M assigns‚Äù**", "answer_score": 0.3, "answer_correctness": 0.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context shows the blank is the concatenated term 'thatMassigns' (no spaces). Model answered 'that M assigns' with spaces, so it does not match the exact expected term, though it is supported by context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 90.528, "llm_ms": 1210.264, "top_contexts": [{"rank": 1, "score": 0.755107, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.4540038356711323, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 21, "text_snippet": "n+1as a continuation for the sequence z1,...,z n. We Ô¨Årst sample up to kcandidate posi- tions for doing API calls by computing, for each i‚àà{1,...,n}, the probability pi=pM(<API>|P(x),x1:i‚àí1) thatMassigns to starting an API call at position "}, {"rank": 2, "score": 0.58407235, "source": "dense+rrf", "rrf_score": 0.030090497737556562, "rerank_score": 0.26824301209760143, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 134, "text_snippet": "niel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. Opt: Open pre- trained transformer language models.  A API Details When sampling and Ô¨Åltering API calls, by default we use values of œÑs= 0.05andœÑf= 1.0‚Äì"}, {"rank": 3, "score": 0.58452415, "source": "dense+rrf", "rrf_score": 0.029850746268656716, "rerank_score": 0.2573053813782815, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 27, "text_snippet": "atterns in the pretraining corpus, thus hurting perplexity.  x1:i‚àí1,e(ci,ri),xi:n; we proceed analogously for texts with multiple API calls. Doing this for all x‚àà Cresults in the new dataset C‚àóaugmented with API calls. We use this new datas"}, {"rank": 4, "score": 0.617629, "source": "dense+rrf", "rrf_score": 0.031746031746031744, "rerank_score": 0.24797668488556054, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 20, "text_snippet": "te API calls for the question answering tool. Mitself on this dataset. Each of these steps is described in more detail below. Sampling API Calls For each API, we write a promptP(x)that encourages the LM to anno- tate an example x=x1,...,x n"}, {"rank": 5, "score": 0.550337, "source": "dense+rrf", "rrf_score": 0.027692895339954164, "rerank_score": 0.22635955805754038, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 1, "text_snippet": "metic or fac- tual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer , a model traine"}]}
{"case_index": 177, "query": "From 2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt, what exact term fills [BLANK] in: \"LongLoRA can [BLANK] Llama2 7B up to 100k context, or a 70B model up to 32k, on a single 8√óA100 machine.\"?", "gold": "fine-tune", "answer": "The exact term is **‚Äúfine-tune‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'fine-tune' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 66.601, "llm_ms": 1141.35, "top_contexts": [{"rank": 1, "score": 0.70691156, "source": "dense+rrf", "rrf_score": 0.032266458495966696, "rerank_score": 0.3839338640358059, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 20, "text_snippet": "embeddings. The trained models achieve comparable performance to the full-attention and fully fine-tuned results, while the computational cost is much less as shown in Figure 1. LongLoRA can fine-tune Llama2 7B up to 100k context, or a 70B "}, {"rank": 2, "score": 0.71147645, "source": "dense+rrf", "rrf_score": 0.032018442622950824, "rerank_score": 0.32978452190636093, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 58, "text_snippet": "lama2 13B model, we observe that the perplexity reduces by -0.28. In Table 4, we further examine the maximum context length that we can fine-tune on a single 8 √ó A100 machine. We extend Llama2 7B, 13B, and 70B to 100k, 65536, and 32768 cont"}, {"rank": 3, "score": 0.67059946, "source": "dense+rrf", "rrf_score": 0.03125763125763126, "rerank_score": 0.32798518131866866, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 3, "text_snippet": "ll under the premise of trainable embedding and normalization. LongLoRA combines this improved LoRA with S2-Attn. LongLoRA demonstrates strong empirical results on various tasks on Llama2 models from 7B/13B to 70B. LongLoRA extends Llama2 7"}, {"rank": 4, "score": 0.70832276, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.3256216123332332, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 77, "text_snippet": "l standard attention architecture during inference, making most pre-existing infrastructure and optimization reusable. At the training level, we bridge the gap between LoRA and full fine-tuning with trainable normalization and embedding. Ou"}, {"rank": 5, "score": 0.6404656, "source": "dense+rrf", "rrf_score": 0.02758751902587519, "rerank_score": 0.23959979819743166, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 51, "text_snippet": " We use AdamW (Loshchilov & Hutter, 2019) with Œ≤1= 0.9andŒ≤2= 0.95. The learning rate is set to 2√ó10‚àí5for 7B and 13B models, and 10‚àí5for 70B models. We also use a linear learning rate warmup. The weight decay is 6  Published as a conference "}]}
{"case_index": 178, "query": "From 2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt, what exact term fills [BLANK] in: \"On modern 1arXiv:2205.14135v2 [cs.LG] 23 Jun 2022 [BLANK] Hierarchy with Bandwidth & Memory SizeAttention on GPT-2 Flas\"?", "gold": "flashattentionmemory", "answer": "The exact term that fills **[BLANK]** is:\n\n**FlashAttentionMemory**", "answer_score": 0.3, "answer_correctness": 0.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context shows the term as \"FlashAttentionMemory\" (with capitalization), but expected answer is the exact string \"flashattentionmemory\". Answer does not match exact term requirement.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 50.351, "llm_ms": 1190.33, "top_contexts": [{"rank": 1, "score": 0.75670326, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.4029606437765324, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 7, "text_snippet": " with wall-clock speed) and tend to ignore overheads from memory access (IO). In this paper, we argue that a missing principle is making attention algorithms IO-aware [1]‚Äîthat is, carefully accounting for reads and writes to diÔ¨Äerent levels"}, {"rank": 2, "score": 0.69213843, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.24844766969602858, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 100, "text_snippet": "n, Atri Rudra, and Christopher R√©. 2022. FlashAttention: Fast and memory-efficient exact attention with IO- awareness. ArXiv:2205.14135. Hermann Ebbinghaus. 1913. Memory: A contribu- tion to experimental psychology. H. A. Ruger & C. E. Buss"}, {"rank": 3, "score": 0.6256002, "source": "dense+rrf", "rrf_score": 0.02871794871794872, "rerank_score": 0.22869175042884643, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 84, "text_snippet": "mputer/RedPajama-Data . Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. CoRR , abs/2307.08691, 2023. Tri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra, and Christopher R ¬¥e. Flashattention: Fast an"}, {"rank": 4, "score": 0.6687616, "source": "dense+rrf", "rrf_score": 0.030776515151515152, "rerank_score": 0.22422451123394033, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 0, "text_snippet": "FlashAttention : Fast and Memory-EÔ¨Écient Exact Attention with IO-Awareness Tri Daoy, Daniel Y. Fuy, Stefano Ermony, Atri Rudraz, and Christopher R√©y yDepartment of Computer Science, Stanford University zDepartment of Computer Science and En"}, {"rank": 5, "score": 0.6312308, "source": "dense+rrf", "rrf_score": 0.029957522915269395, "rerank_score": 0.20277231938883888, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 106, "text_snippet": "o solve math word problems. arXiv preprint arXiv:2110.14168 . Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Car- bonell, Quoc V Le, and Ruslan Salakhutdinov. 2019. Transformer-xl: Attentive language mod- els beyond a Ô¨Åxed-length context. arXi"}]}
{"case_index": 179, "query": "From 2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt, what exact term fills [BLANK] in: \"We design various types of questions for [BLANK] papers, science fiction, and other books.\"?", "gold": "technical", "answer": "The exact term is **‚Äútechnical‚Äù** (as in ‚Äútechnical papers‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context explicitly says 'questions for technical papers, science fiction, and other books.' Answer matches expected term.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 133.678, "llm_ms": 1010.911, "top_contexts": [{"rank": 1, "score": 0.6347383, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.31989715776942956, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 21, "text_snippet": " further fine-tuned with long questions and the corresponding answers. We design various types of questions for technical papers, science fiction, and other books. SFT is important for improving the chat ability of LLMs. We introduce our SF"}, {"rank": 2, "score": 34.99068039840905, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.22416371474720542, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 0, "text_snippet": "Published as a conference paper at ICLR 2024 LONG LORA: E FFICIENT FINE-TUNING OF LONG - CONTEXT LARGE LANGUAGE MODELS Yukang Chen1Shengju Qian1Haotian Tang2Xin Lai1 Zhijian Liu2Song Han2,3Jiaya Jia1 1CUHK2MIT3NVIDIA ABSTRACT We present Lon"}, {"rank": 3, "score": 26.967198611345516, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.20230364129353623, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 42, "text_snippet": " Child et al., 2019), designed for training from scratch, have gaps to the standard full attention, which is used in pre-training. In Table 6, we show that S2-Attn not only enables efficient fine-tuning but also supports fullattention testi"}, {"rank": 4, "score": 28.09405874774162, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.1946600247258666, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 20, "text_snippet": "embeddings. The trained models achieve comparable performance to the full-attention and fully fine-tuned results, while the computational cost is much less as shown in Figure 1. LongLoRA can fine-tune Llama2 7B up to 100k context, or a 70B "}, {"rank": 5, "score": 26.333550537027463, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.18877226125397062, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 107, "text_snippet": "eprint arXiv:2402.01613 , 2024. OpenAI. Hello gpt4-o. 2024. Bowen Peng and Jeffrey Quesnelle. Ntk-aware scaled rope allows llama models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation. https://"}]}
{"case_index": 180, "query": "From 2305.06983__active_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"2.1 Notations and [BLANK] Given a user input xand a document corpus D= {di}|D| i=1(such as all Wikipedia articles), the goal of retrieval augmented LMs is to generate the answer y= [s1,s2, ...,s\"?", "gold": "definitions", "answer": "The exact term is **‚ÄúDefinitions‚Äù** (i.e., ‚Äú2.1 Notations and Definitions‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context explicitly shows the section title '2.1 Notations and Definitions', so [BLANK]=definitions. Model answer matches exactly.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 60.608, "llm_ms": 1144.331, "top_contexts": [{"rank": 1, "score": 0.6806741, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.5798994860941897, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 19, "text_snippet": "- petitive performance compared to single-time and multi-time retrieval baselines, demonstrating the effectiveness and generalizability of our method. 2 Retrieval Augmented Generation We formally define single-time retrieval augmented gener"}, {"rank": 2, "score": 0.6204725, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.3170325195123387, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 20, "text_snippet": "trieval augmented LMs is to generate the answer y= [s1,s2, ...,sm] = [w1, w2, ..., w n]containing msentences or ntokens leveraging information retrieved from the corpus. In retrieval augmented LM, the LM typically pairs with a retriever tha"}, {"rank": 3, "score": 0.56834316, "source": "dense+rrf", "rrf_score": 0.030158730158730156, "rerank_score": 0.3041741430317124, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 22, "text_snippet": " generate the complete answer at once y=LM([Dx,x]). 2.3 Active Retrieval Augmented Generation To aid long-form generation with retrieval, we pro- pose active retrieval augmented generation. It is a generic framework that actively decides wh"}, {"rank": 4, "score": 0.6227974, "source": "dense+rrf", "rrf_score": 0.03128054740957967, "rerank_score": 0.26591717378065605, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 23, "text_snippet": "and previously generated output y<t= [y0, ...,yt‚àí1]: qt=qry(x,y<t), where qry(¬∑)is the query formulation function. At the beginning ( t= 1), the previous generation is empty ( y<1=‚àÖ), and the user input is used as the initial query ( q1=x)."}, {"rank": 5, "score": 0.56899226, "source": "dense+rrf", "rrf_score": 0.029418126757516764, "rerank_score": 0.2554354121134738, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 11, "text_snippet": "eddocumentsLMGeneration$%$%% Figure 1: An illustration of forward-looking active retrieval augmented generation (FLARE). Starting with the user input xand initial retrieval results Dx, FLARE iteratively generates a temporary next sentence ("}]}
{"case_index": 181, "query": "From 2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt, what exact term fills [BLANK] in: \"the input question to a [BLANK] vector, and retrieves kpassages of which vectors are the closest to the question vector.\"?", "gold": "d-dimensional", "answer": "The exact term is **‚Äúd-dimensional‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'd-dimensional' and is directly supported by the provided context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 70.464, "llm_ms": 1081.54, "top_contexts": [{"rank": 1, "score": 0.74933445, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.30842402065658325, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 20, "text_snippet": " the input question to a d-dimensional vector, and retrieves kpassages of which vectors are the closest to the question vector. We deÔ¨Åne the similarity between the question and the passage using the dot product of their vectors: sim(q,p) =E"}, {"rank": 2, "score": 0.6185043, "source": "dense+rrf", "rrf_score": 0.03128054740957967, "rerank_score": 0.2914054978685419, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 17, "text_snippet": "rieval accuracy , which is the fraction of ques- tions for whichCFcontains a span that answers the question. 3 Dense Passage Retriever (DPR) We focus our research in this work on improv- ing the retrieval component in open-domain QA. Given "}, {"rank": 3, "score": 0.62804484, "source": "dense+rrf", "rrf_score": 0.030798389007344232, "rerank_score": 0.26015993180184455, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 18, "text_snippet": "evant to the input question for the reader at run-time. Note that Mcan be very large (e.g., 21 million passages in our experiments, de- scribed in Section 4.1) and kis usually small, such as20‚Äì100. 3.1 Overview Our dense passage retriever ("}, {"rank": 4, "score": 0.57504624, "source": "dense+rrf", "rrf_score": 0.028594771241830064, "rerank_score": 0.2514724897114043, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 19, "text_snippet": "t passage are func- tions of both the retriever and reader. We also experimented with natural paragraphs in our preliminary trials and found that using Ô¨Åxed-length passages performs better in both retrieval and Ô¨Ånal QA accuracy, as observed"}, {"rank": 5, "score": 31.556084967625747, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.2451960030663677, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 81, "text_snippet": "the dual-encoder framework, they introduced a late-interaction operator on top of the BERT encoders. Dense retrieval for open-domain QA has been explored by Das et al. (2019), who propose to re- trieve relevant passages iteratively using re"}]}
{"case_index": 182, "query": "From 2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt, what exact term fills [BLANK] in: \"rieval accuracy , which is the fraction of ques- tions for [BLANK] a span that answers the question.\"?", "gold": "whichcfcontains", "answer": "The exact term that fills **[BLANK]** is **‚ÄúwhichCFcontains‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term exactly and is directly supported by Context 1 showing 'whichCFcontains'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 54.261, "llm_ms": 1082.592, "top_contexts": [{"rank": 1, "score": 0.6702811, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3808159493066269, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 17, "text_snippet": "rieval accuracy , which is the fraction of ques- tions for whichCFcontains a span that answers the question. 3 Dense Passage Retriever (DPR) We focus our research in this work on improv- ing the retrieval component in open-domain QA. Given "}, {"rank": 2, "score": 34.625960963663, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.27600951921906935, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 16, "text_snippet": ". As a result, any open-domain QA system needs to include an efÔ¨Åcient retriever component that can se- lect a small set of relevant texts, before applying the reader to extract the answer (Chen et al., 2017).4 Formally speaking, a retriever"}, {"rank": 3, "score": 0.53638244, "source": "dense+rrf", "rrf_score": 0.027364110201042444, "rerank_score": 0.24941265348686117, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 2, "text_snippet": "9% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.1 1 Introduction Open-domain question answering (QA) (V oorhees, 1999) is a t"}, {"rank": 4, "score": 34.92265277642143, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.2453915064876776, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 81, "text_snippet": "the dual-encoder framework, they introduced a late-interaction operator on top of the BERT encoders. Dense retrieval for open-domain QA has been explored by Das et al. (2019), who propose to re- trieve relevant passages iteratively using re"}, {"rank": 5, "score": 30.42612036635953, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.24025322317030065, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 82, "text_snippet": " ques- tions efÔ¨Åciently. Using additional pretraining with the objective that matches surrogates of questions and relevant passages, Lee et al. (2019) jointly train the question encoder and reader. Their approach outperforms the BM25 plus r"}]}
{"case_index": 183, "query": "From 2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt, what exact term fills [BLANK] in: \"Published as a conference paper at ICLR 2023 [BLANK] IMPROVES CHAIN OF THOUGHT REASONING IN LANGUAGE MODELS Xuezhi Wang‚Ä†‚Ä°Jason Wei‚Ä†Dale Schuurmans‚Ä†Quoc Le‚Ä†Ed H.\"?", "gold": "self-consistency", "answer": "SELF-CONSISTENCY", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "The blank is filled by the paper title term 'SELF-CONSISTENCY', matching expected 'self-consistency' and directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 63.09, "llm_ms": 1097.309, "top_contexts": [{"rank": 1, "score": 0.7432663, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.4704605869237562, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 0, "text_snippet": "Published as a conference paper at ICLR 2023 SELF-CONSISTENCY IMPROVES CHAIN OF THOUGHT REASONING IN LANGUAGE MODELS Xuezhi Wang‚Ä†‚Ä°Jason Wei‚Ä†Dale Schuurmans‚Ä†Quoc Le‚Ä†Ed H. Chi‚Ä† Sharan Narang‚Ä†Aakanksha Chowdhery‚Ä†Denny Zhou‚Ä†¬ß ‚Ä†Google Research, "}, {"rank": 2, "score": 0.6704259, "source": "dense+rrf", "rrf_score": 0.03125763125763126, "rerank_score": 0.3829430084274899, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 120, "text_snippet": "Zhou. 2022. Self- consistency improves chain of thought reasoning in language models. CoRR , abs/2203.11171. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicit"}, {"rank": 3, "score": 0.74782825, "source": "dense+rrf", "rrf_score": 0.03177805800756621, "rerank_score": 0.3783480555817278, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 133, "text_snippet": " Proceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence , 32, Apr. 2018. URL https: //ojs.aaai.org/index.php/AAAI/article/view/12340 . Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and"}, {"rank": 4, "score": 0.68062466, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.36905603871833853, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 113, "text_snippet": "on. arXiv preprint arXiv:1803.05355 , 2018. Lev S Vygotsky. Thinking and speech. The collected works of LS Vygotsky , 1:39‚Äì285, 1987. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdh- ery, and Denny "}, {"rank": 5, "score": 0.62959933, "source": "dense+rrf", "rrf_score": 0.030776515151515152, "rerank_score": 0.35964308263058803, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 109, "text_snippet": ", Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022. LaMDA: Language models for dialog applications. arXiv preprint arXiv:2201.08239 . Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and"}]}
{"case_index": 184, "query": "From 2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt, what exact term fills [BLANK] in: \"For [BLANK] reasoning, we parse the Ô¨Årst numerical part as the Ô¨Ånal answer after the model generates ‚ÄúThe answer is ‚Äù.\"?", "gold": "arithmetic", "answer": "The exact term is **‚Äúarithmetic‚Äù** (as in: ‚ÄúFor **arithmetic** reasoning, we parse the Ô¨Årst numerical part as the Ô¨Ånal answer after the model generates ‚ÄòThe answer is ‚Äô.‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'arithmetic' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 53.839, "llm_ms": 1654.025, "top_contexts": [{"rank": 1, "score": 0.67253447, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.31555228878374414, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 28, "text_snippet": "9; Wei et al., 2022; Chowdhery et al., 2022). However, we have found that even when the desired answer is Ô¨Åxed, introducing diversity in the reasoning processes can be highly beneÔ¨Åcial; therefore we leverage 1The parser is task dependent. F"}, {"rank": 2, "score": 35.30709895914406, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.2956531284417128, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 144, "text_snippet": "ed reasoning paths from the PaLM-540B model in Table 13. We see that the diversity in the additionally sampled reasoning paths indeed helps the model arrive at a more correct Ô¨Ånal answer after aggregation. A.1.2 R OBUSTNESS TO DIFFERENT SET"}, {"rank": 3, "score": 0.6136515, "source": "dense+rrf", "rrf_score": 0.029850746268656716, "rerank_score": 0.2851154123732661, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 6, "text_snippet": "aths that reach a correct answer (Stanovich & West, 2000). The more that deliberate thinking and analysis is required for a problem (Evans, 2010), the greater the diversity of reasoning paths that can recover the answer. Figure 1 illustrate"}, {"rank": 4, "score": 30.75344170803744, "source": "dense+rrf", "rrf_score": 0.014285714285714285, "rerank_score": 0.2795429284035663, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 11, "text_snippet": "mainder for $2 per egg, so in  total she sells 7 * $2 = $14 per day.  The answer is $14. The answer is $14. Greedy decode Figure 1: The self-consistency method contains three steps: (1) prompt a language model using chain-of-thought (CoT) p"}, {"rank": 5, "score": 0.5990163, "source": "dense+rrf", "rrf_score": 0.02758751902587519, "rerank_score": 0.2777347262430018, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 40, "text_snippet": " 4  Published as a conference paper at ICLR 2023 3.2 M AINRESULTS We report the results of self-consistency averaged over 10 runs, where we sampled 40 outputs independently from the decoder in each run. The baseline we compare to is chain-o"}]}
{"case_index": 185, "query": "From 2005.14165__language_models_are_few_shot_learners.txt, what exact term fills [BLANK] in: \"SpeciÔ¨Åcally, we evaluate GPT-3 on over two dozen NLP datasets, as well as several novel tasks designed to test rapid [BLANK]\"?", "gold": "adaptatio", "answer": "The exact term is **‚Äúadaptation‚Äù**.", "answer_score": 0.9299999999999999, "answer_correctness": 0.9, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.9, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context shows the phrase 'test rapid adaptatio[n]' i.e., 'adaptation'. Expected answer is truncated ('adaptatio'), but the full exact term in the sentence is 'adaptation'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 58.153, "llm_ms": 1087.023, "top_contexts": [{"rank": 1, "score": 0.647251, "source": "dense+rrf", "rrf_score": 0.032266458495966696, "rerank_score": 0.4242115766813483, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 28, "text_snippet": "ew-shot performance increases more rapidly, demonstrating that larger models are more proÔ¨Åcient at in-context learning. See Figure 3.8 for a more detailed analysis on SuperGLUE, a standard NLP benchmark suite. In this paper, we test this hy"}, {"rank": 2, "score": 0.6839413, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.3095591338445819, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 29, "text_snippet": " as well as several novel tasks designed to test rapid adaptation to tasks unlikely to be directly contained in the training set. For each task, we evaluate GPT-3 under 3 conditions: (a) ‚Äúfew-shot learning‚Äù, or in-context learning where we "}, {"rank": 3, "score": 29.564211303857448, "source": "dense+rrf", "rrf_score": 0.0136986301369863, "rerank_score": 0.2657781057159628, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 86, "text_snippet": "labelers. We omit GPT (prompted) from the evals on prompts submitted to GPT-3 models (left) as these prompts are already designed to perform well for GPT-3, as opposed to prompts submitted to InstructGPT models (right). 4.1 Results on the A"}, {"rank": 4, "score": 0.68614554, "source": "dense+rrf", "rrf_score": 0.032266458495966696, "rerank_score": 0.2530517726572242, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 3, "text_snippet": "test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or Ô¨Åne-tuning, with tasks and few-shot demonstrations speciÔ¨Åed purely via text interaction with the model. GPT-3 achieves strong perf"}, {"rank": 5, "score": 0.611979, "source": "dense+rrf", "rrf_score": 0.027972027972027972, "rerank_score": 0.24483007244307084, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 146, "text_snippet": " several tasks that involve rearranging or unscrambling the letters in a word, tasks which are unlikely to have been exactly seen during training. Third, we test GPT-3‚Äôs ability to solve SAT-style analogy problems few-shot. Finally, we test"}]}
{"case_index": 186, "query": "From 2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt, what exact term fills [BLANK] in: \"However, this ‚Äú[BLANK]‚Äù reasoning is a static black box, in that the model uses its own internal representations to generate thoughts and is not grounded in the external world, which limits its ability to reason reactively or update its knowledge.\"?", "gold": "chain-of-thought", "answer": "The exact term is **‚Äúchain-of-thought‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "The blank is explicitly filled by 'chain-of-thought' in the provided context, and the model answer matches exactly.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 125.938, "llm_ms": 1586.427, "top_contexts": [{"rank": 1, "score": 0.6935313, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.5104339075562805, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 16, "text_snippet": "nerated by the model (Act, Thought) and the environment (Obs). answers from questions in arithmetic, commonsense, and symbolic reasoning tasks (Wei et al., 2022). However, this ‚Äúchain-of-thought‚Äù reasoning is a static black box, in that the"}, {"rank": 2, "score": 33.44702859532284, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.25095556017328186, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 76, "text_snippet": " or ambiguous, we rely on judgment from our labelers, and our main metric is labeler preference ratings. However, since our labelers are not the users who generated the prompts, there could be a divergence between what a user actually inten"}, {"rank": 3, "score": 0.57110417, "source": "dense+rrf", "rrf_score": 0.027809742999616416, "rerank_score": 0.2353980891523464, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 28, "text_snippet": " 1, thus keep producing hallucinating actions. The idea of ReAct is simple: we augment the agent‚Äôs action space to ÀÜA=A‚à™L , whereLis the space of language. An action ÀÜat‚ààL in the language space, which we will refer to as a thought or a reas"}, {"rank": 4, "score": 0.60942936, "source": "dense+rrf", "rrf_score": 0.028438886647841874, "rerank_score": 0.23166085047597695, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 86, "text_snippet": "n the visual environments for Ô¨Ånal prediction. Inner Monologue made further improvements by adding the eponymous ‚Äúinner monologue\", which is implemented as injected feedback from the environment. To our knowledge, Inner Monologue is the Ô¨Års"}, {"rank": 5, "score": 0.608293, "source": "dense+rrf", "rrf_score": 0.028991596638655463, "rerank_score": 0.22218556844994322, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 19, "text_snippet": "udies on how reasoning and acting can be combined in a synergistic manner for general task solving, and if such a combination can bring systematic beneÔ¨Åts compared to reasoning or acting alone. In this work, we present ReAct , a general par"}]}
{"case_index": 187, "query": "From 2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt, what exact term fills [BLANK] in: \"remove all training documents with high similarity (0.8 or higher) to a [BLANK] or test set document.\"?", "gold": "validation", "answer": "The exact term is **‚Äúvalidation‚Äù** (i.e., ‚Äúremove all training documents with high similarity (0.8 or higher) to a **validation** or test set document.‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches the context exactly: the phrase is 'validation or test set document.'", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 42.84, "llm_ms": 1486.403, "top_contexts": [{"rank": 1, "score": 0.63676465, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3754541921256616, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 21, "text_snippet": "remove all training documents with high similarity (0.8 or higher) to a validation or test set document. Additionally, we remove all validation and test articles from Wikitext103 (Merity et al., 2017) from our Wikipedia training data. 2.2. "}, {"rank": 2, "score": 0.5892596, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.3002527643031909, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 20, "text_snippet": "f the same subsets as the training data, in proportion that matches the training sampling frequencies. During evaluation the retrieval database consists in the full union of these datasets, with the exception of books for which we use a sub"}, {"rank": 3, "score": 0.5641959, "source": "dense+rrf", "rrf_score": 0.031024531024531024, "rerank_score": 0.2827718282538237, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 78, "text_snippet": "struct the dataset with articles from the ‚Äúfuture‚Äù and manually remove new articles that strongly overlap documents in our training data. This guarantees that the evaluation documents are not leaked in our training data. 10  Improving langu"}, {"rank": 4, "score": 32.44795514915959, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.25975250224481633, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 69, "text_snippet": "e stem from their ability to memorise training data, their high training cost, the static nature of their training data (Lazaridou et al., 2021), their tendency of amplifying inherent biases in the training data, and their ability to genera"}, {"rank": 5, "score": 28.97712885403631, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.25304987181275973, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 113, "text_snippet": "formers ( R/e.sc/t.sc/r.sc/o.sc), a method for modelling arbitrary text se- quences whilstretrievingfromdatabaseswithtrillions oftokens‚Äîscalingthedataavailable to models by an order of magnitude compared to what is typically consumed during"}]}
{"case_index": 188, "query": "From 2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt, what exact term fills [BLANK] in: \"This step is crucial for enabling efficient similarity searches in the [BLANK] retrieval phase.\"?", "gold": "subsequent", "answer": "The exact term is **‚Äúsubsequent‚Äù**, as in: ‚Äúefficient similarity searches in the **subsequent** retrieval phase.‚Äù", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'subsequent' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 56.021, "llm_ms": 1302.585, "top_contexts": [{"rank": 1, "score": 47.5606507131782, "source": "dense+rrf", "rrf_score": 0.01639344262295082, "rerank_score": 0.28895951192057745, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 21, "text_snippet": "to smaller, digestible chunks. Chunks are then encoded into vector representations using an embedding model and stored in vector database. This step is crucial for enabling efficient similarity searches in the subsequent retrieval phase. Re"}, {"rank": 2, "score": 20.369770954365954, "source": "dense+rrf", "rrf_score": 0.014285714285714285, "rerank_score": 0.17793238667010244, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 111, "text_snippet": "val augmented language models. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea Madotto, and Pascale Fung. 2022. Survey of hallu- cination in natural language generation. ACM Com- puting Surv"}, {"rank": 3, "score": 20.97899025083508, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.17709371476099942, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 32, "text_snippet": "g‚Ä≤, including 512 tokens, 4K tokens, and encoding the entire gcompletely. The empirical study about this settings is in Table 4. With this similarity score setup, we will retrieve the top kretrieval units closest to the given query. For eff"}, {"rank": 4, "score": 23.027800791910412, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.1743607355461698, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 0, "text_snippet": "1 Retrieval-Augmented Generation for Large Language Models: A Survey Yunfan Gaoa, Yun Xiongb, Xinyu Gaob, Kangxiang Jiab, Jinliu Panb, Yuxi Bic, Yi Daia, Jiawei Suna, Meng Wangc, and Haofen Wanga,c aShanghai Research Institute for Intellige"}, {"rank": 5, "score": 20.458032509978615, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.17269853775380184, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 11, "text_snippet": "h a detailed and structured understanding of both large models and RAG. It aims to illuminate the evolution of retrieval augmentation techniques, assess the strengths and weaknesses of various approaches in their respective contexts, and sp"}]}
{"case_index": 189, "query": "From 2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt, what exact term fills [BLANK] in: \"One limitation of these works is that these [BLANK] have a large gap to full attention, making it infeasible to\"?", "gold": "compressions", "answer": "The exact term is **‚Äúcompressions‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'compressions' and is directly supported by Context 1 sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 52.073, "llm_ms": 1130.338, "top_contexts": [{"rank": 1, "score": 0.62550104, "source": "dense+rrf", "rrf_score": 0.03028233151183971, "rerank_score": 0.3229420369600373, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 23, "text_snippet": " et al., 2022; Ding et al., 2023; Qiu et al., 2020). They alleviate the quadratic complexity of the self-attention computation. For example, Longformer (Beltagy et al., 2020) and BigBird (Zaheer et al., 2020) use sparse attention to handle "}, {"rank": 2, "score": 0.6433313, "source": "dense+rrf", "rrf_score": 0.03125763125763126, "rerank_score": 0.2982019039554625, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 75, "text_snippet": "ed in training-from- scratch transformers. This experiment is to examine their capability of fine-tuning on pre-trained LLMs (Touvron et al., 2023b), toward long context adaptation. Dilated attention performs well in full fine-tuning but is"}, {"rank": 3, "score": 0.71210074, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.2683659632593882, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 24, "text_snippet": "ons have a large gap to full attention, making it infeasible to fine-tune pre-trained LLMs. Although our work also involves an approximation of attention mechanism, it has a similar shape and a small gap to standard attention. This enables "}, {"rank": 4, "score": 0.6484123, "source": "dense+rrf", "rrf_score": 0.030798389007344232, "rerank_score": 0.25396453947078396, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 41, "text_snippet": "eads respectively. This manner does not increase additional computation costs but enables the information flow between different groups. We show that it gets close to the standard attention baseline in Table 1. Consistency to Full Attention"}, {"rank": 5, "score": 0.6416578, "source": "dense+rrf", "rrf_score": 0.030303030303030304, "rerank_score": 0.24845305305649162, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 42, "text_snippet": " Child et al., 2019), designed for training from scratch, have gaps to the standard full attention, which is used in pre-training. In Table 6, we show that S2-Attn not only enables efficient fine-tuning but also supports fullattention testi"}]}
{"case_index": 190, "query": "From 2302.13971__llama_open_and_efficient_foundation_language_models.txt, what exact term fills [BLANK] in: \"We use 2,000warmup 0 200 400 600 800 1000 1200 1400 Billion of tokens1.51.61.71.81.92.02.12.2Training [BLANK] 7B LLaMA 13B LLaMA 33B LLaMA 65BFigure 1: Training loss over train tokens for the 7B, 13B, 33B, and 65 models.\"?", "gold": "lossllama", "answer": "The exact term that fills **[BLANK]** is:\n\n**loss**", "answer_score": 0.3, "answer_correctness": 0.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context shows the phrase 'Training loss' before the model names, so 'loss' is supported. However, the expected answer is 'lossllama', and the model answer omitted 'llama', so it does not match the expected exact term.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 67.09, "llm_ms": 1345.644, "top_contexts": [{"rank": 1, "score": 0.7448874, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.5340843755372021, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 23, "text_snippet": "of 1.0. We use 2,000warmup 0 200 400 600 800 1000 1200 1400 Billion of tokens1.51.61.71.81.92.02.12.2Training lossLLaMA 7B LLaMA 13B LLaMA 33B LLaMA 65BFigure 1: Training loss over train tokens for the 7B, 13B, 33B, and 65 models. LLaMA-33B"}, {"rank": 2, "score": 60.0654531413079, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.2564185162196611, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 22, "text_snippet": " (2021), at each layer of the network. The details of the hyper-parameters for our dif- ferent models are given in Table 2. 2.3 Optimizer Our models are trained using the AdamW opti- mizer (Loshchilov and Hutter, 2017), with the fol- lowing"}, {"rank": 3, "score": 46.875470233158175, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.22522904717567915, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 58, "text_snippet": "0758085HellaSwag 0 250 500 750 1000 1250 150005101520253035NaturalQuestions 0 250 500 750 1000 1250 1500 Billion of tokens40424446485052AccuracySIQA 0 250 500 750 1000 1250 1500 Billion of tokens50556065707580WinoGrande 0 250 500 750 1000 1"}, {"rank": 4, "score": 36.27440281092361, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.20902592054989566, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 138, "text_snippet": "fferent Masking Procedures In Section 3.1, we mention that BERT uses a mixed strategy for masking the target tokens when pre-training with the masked language model (MLM) objective. The following is an ablation study to evaluate the effect "}, {"rank": 5, "score": 0.6100452, "source": "dense+rrf", "rrf_score": 0.014285714285714285, "rerank_score": 0.20146867545407376, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 137, "text_snippet": "s) of Llama-2 models of varying sizes (7B, 13B, 70B parameters), with and without additional su- pervised fine-tuning and reinforcement learning from human feedback (‚Äú -chat- ‚Äù models).  F Token Counts Table 2, Table 3, and Table 4 present "}]}
{"case_index": 191, "query": "From 2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt, what exact term fills [BLANK] in: \"First, a language model is prompted with a set of manually written [BLANK] exemplars (Wei et al., 2022).\"?", "gold": "chain-of-thought", "answer": "The exact term is **‚Äúchain-of-thought‚Äù** (as in ‚Äúmanually written **chain-of-thought exemplars**‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term and is directly supported by Context 1 ('manually written chain-of-thought exemplars').", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 40.227, "llm_ms": 1164.25, "top_contexts": [{"rank": 1, "score": 0.64956546, "source": "dense+rrf", "rrf_score": 0.03177805800756621, "rerank_score": 0.3725457781751564, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 18, "text_snippet": "., in Output 2), but such solutions are less likely to arrive at the same answer. That is, we hypothesize that correct reasoning processes, even if they are diverse, tend to have greater agreement in their Ô¨Ånal answer than incorrect process"}, {"rank": 2, "score": 38.76968870963002, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.3092746782728582, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 40, "text_snippet": " 4  Published as a conference paper at ICLR 2023 3.2 M AINRESULTS We report the results of self-consistency averaged over 10 runs, where we sampled 40 outputs independently from the decoder in each run. The baseline we compare to is chain-o"}, {"rank": 3, "score": 47.79768024744302, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.2924054020147941, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 109, "text_snippet": ", Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022. LaMDA: Language models for dialog applications. arXiv preprint arXiv:2201.08239 . Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and"}, {"rank": 4, "score": 0.63147426, "source": "dense+rrf", "rrf_score": 0.030117753623188408, "rerank_score": 0.2851688138441724, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 5, "text_snippet": "odel performance across a variety of multi-step reasoning tasks (Wei et al., 2022). In this paper, we introduce a novel decoding strategy called self-consistency to replace the greedy decoding strategy used in chain-of-thought prompting (We"}, {"rank": 5, "score": 0.6362523, "source": "dense+rrf", "rrf_score": 0.02862400327131466, "rerank_score": 0.2776805661138588, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 79, "text_snippet": "pecialized approaches for improving reasoning (Andor et al., 2019; Ran et al., 2019; Geva et al., 2020; PiÀõ ekos et al., 2021). Compared to prior work, self-consistency is applicable to a wide range of reasoning tasks without any additional"}]}
{"case_index": 192, "query": "From 2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"First, LLMs are not able to answer [BLANK] about events that have happened after they were trained.\"?", "gold": "questions", "answer": "The exact term that fills **[BLANK]** is **‚Äúquestions‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'questions' and is directly supported by Context 1 sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 53.883, "llm_ms": 1260.995, "top_contexts": [{"rank": 1, "score": 0.65826327, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.31280175229173546, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 4, "text_snippet": "ent Large Language Models (LLMs) capture enough knowledge to rival human performance across a wide variety of question answering bench- marks (Bubeck et al., 2023), the idea of using LLMs as knowledge bases still has two fundamen- tal limit"}, {"rank": 2, "score": 21.605822932345603, "source": "dense+rrf", "rrf_score": 0.014084507042253521, "rerank_score": 0.19137870972485935, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 348, "text_snippet": "ed that identifying as lesbian gave me access to the community and support that id been craving. the term itself just made me feel right in myself - lesbian is a political identity and not just about who you have sex with. i am a feminist a"}, {"rank": 3, "score": 23.950947686186495, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.17354564771134998, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 25, "text_snippet": "n when they do not have the necessary knowledge to avoid unnecessary or inappropriate retrieval, and (2) the retrieval queries should reflect the intents of future generations. We propose two forward-looking active retrieval augmented gener"}, {"rank": 4, "score": 31.67475227445187, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.17200513236829607, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 9, "text_snippet": "we present Ragas1, a framework for the automated assessment 1Ragas is available at https://github.com/ explodinggradients/ragas .arXiv:2309.15217v2 [cs.CL] 28 Apr 2025  of retrieval augmented generation systems. We fo- cus on settings where"}, {"rank": 5, "score": 0.55975837, "source": "dense+rrf", "rrf_score": 0.031009615384615385, "rerank_score": 0.17077603859461732, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 4, "text_snippet": "l, retrieval-augmented gen- eration, natural language processing, information retrieval I. I NTRODUCTION LARGE language models (LLMs) have achieved remark- able success, though they still face significant limitations, especially in domain-s"}]}
{"case_index": 193, "query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \"rs, and all of the param- eters are Ô¨Åne-tuned using labeled data from the [BLANK] tasks.\"?", "gold": "downstream", "answer": "The exact term is **‚Äúdownstream‚Äù** (i.e., ‚Äú‚Ä¶using labeled data from the **downstream** tasks.‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'downstream' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 63.128, "llm_ms": 1219.156, "top_contexts": [{"rank": 1, "score": 0.6131934, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.32742962854894286, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 23, "text_snippet": "rs, and all of the param- eters are Ô¨Åne-tuned using labeled data from the downstream tasks. Each downstream task has sep- arate Ô¨Åne-tuned models, even though they are ini- tialized with the same pre-trained parameters. The question-answerin"}, {"rank": 2, "score": 0.59969383, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.324753945191684, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 22, "text_snippet": "dels, where an effective recipe is to Ô¨Åne-tune models pre-trained with Ima- geNet (Deng et al., 2009; Yosinski et al., 2014). 3 BERT We introduce BERT and its detailed implementa- tion in this section. There are two steps in our framework: "}, {"rank": 3, "score": 34.78367133429458, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.29049342434032904, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 109, "text_snippet": "reading books. In Proceedings of the IEEE international conference on computer vision , pages 19‚Äì27. Appendix for ‚ÄúBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding‚Äù We organize the appendix into three section"}, {"rank": 4, "score": 45.24568943893777, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.266355281158505, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 476, "text_snippet": "heng, and Jingjing Liu. Uniter: Learning universal image-text representations. arXiv preprint arXiv:1909.11740 , 2019. [Cra17] Kate Crawford. The trouble with bias. NIPS 2017 Keynote , 2017. [DCLT18] Jacob Devlin, Ming-Wei Chang, Kenton Lee"}, {"rank": 5, "score": 0.5620816, "source": "dense+rrf", "rrf_score": 0.028258706467661692, "rerank_score": 0.25183472325219275, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 1, "text_snippet": ", 2018), BERT is designed to pre- train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a re- sult, the pre-trained BERT model can be Ô¨Åne- tuned with just one a"}]}
{"case_index": 194, "query": "From 2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt, what exact term fills [BLANK] in: \"nput context and the position of the relevant information and measure changes in task [BLANK].\"?", "gold": "performance", "answer": "The exact term is **‚Äúperformance‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'performance' and is directly supported by Context 3 phrase 'measure changes in task performance.'", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.3333333333333333, "hit_rank": 3, "matched_by": "doc_id+chunk_id", "retrieval_ms": 70.602, "llm_ms": 1179.144, "top_contexts": [{"rank": 1, "score": 0.622517, "source": "dense+rrf", "rrf_score": 0.029513888888888888, "rerank_score": 0.3235294446484676, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 49, "text_snippet": "UIDs. The relevant key-value pair for answering the query is bolded here within the input context for clarity. indicate that extended-context models are not nec- essarily better than their non-extended counterparts at using their input cont"}, {"rank": 2, "score": 0.6467291, "source": "dense+rrf", "rrf_score": 0.030776515151515152, "rerank_score": 0.2891785726330865, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 2, "text_snippet": "urs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts, even for explicitly long-context models. Our analysis provides a better understanding "}, {"rank": 3, "score": 0.6269379, "source": "dense+rrf", "rrf_score": 0.030886196246139225, "rerank_score": 0.28410057276108147, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 23, "text_snippet": "nput context and the position of the relevant information and measure changes in task performance. 2.1 Experimental Setup In the multi-document question answering task, the model inputs are (i) a question to answer and (ii) k documents (e.g"}, {"rank": 4, "score": 0.62027955, "source": "dense+rrf", "rrf_score": 0.02821939586645469, "rerank_score": 0.28356721268514457, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 58, "text_snippet": "s at the very start or end of the context, and rapidly degrades when models must retrieve from the middle of the input context. placed at the start of the input context, LongChat- 13B (16K) tends to generate code to retrieve the key, rather"}, {"rank": 5, "score": 0.6262858, "source": "dense+rrf", "rrf_score": 0.02967032967032967, "rerank_score": 0.2790235814267949, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 16, "text_snippet": "that occur in the middle of their input context and continue to exhibit a U-shaped performance curve. To better understand why language models strug- gle to robustly access and use information in their input contexts, we study the role of m"}]}
{"case_index": 195, "query": "From 2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt, what exact term fills [BLANK] in: \"Although some models perform the [BLANK] key-value retrieval task perfectly, other models struggle to simply retrieve matching tokens that occur in the middle of their input context and continue to\"?", "gold": "synthetic", "answer": "The exact term is **‚Äúsynthetic‚Äù** (as in ‚Äúthe **synthetic** key-value retrieval task‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "The blank is 'synthetic', explicitly stated in the provided context ('synthetic key-value retrieval task').", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 60.365, "llm_ms": 1217.158, "top_contexts": [{"rank": 1, "score": 0.6174228, "source": "dense+rrf", "rrf_score": 0.03047794966520434, "rerank_score": 0.4639118056042724, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 15, "text_snippet": " are given a collection of JSON-formatted key-value pairs and must return the value associated with a specific key. Similar to the multi-document QA task, the key-value retrieval task admits controlled changes to the input context length (a"}, {"rank": 2, "score": 0.6485443, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.3858020150936766, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 49, "text_snippet": "UIDs. The relevant key-value pair for answering the query is bolded here within the input context for clarity. indicate that extended-context models are not nec- essarily better than their non-extended counterparts at using their input cont"}, {"rank": 3, "score": 0.6916083, "source": "dense+rrf", "rrf_score": 0.032266458495966696, "rerank_score": 0.34860423989747186, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 14, "text_snippet": "r at using their input context (¬ß2.3). Given that language models struggle to retrieve and use relevant information in the multi-document question answering task, to what extent can lan- guage models even retrieve from their input con- text"}, {"rank": 4, "score": 0.6820662, "source": "dense+rrf", "rrf_score": 0.03128054740957967, "rerank_score": 0.3031791050651459, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 58, "text_snippet": "s at the very start or end of the context, and rapidly degrades when models must retrieve from the middle of the input context. placed at the start of the input context, LongChat- 13B (16K) tends to generate code to retrieve the key, rather"}, {"rank": 5, "score": 0.6183854, "source": "dense+rrf", "rrf_score": 0.029910714285714284, "rerank_score": 0.27948752636026875, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 50, "text_snippet": "wering task, to what extent can they simply retrieve from input contexts? We study this question with a syn- thetic key-value retrieval task, which is designed to provide a minimal testbed for the basic ability to retrieve matching tokens f"}]}
{"case_index": 196, "query": "From 2305.06983__active_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"[BLANK]$%$%% Figure 1: An illustration of forward-looking active retrieval augmented generation (FLARE).\"?", "gold": "eddocumentslmgeneration", "answer": "The exact term that fills **[BLANK]** is:\n\n**eddocumentsLMGeneration**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term and is directly supported by Context 1 showing 'eddocumentsLMGeneration$%$%%' before Figure 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 46.098, "llm_ms": 1142.563, "top_contexts": [{"rank": 1, "score": 0.67704284, "source": "dense+rrf", "rrf_score": 0.03177805800756621, "rerank_score": 0.2792848297638746, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 11, "text_snippet": "eddocumentsLMGeneration$%$%% Figure 1: An illustration of forward-looking active retrieval augmented generation (FLARE). Starting with the user input xand initial retrieval results Dx, FLARE iteratively generates a temporary next sentence ("}, {"rank": 2, "score": 40.30507690584842, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.2590666660783386, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 28, "text_snippet": "University)][Search(Joe Biden degree)]the University of Pennsylvania, where he earneda law degree.Generate a summary about Joe Biden.Input$&$&#%$&%%%GenerationRetriever$%$%%Figure 2: An illustration of forward-looking active re- trieval aug"}, {"rank": 3, "score": 0.69925225, "source": "dense+rrf", "rrf_score": 0.0315136476426799, "rerank_score": 0.2405856945914982, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 16, "text_snippet": ". When deciding what to retrieve , it is important to consider what LMs intend to gen- erate in the future, as the goal of active retrieval is to benefit future generations. Therefore, we propose anticipating the future by generating a temp"}, {"rank": 4, "score": 0.63784313, "source": "dense+rrf", "rrf_score": 0.03007688828584351, "rerank_score": 0.2339365305764381, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 24, "text_snippet": "t, and the input to LMs is the concatena- tion of the retrieved documents Dqt, the user input x, and the previous generation y<t. We discard previously retrieved documents ‚à™t‚Ä≤<tDqt‚Ä≤and only use the retrieved documents from the current step "}, {"rank": 5, "score": 0.6820776, "source": "dense+rrf", "rrf_score": 0.031746031746031744, "rerank_score": 0.2152423934960377, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 25, "text_snippet": "n when they do not have the necessary knowledge to avoid unnecessary or inappropriate retrieval, and (2) the retrieval queries should reflect the intents of future generations. We propose two forward-looking active retrieval augmented gener"}]}
{"case_index": 197, "query": "From 2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt, what exact term fills [BLANK] in: \"Finally, we aggregate the answers by [BLANK] out the sampled reasoning paths and choosing the answer that is the most consistent among the generated answers.\"?", "gold": "marginalizing", "answer": "The exact term is **‚Äúmarginalizing‚Äù** (as in: ‚Äúaggregate the answers by **marginalizing** out the sampled reasoning paths‚Ä¶‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Model answer matches expected term 'marginalizing' and is directly supported by the provided context quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 59.583, "llm_ms": 1936.274, "top_contexts": [{"rank": 1, "score": 0.75627494, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.40722064885376646, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 21, "text_snippet": "ampling (Fan et al., 2018; Holtzman et al., 2018; Radford et al., 2019), and nucleus sampling (Holtzman et al., 2020). Finally, we aggregate the answers by marginalizing out the sampled reasoning paths and choosing the answer that is the mo"}, {"rank": 2, "score": 55.83413331522398, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.3516286514990791, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 11, "text_snippet": "mainder for $2 per egg, so in  total she sells 7 * $2 = $14 per day.  The answer is $14. The answer is $14. Greedy decode Figure 1: The self-consistency method contains three steps: (1) prompt a language model using chain-of-thought (CoT) p"}, {"rank": 3, "score": 0.7103455, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.312023388810376, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 1, "text_snippet": "aper, we propose a new decoding strategy, self-consistency , to replace the naive greedy decoding used in chain-of-thought prompting. It Ô¨Årst samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects t"}, {"rank": 4, "score": 0.6646787, "source": "dense+rrf", "rrf_score": 0.029850746268656716, "rerank_score": 0.28717000535852316, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 5, "text_snippet": "odel performance across a variety of multi-step reasoning tasks (Wei et al., 2022). In this paper, we introduce a novel decoding strategy called self-consistency to replace the greedy decoding strategy used in chain-of-thought prompting (We"}, {"rank": 5, "score": 0.6254962, "source": "dense+rrf", "rrf_score": 0.02690501986276634, "rerank_score": 0.2848592823916549, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 86, "text_snippet": "ength sequence in recurrent language models. Nye et al. (2021) improve the logical consistency of samples from a System 1 model by adding a System 2-inspired logical reasoning module. In this paper we focus on a slightly different notion of"}]}
{"case_index": 198, "query": "From 2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt, what exact term fills [BLANK] in: \"ng low-rank matrices, which are generally efficient and reduce the number of trainable [BLANK].\"?", "gold": "parameters", "answer": "The exact term is **‚Äúparameters‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'parameters' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 46.29, "llm_ms": 1110.69, "top_contexts": [{"rank": 1, "score": 0.6779431, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.33248606280223963, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 11, "text_snippet": "ng low-rank matrices, which are generally efficient and reduce the number of trainable parameters. However, our empirical findings indicate that training long context models in this manner is neither sufficiently effective nor efficient. In"}, {"rank": 2, "score": 0.6125429, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.246549632102733, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 10, "text_snippet": "al., 2023) used 32 TPUs for standard transformer training and 128 TPUs for LongLLaMA. These computation resources are typically unaffordable for common researchers, which naturally leads us to question: can we extend the context window of L"}, {"rank": 3, "score": 35.74207978896411, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.2366614958094053, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 75, "text_snippet": "ed in training-from- scratch transformers. This experiment is to examine their capability of fine-tuning on pre-trained LLMs (Touvron et al., 2023b), toward long context adaptation. Dilated attention performs well in full fine-tuning but is"}, {"rank": 4, "score": 34.23552320145783, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.2240629082955925, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 0, "text_snippet": "Published as a conference paper at ICLR 2024 LONG LORA: E FFICIENT FINE-TUNING OF LONG - CONTEXT LARGE LANGUAGE MODELS Yukang Chen1Shengju Qian1Haotian Tang2Xin Lai1 Zhijian Liu2Song Han2,3Jiaya Jia1 1CUHK2MIT3NVIDIA ABSTRACT We present Lon"}, {"rank": 5, "score": 33.928981690991684, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.21688591660685777, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 45, "text_snippet": "edding layers is crucial for low-rank long-context adaptation . Llama2 7B (Touvron et al., 2023b) models with the proposed S2-Attn are trained on the RedPajama (Computer, 2023) dataset. The target context length is 32768. ‚Äò+ Normal / Embed‚Äô"}]}
{"case_index": 199, "query": "From 2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"To address these issues, in this paper we present Ragas1, a framework for the automated [BLANK] 1Rag\"?", "gold": "assessment", "answer": "The exact term is **‚Äúassessment‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'assessment' and is directly supported by the provided context sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 60.647, "llm_ms": 1085.621, "top_contexts": [{"rank": 1, "score": 0.69718504, "source": "dense+rrf", "rrf_score": 0.03177805800756621, "rerank_score": 0.3132948921515377, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 8, "text_snippet": "e not always predictive of downstream performance (Wang et al., 2023c). Moreover, this evaluation strategy relies on the LM probabilities, which are not accessible for some closed models (e.g. ChatGPT and GPT-4). Ques- tion answering is ano"}, {"rank": 2, "score": 0.7630018, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.28523999169940034, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 9, "text_snippet": "we present Ragas1, a framework for the automated assessment 1Ragas is available at https://github.com/ explodinggradients/ragas .arXiv:2309.15217v2 [cs.CL] 28 Apr 2025  of retrieval augmented generation systems. We fo- cus on settings where"}, {"rank": 3, "score": 26.253025486210902, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.24223638898242003, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 46, "text_snippet": "of such a framework, we have introduced WikiEval , a dataset which human judgements of these three different aspects. Finally, we have also described Ragas, our implementation of the three considered quality aspects. This framework is easy "}, {"rank": 4, "score": 0.7672877, "source": "dense+rrf", "rrf_score": 0.032018442622950824, "rerank_score": 0.23453306136527785, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 0, "text_snippet": "Ragas: Automated Evaluation of Retrieval Augmented Generation Shahul Es‚Ä†, Jithin James‚Ä†, Luis Espinosa-Anke‚àó‚ô¢, Steven Schockaert‚àó ‚Ä†Exploding Gradients ‚àóCardiffNLP, Cardiff University, United Kingdom ‚ô¢AMPLYFI, United Kingdom shahules786@gmai"}, {"rank": 5, "score": 0.6815432, "source": "dense+rrf", "rrf_score": 0.029236022193768675, "rerank_score": 0.23224718714430592, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 3, "text_snippet": "augmentation techniques. The paper highlights the state-of-the- art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-"}]}
{"case_index": 200, "query": "From 2005.14165__language_models_are_few_shot_learners.txt, what exact term fills [BLANK] in: \"The steeper ‚Äúin-context learning curves‚Äù for large models demonstrate improved ability to learn a task from contextual [BLANK].\"?", "gold": "information", "answer": "The exact term that fills **[BLANK]** is **‚Äúinformation‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context 1 explicitly contains the phrase 'learn a task from contextual information,' matching the expected blank. Answer is correct and fully grounded.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 61.074, "llm_ms": 1307.602, "top_contexts": [{"rank": 1, "score": 0.7326833, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.35609813491106046, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 19, "text_snippet": "Larger models make increasingly efÔ¨Åcient use of in-context information. We show in-context learning performance on a simple task requiring the model to remove random symbols from a word, both with and without a natural language task descrip"}, {"rank": 2, "score": 26.689515361347144, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.24524262311298778, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 45, "text_snippet": " major reduction in the need for task-speciÔ¨Åc data and reduced potential to learn an overly narrow distribution from a large but narrow Ô¨Åne-tuning dataset. The main disadvantage is that results from this method have so far been much worse t"}, {"rank": 3, "score": 21.890737446380164, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.23823121826937102, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 14, "text_snippet": "r at using their input context (¬ß2.3). Given that language models struggle to retrieve and use relevant information in the multi-document question answering task, to what extent can lan- guage models even retrieve from their input con- text"}, {"rank": 4, "score": 0.61425096, "source": "dense+rrf", "rrf_score": 0.03055037313432836, "rerank_score": 0.23430978612825837, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 166, "text_snippet": "r to learn these tasks at test time, as the model cannot perform them zero-shot and their artiÔ¨Åcial nature makes them unlikely to appear in the pre-training data (although we cannot conÔ¨Årm this with certainty). We can further quantify perfo"}, {"rank": 5, "score": 29.530820123454227, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.22769894575318636, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 321, "text_snippet": "del and data scaling decisions for the research. Ben Mann implemented sampling without replacement during training. Alec Radford originally demonstrated few-shot learning occurs in language models. Jared Kaplan and Sam McCandlish showed tha"}]}
{"case_index": 201, "query": "From 2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt, what exact term fills [BLANK] in: \"Orca 2 [BLANK] surpasses models of similar size and attains performance levels similar or better to those of models 5-10x larger, as assessed on complex tasks that test advanced reasoning abilities in zero-shot settings.\"?", "gold": "significantly", "answer": "The exact term is **‚Äúsignificantly‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'significantly' and is directly supported by Context 1/2.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 66.086, "llm_ms": 1129.009, "top_contexts": [{"rank": 1, "score": 0.76165986, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.4711679425791736, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 3, "text_snippet": "valuate Orca 2 using a comprehensive set of 15 diverse benchmarks (corresponding to approximately 100 tasks and over 36K unique prompts). Orca 2 significantly surpasses models of similar size and attains performance levels similar or better"}, {"rank": 2, "score": 0.7472865, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.280499725171888, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 20, "text_snippet": "problem solving, reading comprehension, summarization, groundedness, truthfulness and toxic content generation and identification. Our preliminary results indicate that Orca 2 significantly surpasses models of a similar size, even matching "}, {"rank": 3, "score": 0.7608123, "source": "dense+rrf", "rrf_score": 0.03128054740957967, "rerank_score": 0.25690743494523643, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 147, "text_snippet": "ques and recognizing the most effective solution strategy for each task, achieve performance levels comparable to, and often exceeding, models that are much larger, especially on zero-shot reasoning tasks. Though these models still exhibit "}, {"rank": 4, "score": 0.7338414, "source": "dense+rrf", "rrf_score": 0.029513888888888888, "rerank_score": 0.24754128065490447, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 1, "text_snippet": "n benchmarks like BigBench Hard and AGIEval. In Orca 2, we continue exploring how improved training signals can enhance smaller LMs‚Äô reasoning abilities. Research on training small LMs has often relied on imitation learning to replicate the"}, {"rank": 5, "score": 37.30146525035493, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.24672075911752034, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 64, "text_snippet": "sks selected to assess open-ended generation, summarization, safety, bias, reasoning, and comprehension capacities of Orca 2. Except where specified otherwise, evaluations were conducted using the test split of each dataset. We conduct eval"}]}
{"case_index": 202, "query": "From 2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt, what exact term fills [BLANK] in: \"Instruction tuning has been shown to improve the model‚Äôs ability to follow [BLANK] on both seen and unseen tasks [ 47], improve the overall quality of the generations [ 7] and give models enhanced zero-shot and reasoning ab\"?", "gold": "instructions", "answer": "The exact term that fills **[BLANK]** is **‚Äúinstructions‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'instructions' and is directly supported by the provided context sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 68.409, "llm_ms": 1186.764, "top_contexts": [{"rank": 1, "score": 0.7361864, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.5176766760910945, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 22, "text_snippet": "nstruction Tuning Instruction tuning [ 46,38,62,61] has emerged as a crucial step in training language models. Instruction tuning involves learning from input-output pairs where the input is natural language task description,and the output "}, {"rank": 2, "score": 0.7273271, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.28858985924666014, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 23, "text_snippet": "rations [ 7] and give models enhanced zero-shot and reasoning abilities [62]. Several studies, including Alpaca [ 55], Vicuna [ 6], WizardLM [ 64], Baize [ 65], and Koala [ 12], have adopted instruction tuning to train smaller ‚Äústudent‚Äù lan"}, {"rank": 3, "score": 32.47575707488437, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.24149899222683882, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 61, "text_snippet": "l state-of-the-art models. All baseline models are instruction-tuned models. We use the instruction-tuned versions because they have been shown to be much better at following instructions, have stronger reasoning capabilities, and are much "}, {"rank": 4, "score": 31.669798614469748, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.23383865840048385, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 67, "text_snippet": " thought in the few-shot exemplars; all the model has to do is repeat the same steps with the new symbols in the test-time example. And yet, small models still fail‚Äîthe ability to perform abstract manipulations on unseen symbols for these t"}, {"rank": 5, "score": 0.6283214, "source": "dense+rrf", "rrf_score": 0.027984344422700584, "rerank_score": 0.22556754141656601, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 25, "text_snippet": "skills and tasks within the parameters of the model, it is plausible that in-context learning abilities might show similarly strong gains with scale. 1In the context of language models this has sometimes been called ‚Äúzero-shot transfer‚Äù, bu"}]}
{"case_index": 203, "query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"Input: x Output: Figure 3: An [BLANK] prompt P(x)used to generate API calls for the question answering tool.\"?", "gold": "exemplary", "answer": "The exact term is **‚Äúexemplary‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'exemplary' and is directly supported by Context 1 showing 'An exemplary prompt'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 47.881, "llm_ms": 948.386, "top_contexts": [{"rank": 1, "score": 0.54207075, "source": "dense+rrf", "rrf_score": 0.02921395544346364, "rerank_score": 0.37564292190550364, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 19, "text_snippet": " born in [QA(\"Where was Joe  Biden born?\")] Scranton, [QA(\"In which state is  Scranton?\")] Pennsylvania.  Input: Coca-Cola, or Coke, is a carbonated soft drink  manufactured by the Coca-Cola Company.  Output: Coca-Cola, or [QA(\"What other n"}, {"rank": 2, "score": 40.45982183763993, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.2618903946020707, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 142, "text_snippet": " sample API calls for each tool considered. Question Answering We use the following prompt for the question answering tool: Your task is to add calls to a Question Answering API to a piece of text. The questions should help you get informat"}, {"rank": 3, "score": 0.6788087, "source": "dense+rrf", "rrf_score": 0.031754032258064516, "rerank_score": 0.2525337884102733, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 20, "text_snippet": "te API calls for the question answering tool. Mitself on this dataset. Each of these steps is described in more detail below. Sampling API Calls For each API, we write a promptP(x)that encourages the LM to anno- tate an example x=x1,...,x n"}, {"rank": 4, "score": 42.479923702917866, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.24935758513326356, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 93, "text_snippet": "mitations While our approach enables LMs to learn how to use a variety of tools in a self-supervised way, there are some clear limitations to what can be achieved with our method in its current form. One such limi- tation is the inability o"}, {"rank": 5, "score": 40.32573212358057, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.24600437512439832, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 147, "text_snippet": "tput: From this, we have 4 *30 minutes = [Calculator(4 *30)] 120 minutes. Input: x Output: Wikipedia Search We use the following prompt for the Wikipedia search tool: Your task is to complete a given piece of text. You can use a Wikipedia S"}]}
{"case_index": 204, "query": "From 2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt, what exact term fills [BLANK] in: \"3 Published as a conference paper at ICLR 2023 sampling, as commonly used for [BLANK] text generation (Radford et al., 2019; Brown et al., 2020; Thoppilan et al., 2022), to achieve this goal.\"?", "gold": "open-ended", "answer": "The exact term that fills **[BLANK]** is **‚Äúopen-ended‚Äù**, as in:\n\n‚Äúsampling, as commonly used for **open-ended** text generation (Radford et al., 2019; Brown et al., 2020; Thoppilan et al., 2022), to achieve this goal.‚Äù", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'open-ended' and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 59.387, "llm_ms": 2000.356, "top_contexts": [{"rank": 1, "score": 0.7413776, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.4711679425791736, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 30, "text_snippet": "l., 2021; Thoppilan et al., 2022). 3  Published as a conference paper at ICLR 2023 sampling, as commonly used for open-ended text generation (Radford et al., 2019; Brown et al., 2020; Thoppilan et al., 2022), to achieve this goal. One shoul"}, {"rank": 2, "score": 57.8588329121743, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.37789416510321655, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 40, "text_snippet": " 4  Published as a conference paper at ICLR 2023 3.2 M AINRESULTS We report the results of self-consistency averaged over 10 runs, where we sampled 40 outputs independently from the decoder in each run. The baseline we compare to is chain-o"}, {"rank": 3, "score": 0.64515036, "source": "dense+rrf", "rrf_score": 0.031746031746031744, "rerank_score": 0.30356425305605567, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 0, "text_snippet": "Published as a conference paper at ICLR 2023 SELF-CONSISTENCY IMPROVES CHAIN OF THOUGHT REASONING IN LANGUAGE MODELS Xuezhi Wang‚Ä†‚Ä°Jason Wei‚Ä†Dale Schuurmans‚Ä†Quoc Le‚Ä†Ed H. Chi‚Ä† Sharan Narang‚Ä†Aakanksha Chowdhery‚Ä†Denny Zhou‚Ä†¬ß ‚Ä†Google Research, "}, {"rank": 4, "score": 45.81149688960653, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.29913802135401923, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 18, "text_snippet": "., in Output 2), but such solutions are less likely to arrive at the same answer. That is, we hypothesize that correct reasoning processes, even if they are diverse, tend to have greater agreement in their Ô¨Ånal answer than incorrect process"}, {"rank": 5, "score": 51.55074571569866, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.2900707221665879, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 14, "text_snippet": "et al., 2022) and GPT-3-175B (Brown et al., 2020), and two densely-activated decoder-only language models: LaMDA-137B (Thoppilan et al., 2022) and PaLM-540B (Chowdhery et al., 2022). On all four language models, self-consistency improves ov"}]}
{"case_index": 205, "query": "From 2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt, what exact term fills [BLANK] in: \"To measure toxicity, we use the [BLANK] dataset (Gehman et al., 2020) and conduct both automatic and human evaluations.\"?", "gold": "realtoxicityprompts", "answer": "The exact term is **‚ÄúRealToxicityPrompts‚Äù**.", "answer_score": 0.3, "answer_correctness": 0.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context shows the dataset name as RealToxicityPrompts, but the expected exact fill is the lowercase token 'realtoxicityprompts'. Answer does not match expected exact term.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 59.037, "llm_ms": 1162.528, "top_contexts": [{"rank": 1, "score": 0.6472566, "source": "dense+rrf", "rrf_score": 0.031099324975891997, "rerank_score": 0.3455935183188918, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 18, "text_snippet": "1% hallucination rate, respectively). InstructGPT shows small improvements in toxicity over GPT-3, but not bias. To measure toxicity, we use the RealToxicityPrompts dataset (Gehman et al., 2020) and conduct both automatic and human evaluati"}, {"rank": 2, "score": 31.839431478405892, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.2711253282114185, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 9, "text_snippet": "fabricate information or mislead the user), and harmless (they should not cause physical, psychological, or social harm to people or the environment). We elaborate on the evaluation of these criteria in Section 3.6. We focus on Ô¨Åne-tuning a"}, {"rank": 3, "score": 0.62811226, "source": "dense+rrf", "rrf_score": 0.028370221327967807, "rerank_score": 0.26495553368645364, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 56, "text_snippet": "tion Large language models have been showed to re- produce and amplify biases that are existing in the training data (Sheng et al., 2019; Kurita et al., 2019), and to generate toxic or offensive con- tent (Gehman et al., 2020). As our train"}, {"rank": 4, "score": 40.62406166004051, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.2452218632444157, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 99, "text_snippet": "ustin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. 2021. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114 . Long Ouyang, Jeff Wu, Xu Jiang, Diog"}, {"rank": 5, "score": 0.66476667, "source": "dense+rrf", "rrf_score": 0.03149801587301587, "rerank_score": 0.23850262913293338, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 83, "text_snippet": "ct a range of metadata for each model output (see Table 3). Evaluations on public NLP datasets. We evaluate on two types of public datasets: those that capture an aspect of language model safety, particularly truthfulness, toxicity, and bia"}]}
{"case_index": 206, "query": "From 2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt, what exact term fills [BLANK] in: \"1 [BLANK] Transformer models [ 82] have emerged as the most widely used architecture in applications such as natural language processing and image classiÔ¨Åcation.\"?", "gold": "introduction", "answer": "The exact term is **‚ÄúIntroduction‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context shows the line '1 Introduction' immediately before the quoted sentence, so the blank is 'Introduction'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 46.465, "llm_ms": 964.064, "top_contexts": [{"rank": 1, "score": 69.97348908458481, "source": "dense+rrf", "rrf_score": 0.01639344262295082, "rerank_score": 0.3687294247821033, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 4, "text_snippet": "T-2 and 6.4 points of lift on long-document classiÔ¨Åcation) and entirely new capabilities: the Ô¨Årst Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64"}, {"rank": 2, "score": 0.67900217, "source": "dense+rrf", "rrf_score": 0.032018442622950824, "rerank_score": 0.2902336385993442, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 134, "text_snippet": " well as the (dense) FlashAttention on the Long-range Arena tasks. EÔ¨Écient Transformer. Transformer-based models have become the most widely-used architecture in natural language processing [ 22] and computer vision [ 24,91]. However, one o"}, {"rank": 3, "score": 55.544890396986546, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.2291774186479612, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 100, "text_snippet": "n, Atri Rudra, and Christopher R√©. 2022. FlashAttention: Fast and memory-efficient exact attention with IO- awareness. ArXiv:2205.14135. Hermann Ebbinghaus. 1913. Memory: A contribu- tion to experimental psychology. H. A. Ruger & C. E. Buss"}, {"rank": 4, "score": 36.53787558929573, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.20643785741901385, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 10, "text_snippet": "mplementation of attention on GPT-2. FlashAttention does not read and write the large ùëÅ\u0002ùëÅattention matrix to HBM, resulting in an 7.6 \u0002 speedup on the attention computation. GPUs, compute speed has out-paced memory speed [ 61,62,63], and mo"}, {"rank": 5, "score": 0.62408864, "source": "dense+rrf", "rrf_score": 0.03125763125763126, "rerank_score": 0.20529686686277016, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 106, "text_snippet": "o solve math word problems. arXiv preprint arXiv:2110.14168 . Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Car- bonell, Quoc V Le, and Ruslan Salakhutdinov. 2019. Transformer-xl: Attentive language mod- els beyond a Ô¨Åxed-length context. arXi"}]}
{"case_index": 207, "query": "From 2005.14165__language_models_are_few_shot_learners.txt, what exact term fills [BLANK] in: \"Third, humans do not require large [BLANK] datasets to learn most language tasks ‚Äì a brief directive in natural language (e.g.\"?", "gold": "supervised", "answer": "The exact term is **‚Äúsupervised‚Äù**, i.e., ‚Äúlarge **supervised** datasets.‚Äù", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'supervised' and is directly supported by the provided context snippet.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 61.228, "llm_ms": 1131.939, "top_contexts": [{"rank": 1, "score": 0.610012, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.332545353917465, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 16, "text_snippet": "neralization achieved under this paradigm can be poor because the model is overly speciÔ¨Åc to the training distribution and does not generalize well outside it [YdC+19,MPL19 ]. Thus, the performance of Ô¨Åne-tuned models on speciÔ¨Åc benchmarks,"}, {"rank": 2, "score": 24.216740626048818, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.22262109515570797, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 43, "text_snippet": " mod- els, respectively. 4.2 Downstream Tasks We evaluate all models on a variety of downstream tasks. In all cases, we consider a prompted zero- shot setup ‚Äì i.e., models are instructed to solve each task in natural language, but we do not"}, {"rank": 3, "score": 24.136975364457854, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.20868886551749327, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 327, "text_snippet": "ch is mitigated by adding updates on the pretraining data during Ô¨Åne-tuning. Few-shot performance is shown in Figure 29. Error bars for translation are not available because we use a software package that does not report them. E.1 Performan"}, {"rank": 4, "score": 23.213424570638736, "source": "dense+rrf", "rrf_score": 0.014285714285714285, "rerank_score": 0.20551330169909815, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 37, "text_snippet": "lts, and we either do not report results on these datasets or we note them with an asterisk, depending on the severity. In addition to all the above, we also train a series of smaller models (ranging from 125 million parameters to 13 billio"}, {"rank": 5, "score": 33.10324401421807, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.19936337387608555, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 181, "text_snippet": " S., von Arx, S., Bernstein, M. S., Bohg, J., Bosselut, A., Brunskill, E., et al. (2021). On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258 . Bostrom, N. (2014). Superintelligence . Dunod. Brown, T. B., Ma"}]}
{"case_index": 208, "query": "From 2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt, what exact term fills [BLANK] in: \"Each kernel loads inputs from HBM to [BLANK] and SRAM, computes, then writes outputs to HBM.\"?", "gold": "registers", "answer": "The exact term is **‚Äúregisters‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'registers' and is directly supported by Context 1 sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 42.523, "llm_ms": 1156.838, "top_contexts": [{"rank": 1, "score": 67.00851998126417, "source": "dense+rrf", "rrf_score": 0.01639344262295082, "rerank_score": 0.272958633509881, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 23, "text_snippet": "peed [ 61,62,63], operations are increasingly bottlenecked by memory (HBM) accesses. Thus exploiting fast SRAM becomes more important. Execution Model. GPUs have a massive number of threads to execute an operation (called a kernel). Each ke"}, {"rank": 2, "score": 54.73686116546747, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.26085028193941917, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 100, "text_snippet": "n, Atri Rudra, and Christopher R√©. 2022. FlashAttention: Fast and memory-efficient exact attention with IO- awareness. ArXiv:2205.14135. Hermann Ebbinghaus. 1913. Memory: A contribu- tion to experimental psychology. H. A. Ruger & C. E. Buss"}, {"rank": 3, "score": 0.63299525, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.2349856404034172, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 7, "text_snippet": " with wall-clock speed) and tend to ignore overheads from memory access (IO). In this paper, we argue that a missing principle is making attention algorithms IO-aware [1]‚Äîthat is, carefully accounting for reads and writes to diÔ¨Äerent levels"}, {"rank": 4, "score": 36.00596494635844, "source": "dense+rrf", "rrf_score": 0.014285714285714285, "rerank_score": 0.22726894231898614, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 32, "text_snippet": " techniques (tiling, recomputation) to overcome the technical challenge of computing exact attention in sub-quadratic HBM accesses. We describe this in Algorithm 1. The main idea is that we split the inputs Q¬ñK¬ñVinto blocks, load them from "}, {"rank": 5, "score": 39.956400076416315, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.22211932603236448, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 84, "text_snippet": "mputer/RedPajama-Data . Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. CoRR , abs/2307.08691, 2023. Tri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra, and Christopher R ¬¥e. Flashattention: Fast an"}]}
{"case_index": 209, "query": "From 2005.14165__language_models_are_few_shot_learners.txt, what exact term fills [BLANK] in: \"Model [BLANK] improves with the addition of a natural language task description, and with the number of examples in the model‚Äôs context, K.\"?", "gold": "performance", "answer": "The exact term is **‚Äúperformance‚Äù** (‚ÄúModel performance improves with the addition of a natural language task description, and with the number of examples in the model‚Äôs context, K.‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'performance' and is directly supported by Context 1 sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 52.643, "llm_ms": 1340.078, "top_contexts": [{"rank": 1, "score": 0.5833194, "source": "dense+rrf", "rrf_score": 0.03131881575727918, "rerank_score": 0.3982469179563361, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 30, "text_snippet": "ions are allowed and only an instruction in natural language is given to the model. GPT-3 could also in principle be evaluated in the traditional Ô¨Åne-tuning setting, but we leave this to future work. Figure 1.2 illustrates the conditions we"}, {"rank": 2, "score": 0.5706356, "source": "dense+rrf", "rrf_score": 0.029910714285714284, "rerank_score": 0.29333458946483243, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 140, "text_snippet": "inally, we note that the few-shot SuperGLUE score steadily improves with both model size and with number of examples in the context showing increasing beneÔ¨Åts from in-context learning (Figure 3.8). We scale Kup to 32 examples per task, afte"}, {"rank": 3, "score": 0.6100534, "source": "dense+rrf", "rrf_score": 0.031746031746031744, "rerank_score": 0.27795644447675577, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 31, "text_snippet": " the number of examples in the model‚Äôs context, K. Few-shot learning also improves dramatically with model size. Though the results in this case are particularly striking, the general trends with both model size and number of examples in-co"}, {"rank": 4, "score": 0.5627654, "source": "dense+rrf", "rrf_score": 0.027799227799227798, "rerank_score": 0.27282108747226397, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 8, "text_snippet": "017) or Ô¨Ånetuning a pretrained model (Cobbe et al., 2021), in addition to neuro-symbolic methods that use formal lan- guages instead of natural language (Roy and Roth, 2015; Chiang and Chen, 2019; Amini et al., 2019; Chen et al., 2019). Sec"}, {"rank": 5, "score": 0.6565984, "source": "dense+rrf", "rrf_score": 0.029906956136464335, "rerank_score": 0.259481480146339, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 19, "text_snippet": "Larger models make increasingly efÔ¨Åcient use of in-context information. We show in-context learning performance on a simple task requiring the model to remove random symbols from a word, both with and without a natural language task descrip"}]}
{"case_index": 210, "query": "From 2005.14165__language_models_are_few_shot_learners.txt, what exact term fills [BLANK] in: \"ostic, there is still a need for task-speciÔ¨Åc datasets and task-speciÔ¨Åc Ô¨Åne-tuning: to achieve strong [BLANK] on a desired task typically requires Ô¨Åne-tuning on a dataset of thousands to hundreds of thousands of examples speciÔ¨Åc to that task.\"?", "gold": "performance", "answer": "The exact term that fills **[BLANK]** is **‚Äúperformance‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'performance' and is directly supported by the provided context sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 57.688, "llm_ms": 1053.956, "top_contexts": [{"rank": 1, "score": 0.72005975, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.5000670497407692, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 13, "text_snippet": "ostic, there is still a need for task-speciÔ¨Åc datasets and task-speciÔ¨Åc Ô¨Åne-tuning: to achieve strong performance on a desired task typically requires Ô¨Åne-tuning on a dataset of thousands to hundreds of thousands of examples speciÔ¨Åc to that"}, {"rank": 2, "score": 0.66748524, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.3291790435405815, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 42, "text_snippet": "iÔ¨Åc to the desired task. Typically thousands to hundreds of thousands of labeled examples are used. The main advantage of Ô¨Åne-tuning is strong performance on many benchmarks. The main disadvantages are the need for a new large dataset for e"}, {"rank": 3, "score": 0.5824914, "source": "dense+rrf", "rrf_score": 0.030330882352941176, "rerank_score": 0.3144918405992758, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 45, "text_snippet": " major reduction in the need for task-speciÔ¨Åc data and reduced potential to learn an overly narrow distribution from a large but narrow Ô¨Åne-tuning dataset. The main disadvantage is that results from this method have so far been much worse t"}, {"rank": 4, "score": 0.57798314, "source": "dense+rrf", "rrf_score": 0.030621785881252923, "rerank_score": 0.29816216881305735, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 1, "text_snippet": "ec Radford Ilya Sutskever Dario Amodei OpenAI Abstract Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by Ô¨Åne-tuning on a speciÔ¨Åc task. While typically task-"}, {"rank": 5, "score": 36.69761149250606, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.2796977035283305, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 44, "text_snippet": " a typical dataset an example has a context and a desired completion (for example an English sentence and the French translation), and few-shot works by giving Kexamples of context and completion, and then one Ô¨Ånal example of context, with "}]}
{"case_index": 211, "query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \"Dur- ing [BLANK], the model is trained on unlabeled data over different pre-training tasks.\"?", "gold": "pre-training", "answer": "The exact term is **‚Äúpre-training‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected blank term 'pre-training' and is directly supported by Context 1 sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 70.179, "llm_ms": 930.054, "top_contexts": [{"rank": 1, "score": 0.54398, "source": "dense+rrf", "rrf_score": 0.031544957774465976, "rerank_score": 0.3295985352030807, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 22, "text_snippet": "dels, where an effective recipe is to Ô¨Åne-tune models pre-trained with Ima- geNet (Deng et al., 2009; Yosinski et al., 2014). 3 BERT We introduce BERT and its detailed implementa- tion in this section. There are two steps in our framework: "}, {"rank": 2, "score": 33.294640937765706, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.2740099147447783, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 109, "text_snippet": "reading books. In Proceedings of the IEEE international conference on computer vision , pages 19‚Äì27. Appendix for ‚ÄúBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding‚Äù We organize the appendix into three section"}, {"rank": 3, "score": 0.55165213, "source": "dense+rrf", "rrf_score": 0.03076923076923077, "rerank_score": 0.2728205128205073, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 1, "text_snippet": ", 2018), BERT is designed to pre- train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a re- sult, the pre-trained BERT model can be Ô¨Åne- tuned with just one a"}, {"rank": 4, "score": 0.5161444, "source": "dense+rrf", "rrf_score": 0.027425373134328357, "rerank_score": 0.2627329523406234, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 83, "text_snippet": "ted that rich, unsupervised pre-training is an integral part of many language understanding systems. In particular, these results enable even low-resource tasks to beneÔ¨Åt from deep unidirectional architec- tures. Our major contribution is f"}, {"rank": 5, "score": 31.7731327892335, "source": "dense+rrf", "rrf_score": 0.014285714285714285, "rerank_score": 0.25823410208476194, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 44, "text_snippet": "U, or a few hours on a GPU, starting from the exact same pre-trained model.7We de- scribe the task-speciÔ¨Åc details in the correspond- ing subsections of Section 4. More details can be found in Appendix A.5. 4 Experiments In this section, we"}]}
{"case_index": 212, "query": "From 2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt, what exact term fills [BLANK] in: \"We Ô¨Ånd that [BLANK] robustly improves reasoning accuracy for every language model considered, spanning a wide range of model scales.\"?", "gold": "self-consistency", "answer": "The exact term is **‚Äúself-consistency‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'self-consistency' and is directly supported by the provided context sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 63.915, "llm_ms": 1300.312, "top_contexts": [{"rank": 1, "score": 0.66646934, "source": "dense+rrf", "rrf_score": 0.03009207275993712, "rerank_score": 0.35974456518028514, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 31, "text_snippet": "f consistency can be deÔ¨Åned between multiple generations, e.g., whether two answers agree or contradict each other. 3 E XPERIMENTS We conducted a series of experiments to compare the proposed self-consistency method with existing approaches"}, {"rank": 2, "score": 0.719064, "source": "dense+rrf", "rrf_score": 0.031024531024531024, "rerank_score": 0.28535016932444096, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 87, "text_snippet": "rved that it signiÔ¨Åcantly improves accuracy in a range of arithmetic and commonsense reasoning tasks, across four large language models with varying scales. Beyond accuracy gains, self-consistency is also useful for collecting rationales wh"}, {"rank": 3, "score": 0.6971091, "source": "dense+rrf", "rrf_score": 0.029418126757516764, "rerank_score": 0.2670001092850444, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 86, "text_snippet": "ength sequence in recurrent language models. Nye et al. (2021) improve the logical consistency of samples from a System 1 model by adding a System 2-inspired logical reasoning module. In this paper we focus on a slightly different notion of"}, {"rank": 4, "score": 0.7283392, "source": "dense+rrf", "rrf_score": 0.03057889822595705, "rerank_score": 0.26324551825847586, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 41, "text_snippet": "The results are shown in Table 2.7Self-consistency improves the arithmetic reasoning performance over all four language models signiÔ¨Åcantly over chain-of-thought prompting. More surprisingly, the gains become more signiÔ¨Åcant when the langua"}, {"rank": 5, "score": 0.7213901, "source": "dense+rrf", "rrf_score": 0.029513888888888888, "rerank_score": 0.2478286724813746, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 5, "text_snippet": "odel performance across a variety of multi-step reasoning tasks (Wei et al., 2022). In this paper, we introduce a novel decoding strategy called self-consistency to replace the greedy decoding strategy used in chain-of-thought prompting (We"}]}
{"case_index": 213, "query": "From 2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt, what exact term fills [BLANK] in: \"LongRAG processes each [BLANK] document as a single (long) unit rather than chunking them into smaller units.\"?", "gold": "individual", "answer": "The exact term is **‚Äúindividual‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context 1 explicitly states: 'LongRAG processes each individual document as a single (long) unit...' Model answer matches expected term.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 51.616, "llm_ms": 1322.372, "top_contexts": [{"rank": 1, "score": 0.633891, "source": "dense+rrf", "rrf_score": 0.03028233151183971, "rerank_score": 0.3115879621411623, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 4, "text_snippet": "‚âà30K tokens) to an existing long-context LLM to perform zero-shot answer generation. Without requiring any training, LongRAG achieves an EM of 62.7% on NQ and 64.3% on HotpotQA, which are on par with the (fully-trained) SoTA model. Furtherm"}, {"rank": 2, "score": 58.29904638758907, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.260515487157966, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 17, "text_snippet": "anding ability of GPT-4o, LongRAG can achieve an EM of 62.7% on NQ and 64.3% on HotpotQA. These results could be comparable to the strongest fully trained RAG models like Atlas(Izacardetal.,2022)andMDR(Xiongetal.,2020b). Furthermore,wetesto"}, {"rank": 3, "score": 0.7473578, "source": "dense+rrf", "rrf_score": 0.032266458495966696, "rerank_score": 0.2540615971390017, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 28, "text_snippet": " then fed into a long reader. Compared to traditional RAG, which retrieves hundreds of short units, our proposed LongRAG reduces the likelihood of retrieving hard negatives during the retrieval stage and more effectively leverages recent ad"}, {"rank": 4, "score": 0.639459, "source": "dense+rrf", "rrf_score": 0.02964426877470356, "rerank_score": 0.23222980649734296, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 5, "text_snippet": "ather than chunking them into smaller units. By doing so, we achieve an F1 score of 25.9% on Qasper (previously 22.5%) and 57.5% on MultiFieldQA-en (previously 51.2%). Our study offers insights into the future roadmap for combining RAG with"}, {"rank": 5, "score": 0.6347184, "source": "dense+rrf", "rrf_score": 0.02900988017658188, "rerank_score": 0.22864774950016295, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 21, "text_snippet": "operates on long retrieval units, with only a few (typically fewer than 10) being fed into the reader. An illustrative example is shown in Figure 2. 2.1 Long Retriever The traditional RAG framework employs smaller retrieval units and priori"}]}
{"case_index": 214, "query": "From 2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt, what exact term fills [BLANK] in: \"s an example, the result of a game in premier league in a particular day might be good training data for frontier models, but we need to remove such [BLANK] to leave more model capacity for ‚Äúreasoning‚Äù for the mini size models.\"?", "gold": "information", "answer": "The exact term that fills **[BLANK]** is:\n\n**information**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'information' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 79.28, "llm_ms": 1202.372, "top_contexts": [{"rank": 1, "score": 0.64865184, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.5100636019546028, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 21, "text_snippet": "s an example, the result of a game in premier league in a particular day might be good training data for frontier models, but we need to remove such information to leave more model capacity for ‚Äúreasoning‚Äù for the mini size models. We compa"}, {"rank": 2, "score": 0.6383326, "source": "dense+rrf", "rrf_score": 0.031754032258064516, "rerank_score": 0.3408841654847698, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 20, "text_snippet": "hat train language models in either ‚Äúcompute optimal regime‚Äù [HBM+22] or ‚Äúover-train regime‚Äù, we mainly focus on the quality of data for a given scale .3 We try to calibrate the training data to be closer to the ‚Äúdata optimal‚Äù regime for sm"}, {"rank": 3, "score": 66.96124105526451, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.28948982784984467, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 0, "text_snippet": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone Microsoft Abstract We introduce phi-3-mini , a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both"}, {"rank": 4, "score": 51.8962949753799, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.26907941270562075, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 16, "text_snippet": "3. The table shows the Keys/values a query token in block 8 attended to. Blue=local blocks, orange=remote/vertical blocks, gray=blocks skipped. total parameters. Additionally, we utilize the SparseMixer approach [LGC23, LDL+23] for training"}, {"rank": 5, "score": 55.29949078157031, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.26716113481757675, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 6, "text_snippet": "7B parameters), matched the performance of models 25 times larger trained on regular data. In this report we present a new model, phi-3-mini (3.8B parameters), trained for 3.3T tokens on larger and more advanced versions of the datasets use"}]}
{"case_index": 215, "query": "From 2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt, what exact term fills [BLANK] in: \"the next token on a webpage from the internet‚Äîis different from the objective ‚Äúfollow the user‚Äôs [BLANK] helpfully and safely‚Äù (Radford et al., 2019; Brown et al., 2020; Fedus et al., 2021; Rae et al., 2021; Thoppilan et al., 2022).\"?", "gold": "instructions", "answer": "The exact term that fills **[BLANK]** is **‚Äúinstructions‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 89.839, "llm_ms": 1178.847, "top_contexts": [{"rank": 1, "score": 0.7311493, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.5344391192130578, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 7, "text_snippet": "the next token on a webpage from the internet‚Äîis different from the objective ‚Äúfollow the user‚Äôs instructions helpfully and safely‚Äù (Radford et al., 2019; Brown et al., 2020; Fedus et al., 2021; Rae et al., 2021; Thoppilan et al., 2022). Th"}, {"rank": 2, "score": 47.95187250345022, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.286461083601581, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 43, "text_snippet": "mmarization domains. We start with a pretrained language model (Radford et al., 2019; Brown et al., 2020; Fedus et al., 2021; Rae et al., 2021; Thoppilan et al., 2022), a distribution of prompts on which we want our model to produce aligned"}, {"rank": 3, "score": 0.5515051, "source": "dense+rrf", "rrf_score": 0.027650648360030512, "rerank_score": 0.2717862238601112, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 28, "text_snippet": "eedback. We build on previous techniques to align models with human intentions, particularly reinforcement learning from human feed- back (RLHF). Originally developed for training simple robots in simulated environments and Atari games (Chr"}, {"rank": 4, "score": 39.48208760813926, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.25845470001491655, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 87, "text_snippet": "s. Later, scaling transformers lead to improvement on many NLP tasks. Notable models include BERT (Devlin et al., 2018), GPT-2 (Radford et al., 2019), Megatron- LM (Shoeybi et al., 2019), and T5 (Raffel et al., 2020). A signiÔ¨Åcant breakthro"}, {"rank": 5, "score": 0.57794887, "source": "dense+rrf", "rrf_score": 0.029211087420042643, "rerank_score": 0.25352467615170005, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 9, "text_snippet": "fabricate information or mislead the user), and harmless (they should not cause physical, psychological, or social harm to people or the environment). We elaborate on the evaluation of these criteria in Section 3.6. We focus on Ô¨Åne-tuning a"}]}
{"case_index": 216, "query": "From 2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt, what exact term fills [BLANK] in: \"Remarkably, this has been successful for a range of simple [BLANK] tasks (Brown et al., 2020).\"?", "gold": "question-answering", "answer": "The exact term is **‚Äúquestion-answering‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context 1 explicitly contains the phrase 'simple question-answering tasks', matching the expected blank.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 54.804, "llm_ms": 1247.838, "top_contexts": [{"rank": 1, "score": 52.394047159724664, "source": "dense+rrf", "rrf_score": 0.01639344262295082, "rerank_score": 0.3141988208276055, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 9, "text_snippet": "mpt‚Äù the model with a few input‚Äìoutput exemplars demonstrating the task. Remarkably, this has been successful for a range of simple question-answering tasks (Brown et al., 2020). Both of the above ideas, however, have key limitations. For r"}, {"rank": 2, "score": 0.67414564, "source": "dense+rrf", "rrf_score": 0.030621785881252923, "rerank_score": 0.2962105069998195, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 71, "text_snippet": "revailing theme (Wei et al., 2022b). For many reasoning tasks where standard prompting has a Ô¨Çat scaling curve, chain- of-thought prompting leads to dramatically increasing scaling curves. Chain-of-thought prompting appears to expand the se"}, {"rank": 3, "score": 36.180837613093345, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.29307122445043454, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 79, "text_snippet": "pecialized approaches for improving reasoning (Andor et al., 2019; Ran et al., 2019; Geva et al., 2020; PiÀõ ekos et al., 2021). Compared to prior work, self-consistency is applicable to a wide range of reasoning tasks without any additional"}, {"rank": 4, "score": 0.6633178, "source": "dense+rrf", "rrf_score": 0.029631255487269532, "rerank_score": 0.2912172582603232, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 70, "text_snippet": "t generally applicable (Section 4). Finally, we showed that for symbolic reasoning, chain-of-thought prompting facilitates OOD generalization to longer sequence lengths (Section 5). In all experiments, chain-of-thought reasoning is elicited"}, {"rank": 5, "score": 0.6361948, "source": "dense+rrf", "rrf_score": 0.02658450704225352, "rerank_score": 0.2838411509375612, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 10, "text_snippet": " For the traditional few- shot prompting method used in Brown et al. (2020), it works poorly on tasks that require reasoning abilities, and often does not improve substantially with increasing language model scale (Rae et al., 2021). In thi"}]}
{"case_index": 217, "query": "From 2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt, what exact term fills [BLANK] in: \"On all four language models, [BLANK] improves over chain-of-thought prompting by a striking margin across all tasks.\"?", "gold": "self-consistency", "answer": "**self-consistency**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "The blank is explicitly filled by 'self-consistency' in the provided context, and the model answer matches exactly.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 42.4, "llm_ms": 964.353, "top_contexts": [{"rank": 1, "score": 0.7519746, "source": "dense+rrf", "rrf_score": 0.032018442622950824, "rerank_score": 0.3991243375828159, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 14, "text_snippet": "et al., 2022) and GPT-3-175B (Brown et al., 2020), and two densely-activated decoder-only language models: LaMDA-137B (Thoppilan et al., 2022) and PaLM-540B (Chowdhery et al., 2022). On all four language models, self-consistency improves ov"}, {"rank": 2, "score": 0.72547054, "source": "dense+rrf", "rrf_score": 0.0315136476426799, "rerank_score": 0.318590752872059, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 5, "text_snippet": "odel performance across a variety of multi-step reasoning tasks (Wei et al., 2022). In this paper, we introduce a novel decoding strategy called self-consistency to replace the greedy decoding strategy used in chain-of-thought prompting (We"}, {"rank": 3, "score": 0.7522968, "source": "dense+rrf", "rrf_score": 0.03125763125763126, "rerank_score": 0.2939101342358364, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 41, "text_snippet": "The results are shown in Table 2.7Self-consistency improves the arithmetic reasoning performance over all four language models signiÔ¨Åcantly over chain-of-thought prompting. More surprisingly, the gains become more signiÔ¨Åcant when the langua"}, {"rank": 4, "score": 0.77165866, "source": "dense+rrf", "rrf_score": 0.032018442622950824, "rerank_score": 0.2682651568077753, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 2, "text_snippet": "ng to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmark"}, {"rank": 5, "score": 0.69857043, "source": "dense+rrf", "rrf_score": 0.027809742999616416, "rerank_score": 0.26395202599989254, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 1, "text_snippet": "such reasoning abilities emerge naturally in sufÔ¨Åciently large language models via a simple method called chain-of- thought prompting , where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three"}]}
{"case_index": 218, "query": "From 2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt, what exact term fills [BLANK] in: \"For the tasks where reasoning is of primary importance (Figure 1(1)), we alternate the generation of thoughts and actions so that the task-solving trajectory consists of multiple [BLANK] steps.\"?", "gold": "thought-action-observation", "answer": "The exact term is **‚Äúthought-action-observation‚Äù**, i.e., the trajectory consists of multiple **thought-action-observation steps**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'thought-action-observation' and is directly supported by the provided context sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 70.329, "llm_ms": 1345.21, "top_contexts": [{"rank": 1, "score": 0.6656178, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.42764106874220814, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 31, "text_snippet": "xt example is a human trajectory of actions, thoughts, and environment observations to solve a task instance (see Appendix C). For the tasks where reasoning is of primary importance (Figure 1(1)), we alternate the generation of thoughts and"}, {"rank": 2, "score": 0.6179539, "source": "dense+rrf", "rrf_score": 0.03021353930031804, "rerank_score": 0.29536976315827285, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 40, "text_snippet": " Wikipedia, and force models to retrieve via explicit reasoning in language. 3.2 M ETHODS ReAct Prompting For HotpotQA and Fever, we randomly select 6 and 3 cases2from the training set and manually compose ReAct -format trajectories to use "}, {"rank": 3, "score": 0.602576, "source": "dense+rrf", "rrf_score": 0.027809742999616416, "rerank_score": 0.27648043338238043, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 24, "text_snippet": "o understand the decision basis of model actions. To summarize, our key contributions are the following: (1) we introduce ReAct , a novel prompt- based paradigm to synergize reasoning and acting in language models for general task solving; "}, {"rank": 4, "score": 0.66045225, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.27446691093289555, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 19, "text_snippet": "udies on how reasoning and acting can be combined in a synergistic manner for general task solving, and if such a combination can bring systematic beneÔ¨Åts compared to reasoning or acting alone. In this work, we present ReAct , a general par"}, {"rank": 5, "score": 0.6116344, "source": "dense+rrf", "rrf_score": 0.0293236301369863, "rerank_score": 0.26663282223549817, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 30, "text_snippet": " Thought 3), and so on. However, as the language space Lis unlimited, learning in this augmented action space is difÔ¨Åcult and requires strong language priors. In this paper, we mainly focus on the setup where a frozen large language model, "}]}
{"case_index": 219, "query": "From 2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt, what exact term fills [BLANK] in: \"rformance across tasks in language [BLANK] and interactive decision making, their abilities for reasoning (e.g.\"?", "gold": "understanding", "answer": "The exact term is **‚Äúunderstanding‚Äù** (i.e., ‚Äúperformance across tasks in language **understanding** and interactive decision making, their abilities for reasoning ‚Ä¶‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "The blank is 'understanding', explicitly present in Context 1 matching the quoted sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 44.996, "llm_ms": 1463.115, "top_contexts": [{"rank": 1, "score": 0.66643083, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.3088760733835777, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 1, "text_snippet": "rformance across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In th"}, {"rank": 2, "score": 0.614823, "source": "dense+rrf", "rrf_score": 0.030621785881252923, "rerank_score": 0.24990411726426248, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 88, "text_snippet": " the development of versatile and generalist agents like Reed et al. (2022). 6 C ONCLUSION We have proposed ReAct ‚Äì a simple yet effective method for synergizing reasoning and acting in large language models. Through a diverse set of experi"}, {"rank": 3, "score": 0.63225424, "source": "dense+rrf", "rrf_score": 0.03055037313432836, "rerank_score": 0.24433007477988303, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 82, "text_snippet": "actions and their corresponding observations into a coherent stream of inputs for the model to reason more accurately and tackle tasks beyond reasoning (e.g. interactive decision making). Language model for decision making The strong capabi"}, {"rank": 4, "score": 0.66065866, "source": "dense+rrf", "rrf_score": 0.03125763125763126, "rerank_score": 0.22652704295309464, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 19, "text_snippet": "udies on how reasoning and acting can be combined in a synergistic manner for general task solving, and if such a combination can bring systematic beneÔ¨Åts compared to reasoning or acting alone. In this work, we present ReAct , a general par"}, {"rank": 5, "score": 0.59692746, "source": "dense+rrf", "rrf_score": 0.02761904761904762, "rerank_score": 0.20235094204585383, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 75, "text_snippet": "mbined reasoning and action using an LLM applied to an interactive environment within a closed-loop system. Perhaps the closest prior work is Inner Monologue (IM), from Huang et al. (2022b), in which actions from an embodied agent are motiv"}]}
{"case_index": 220, "query": "From 2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt, what exact term fills [BLANK] in: \"Even though [BLANK] still makes simple mistakes, our results show that Ô¨Åne-tuning with human feedback is a promising direction for aligning language models with human intent.\"?", "gold": "instructgpt", "answer": "In the sentence, **[BLANK]** is filled by **‚ÄúInstructGPT‚Äù**:\n\n‚ÄúEven though **InstructGPT** still makes simple mistakes, our results show that Ô¨Åne-tuning with human feedback is a promising direction for aligning language models with human intent.‚Äù", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer correctly identifies the blank as 'InstructGPT', which is explicitly present in the provided context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 61.502, "llm_ms": 1643.541, "top_contexts": [{"rank": 1, "score": 0.66380495, "source": "dense+rrf", "rrf_score": 0.031544957774465976, "rerank_score": 0.42808468649486725, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 3, "text_snippet": "T-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT"}, {"rank": 2, "score": 0.69061905, "source": "dense+rrf", "rrf_score": 0.031754032258064516, "rerank_score": 0.32344682056737384, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 25, "text_snippet": "ent even on tasks for which they get very little direct supervision signal. InstructGPT still makes simple mistakes. For example, InstructGPT can still fail to follow instructions, make up facts, give long hedging answers to simple question"}, {"rank": 3, "score": 31.640979325315342, "source": "dense+rrf", "rrf_score": 0.013888888888888888, "rerank_score": 0.25275107917441514, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 139, "text_snippet": "ning ML models on public NLP datasets (Ziegler et al., 2019; Stiennon et al., 2020). Our work provides grounding for alignment research in AI systems that are being used in production in 17  the real world with customers.10This enables an i"}, {"rank": 4, "score": 0.6996955, "source": "dense+rrf", "rrf_score": 0.03128054740957967, "rerank_score": 0.24770920045757122, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 1, "text_snippet": ". For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language m"}, {"rank": 5, "score": 0.69995975, "source": "dense+rrf", "rrf_score": 0.029551337359792925, "rerank_score": 0.24736335844761387, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 28, "text_snippet": "eedback. We build on previous techniques to align models with human intentions, particularly reinforcement learning from human feed- back (RLHF). Originally developed for training simple robots in simulated environments and Atari games (Chr"}]}
{"case_index": 221, "query": "From 2305.06983__active_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"We discard [BLANK] retrieved documents ‚à™t‚Ä≤<tDqt‚Ä≤and only use the retrieved documents from the current step to condition the next generation to prevent reaching the input length limit of LMs.\"?", "gold": "previously", "answer": "The exact term is **‚Äúpreviously‚Äù**, i.e., **‚ÄúWe discard previously retrieved documents ‚à™t‚Ä≤<tDqt‚Ä≤ ‚Ä¶‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Model answer matches expected term 'previously' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 50.804, "llm_ms": 1229.745, "top_contexts": [{"rank": 1, "score": 0.7204939, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.48762761249153347, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 24, "text_snippet": "t, and the input to LMs is the concatena- tion of the retrieved documents Dqt, the user input x, and the previous generation y<t. We discard previously retrieved documents ‚à™t‚Ä≤<tDqt‚Ä≤and only use the retrieved documents from the current step "}, {"rank": 2, "score": 0.58846045, "source": "dense+rrf", "rrf_score": 0.030834914611005692, "rerank_score": 0.2808881108195729, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 16, "text_snippet": ". When deciding what to retrieve , it is important to consider what LMs intend to gen- erate in the future, as the goal of active retrieval is to benefit future generations. Therefore, we propose anticipating the future by generating a temp"}, {"rank": 3, "score": 27.907237180038532, "source": "dense+rrf", "rrf_score": 0.013888888888888888, "rerank_score": 0.24790812858910846, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 11, "text_snippet": "r of documents in the input context (akin to retrieving more or less documents in retrieval-augmented generation), and (ii) control the position of the relevant information within the input context by changing the order of the documents to "}, {"rank": 4, "score": 0.6489489, "source": "dense+rrf", "rrf_score": 0.0315136476426799, "rerank_score": 0.24217951957431819, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 23, "text_snippet": "and previously generated output y<t= [y0, ...,yt‚àí1]: qt=qry(x,y<t), where qry(¬∑)is the query formulation function. At the beginning ( t= 1), the previous generation is empty ( y<1=‚àÖ), and the user input is used as the initial query ( q1=x)."}, {"rank": 5, "score": 0.60175323, "source": "dense+rrf", "rrf_score": 0.03125763125763126, "rerank_score": 0.23958485958485473, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 22, "text_snippet": " generate the complete answer at once y=LM([Dx,x]). 2.3 Active Retrieval Augmented Generation To aid long-form generation with retrieval, we pro- pose active retrieval augmented generation. It is a generic framework that actively decides wh"}]}
{"case_index": 222, "query": "From 2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt, what exact term fills [BLANK] in: \"Some of these approaches are [BLANK] (Karpukhin et al., 2020; Izacard et al., 2022; Gu\"?", "gold": "retrieval-based", "answer": "The exact term is **‚Äúretrieval-based‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term and is explicitly supported by Context 1 ('Some of these approaches are retrieval-based').", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 50.727, "llm_ms": 1093.569, "top_contexts": [{"rank": 1, "score": 47.93910130776057, "source": "dense+rrf", "rrf_score": 0.01639344262295082, "rerank_score": 0.3039459165586432, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 21, "text_snippet": " further fine-tuned with long questions and the corresponding answers. We design various types of questions for technical papers, science fiction, and other books. SFT is important for improving the chat ability of LLMs. We introduce our SF"}, {"rank": 2, "score": 0.63415235, "source": "dense+rrf", "rrf_score": 0.031054405392392875, "rerank_score": 0.2476486993958438, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 81, "text_snippet": " al., 2022), fine-tuning the retriever and reader jointly (Yu, 2022; Izacard et al., 2022; Singh et al., 2021; Izacard & Grave, 2020a), and integrating the retriever with the black-box language model (Yu et al., 2023; Shi et al., 2023; Triv"}, {"rank": 3, "score": 30.54599340514203, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.23123001453169573, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 41, "text_snippet": "eads respectively. This manner does not increase additional computation costs but enables the information flow between different groups. We show that it gets close to the standard attention baseline in Table 1. Consistency to Full Attention"}, {"rank": 4, "score": 33.16906791120817, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.21433175872441543, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 80, "text_snippet": "le (Chen et al., 2017; Guu et al., 2020), where the input query re- trieves information from a corpus, and a language model uses this information as additional context to make a final prediction. Recent work has focused on improving the ret"}, {"rank": 5, "score": 0.6780902, "source": "dense+rrf", "rrf_score": 0.03177805800756621, "rerank_score": 0.20560980415619548, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 29, "text_snippet": "g (Zhu et al., 2023), and methods based on out-of-distribution analysis (Han et al., 2023). Our method focuses on efficient fine-tuning and retaining the original architecture during inference, which is orthogonal to these position embeddin"}]}
{"case_index": 223, "query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \"[CLS] is a special symbol added in front of every input example, and [SEP] is a special [BLANK] token (e.g.\"?", "gold": "separator", "answer": "The exact term is **‚Äúseparator‚Äù** (as in ‚Äúa special **separator** token‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'separator' and is directly supported by Context 1 ('[SEP] is a special separator token').", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 58.43, "llm_ms": 1231.965, "top_contexts": [{"rank": 1, "score": 0.62946284, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3755536395752878, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 20, "text_snippet": "or BERT. Apart from output layers, the same architec- tures are used in both pre-training and Ô¨Åne-tuning. The same pre-trained model parameters are used to initialize models for different down-stream tasks. During Ô¨Åne-tuning, all parameters"}, {"rank": 2, "score": 32.83493084446756, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.2621324653357797, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 109, "text_snippet": "reading books. In Proceedings of the IEEE international conference on computer vision , pages 19‚Äì27. Appendix for ‚ÄúBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding‚Äù We organize the appendix into three section"}, {"rank": 3, "score": 0.49302188, "source": "dense+rrf", "rrf_score": 0.0293236301369863, "rerank_score": 0.2425079173513769, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 127, "text_snippet": " Our task-speciÔ¨Åc models are formed by incorporating BERT with one additional output layer, so a minimal num- ber of parameters need to be learned from scratch. Among the tasks, (a) and (b) are sequence-level tasks while (c) and (d) are tok"}, {"rank": 4, "score": 44.97033704715814, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.2417397240515854, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 476, "text_snippet": "heng, and Jingjing Liu. Uniter: Learning universal image-text representations. arXiv preprint arXiv:1909.11740 , 2019. [Cra17] Kate Crawford. The trouble with bias. NIPS 2017 Keynote , 2017. [DCLT18] Jacob Devlin, Ming-Wei Chang, Kenton Lee"}, {"rank": 5, "score": 41.521911403702426, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.2380866579299632, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 0, "text_snippet": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova Google AI Language {jacobdevlin,mingweichang,kentonl,kristout }@google.com Abstract We introduce a ne"}]}
{"case_index": 224, "query": "From 2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt, what exact term fills [BLANK] in: \"where [BLANK] sparse vector space models, such as TF-IDF or BM25, are the de facto method.\"?", "gold": "traditional", "answer": "The exact term is **‚Äútraditional‚Äù**, as in: ‚Äúwhere **traditional** sparse vector space models, such as TF-IDF or BM25, are the de facto method.‚Äù", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'traditional' and is directly supported by Context 1/2.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 47.663, "llm_ms": 1552.759, "top_contexts": [{"rank": 1, "score": 0.6745073, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.37338539955046696, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 1, "text_snippet": "where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented us- ingdense representations alone, where em- beddings are learned from a sma"}, {"rank": 2, "score": 0.62477267, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.32259767969177144, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 0, "text_snippet": "Dense Passage Retrieval for Open-Domain Question Answering Vladimir Karpukhin‚àó, Barlas O Àòguz‚àó, Sewon Min‚Ä†, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen‚Ä°, Wen-tau Yih Facebook AI‚Ä†University of Washington‚Ä°Princeton University {vladk, "}, {"rank": 3, "score": 0.6387669, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.26158512093145897, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 77, "text_snippet": "y comparable to ORQA‚Äôs 5-passage setup. 7 Related Work Passage retrieval has been an important compo- nent for open-domain QA (V oorhees, 1999). It not only effectively reduces the search space for answer extraction, but also identiÔ¨Åes the "}, {"rank": 4, "score": 37.14167344847099, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.2063353303753862, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 4, "text_snippet": " degradation is often observed in practice2, indicating the needs of improving retrieval. ‚àóEqual contribution 1The code and trained models have been released at https://github.com/facebookresearch/DPR. 2For instance, the exact match score o"}, {"rank": 5, "score": 0.5631106, "source": "dense+rrf", "rrf_score": 0.027402402402402402, "rerank_score": 0.19950898050954294, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 99, "text_snippet": "las Joseph, Ben Mann, Sam McCandlish, Chris Olah, and Jared Kaplan. 2022. Language models (mostly) know what they know. CoRR , abs/2207.05221. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H. Lewis, Ledell Wu, Sergey Edunov, Danqi "}]}
{"case_index": 225, "query": "From 2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt, what exact term fills [BLANK] in: \"The Naive RAG follows a traditional process that includes indexing, retrieval, and generation, which is also characterized as a ‚Äú[BLANK]‚Äù framework [7].\"?", "gold": "retrieve-read", "answer": "The exact term is **‚ÄúRetrieve-Read‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context explicitly states the Naive RAG is characterized as a ‚ÄúRetrieve-Read‚Äù framework; model answer matches expected term (case-insensitive).", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 56.23, "llm_ms": 1048.819, "top_contexts": [{"rank": 1, "score": 0.66412264, "source": "dense+rrf", "rrf_score": 0.032018442622950824, "rerank_score": 0.39905012031879616, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 20, "text_snippet": "er. widespread adoption of ChatGPT. The Naive RAG follows a traditional process that includes indexing, retrieval, and generation, which is also characterized as a ‚ÄúRetrieve-Read‚Äù framework [7]. Indexing starts with the cleaning and extract"}, {"rank": 2, "score": 0.61304855, "source": "dense+rrf", "rrf_score": 0.028949545078577336, "rerank_score": 0.22592346248270767, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 11, "text_snippet": "h a detailed and structured understanding of both large models and RAG. It aims to illuminate the evolution of retrieval augmentation techniques, assess the strengths and weaknesses of various approaches in their respective contexts, and sp"}, {"rank": 3, "score": 0.63626987, "source": "dense+rrf", "rrf_score": 0.02976190476190476, "rerank_score": 0.223684802533103, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 28, "text_snippet": "Left) Naive RAG mainly consists of three parts: indexing, retrieval and generation. (Middle) Advanced RAG proposes multiple optimization strategies around pre-retrieval and post-retrieval, with a process similar to the Naive RAG, still foll"}, {"rank": 4, "score": 0.64404714, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.21470154668817965, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 100, "text_snippet": "ohere rerank or bge-raranker-large, and general large language mod- els like GPT [12], [99]. 2) Context Selection/Compression: A common misconcep- tion in the RAG process is the belief that retrieving as many relevant documents as possible "}, {"rank": 5, "score": 27.08301182820931, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.20289855072463353, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 22, "text_snippet": " generate the complete answer at once y=LM([Dx,x]). 2.3 Active Retrieval Augmented Generation To aid long-form generation with retrieval, we pro- pose active retrieval augmented generation. It is a generic framework that actively decides wh"}]}
{"case_index": 226, "query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \"Model Architecture BERT‚Äôs model architec- ture is a multi-layer bidirectional Transformer en- coder based on the original [BLANK] de- scribed in Vaswani et al.\"?", "gold": "implementation", "answer": "The exact term is **‚Äúimplementation‚Äù** (i.e., ‚Äúbased on the original **implementation** described in Vaswani et al.‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'implementation' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 62.931, "llm_ms": 1169.544, "top_contexts": [{"rank": 1, "score": 0.72892296, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.4368203550169144, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 24, "text_snippet": "architec- ture and the Ô¨Ånal downstream architecture. Model Architecture BERT‚Äôs model architec- ture is a multi-layer bidirectional Transformer en- coder based on the original implementation de- scribed in Vaswani et al. (2017) and released "}, {"rank": 2, "score": 0.596993, "source": "dense+rrf", "rrf_score": 0.03128054740957967, "rerank_score": 0.27338694824340865, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 0, "text_snippet": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova Google AI Language {jacobdevlin,mingweichang,kentonl,kristout }@google.com Abstract We introduce a ne"}, {"rank": 3, "score": 0.5548217, "source": "dense+rrf", "rrf_score": 0.027242401779095628, "rerank_score": 0.2679332109782336, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 10, "text_snippet": "ford et al. (2018), which uses unidirec- tional language models for pre-training, BERT uses masked language models to enable pre- trained deep bidirectional representations. This is also in contrast to Peters et al. (2018a), which uses a sh"}, {"rank": 4, "score": 0.6061425, "source": "dense+rrf", "rrf_score": 0.0293236301369863, "rerank_score": 0.2632901181114782, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 114, "text_snippet": "  E1 E2 EN...Figure 3: Differences in pre-training model architectures. BERT uses a bidirectional Transformer. OpenAI GPT uses a left-to-right Transformer. ELMo uses the concatenation of independently trained left-to-right and right-to- lef"}, {"rank": 5, "score": 40.62366295726071, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.24724997295624937, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 19, "text_snippet": "1.5e‚àí44M 1.4T Table 2: Model sizes, architectures, and optimization hyper-parameters. Overall, our entire training dataset contains roughly 1.4T tokens after tokenization. For most of our training data, each token is used only once dur- ing"}]}
{"case_index": 227, "query": "From 2005.14165__language_models_are_few_shot_learners.txt, what exact term fills [BLANK] in: \"ew-shot performance increases more rapidly, [BLANK] that larger models are more proÔ¨Åcient at in-context learning.\"?", "gold": "demonstrating", "answer": "The exact term is **‚Äúdemonstrating‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected blank term 'demonstrating' and is directly supported by Context 1/4.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 48.469, "llm_ms": 942.024, "top_contexts": [{"rank": 1, "score": 0.65687215, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3205522026961079, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 28, "text_snippet": "ew-shot performance increases more rapidly, demonstrating that larger models are more proÔ¨Åcient at in-context learning. See Figure 3.8 for a more detailed analysis on SuperGLUE, a standard NLP benchmark suite. In this paper, we test this hy"}, {"rank": 2, "score": 31.109391906341482, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.22163336894438718, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 38, "text_snippet": "able pattern is that the gap between zero-, one-, and few-shot performance often grows with model capacity, perhaps suggesting that larger models are more proÔ¨Åcient meta-learners. Finally, given the broad spectrum of capabilities displayed "}, {"rank": 3, "score": 29.11384057515182, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.20479866934620183, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 18, "text_snippet": "s at inference time to rapidly adapt to or recognize the desired task. We use the term ‚Äúin-context learning‚Äù to describe the inner loop of this process, which occurs within the forward-pass upon each sequence. The sequences in this diagram "}, {"rank": 4, "score": 0.5946328, "source": "dense+rrf", "rrf_score": 0.02976190476190476, "rerank_score": 0.20472411959153172, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 27, "text_snippet": "of whether the model learns new tasks from scratch at inference time or simply recognizes patterns seen during training ‚Äì this is an important issue which we discuss later in the paper, but ‚Äúmeta-learning‚Äù is intended to encompass both poss"}, {"rank": 5, "score": 22.951313364416844, "source": "dense+rrf", "rrf_score": 0.014285714285714285, "rerank_score": 0.19968427537918718, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 321, "text_snippet": "del and data scaling decisions for the research. Ben Mann implemented sampling without replacement during training. Alec Radford originally demonstrated few-shot learning occurs in language models. Jared Kaplan and Sam McCandlish showed tha"}]}
{"case_index": 228, "query": "From 2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt, what exact term fills [BLANK] in: \"Causality is maintained as [BLANK] of the Ô¨Årst chunk only aÔ¨Äect the last token of the Ô¨Årst chunk and tokens from the second chunk.\"?", "gold": "neighbours", "answer": "The exact term is **‚Äúneighbours‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'neighbours' and isfinally supported verbatim by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 63.819, "llm_ms": 953.795, "top_contexts": [{"rank": 1, "score": 0.65196395, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.365767981102722, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 18, "text_snippet": "c/a.scoperator. Causality is maintained as neighbours of the Ô¨Årst chunk only aÔ¨Äect the last token of the Ô¨Årst chunk and tokens from the second chunk. a new methodology to evaluate language models when an evaluation set is partially present "}, {"rank": 2, "score": 0.5871888, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.3276702116840516, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 38, "text_snippet": "t token of chunk ùê∂ùë¢is the Ô¨Årst to be able to access the retrieved content ùê∏ùë¢while maintaining autoregressivity in(1). Hence, there is a one token overlap between chunk ùê∂ùë¢=\u0010 ùë•¬πùë¢\u00001¬∫ùëö¬∏ùëñ\u0011 ùëñ2¬ª1¬îùëö¬ºand the corresponding attending chunk ùê∂¬∏ ùë¢,¬πùë•ùë¢ùëö¬∏ùëñ"}, {"rank": 3, "score": 27.861196479601162, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.2889373274983086, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 173, "text_snippet": "n Conference of the North American Chapter of the Association for Computational Linguistics , 2018. 23  Improving language models by retrieving from trillions of tokens A. Datasets We provide a full description of MassiveText and of our ext"}, {"rank": 4, "score": 0.54592115, "source": "dense+rrf", "rrf_score": 0.029910714285714284, "rerank_score": 0.28695788633164443, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 22, "text_snippet": " ùïç=¬ª1¬îùë£¬º, obtained using a text tokenizer1. Wespliteach ùëõ-token-longexample ùëã=¬πùë•1¬î¬ì¬ì¬ì¬îùë•ùëõ¬∫intoasequenceof ùëôchunks¬πùê∂1¬î¬ì¬ì¬ì¬îùê∂ùëô¬∫ of sizeùëö=ùëõ ùëô, i.e.ùê∂1,¬πùë•1¬î¬ì¬ì¬ì¬îùë•ùëö¬∫¬î ¬ì¬ì¬ì¬î ùê∂ùëô,¬πùë•ùëõ\u0000ùëö¬∏1¬î¬ì¬ì¬ì¬îùë•ùëõ¬∫2ùïçùëö. We useùëõ=2048andùëö=64. We augment each chunk ùê∂ùë¢with a se"}, {"rank": 5, "score": 0.5649219, "source": "dense+rrf", "rrf_score": 0.030536130536130537, "rerank_score": 0.2712722690521755, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 242, "text_snippet": "37  Improving language models by retrieving from trillions of tokens Table 16jGreat Circle (novel) , from Wikipedia September 21. The article is about a recent novel and chunks ùê∂3andùê∂4are speciÔ¨Åcally about its reception. The name Publishers"}]}
{"case_index": 229, "query": "From 2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt, what exact term fills [BLANK] in: \"Document [1](Title: Asian Americans in science and [BLANK]) Prize in physics for discovery of the subatomic particle J/œà.\"?", "gold": "technology", "answer": "The exact term that fills **[BLANK]** is **‚Äútechnology‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term and is directly supported by Context 1/4 showing the title 'Asian Americans in science and technology'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 49.587, "llm_ms": 2002.192, "top_contexts": [{"rank": 1, "score": 0.70278406, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.30040973172615515, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 29, "text_snippet": "ranked order, we explored randomly ordering the k‚àí1 distractor documents and mentioning that the documents are randomly ordered in the task description, but found the same trends. See Appendix C for more details.  Write a high-quality answe"}, {"rank": 2, "score": 29.326203917867208, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.2595741298925309, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 49, "text_snippet": "UIDs. The relevant key-value pair for answering the query is bolded here within the input context for clarity. indicate that extended-context models are not nec- essarily better than their non-extended counterparts at using their input cont"}, {"rank": 3, "score": 44.682453898816405, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.25843598106150795, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 207, "text_snippet": "‚Äù arXiv preprint arXiv:2310.07554 , 2023. [98] N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni, and P. Liang, ‚ÄúLost in the middle: How language models use long contexts,‚Äù arXiv preprint arXiv:2307.03172 , 2023. [99] Y "}, {"rank": 4, "score": 0.5648881, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.2518292060573683, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 35, "text_snippet": " [1](Title: Asian Americans in science and  technology) ...  Document [2](Title: List of Nobel laureates in  Physics) ...  Document [3](Title: Scientist) ...  Document [4](Title: Norwegian Americans) ...  Document [5](Title: Maria Goeppert "}, {"rank": 5, "score": 26.15324657630917, "source": "dense+rrf", "rrf_score": 0.013888888888888888, "rerank_score": 0.24549299234262711, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 2, "text_snippet": "urs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts, even for explicitly long-context models. Our analysis provides a better understanding "}]}
{"case_index": 230, "query": "From 2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt, what exact term fills [BLANK] in: \"If language models can robustly use information within long input con- texts, then their [BLANK] should be minimally affected by the position of the relevant information in the input context.\"?", "gold": "performance", "answer": "The exact term that fills **[BLANK]** is **‚Äúperformance‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'performance' and is directly supported by the provided context sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 74.847, "llm_ms": 1308.382, "top_contexts": [{"rank": 1, "score": 0.68542856, "source": "dense+rrf", "rrf_score": 0.032266458495966696, "rerank_score": 0.4607641421234377, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 9, "text_snippet": "at require accessing and using information within an input context. In particular, our experi- ments make controlled changes to the input context size and the position of the relevant information within the input context and study their eff"}, {"rank": 2, "score": 0.6515055, "source": "dense+rrf", "rrf_score": 0.031054405392392875, "rerank_score": 0.3580481640434955, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 21, "text_snippet": "eir input context and introduces new evaluation protocols for future long- context models; to claim that a language model can robustly use information within long input con- texts, it is necessary to show that its performance is minimally a"}, {"rank": 3, "score": 0.64257014, "source": "dense+rrf", "rrf_score": 0.030158730158730156, "rerank_score": 0.3181913350604225, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 49, "text_snippet": "UIDs. The relevant key-value pair for answering the query is bolded here within the input context for clarity. indicate that extended-context models are not nec- essarily better than their non-extended counterparts at using their input cont"}, {"rank": 4, "score": 0.6405988, "source": "dense+rrf", "rrf_score": 0.029236022193768675, "rerank_score": 0.2994993437930661, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 11, "text_snippet": "r of documents in the input context (akin to retrieving more or less documents in retrieval-augmented generation), and (ii) control the position of the relevant information within the input context by changing the order of the documents to "}, {"rank": 5, "score": 40.102376211454335, "source": "dense+rrf", "rrf_score": 0.013888888888888888, "rerank_score": 0.28073427834428993, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 14, "text_snippet": "r at using their input context (¬ß2.3). Given that language models struggle to retrieve and use relevant information in the multi-document question answering task, to what extent can lan- guage models even retrieve from their input con- text"}]}
{"case_index": 231, "query": "From 2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt, what exact term fills [BLANK] in: \"Sample a diverse set of [BLANK] paths She eats 3 for breakfast, so she has 16 - 3 = 13 left.\"?", "gold": "reasoning", "answer": "The exact term is **‚Äúreasoning‚Äù**, as in: ‚ÄúSample a diverse set of **reasoning** paths ‚Ä¶‚Äù.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'reasoning' and is directly supported by the provided context phrase 'Sample a diverse set of reasoning paths'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 43.384, "llm_ms": 1419.939, "top_contexts": [{"rank": 1, "score": 0.5263455, "source": "dense+rrf", "rrf_score": 0.031099324975891997, "rerank_score": 0.3198567911408747, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 9, "text_snippet": "rking lot? A: There are 3 cars in the parking lot  already. 2 more arrive. Now there are  3 + 2 = 5 cars. The answer is 5.  ‚Ä¶ Q: Janet‚Äôs ducks lay 16 eggs per day.  She eats three for breakfast every  morning and bakes muffins for her  frie"}, {"rank": 2, "score": 0.5073807, "source": "dense+rrf", "rrf_score": 0.029957522915269395, "rerank_score": 0.31492288287154, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 11, "text_snippet": "mainder for $2 per egg, so in  total she sells 7 * $2 = $14 per day.  The answer is $14. The answer is $14. Greedy decode Figure 1: The self-consistency method contains three steps: (1) prompt a language model using chain-of-thought (CoT) p"}, {"rank": 3, "score": 0.59578085, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.2897086679337331, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 10, "text_snippet": "g paths She eats 3 for breakfast, so  she has 16 - 3 = 13 left. Then  she bakes muffins, so she  has 13 - 4 = 9 eggs left. So  she has 9 eggs * $2 = $18. This means she she sells the  remainder for $2 * (16 - 4 - 3)  = $26 per day. The answ"}, {"rank": 4, "score": 0.5756002, "source": "dense+rrf", "rrf_score": 0.03128054740957967, "rerank_score": 0.24450006217636042, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 22, "text_snippet": " a prompt and a question, self-consistency introduces an additional latent variable ri, which is a sequence of tokens representing the reasoning path in the i-th output, then couples the generation of (ri,ai)where ri‚Üíai, i.e., generating a "}, {"rank": 5, "score": 38.731120659519064, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.23773805585014962, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 109, "text_snippet": ", Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022. LaMDA: Language models for dialog applications. arXiv preprint arXiv:2201.08239 . Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and"}]}
{"case_index": 232, "query": "From 2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt, what exact term fills [BLANK] in: \"operates on long retrieval units, with only a few ([BLANK] fewer than 10) being fed into the reader.\"?", "gold": "typically", "answer": "The exact term that fills **[BLANK]** is **‚Äútypically‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'typically' and is directly supported by Context 1/2 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 44.602, "llm_ms": 1435.372, "top_contexts": [{"rank": 1, "score": 0.6811534, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.3836281116090204, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 21, "text_snippet": "operates on long retrieval units, with only a few (typically fewer than 10) being fed into the reader. An illustrative example is shown in Figure 2. 2.1 Long Retriever The traditional RAG framework employs smaller retrieval units and priori"}, {"rank": 2, "score": 0.64171034, "source": "dense+rrf", "rrf_score": 0.030158730158730156, "rerank_score": 0.2665673249433118, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 20, "text_snippet": "ontext methods evolve, the performance of LongRAG will continue to improve. Therefore, we believe the modern RAG systems should re-consider the granularity of their retrieval units to exploit the advantages of the current long-context LLMs."}, {"rank": 3, "score": 0.6685934, "source": "dense+rrf", "rrf_score": 0.031754032258064516, "rerank_score": 0.2550294183781884, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 7, "text_snippet": "assive corpus with up to tens of millions of information units). Subsequently, the retrieved units are passed to the reader to generate the final response. On the contrary, the reader only needs to extract answers from 1arXiv:2406.15319v3 ["}, {"rank": 4, "score": 40.047803872884266, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.24683371833797169, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 27, "text_snippet": "stralian Football League‚Ä¶  Reader  Long  Reader Figure 2: LongRAG example. We form the long retrieval unit which is at least 4K tokens by using the entire document or grouping related documents, depending on the orinal docuemnt size. In thi"}, {"rank": 5, "score": 0.6122034, "source": "dense+rrf", "rrf_score": 0.028309409888357256, "rerank_score": 0.23938450096492345, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 8, "text_snippet": "ever needs to scan over a massive amount of units to find the relevant piece. In contrast, LongRAG operates on long retrieval units (30x longer). The retriever of LongRAG has a significantly reduced workload, achieving strong retrieval qual"}]}
{"case_index": 233, "query": "From 2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt, what exact term fills [BLANK] in: \"RAG bridges this information gap by sourcing and [BLANK] knowledge from external databases.\"?", "gold": "incorporating", "answer": "The exact term is **‚Äúincorporating‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'incorporating' and is directly supported by Context 2.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 60.508, "llm_ms": 1141.822, "top_contexts": [{"rank": 1, "score": 0.6491682, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.2873307359455095, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 1, "text_snippet": "dels (LLMs) showcase impres- sive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution b"}, {"rank": 2, "score": 48.3825626777341, "source": "dense+rrf", "rrf_score": 0.01639344262295082, "rerank_score": 0.2549799705108616, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 17, "text_snippet": "ning data, it initially lacks the capacity to provide up- dates on recent developments. RAG bridges this information gap by sourcing and incorporating knowledge from external databases. In this case, it gathers relevant news articles relate"}, {"rank": 3, "score": 0.60551476, "source": "dense+rrf", "rrf_score": 0.026171741361614778, "rerank_score": 0.23381957904952408, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 79, "text_snippet": "on the long-context understanding ability of LLMs, and we still have a long way to go in harnessing open-source LLMs within our framework. 10  4 Related Work 4.1 Retrieval-Augmented Generation. Augmenting language models with information re"}, {"rank": 4, "score": 27.299152765923438, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.2273529759324315, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 238, "text_snippet": " maximizing llm performance,‚Äù https://community.openai. com/t/openai-dev-day-2023-breakout-sessions/505213# a-survey-of-techniques-for-maximizing-llm-performance-2, 2023. [167] J. Chen, H. Lin, X. Han, and L. Sun, ‚ÄúBenchmarking large lan- g"}, {"rank": 5, "score": 0.58088785, "source": "dense+rrf", "rrf_score": 0.02804284323271665, "rerank_score": 0.22642954763609158, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 239, "text_snippet": " against external counterfactual knowledge,‚Äù arXiv preprint arXiv:2311.08147 , 2023. [169] Y . Lyu, Z. Li, S. Niu, F. Xiong, B. Tang, W. Wang, H. Wu, H. Liu, T. Xu, and E. Chen, ‚ÄúCrud-rag: A comprehensive chinese benchmark for retrieval-aug"}]}
{"case_index": 234, "query": "From 2302.13971__llama_open_and_efficient_foundation_language_models.txt, what exact term fills [BLANK] in: \"Correspondence: {htouvron, thibautlav,gizacard,egrave,glample}@meta.com 1https://github.com/facebookresearch/[BLANK], a smaller one trained longer will ultimately be cheaper at inference.\"?", "gold": "llamaperformance", "answer": "The exact term is **`llama`**.", "answer_score": 0.0, "answer_correctness": 0.0, "answer_groundedness": 0.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 0.0, "abstained": false, "should_abstain": false, "notes": "Context explicitly shows the URL ends with facebookresearch/llamaperformance, not 'llama'. The answer is incorrect and not supported by the provided context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 46.567, "llm_ms": 1122.313, "top_contexts": [{"rank": 1, "score": 0.73454285, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.4234955762742548, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 4, "text_snippet": "ng a language model at scale. In this context, given a target level of performance, the preferred model is not the fastest to train but the fastest at inference, and although it may be cheaper to train a large model to reach a certain level"}, {"rank": 2, "score": 31.311554776769327, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.17816872309129733, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 4, "text_snippet": " degradation is often observed in practice2, indicating the needs of improving retrieval. ‚àóEqual contribution 1The code and trained models have been released at https://github.com/facebookresearch/DPR. 2For instance, the exact match score o"}, {"rank": 3, "score": 0.5647787, "source": "dense+rrf", "rrf_score": 0.013888888888888888, "rerank_score": 0.16417178648945516, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 3, "text_snippet": ", recent work from Hoffmann et al. (2022) shows that, for a given compute budget, the best performances are not achieved by the largest mod- els, but by smaller models trained on more data. The objective of the scaling laws from Hoff- mann "}, {"rank": 4, "score": 21.461993486745364, "source": "dense+rrf", "rrf_score": 0.014285714285714285, "rerank_score": 0.15935928346648762, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 21, "text_snippet": "his same Ô¨Çuidity and generality. One potential route towards addressing these issues is meta-learning1‚Äì which in the context of language models means the model develops a broad set of skills and pattern recognition abilities at training tim"}, {"rank": 5, "score": 0.6421963, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.14730353945163607, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 116, "text_snippet": "Marie-Anne Lachaux, Timoth¬¥ ee Lacroix, Baptiste Rozi` ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. arXiv preprint"}]}
{"case_index": 235, "query": "From 2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt, what exact term fills [BLANK] in: \"Attention only computes in each group in ours while the [BLANK] flows between groups via shifting.\"?", "gold": "information", "answer": "The exact term is **‚Äúinformation‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected blank term 'information' and is directly supported by Context 1 sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 50.294, "llm_ms": 992.194, "top_contexts": [{"rank": 1, "score": 0.6808676, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.2740176962643702, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 18, "text_snippet": "are shifted by half of the group size. Third, we split tokens into groups and reshape them into batch dimensions. Attention only computes in each group in ours while the information flows between groups via shifting. Potential information l"}, {"rank": 2, "score": 0.5626753, "source": "dense+rrf", "rrf_score": 0.028629032258064516, "rerank_score": 0.21969406182376486, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 42, "text_snippet": " Child et al., 2019), designed for training from scratch, have gaps to the standard full attention, which is used in pre-training. In Table 6, we show that S2-Attn not only enables efficient fine-tuning but also supports fullattention testi"}, {"rank": 3, "score": 0.6652268, "source": "dense+rrf", "rrf_score": 0.031054405392392875, "rerank_score": 0.21037379790684865, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 41, "text_snippet": "eads respectively. This manner does not increase additional computation costs but enables the information flow between different groups. We show that it gets close to the standard attention baseline in Table 1. Consistency to Full Attention"}, {"rank": 4, "score": 0.6086488, "source": "dense+rrf", "rrf_score": 0.030117753623188408, "rerank_score": 0.20867970956659296, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 13, "text_snippet": "nds the context windows of pre-trained LLMs, e.g., Llama2 (Touvron et al., 2023b). LoRA (Hu et al., 2022) uses low-rank weight updates to approximate full fine-tuning. Similarly, we find that short attention is also able to approximate long"}, {"rank": 5, "score": 33.984422551458564, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.20260883117830675, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 0, "text_snippet": "Published as a conference paper at ICLR 2024 LONG LORA: E FFICIENT FINE-TUNING OF LONG - CONTEXT LARGE LANGUAGE MODELS Yukang Chen1Shengju Qian1Haotian Tang2Xin Lai1 Zhijian Liu2Song Han2,3Jiaya Jia1 1CUHK2MIT3NVIDIA ABSTRACT We present Lon"}]}
{"case_index": 236, "query": "From 2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt, what exact term fills [BLANK] in: \"One main reason is that they focus on FLOP reduction (which may not correlate with [BLANK] speed) and tend to ignore overheads from memory\"?", "gold": "wall-clock", "answer": "The exact term is **‚Äúwall-clock‚Äù**, i.e., ‚Äú‚Ä¶may not correlate with **wall-clock** speed‚Ä¶‚Äù.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'wall-clock' and is directly supported by the provided context quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 52.676, "llm_ms": 1308.066, "top_contexts": [{"rank": 1, "score": 0.6872138, "source": "dense+rrf", "rrf_score": 0.032018442622950824, "rerank_score": 0.40576522044001806, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 6, "text_snippet": "memory requirements of attention. These methods range from sparse-approximation [ 51,74] to low-rank approximation [ 12,50,84], and their combinations [ 3,9,92]. Although these methods reduce the compute requirements to linear or near-linea"}, {"rank": 2, "score": 0.7158102, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.2721011277101303, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 7, "text_snippet": " with wall-clock speed) and tend to ignore overheads from memory access (IO). In this paper, we argue that a missing principle is making attention algorithms IO-aware [1]‚Äîthat is, carefully accounting for reads and writes to diÔ¨Äerent levels"}, {"rank": 3, "score": 54.73686116546747, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.23207995658231906, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 100, "text_snippet": "n, Atri Rudra, and Christopher R√©. 2022. FlashAttention: Fast and memory-efficient exact attention with IO- awareness. ArXiv:2205.14135. Hermann Ebbinghaus. 1913. Memory: A contribu- tion to experimental psychology. H. A. Ruger & C. E. Buss"}, {"rank": 4, "score": 0.6714113, "source": "dense+rrf", "rrf_score": 0.029411764705882353, "rerank_score": 0.22460899166323414, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 11, "text_snippet": "on of the runtime‚Äîsuch as database joins [71], image processing [ 70], numerical linear algebra [ 4], and more [ 40,85]. However, common Python interfaces to deep learning such as PyTorch and TensorÔ¨Çow do not allow Ô¨Åne-grained control of me"}, {"rank": 5, "score": 0.647033, "source": "dense+rrf", "rrf_score": 0.027598020555767037, "rerank_score": 0.21000647907545544, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 138, "text_snippet": "ory linear instead of quadratic in the sequence length). Though they reduce the amount of extra memory required, naively they still incur quadratic HBM accesses, resulting in slower execution speed. We describe the FlashAttention algorithm "}]}
{"case_index": 237, "query": "From 2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt, what exact term fills [BLANK] in: \"To this end, we analyze model performance on [BLANK] question answering, which requires models to find relevant information within an input context and use it to answer the question.\"?", "gold": "multi-document", "answer": "The exact term is **‚Äúmulti-document‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term and is directly supported by Context 1/4/5 stating 'multi-document question answering'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 60.275, "llm_ms": 1060.203, "top_contexts": [{"rank": 1, "score": 0.6870165, "source": "dense+rrf", "rrf_score": 0.032266458495966696, "rerank_score": 0.48251057078647575, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 22, "text_snippet": "e models use their input context, we release our code and evaluation data.1 2 Multi-Document Question Answering Our goal is to better understand how language mod- els use their input context. To this end, we analyze model performance on mul"}, {"rank": 2, "score": 0.70654166, "source": "dense+rrf", "rrf_score": 0.032266458495966696, "rerank_score": 0.32367116272846597, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 49, "text_snippet": "UIDs. The relevant key-value pair for answering the query is bolded here within the input context for clarity. indicate that extended-context models are not nec- essarily better than their non-extended counterparts at using their input cont"}, {"rank": 3, "score": 43.64345204087972, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.32107309812585405, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 11, "text_snippet": "r of documents in the input context (akin to retrieving more or less documents in retrieval-augmented generation), and (ii) control the position of the relevant information within the input context by changing the order of the documents to "}, {"rank": 4, "score": 0.6492506, "source": "dense+rrf", "rrf_score": 0.027071520029266508, "rerank_score": 0.32032359613114714, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 9, "text_snippet": "at require accessing and using information within an input context. In particular, our experi- ments make controlled changes to the input context size and the position of the relevant information within the input context and study their eff"}, {"rank": 5, "score": 0.6671221, "source": "dense+rrf", "rrf_score": 0.030330882352941176, "rerank_score": 0.3151139746282349, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 1, "text_snippet": " on two tasks that require identifying relevant information in their in- put contexts: multi-document question an- swering and key-value retrieval. We find that performance can degrade significantly when changing the position of relevant in"}]}
{"case_index": 238, "query": "From 2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt, what exact term fills [BLANK] in: \"Rather than naively imitating powerful LLMs, we treat them as a [BLANK] of behaviors from which we carefully select those best suited for the task at hand.\"?", "gold": "reservoir", "answer": "**reservoir**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "The blank is explicitly 'reservoir' in the provided context, and the model answered 'reservoir'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 64.139, "llm_ms": 796.288, "top_contexts": [{"rank": 1, "score": 0.66455853, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.44755694638573995, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 18, "text_snippet": "roach a particular task. Rather than naively imitating powerful LLMs, we treat them as a reservoir of behaviors from which we carefully select those best suited for the task at hand. Some previous studies on training small models are limite"}, {"rank": 2, "score": 0.5928266, "source": "dense+rrf", "rrf_score": 0.030776515151515152, "rerank_score": 0.2695510971046985, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 16, "text_snippet": "owing them to perform at their best, irrespective of their size. Like Orca 1, we utilize more capable LLMs to demonstrate various reasoning strategies across various tasks. However, in Orca 2, the reasoning strategies are carefully tailored"}, {"rank": 3, "score": 36.13552577331217, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.2686229225756683, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 15, "text_snippet": ", we continue to pursue the question of how we can teach smaller LMs to reason. The objectives of Orca 2 are two-fold. Firstly, we aim to teach smaller models howto use a suite of reasoning techniques, such as step-by-step processing, recal"}, {"rank": 4, "score": 0.5740231, "source": "dense+rrf", "rrf_score": 0.02946236559139785, "rerank_score": 0.248006005232686, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 17, "text_snippet": "fic strategic behaviors ‚Äì and more accurate results ‚Äì as exemplified in Figure 3. Furthermore, during the training phase, the smaller model is exposed only to the task and the resultant behavior, without visibility into the original prompts"}, {"rank": 5, "score": 0.5763219, "source": "dense+rrf", "rrf_score": 0.02976190476190476, "rerank_score": 0.23836043454743575, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 37, "text_snippet": ", naively teaching a smaller model to ‚Äúimitate‚Äù the reasoning behavior of a more powerful one may be sub-optimal. Although training smaller models towards step-by-step-explained answers has proven beneficial, training them on a plurality of"}]}
{"case_index": 239, "query": "From 2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt, what exact term fills [BLANK] in: \"Typically, training LLMs with long context sizes is [BLANK] expensive, requiring extensive training hours and GPU re\"?", "gold": "computationally", "answer": "The exact term is **‚Äúcomputationally‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'computationally' and is directly supported by Context 1 sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 54.742, "llm_ms": 1222.238, "top_contexts": [{"rank": 1, "score": 0.58385617, "source": "dense+rrf", "rrf_score": 0.031099324975891997, "rerank_score": 0.4232187053665587, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 0, "text_snippet": "Published as a conference paper at ICLR 2024 LONG LORA: E FFICIENT FINE-TUNING OF LONG - CONTEXT LARGE LANGUAGE MODELS Yukang Chen1Shengju Qian1Haotian Tang2Xin Lai1 Zhijian Liu2Song Han2,3Jiaya Jia1 1CUHK2MIT3NVIDIA ABSTRACT We present Lon"}, {"rank": 2, "score": 0.6457614, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.2540599183430342, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 1, "text_snippet": "ionally expensive, requiring extensive training hours and GPU resources. For example, training on the context length of 8192 needs 16 √ócomputational costs in self-attention layers as that of 2048. In this paper, we speed up the context exte"}, {"rank": 3, "score": 35.06034502144373, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.23497354649443444, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 76, "text_snippet": "pose LongLoRA that can efficiently extend the context length of LLMs to be significantly larger. LongLoRA has less GPU memory cost and training time than standard full fine-tuning, with minimal accuracy compromise. At the architecture level"}, {"rank": 4, "score": 33.259252117377805, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.23304699790839348, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 75, "text_snippet": "ed in training-from- scratch transformers. This experiment is to examine their capability of fine-tuning on pre-trained LLMs (Touvron et al., 2023b), toward long context adaptation. Dilated attention performs well in full fine-tuning but is"}, {"rank": 5, "score": 0.6688254, "source": "dense+rrf", "rrf_score": 0.031544957774465976, "rerank_score": 0.227657619992032, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 9, "text_snippet": "rkowski et al., 2023; Mohtashami & Jaggi, 2023) train or fine-tune LLMs to longer context. However, training an LLM from scratch with long sequences poses computational challenges, and fine-tuning an existing pre-trained LLM is also conside"}]}
{"case_index": 240, "query": "From 2302.13971__llama_open_and_efficient_foundation_language_models.txt, what exact term fills [BLANK] in: \"This imple- [BLANK], available in the xformers library,2is inspired by Rabe and Staats (2021) and uses the backward from Dao et al.\"?", "gold": "mentation", "answer": "The exact term is **‚Äúmentation of the causal multi-head attention‚Äù**.", "answer_score": 0.06, "answer_correctness": 0.0, "answer_groundedness": 0.2, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 0.2, "abstained": false, "should_abstain": false, "notes": "Blank should be filled by the word 'mentation' (as in 'This implementation...'). Model answer adds extra words and does not provide the exact term alone.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 43.352, "llm_ms": 1095.803, "top_contexts": [{"rank": 1, "score": 0.57861847, "source": "dense+rrf", "rrf_score": 0.030886196246139225, "rerank_score": 0.35303009763566096, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 24, "text_snippet": "ze of the model (see Table 2 for details). 2.4 EfÔ¨Åcient implementation We make several optimizations to improve the train- ing speed of our models. First, we use an efÔ¨Åcient implementation of the causal multi-head attention to reduce memory"}, {"rank": 2, "score": 25.43418677351799, "source": "dense+rrf", "rrf_score": 0.014084507042253521, "rerank_score": 0.1992963849116532, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 30, "text_snippet": "e non-publicly available language models GPT-3 (Brown et al., 2020), Gopher (Rae et al., 2021), Chinchilla (Hoffmann et al., 2022) and PaLM (Chowdhery et al., 2022), as well as the open-sourced OPT models (Zhang et al., 2022), GPT-J (Wang a"}, {"rank": 3, "score": 0.57738346, "source": "dense+rrf", "rrf_score": 0.03021353930031804, "rerank_score": 0.19041931218188443, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 116, "text_snippet": "Marie-Anne Lachaux, Timoth¬¥ ee Lacroix, Baptiste Rozi` ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. arXiv preprint"}, {"rank": 4, "score": 23.325833203830097, "source": "dense+rrf", "rrf_score": 0.013888888888888888, "rerank_score": 0.18446260836114747, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 89, "text_snippet": "y for transformer based language models, which were later reÔ¨Åned by Hoffmann et al. (2022), by adapting the learning rate schedule when scaling datasets. Finally, Wei et al. (2022) studied the effect of scal- ing on the abilities of large l"}, {"rank": 5, "score": 25.9185103277363, "source": "dense+rrf", "rrf_score": 0.014285714285714285, "rerank_score": 0.1730313947817461, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 123, "text_snippet": ". Training language models to follow instructions with human feedback. In Advances in Neural Infor- mation Processing Systems . Markus N Rabe and Charles Staats. 2021. Self- attention does not need o(n2)memory. arXiv preprint arXiv:2112.056"}]}
{"case_index": 241, "query": "From 2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt, what exact term fills [BLANK] in: \"1 left) [BLANK] multiple forms of memory of diÔ¨Äerent sizes and speeds, with smaller memor\"?", "gold": "comprises", "answer": "The exact term is **‚Äúcomprises‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context explicitly contains the phrase 'GPU memory hierarchy ... comprises multiple forms of memory...' matching expected blank.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 54.093, "llm_ms": 1247.376, "top_contexts": [{"rank": 1, "score": 0.6155305, "source": "dense+rrf", "rrf_score": 0.02921395544346364, "rerank_score": 0.3067847086339967, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 21, "text_snippet": "ll existing approximate attention methods that we know of. 2 Background We provide some background on the performance characteristics of common deep learning operations on modern hardware (GPUs). We also describe the standard implementation"}, {"rank": 2, "score": 54.585359666794915, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.26085028193941917, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 100, "text_snippet": "n, Atri Rudra, and Christopher R√©. 2022. FlashAttention: Fast and memory-efficient exact attention with IO- awareness. ArXiv:2205.14135. Hermann Ebbinghaus. 1913. Memory: A contribu- tion to experimental psychology. H. A. Ruger & C. E. Buss"}, {"rank": 3, "score": 0.6837713, "source": "dense+rrf", "rrf_score": 0.032266458495966696, "rerank_score": 0.25262200176694843, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 7, "text_snippet": " with wall-clock speed) and tend to ignore overheads from memory access (IO). In this paper, we argue that a missing principle is making attention algorithms IO-aware [1]‚Äîthat is, carefully accounting for reads and writes to diÔ¨Äerent levels"}, {"rank": 4, "score": 40.72925844810009, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.22216594607898454, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 84, "text_snippet": "mputer/RedPajama-Data . Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. CoRR , abs/2307.08691, 2023. Tri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra, and Christopher R ¬¥e. Flashattention: Fast an"}, {"rank": 5, "score": 38.97781246342425, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.21877425901853562, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 22, "text_snippet": "forms of memory of diÔ¨Äerent sizes and speeds, with smaller memory being faster. As an example, the A100 GPU has 40-80GB of high bandwidth memory (HBM) with bandwidth 1.5-2.0TB/s and 192KB of on-chip SRAM per each of 108 streaming multiproce"}]}
{"case_index": 242, "query": "From 2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt, what exact term fills [BLANK] in: \"However, these models often express unintended behaviors such as making up facts, generating biased or toxic text, or simply not following user [BLANK] (Bender et al., 2021; Bommasani et al., 2021; Kenton et al., 2021; Weidinger et al., 2021; Tamkin et al., 2021; Gehman et al., 2020).\"?", "gold": "instructions", "answer": "The exact term that fills **[BLANK]** is **‚Äúinstructions‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term and is directly supported by Context 1 ('not following user instructions').", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 48.539, "llm_ms": 1254.318, "top_contexts": [{"rank": 1, "score": 0.7752293, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.5040721105027098, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 4, "text_snippet": "rompted‚Äù to perform a range of natural language process- ing (NLP) tasks, given some examples of the task as input. However, these models often express unintended behaviors such as making up facts, generating biased or toxic text, or simply"}, {"rank": 2, "score": 0.67229426, "source": "dense+rrf", "rrf_score": 0.031754032258064516, "rerank_score": 0.26760821056628575, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 8, "text_snippet": "progress on aligning language models by training them to act in accordance with the user‚Äôs intention (Leike et al., 2018). This encompasses both explicit intentions such as following instructions and implicit intentions such as staying trut"}, {"rank": 3, "score": 0.65589076, "source": "dense+rrf", "rrf_score": 0.03149801587301587, "rerank_score": 0.2443593261109624, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 7, "text_snippet": "the next token on a webpage from the internet‚Äîis different from the objective ‚Äúfollow the user‚Äôs instructions helpfully and safely‚Äù (Radford et al., 2019; Brown et al., 2020; Fedus et al., 2021; Rae et al., 2021; Thoppilan et al., 2022). Th"}, {"rank": 4, "score": 0.6053384, "source": "dense+rrf", "rrf_score": 0.028629032258064516, "rerank_score": 0.23666591412745847, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 34, "text_snippet": "2021). Evaluating the harms of language models. A goal of modifying the behavior of language models is to mitigate the harms of these models when they‚Äôre deployed in the real world. These risks have been extensively documented (Bender et al"}, {"rank": 5, "score": 42.72853908957167, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.2300929928128777, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 73, "text_snippet": "rieval mechanisms oÔ¨Äer a path to reducing the compute requirements needed to train and update language models that reach a certain performance. Large language models are prone to generating toxic outputs, as shown in Gehman et al. (2020). B"}]}
{"case_index": 243, "query": "From 2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt, what exact term fills [BLANK] in: \"Our work is [BLANK] to these works, as our attention mechanism is unmodified during inference.\"?", "gold": "complementary", "answer": "The exact term is **‚Äúcomplementary‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term and is directly supported by Context 1 sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 60.767, "llm_ms": 1093.33, "top_contexts": [{"rank": 1, "score": 0.6447223, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.275934620598022, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 22, "text_snippet": "etrieval-based (Karpukhin et al., 2020; Izacard et al., 2022; Guu et al., 2020), which augment language models via fetching related documents and including the retrieved results into contexts. Our work is complementary to these works, as ou"}, {"rank": 2, "score": 0.6415845, "source": "dense+rrf", "rrf_score": 0.031754032258064516, "rerank_score": 0.26195466662068456, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 24, "text_snippet": "ons have a large gap to full attention, making it infeasible to fine-tune pre-trained LLMs. Although our work also involves an approximation of attention mechanism, it has a similar shape and a small gap to standard attention. This enables "}, {"rank": 3, "score": 0.6616002, "source": "dense+rrf", "rrf_score": 0.03177805800756621, "rerank_score": 0.2416949737673283, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 29, "text_snippet": "g (Zhu et al., 2023), and methods based on out-of-distribution analysis (Han et al., 2023). Our method focuses on efficient fine-tuning and retaining the original architecture during inference, which is orthogonal to these position embeddin"}, {"rank": 4, "score": 0.6086232, "source": "dense+rrf", "rrf_score": 0.029631255487269532, "rerank_score": 0.23941314373225603, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 75, "text_snippet": "ed in training-from- scratch transformers. This experiment is to examine their capability of fine-tuning on pre-trained LLMs (Touvron et al., 2023b), toward long context adaptation. Dilated attention performs well in full fine-tuning but is"}, {"rank": 5, "score": 34.00412176553493, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.23284000691592585, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 12, "text_snippet": "e. In terms of efficiency , regardless of whether LoRA is employed or not, computational cost increases dramatically as the context size expands, primarily due to the standard self-attention mechanism (Vaswani et al., 2017). As shown in Fig"}]}
{"case_index": 244, "query": "From 2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt, what exact term fills [BLANK] in: \"We prove that block-sparse [BLANK] has better IO complexity than FlashAttention by a factor proportional to the sparsity ratio.\"?", "gold": "flashattention", "answer": "The exact term is **‚ÄúFlashAttention‚Äù** (i.e., ‚ÄúWe prove that block-sparse **FlashAttention** has better IO complexity than FlashAttention by a factor proportional to the sparsity ratio.‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context explicitly states: 'We prove that block-sparse FlashAttention has better IO complexity than FlashAttention...' so [BLANK]=FlashAttention (expected 'flashattention').", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 52.839, "llm_ms": 1680.94, "top_contexts": [{"rank": 1, "score": 0.74578667, "source": "dense+rrf", "rrf_score": 0.032266458495966696, "rerank_score": 0.3709822424454923, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 16, "text_snippet": "lock-sparse FlashAttention , a sparse attention algorithm that is 2-4 \u0002faster than evenFlashAttention , scaling up to sequence length of 64k. We prove that block-sparse FlashAttention has better IO complexity than FlashAttention by a factor"}, {"rank": 2, "score": 0.7482342, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.28181255868984867, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 48, "text_snippet": "s decreases (as we make fewer passes over the input), and runtime decreases. For large enough block size (beyond 256), the runtime is then bottlenecked by other factors (e.g., arithmetic operations). Moreover, larger block size will not Ô¨Åt "}, {"rank": 3, "score": 0.7281206, "source": "dense+rrf", "rrf_score": 0.030536130536130537, "rerank_score": 0.27336846801964454, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 2, "text_snippet": "exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity ofFlashAttention , showing that it requires fewer HBM accesses"}, {"rank": 4, "score": 0.73286575, "source": "dense+rrf", "rrf_score": 0.028612012987012988, "rerank_score": 0.26442706930281923, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 30, "text_snippet": "n O. 3FlashAttention : Algorithm, Analysis, and Extensions We show how to compute exact attention with fewer HBM reads/writes and without storing large intermediate matrices for the backward pass. This yields an attention algorithm that is "}, {"rank": 5, "score": 0.72944003, "source": "dense+rrf", "rrf_score": 0.02946912242686891, "rerank_score": 0.26230021052292074, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 50, "text_snippet": "o blocks of the attention matrix. The algorithm is identical to Algorithm 1, except we skip zero blocks. We reproduce the algorithm description in Algorithm 5 in Appendix B. We also analyze the IO complexity of block-sparse FlashAttention ."}]}
{"case_index": 245, "query": "From 1706.03762__attention_is_all_you_need.txt, what exact term fills [BLANK] in: \"ransformer follows this overall architecture using stacked [BLANK] and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.\"?", "gold": "self-attention", "answer": "The exact term is **‚Äúself-attention‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'self-attention' and is directly supported by Context 1/5.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 57.991, "llm_ms": 1075.685, "top_contexts": [{"rank": 1, "score": 0.7627425, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.48231095130230645, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 16, "text_snippet": "ransformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively. 3.1 Encoder and Decoder Stacks "}, {"rank": 2, "score": 0.53943646, "source": "dense+rrf", "rrf_score": 0.02804284323271665, "rerank_score": 0.22871057511941348, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 114, "text_snippet": "  E1 E2 EN...Figure 3: Differences in pre-training model architectures. BERT uses a bidirectional Transformer. OpenAI GPT uses a left-to-right Transformer. ELMo uses the concatenation of independently trained left-to-right and right-to- lef"}, {"rank": 3, "score": 0.5998183, "source": "dense+rrf", "rrf_score": 0.03125763125763126, "rerank_score": 0.2248947928955705, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 30, "text_snippet": "rd Networks In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transf"}, {"rank": 4, "score": 29.587796706374135, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.21998498296430108, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 38, "text_snippet": "t token of chunk ùê∂ùë¢is the Ô¨Årst to be able to access the retrieved content ùê∏ùë¢while maintaining autoregressivity in(1). Hence, there is a one token overlap between chunk ùê∂ùë¢=\u0010 ùë•¬πùë¢\u00001¬∫ùëö¬∏ùëñ\u0011 ùëñ2¬ª1¬îùëö¬ºand the corresponding attending chunk ùê∂¬∏ ùë¢,¬πùë•ùë¢ùëö¬∏ùëñ"}, {"rank": 5, "score": 0.5982963, "source": "dense+rrf", "rrf_score": 0.03128054740957967, "rerank_score": 0.21952495311262768, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 15, "text_snippet": ",2,35]. Here, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence of continuous representations z= (z1, ..., z n). Given z, the decoder then generates an output sequence (y1, ..., y m)of symbols one elem"}]}
{"case_index": 246, "query": "From 2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt, what exact term fills [BLANK] in: \"We study this question with a [BLANK] key- value retrieval task, which is designed to be a mini- mal testbed for the basic ability to retrieve matching tokens from the input context.\"?", "gold": "synthetic", "answer": "The exact term is **‚Äúsynthetic‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "The context explicitly states 'We study this question with a synthetic key-value retrieval task,' matching the expected blank.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 82.348, "llm_ms": 1179.926, "top_contexts": [{"rank": 1, "score": 0.7451596, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.5023812064899007, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 14, "text_snippet": "r at using their input context (¬ß2.3). Given that language models struggle to retrieve and use relevant information in the multi-document question answering task, to what extent can lan- guage models even retrieve from their input con- text"}, {"rank": 2, "score": 0.7218716, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.41747808521705954, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 50, "text_snippet": "wering task, to what extent can they simply retrieve from input contexts? We study this question with a syn- thetic key-value retrieval task, which is designed to provide a minimal testbed for the basic ability to retrieve matching tokens f"}, {"rank": 3, "score": 0.63432705, "source": "dense+rrf", "rrf_score": 0.03149801587301587, "rerank_score": 0.32502723233043507, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 49, "text_snippet": "UIDs. The relevant key-value pair for answering the query is bolded here within the input context for clarity. indicate that extended-context models are not nec- essarily better than their non-extended counterparts at using their input cont"}, {"rank": 4, "score": 46.1751093580084, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.3237319902926832, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 81, "text_snippet": "ge model can take in 16K tokens, is it actually beneficial to provide 16K tokens of context? The answer to this question is ultimately downstream task-specific since it de- pends on the marginal value of the added context and the model‚Äôs ab"}, {"rank": 5, "score": 0.6422219, "source": "dense+rrf", "rrf_score": 0.03149801587301587, "rerank_score": 0.26839885302375033, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 15, "text_snippet": " are given a collection of JSON-formatted key-value pairs and must return the value associated with a specific key. Similar to the multi-document QA task, the key-value retrieval task admits controlled changes to the input context length (a"}]}
{"case_index": 247, "query": "From 2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt, what exact term fills [BLANK] in: \"Prior work has given models the ability to generate natural language inter- mediate steps by training from scratch (Ling et al., 2017) or Ô¨Ånetuning a [BLANK] model (Cobbe et al., 2021), in ad\"?", "gold": "pretrained", "answer": "The exact term that fills **[BLANK]** is **‚Äúpretrained‚Äù** (i.e., ‚Äúfinetuning a **pretrained** model‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'pretrained' and is directly supported by the provided context quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 56.705, "llm_ms": 1315.371, "top_contexts": [{"rank": 1, "score": 0.64798784, "source": "dense+rrf", "rrf_score": 0.031099324975891997, "rerank_score": 0.5060262791325859, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 7, "text_snippet": "tic, commonsense, and symbolic reasoning (Rae et al., 2021). This work explores how the reasoning ability of large language models can be unlocked by a simple method motivated by two ideas. First, techniques for arithmetic reasoning can ben"}, {"rank": 2, "score": 49.7053334919573, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.29529330051068586, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 35, "text_snippet": "f-thought prompting Prior supervised best 020406080SV AMP solve rate (%) 0.4 81370255075100MAWPS solve rate (%) 0.4 7175 862540 Model scale (# parameters in billions) Figure 4: Chain-of-thought prompting enables large language models to sol"}, {"rank": 3, "score": 0.7227818, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.2897397721261712, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 190, "text_snippet": "h the ability to produce intermediate steps, prior work typically Ô¨Ånetunes models on either manually annotated training datasets (Camburu et al., 2018; Rajani et al., 2019, inter alia ) or generates synthetic datasets (Talmor et al., 2020; "}, {"rank": 4, "score": 0.6416755, "source": "dense+rrf", "rrf_score": 0.027984344422700584, "rerank_score": 0.2827249980955551, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 76, "text_snippet": "015; Chiang and Chen, 2019; Amini et al., 2019; Chen et al., 2019). Cobbe et al. (2021) extend Ling et al. (2017) by creating a larger dataset and using it to Ô¨Ånetune a pretrained language model rather than training a model from scratch. In"}, {"rank": 5, "score": 0.6510997, "source": "dense+rrf", "rrf_score": 0.03007688828584351, "rerank_score": 0.2768401027950872, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 3, "text_snippet": "asks, their ability to demonstrate reasoning is often seen as a limitation, which cannot be overcome solely by increasing model scale (Rae et al., 2021; BIG-bench collaboration, 2021, inter alia ). In an effort to address this shortcoming, "}]}
{"case_index": 248, "query": "From 1706.03762__attention_is_all_you_need.txt, what exact term fills [BLANK] in: \"Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer [BLANK].\"?", "gold": "normalization", "answer": "The exact term is **‚Äúnormalization‚Äù** (i.e., ‚Äúfollowed by layer normalization.‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'normalization' and is directly supported by context phrase 'followed by layer normalization.'", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 50.715, "llm_ms": 1233.081, "top_contexts": [{"rank": 1, "score": 0.61498636, "source": "dense+rrf", "rrf_score": 0.032266458495966696, "rerank_score": 0.388729837368355, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 18, "text_snippet": "s also composed of a stack of N= 6identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the enc"}, {"rank": 2, "score": 0.64789844, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.3389046223579291, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 17, "text_snippet": "ted feed-forward network. We employ a residual connection [ 11] around each of the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is LayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function"}, {"rank": 3, "score": 0.5610212, "source": "dense+rrf", "rrf_score": 0.03055037313432836, "rerank_score": 0.2557180622102658, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 29, "text_snippet": "n layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We"}, {"rank": 4, "score": 0.5743817, "source": "dense+rrf", "rrf_score": 0.03125763125763126, "rerank_score": 0.25151817418254113, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 16, "text_snippet": "ransformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively. 3.1 Encoder and Decoder Stacks "}, {"rank": 5, "score": 0.5727563, "source": "dense+rrf", "rrf_score": 0.030536130536130537, "rerank_score": 0.2443805849797279, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 28, "text_snippet": "the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9]. ‚Ä¢The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries c"}]}
{"case_index": 249, "query": "From 2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt, what exact term fills [BLANK] in: \"and delve into their synergies, elucidating how these com- ponents [BLANK] collaborate to form a cohesive and effective RAG framework.\"?", "gold": "intricately", "answer": "The exact term that fills **[BLANK]** is **‚Äúintricately‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'intricately' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 45.215, "llm_ms": 1242.833, "top_contexts": [{"rank": 1, "score": 0.6022853, "source": "dense+rrf", "rrf_score": 0.03177805800756621, "rerank_score": 0.3419038087246503, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 14, "text_snippet": "and delve into their synergies, elucidating how these com- ponents intricately collaborate to form a cohesive and effective RAG framework. ‚Ä¢We have summarized the current assessment methods of RAG, covering 26 tasks, nearly 50 datasets, out"}, {"rank": 2, "score": 0.6518933, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.3015993598660641, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 13, "text_snippet": "he fine-tuning of LLMs. Researchers have also been exploring ways to enhance language models in the pre-training stage through retrieval-augmented techniques. advanced RAG, and modular RAG. This review contex- tualizes the broader scope of "}, {"rank": 3, "score": 25.88339791780677, "source": "dense+rrf", "rrf_score": 0.014285714285714285, "rerank_score": 0.2166661363870776, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 69, "text_snippet": "e stem from their ability to memorise training data, their high training cost, the static nature of their training data (Lazaridou et al., 2021), their tendency of amplifying inherent biases in the training data, and their ability to genera"}, {"rank": 4, "score": 0.6377987, "source": "dense+rrf", "rrf_score": 0.02928692699490662, "rerank_score": 0.21594767797453282, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 3, "text_snippet": "augmentation techniques. The paper highlights the state-of-the- art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-"}, {"rank": 5, "score": 0.61414474, "source": "dense+rrf", "rrf_score": 0.031009615384615385, "rerank_score": 0.19246877601053186, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 11, "text_snippet": "h a detailed and structured understanding of both large models and RAG. It aims to illuminate the evolution of retrieval augmentation techniques, assess the strengths and weaknesses of various approaches in their respective contexts, and sp"}]}
{"case_index": 250, "query": "From 2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"Moreover, this evaluation strategy relies on the LM [BLANK], which are not accessible for some closed models (e.g.\"?", "gold": "probabilities", "answer": "The exact term is **‚Äúprobabilities‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'probabilities' and is directly supported by the provided context sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 69.664, "llm_ms": 1054.045, "top_contexts": [{"rank": 1, "score": 0.5844326, "source": "dense+rrf", "rrf_score": 0.030886196246139225, "rerank_score": 0.3481099496748092, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 8, "text_snippet": "e not always predictive of downstream performance (Wang et al., 2023c). Moreover, this evaluation strategy relies on the LM probabilities, which are not accessible for some closed models (e.g. ChatGPT and GPT-4). Ques- tion answering is ano"}, {"rank": 2, "score": 0.5978111, "source": "dense+rrf", "rrf_score": 0.03076923076923077, "rerank_score": 0.24439779884829063, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 7, "text_snippet": "lementation requires a significant amount of tuning, as the overall per- formance will be affected by the retrieval model, the considered corpus, the LM, or the prompt for- mulation, among others. Automated evaluation of retrieval-augmented"}, {"rank": 3, "score": 21.144048080433954, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.21032374226132577, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 141, "text_snippet": " open-domain question answering containing 3610 questions. For TriviaQA we evaluate on the dev set of the Ô¨Åltered set. This differs from GPT-3 and PaLM, which evaluate on the test set of the unÔ¨Åltered set for which the online evaluation ser"}, {"rank": 4, "score": 33.1604624062252, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.20277867939384453, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 9, "text_snippet": "we present Ragas1, a framework for the automated assessment 1Ragas is available at https://github.com/ explodinggradients/ragas .arXiv:2309.15217v2 [cs.CL] 28 Apr 2025  of retrieval augmented generation systems. We fo- cus on settings where"}, {"rank": 5, "score": 0.5833136, "source": "dense+rrf", "rrf_score": 0.029437229437229435, "rerank_score": 0.1902640502092667, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 15, "text_snippet": "his is less likely to be the case for hallucinated answers. Automated evaluation of text generation systems LLMs have also been leveraged to automatically evaluate other aspects of generated text fragments, beyond factuality. For instance, "}]}
{"case_index": 251, "query": "From 2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt, what exact term fills [BLANK] in: \"In Table 1, we show the test accuracy over a set of reasoning tasks by using different answer [BLANK] strategies.\"?", "gold": "aggregation", "answer": "The exact term is **‚Äúaggregation‚Äù** (i.e., ‚Äúdifferent answer **aggregation** strategies.‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'aggregation' and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 52.636, "llm_ms": 1524.743, "top_contexts": [{"rank": 1, "score": 0.6250756, "source": "dense+rrf", "rrf_score": 0.03028233151183971, "rerank_score": 0.37505272882847546, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 23, "text_snippet": "nstitutes ri, while the answer 18from the last sentence, ‚Äú The answer is $18 ‚Äù, is parsed as ai.1After sampling multiple (ri,ai)from the model‚Äôs decoder, self-consistency applies a marginalization over riby taking a majority vote over ai, i"}, {"rank": 2, "score": 38.72125769676984, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.3049267209774795, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 144, "text_snippet": "ed reasoning paths from the PaLM-540B model in Table 13. We see that the diversity in the additionally sampled reasoning paths indeed helps the model arrive at a more correct Ô¨Ånal answer after aggregation. A.1.2 R OBUSTNESS TO DIFFERENT SET"}, {"rank": 3, "score": 0.6298462, "source": "dense+rrf", "rrf_score": 0.028991596638655463, "rerank_score": 0.294920848257405, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 5, "text_snippet": "odel performance across a variety of multi-step reasoning tasks (Wei et al., 2022). In this paper, we introduce a novel decoding strategy called self-consistency to replace the greedy decoding strategy used in chain-of-thought prompting (We"}, {"rank": 4, "score": 36.04259393352484, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.29025099733232723, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 86, "text_snippet": "ength sequence in recurrent language models. Nye et al. (2021) improve the logical consistency of samples from a System 1 model by adding a System 2-inspired logical reasoning module. In this paper we focus on a slightly different notion of"}, {"rank": 5, "score": 40.62581219660493, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.28720324417328696, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 143, "text_snippet": "40 #Sampled Reasoning Paths505254565860 ARC (Challenge) Greedy Decode (Single-path) Self Consistency (Multi-path) Figure 7: Self-consistency (blue) signiÔ¨Åcantly improves accuracy across various arithmetic and commonsense reasoning tasks, ov"}]}
{"case_index": 252, "query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \"r [BLANK] such models (Howard and Ruder, 2018; Radford et al., 2018; Dai and Le, 2015).\"?", "gold": "pre-training", "answer": "The exact term is **‚Äúpre-training‚Äù**, i.e., the phrase is: **‚Äúfor pre-training such models (Howard and Ruder, 2018; Radford et al., 2018; Dai and Le, 2015).‚Äù**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer correctly fills [BLANK] with 'pre-training' and is directly supported by the provided context snippet.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 54.895, "llm_ms": 1824.051, "top_contexts": [{"rank": 1, "score": 0.6289407, "source": "dense+rrf", "rrf_score": 0.030886196246139225, "rerank_score": 0.3271296223074558, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 21, "text_snippet": "r pre-training such models (Howard and Ruder, 2018; Radford et al., 2018; Dai and Le, 2015). 2.3 Transfer Learning from Supervised Data There has also been work showing effective trans- fer from supervised tasks with large datasets, such as"}, {"rank": 2, "score": 0.6685582, "source": "dense+rrf", "rrf_score": 0.032266458495966696, "rerank_score": 0.2855985548187281, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 0, "text_snippet": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova Google AI Language {jacobdevlin,mingweichang,kentonl,kristout }@google.com Abstract We introduce a ne"}, {"rank": 3, "score": 0.6658591, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.27277173404769356, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 18, "text_snippet": "for a supervised downstream task (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018). The advantage of these approaches is that few parameters need to be learned from scratch. At least partly due to this advantage, OpenAI GPT ("}, {"rank": 4, "score": 48.35271937445739, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.2662576010608249, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 476, "text_snippet": "heng, and Jingjing Liu. Uniter: Learning universal image-text representations. arXiv preprint arXiv:1909.11740 , 2019. [Cra17] Kate Crawford. The trouble with bias. NIPS 2017 Keynote , 2017. [DCLT18] Jacob Devlin, Ming-Wei Chang, Kenton Lee"}, {"rank": 5, "score": 0.6203789, "source": "dense+rrf", "rrf_score": 0.028814262023217248, "rerank_score": 0.2648629626571562, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 20, "text_snippet": "or BERT. Apart from output layers, the same architec- tures are used in both pre-training and Ô¨Åne-tuning. The same pre-trained model parameters are used to initialize models for different down-stream tasks. During Ô¨Åne-tuning, all parameters"}]}
{"case_index": 253, "query": "From 2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt, what exact term fills [BLANK] in: \"Most decomposable similarity functions are some [BLANK] of Euclidean distance (L2).\"?", "gold": "transformations", "answer": "The exact term is **‚Äútransformations‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 44.32, "llm_ms": 1129.618, "top_contexts": [{"rank": 1, "score": 0.6307746, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.24680505865064126, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 21, "text_snippet": "y function needs to be decomposable so that the represen- tations of the collection of passages can be pre- computed. Most decomposable similarity functions are some transformations of Euclidean distance (L2). For instance, cosine is equiva"}, {"rank": 2, "score": 0.6563735, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.2281479013152108, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 56, "text_snippet": "ss Besides dot product, cosine and Euclidean L2 distance are also commonly used as decomposable similarity functions. We test these alternatives and Ô¨Ånd that L2 performs compara- ble to dot product, and both of them are superior to cosine. "}, {"rank": 3, "score": 25.220869238838823, "source": "dense+rrf", "rrf_score": 0.013888888888888888, "rerank_score": 0.20480828668821482, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 101, "text_snippet": "t long-text understanding with short-text models. Transactions of the Association for Computational Linguistics , 11:284‚Äì299. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Gr"}, {"rank": 4, "score": 26.130738826418906, "source": "dense+rrf", "rrf_score": 0.014285714285714285, "rerank_score": 0.19884872025958306, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 99, "text_snippet": "las Joseph, Ben Mann, Sam McCandlish, Chris Olah, and Jared Kaplan. 2022. Language models (mostly) know what they know. CoRR , abs/2207.05221. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H. Lewis, Ledell Wu, Sergey Edunov, Danqi "}, {"rank": 5, "score": 39.11947626566707, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.19781914346427476, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 100, "text_snippet": " Huiyuan Chen, and Xia Hu. Llm maybe longlm: Self-extend llm context window without tuning. arXiv preprint arXiv:2401.01325 , 2024. Jeff Johnson, Matthijs Douze, and Herv√© J√©gou. Billion-scale similarity search with gpus. IEEE Transactions "}]}
{"case_index": 254, "query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"The former is the weighted loss over all tokens xi,...,x nif the API call and its result are given to Mas a preÔ¨Åx;3 the latter is the minimum of the losses obtained from (i) doing no API call at all and (ii) doing an API call, but not [BLANK] the response.\"?", "gold": "providing", "answer": "The exact term is **‚Äúproviding‚Äù** (as in: ‚Äúdoing an API call, but not **providing** the response.‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'providing' and is directly supported by the provided context sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 91.355, "llm_ms": 1394.949, "top_contexts": [{"rank": 1, "score": 0.68166226, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.5111260360842341, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 24, "text_snippet": "o different instantiations of this loss: L+ i=Li(e(ci,ri)) L‚àí i= min (Li(Œµ),Li(e(ci,Œµ))) whereŒµdenotes an empty sequence. The former is the weighted loss over all tokens xi,...,x nif the API call and its result are given to Mas a preÔ¨Åx;3 th"}, {"rank": 2, "score": 0.5576083, "source": "dense+rrf", "rrf_score": 0.03149801587301587, "rerank_score": 0.27089764022725926, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 25, "text_snippet": "he input andthe output of this call makes it easier for the model to predict future tokens, compared to not receiving the API call at all, or receiving only its input. Given a Ô¨Åltering threshold œÑf, we thus only keep API calls for which L‚àí "}, {"rank": 3, "score": 0.6089138, "source": "dense+rrf", "rrf_score": 0.030834914611005692, "rerank_score": 0.252905924724752, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 10, "text_snippet": " i. We then execute these API calls and Ô¨Ålter out all calls which do not reduce the loss Liover the next tokens. All remaining API calls are interleaved with the original text, resulting in a new text x‚àó. tant not only because of the costs "}, {"rank": 4, "score": 0.55786276, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.24361628266303942, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 23, "text_snippet": "eval system to perform search over a large corpus. The response for each API call cineeds to be a single text sequence ri. Filtering API Calls Letibe the position of the API callciin the sequence x=x1,...,x n, and let ribe the response from"}, {"rank": 5, "score": 36.987524206965766, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.23497556436394346, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 67, "text_snippet": "alone would be of little help. The best course of action for this dataset ‚Äì Ô¨Årst querying the calen- dar API to get the current date, and then querying the question answering system with this date ‚Äì is not only prohibited by our restriction"}]}
{"case_index": 255, "query": "From 2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt, what exact term fills [BLANK] in: \"1 we also report the results by taking a ‚Äúweighted average‚Äù, i.e., each agets a score of its weighted sum divided by‚àëm i=11(ai=a), which results in a much worse [BLANK].\"?", "gold": "performance", "answer": "The exact term that fills **[BLANK]** is **‚Äúperformance‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context 1 explicitly states the phrase ends with 'much worse performance,' matching the expected answer.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 57.441, "llm_ms": 1351.314, "top_contexts": [{"rank": 1, "score": 0.6581143, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.4519191484642953, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 27, "text_snippet": " 1 we also report the results by taking a ‚Äúweighted average‚Äù, i.e., each agets a score of its weighted sum divided by‚àëm i=11(ai=a), which results in a much worse performance. Self-consistency explores an interesting space between open-ended"}, {"rank": 2, "score": 0.6480389, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.31528818749609305, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 26, "text_snippet": "ilities and found this is because for each (ri,ai), the normalized conditional probabilities P(ri,ai|prompt ,question )are quite close to each other, i.e., the language model regards those generations as ‚Äúsimilarly likely‚Äù.2Additionally, wh"}, {"rank": 3, "score": 0.5749967, "source": "dense+rrf", "rrf_score": 0.030798389007344232, "rerank_score": 0.2861092759012914, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 40, "text_snippet": " 4  Published as a conference paper at ICLR 2023 3.2 M AINRESULTS We report the results of self-consistency averaged over 10 runs, where we sampled 40 outputs independently from the decoder in each run. The baseline we compare to is chain-o"}, {"rank": 4, "score": 0.580066, "source": "dense+rrf", "rrf_score": 0.03076923076923077, "rerank_score": 0.27541857927627683, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 23, "text_snippet": "nstitutes ri, while the answer 18from the last sentence, ‚Äú The answer is $18 ‚Äù, is parsed as ai.1After sampling multiple (ri,ai)from the model‚Äôs decoder, self-consistency applies a marginalization over riby taking a majority vote over ai, i"}, {"rank": 5, "score": 0.5418554, "source": "dense+rrf", "rrf_score": 0.028169014084507043, "rerank_score": 0.24738069202451063, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 5, "text_snippet": "odel performance across a variety of multi-step reasoning tasks (Wei et al., 2022). In this paper, we introduce a novel decoding strategy called self-consistency to replace the greedy decoding strategy used in chain-of-thought prompting (We"}]}
{"case_index": 256, "query": "From 2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt, what exact term fills [BLANK] in: \"This can result in the loss of contextual information, which may ultimately harm overall [BLANK].\"?", "gold": "performance", "answer": "The exact term that fills **[BLANK]** is **‚Äúperformance‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'performance' and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 58.256, "llm_ms": 1105.846, "top_contexts": [{"rank": 1, "score": 51.343237670727895, "source": "dense+rrf", "rrf_score": 0.01639344262295082, "rerank_score": 0.3214767400585295, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 10, "text_snippet": " performance. Moreover, short retrieval units can lead to semantic incompleteness due to document truncation. This can result in the loss of contextual information, which may ultimately harm overall performance. This design choice was made "}, {"rank": 2, "score": 24.706397711167572, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.21670754425457237, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 44, "text_snippet": " question, following a design similar to LoCoV1 (Saad-Falcon et al., 2024). 3.2 Retrieval Performance In this section, we present the retrieval performance on two extractive QA datasets, NQ and HotpotQA, to demonstrate that comparable retri"}, {"rank": 3, "score": 0.58647233, "source": "dense+rrf", "rrf_score": 0.02964254577157803, "rerank_score": 0.21325693129383755, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 0, "text_snippet": "LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs ‚ô†Ziyan Jiang,‚ô†Xueguang Ma,‚ô†Wenhu Chen ‚ô†University of Waterloo ziyanjiang528@gmail.com ,{x93ma ,wenhuchen}@uwaterloo.ca Project Website: https://tiger-ai-lab.github.io/"}, {"rank": 4, "score": 0.6238817, "source": "dense+rrf", "rrf_score": 0.031054405392392875, "rerank_score": 0.21051244846057388, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 79, "text_snippet": "on the long-context understanding ability of LLMs, and we still have a long way to go in harnessing open-source LLMs within our framework. 10  4 Related Work 4.1 Retrieval-Augmented Generation. Augmenting language models with information re"}, {"rank": 5, "score": 25.065257677065812, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.20734179507167538, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 19, "text_snippet": "e end performance dips significantly due to the huge amount of ‚Äúhard negatives‚Äù, which confuses the reader. With ‚Äúlong retriever units‚Äù, we observe an entirely different trend. As we recall more units (from 1 to 8 units), both the recall an"}]}
{"case_index": 257, "query": "From 2302.13971__llama_open_and_efficient_foundation_language_models.txt, what exact term fills [BLANK] in: \"ow that it is possible to train [BLANK] models using publicly avail- able datasets exclusively, without resorting to proprietary and inaccessible datasets.\"?", "gold": "state-of-the-art", "answer": "The exact term that fills **[BLANK]** is:\n\n**state-of-the-art**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term and is directly supported by the provided context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 51.81, "llm_ms": 1133.809, "top_contexts": [{"rank": 1, "score": 0.6732214, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.37258912260456195, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 1, "text_snippet": "ow that it is possible to train state-of-the-art models using publicly avail- able datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and L"}, {"rank": 2, "score": 0.58373904, "source": "dense+rrf", "rrf_score": 0.0304147465437788, "rerank_score": 0.25565004855106144, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 90, "text_snippet": "3 while being more than 10 √ósmaller, and LLaMA-65B is competitive with Chinchilla-70B and PaLM-540B. Unlike previous studies, we show that it is possible to achieve state-of-the-art performance by training exclusively on publicly available "}, {"rank": 3, "score": 0.5960492, "source": "dense+rrf", "rrf_score": 0.030776515151515152, "rerank_score": 0.2373905419545918, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 0, "text_snippet": "LLaMA: Open and EfÔ¨Åcient Foundation Language Models Hugo Touvron‚àó, Thibaut Lavril‚àó, Gautier Izacard‚àó, Xavier Martinet Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi√®re, Naman Goyal Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand "}, {"rank": 4, "score": 24.866342310306095, "source": "dense+rrf", "rrf_score": 0.014285714285714285, "rerank_score": 0.2070662451666667, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 206, "text_snippet": "k it to correct 5 more (again without any omissions or repeats). Results are shown in Figure 3.17. 4 Measuring and Preventing Memorization Of Benchmarks Since our training dataset is sourced from the internet, it is possible that our model "}, {"rank": 5, "score": 0.5976682, "source": "dense+rrf", "rrf_score": 0.0288981288981289, "rerank_score": 0.19591938468825656, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 10, "text_snippet": " mains. For the most part, we reuse data sources that have been leveraged to train other LLMs, with the restriction of only using data that is publicly available, and compatible with open sourcing. This leads to the following mixture of dat"}]}
{"case_index": 258, "query": "From 2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt, what exact term fills [BLANK] in: \"1.First, chain of thought, in principle, allows models to decompose multi-step problems into [BLANK] steps, which means that additional computation can be allocated to problems that require more reason\"?", "gold": "intermediate", "answer": "The exact term that fills **[BLANK]** is:\n\n**‚Äúintermediate‚Äù**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'intermediate' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 52.228, "llm_ms": 1176.688, "top_contexts": [{"rank": 1, "score": 0.78212166, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.4864614058395741, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 16, "text_snippet": " for arriving at the answer (and also, solutions/explanations typically come after the Ô¨Ånal answer (Narang et al., 2020; Wiegreffe et al., 2022; Lampinen et al., 2022, inter alia )). Chain-of-thought prompting has several attractive propert"}, {"rank": 2, "score": 39.32544512620325, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.27268476774830946, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 42, "text_snippet": "tput only a mathematical equation before giving the answer. Figure 5 shows that equation only prompting does not help much for GSM8K, which implies that the semantics of the questions in GSM8K are too challenging to directly translate into "}, {"rank": 3, "score": 0.7163067, "source": "dense+rrf", "rrf_score": 0.03125763125763126, "rerank_score": 0.2601278474540874, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 18, "text_snippet": " used for tasks such as math word problems, commonsense reasoning, and symbolic manipulation, and is potentially applicable (at least in principle) to any task that humans can solve via language. 4.Finally, chain-of-thought reasoning can be"}, {"rank": 4, "score": 40.97459035143819, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.24497365459395168, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 44, "text_snippet": "t allows the model to spend more computation (i.e., intermediate tokens) on harder problems. To isolate the effect of variable computation from chain-of-thought reasoning, we test a conÔ¨Åguration where the model is prompted to output a only "}, {"rank": 5, "score": 0.64100057, "source": "dense+rrf", "rrf_score": 0.026171741361614778, "rerank_score": 0.23419028882789764, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 152, "text_snippet": "ting improve performance for my task of interest? While chain-of-thought prompting is in principle applicable for any text-to-text task, it is more helpful for some tasks than others. Based on the experiments in this paper, our intuition is"}]}
{"case_index": 259, "query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \"We argue that current techniques restrict the power of the pre-trained [BLANK], espe- cially for the Ô¨Åne-tuning approaches.\"?", "gold": "representations", "answer": "The exact term is **‚Äúrepresentations‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'representations' and is directly supported by the provided context sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 51.23, "llm_ms": 1005.459, "top_contexts": [{"rank": 1, "score": 0.6767187, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.34690588089838675, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 6, "text_snippet": " representations. We argue that current techniques restrict the power of the pre-trained representations, espe- cially for the Ô¨Åne-tuning approaches. The ma- jor limitation is that standard language models are unidirectional, and this limit"}, {"rank": 2, "score": 0.63536024, "source": "dense+rrf", "rrf_score": 0.031024531024531024, "rerank_score": 0.27643131567152707, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 10, "text_snippet": "ford et al. (2018), which uses unidirec- tional language models for pre-training, BERT uses masked language models to enable pre- trained deep bidirectional representations. This is also in contrast to Peters et al. (2018a), which uses a sh"}, {"rank": 3, "score": 33.69145188151025, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.2659603780874293, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 109, "text_snippet": "reading books. In Proceedings of the IEEE international conference on computer vision , pages 19‚Äì27. Appendix for ‚ÄúBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding‚Äù We organize the appendix into three section"}, {"rank": 4, "score": 0.6268704, "source": "dense+rrf", "rrf_score": 0.0315136476426799, "rerank_score": 0.2585140450307155, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 5, "text_snippet": "ude the pre-trained representations as addi- tional features. The Ô¨Åne-tuning approach, such as the Generative Pre-trained Transformer (OpenAI GPT) (Radford et al., 2018), introduces minimal task-speciÔ¨Åc parameters, and is trained on the dow"}, {"rank": 5, "score": 44.8617148264114, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.24517065996740595, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 476, "text_snippet": "heng, and Jingjing Liu. Uniter: Learning universal image-text representations. arXiv preprint arXiv:1909.11740 , 2019. [Cra17] Kate Crawford. The trouble with bias. NIPS 2017 Keynote , 2017. [DCLT18] Jacob Devlin, Ming-Wei Chang, Kenton Lee"}]}
{"case_index": 260, "query": "From 2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt, what exact term fills [BLANK] in: \"All our code, models, dataset, and demo are available at github.com/[BLANK]/LongLoRA.\"?", "gold": "dvlab-research", "answer": "The exact term is **`dvlab-research`**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term and is explicitly supported by Context 1 (github.com/dvlab-research/LongLoRA).", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 58.096, "llm_ms": 1072.882, "top_contexts": [{"rank": 1, "score": 0.6036999, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.3027171474471385, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 4, "text_snippet": "ike Flash-Attention2. In addition, we further conduct supervised fine-tuning with LongLoRA and our long instruction-following LongAlpaca dataset. All our code, models, dataset, and demo are available at github.com/dvlab-research/LongLoRA. 2"}, {"rank": 2, "score": 0.6387268, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.2309776342777765, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 0, "text_snippet": "Published as a conference paper at ICLR 2024 LONG LORA: E FFICIENT FINE-TUNING OF LONG - CONTEXT LARGE LANGUAGE MODELS Yukang Chen1Shengju Qian1Haotian Tang2Xin Lai1 Zhijian Liu2Song Han2,3Jiaya Jia1 1CUHK2MIT3NVIDIA ABSTRACT We present Lon"}, {"rank": 3, "score": 29.896303370807022, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.21070792065469268, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 107, "text_snippet": "eprint arXiv:2402.01613 , 2024. OpenAI. Hello gpt4-o. 2024. Bowen Peng and Jeffrey Quesnelle. Ntk-aware scaled rope allows llama models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation. https://"}, {"rank": 4, "score": 24.698742689658662, "source": "dense+rrf", "rrf_score": 0.014084507042253521, "rerank_score": 0.21035021888854022, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 65, "text_snippet": "2023). All models are evaluated on 16-shot in-context learning with direct prompting; The second group is ‚Äú Fully-supervised RAG‚Äù, and these baselines involve full-supervised fine-tuning on the training dataset. The third group is ‚ÄúNo Fine-"}, {"rank": 5, "score": 27.571618006591198, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.20618527782716622, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 11, "text_snippet": "e Ô¨Årst Ô¨Åne- tuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level andtoken-level tasks, outper- forming many task-speciÔ¨Åc architectures. ‚Ä¢ BERT advances the state of the art for elev"}]}
{"case_index": 261, "query": "From 2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt, what exact term fills [BLANK] in: \"We follow the sequence of works initiated in ‚ÄúTextbooks Are All You Need‚Äù [GZA+23], which utilize high quality training data to improve the [BLANK] of small language mod\"?", "gold": "performance", "answer": "The exact term that fills **[BLANK]** is **‚Äúperformance‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'performance' and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 77.305, "llm_ms": 1316.66, "top_contexts": [{"rank": 1, "score": 0.6493648, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.45116070929931756, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 17, "text_snippet": " running locally on a cell-phone. Thanks to its small size, phi- 3-mini can be quantized to 4-bits so that it only occupies ‚âà1.8GB of memory. We tested the quantized model by deploying phi-3-mini on iPhone 14 with A16 Bionic chip running na"}, {"rank": 2, "score": 52.93337636875514, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.27404232660208977, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 16, "text_snippet": "3. The table shows the Keys/values a query token in block 8 attended to. Blue=local blocks, orange=remote/vertical blocks, gray=blocks skipped. total parameters. Additionally, we utilize the SparseMixer approach [LGC23, LDL+23] for training"}, {"rank": 3, "score": 0.6305059, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.271400266252886, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 0, "text_snippet": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone Microsoft Abstract We introduce phi-3-mini , a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both"}, {"rank": 4, "score": 0.613493, "source": "dense+rrf", "rrf_score": 0.03125, "rerank_score": 0.2218408917930252, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 6, "text_snippet": "7B parameters), matched the performance of models 25 times larger trained on regular data. In this report we present a new model, phi-3-mini (3.8B parameters), trained for 3.3T tokens on larger and more advanced versions of the datasets use"}, {"rank": 5, "score": 0.58131015, "source": "dense+rrf", "rrf_score": 0.029273504273504274, "rerank_score": 0.2196976404027166, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 5, "text_snippet": "he existence of frontier LLMs themselves, which allow us to interact with data in novel ways. In our previous works on the phi models [GZA+23, LBE+23, JBA+23] it was shown that a combination of LLM-based filtering of publicly available web "}]}
{"case_index": 262, "query": "From 2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt, what exact term fills [BLANK] in: \"As most of the datasets only have an [BLANK] split, we manually composed a set of eight few-shot exemplars with chains of thought for prompting‚ÄîFigure 1 (right) shows one chain of thought exemplar, and the full set of exemplars is given in Appendix Table 20.\"?", "gold": "evaluation", "answer": "The exact term is **‚Äúevaluation‚Äù** (i.e., ‚ÄúAs most of the datasets only have an **evaluation** split, ‚Ä¶‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'evaluation' and is directly supported by the provided context quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 59.925, "llm_ms": 1285.145, "top_contexts": [{"rank": 1, "score": 0.8123864, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.5376883609882763, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 23, "text_snippet": "ed answer, as illustrated in Figure 1 (right). As most of the datasets only have an evaluation split, we manually composed a set of eight few-shot exemplars with chains of thought for prompting‚ÄîFigure 1 (right) shows one chain of thought ex"}, {"rank": 2, "score": 0.68850034, "source": "dense+rrf", "rrf_score": 0.03149801587301587, "rerank_score": 0.3357124452327364, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 22, "text_snippet": "rd few-shot prompting, popularized by Brown et al. (2020), in which a language model is given in-context exemplars of input‚Äìoutput pairs before outputting a prediction for a test-time example. Exemplars are formatted as questions and answer"}, {"rank": 3, "score": 0.68769336, "source": "dense+rrf", "rrf_score": 0.031009615384615385, "rerank_score": 0.33025810176803694, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 30, "text_snippet": " output ‚ü©triples for arithmetic, commonsense, and symbolic reasoning benchmarks. Chains of thought are highlighted. Full prompts in Appendix G. math word problems, we used this single set of eight chain of thought exemplars for all benchmar"}, {"rank": 4, "score": 0.71656317, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.3297276905863037, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 56, "text_snippet": "ete set. Figure 3 shows examples with chain of thought annotations for all datasets. Prompts. We follow the same experimental setup as the prior section. For CSQA and StrategyQA, we randomly selected examples from the training set and manua"}, {"rank": 5, "score": 0.6117474, "source": "dense+rrf", "rrf_score": 0.02813852813852814, "rerank_score": 0.31153766273397654, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 143, "text_snippet": "hain of thought still works with reasoning chains from an independent source without a background in machine learning. So we randomly sampled three sets of eight exemplars with chains of thought from GSM8K. These chain of thought annotation"}]}
{"case_index": 263, "query": "From 2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt, what exact term fills [BLANK] in: \"Like Orca 1, we utilize more capable LLMs to [BLANK] various reasoning strategies across various tasks.\"?", "gold": "demonstrate", "answer": "The exact term is **‚Äúdemonstrate‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 55.597, "llm_ms": 1079.902, "top_contexts": [{"rank": 1, "score": 0.7045712, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.29847116591773865, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 16, "text_snippet": "owing them to perform at their best, irrespective of their size. Like Orca 1, we utilize more capable LLMs to demonstrate various reasoning strategies across various tasks. However, in Orca 2, the reasoning strategies are carefully tailored"}, {"rank": 2, "score": 0.6951335, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.2823536395171741, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 1, "text_snippet": "n benchmarks like BigBench Hard and AGIEval. In Orca 2, we continue exploring how improved training signals can enhance smaller LMs‚Äô reasoning abilities. Research on training small LMs has often relied on imitation learning to replicate the"}, {"rank": 3, "score": 0.6687013, "source": "dense+rrf", "rrf_score": 0.031024531024531024, "rerank_score": 0.25014473765155887, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 0, "text_snippet": "Orca 2: Teaching Small Language Models How to Reason Arindam Mitra, Luciano Del Corro‚Ä†, Shweti Mahajan‚Ä†, Andres Codas‚Ä° Clarisse Simoes‚Ä°, Sahaj Agarwal, Xuxi Chen‚àó, Anastasia Razdaibiedina‚àó Erik Jones‚àó, Kriti Aggarwal‚àó, Hamid Palangi, Guoqin"}, {"rank": 4, "score": 26.71705569960646, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.19179165225462685, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 148, "text_snippet": "synthetic data for post-training. While Orca 2 models have not gone through RLHF training for safety, we believe that the use of synthetic data for post-training that has been filtered with various content safety filters could provide anoth"}, {"rank": 5, "score": 0.67980194, "source": "dense+rrf", "rrf_score": 0.029910714285714284, "rerank_score": 0.19147770116120588, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 2, "text_snippet": "asks, potentially different from the one used by the larger model. For example, while larger models might provide a direct answer to a complex task, smaller models may not have the same capacity. In Orca 2, we teach the model various reason"}]}
{"case_index": 264, "query": "From 2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt, what exact term fills [BLANK] in: \"These computation resources are typically [BLANK] for common researchers, which naturally leads us to question: can we extend the context window of LLMs efficiently?\"?", "gold": "unaffordable", "answer": "The exact term is **‚Äúunaffordable‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'unaffordable' and is directly supported by Context 1 sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 67.09, "llm_ms": 957.773, "top_contexts": [{"rank": 1, "score": 0.6388363, "source": "dense+rrf", "rrf_score": 0.032266458495966696, "rerank_score": 0.4003813816200424, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 10, "text_snippet": "al., 2023) used 32 TPUs for standard transformer training and 128 TPUs for LongLLaMA. These computation resources are typically unaffordable for common researchers, which naturally leads us to question: can we extend the context window of L"}, {"rank": 2, "score": 44.822220474365025, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.2729181944523244, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 75, "text_snippet": "ed in training-from- scratch transformers. This experiment is to examine their capability of fine-tuning on pre-trained LLMs (Touvron et al., 2023b), toward long context adaptation. Dilated attention performs well in full fine-tuning but is"}, {"rank": 3, "score": 35.80989905638477, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.26256032237324106, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 41, "text_snippet": "eads respectively. This manner does not increase additional computation costs but enables the information flow between different groups. We show that it gets close to the standard attention baseline in Table 1. Consistency to Full Attention"}, {"rank": 4, "score": 0.57016325, "source": "dense+rrf", "rrf_score": 0.030834914611005692, "rerank_score": 0.2501520867308696, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 0, "text_snippet": "Published as a conference paper at ICLR 2024 LONG LORA: E FFICIENT FINE-TUNING OF LONG - CONTEXT LARGE LANGUAGE MODELS Yukang Chen1Shengju Qian1Haotian Tang2Xin Lai1 Zhijian Liu2Song Han2,3Jiaya Jia1 1CUHK2MIT3NVIDIA ABSTRACT We present Lon"}, {"rank": 5, "score": 31.738204135550074, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.22002888112057853, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 84, "text_snippet": " An et al., 2024), Position interpolation (Chen et al., 2023a; Peng et al., 2023; Liu et al., 2024). Furthermore, alternative architectures beyond the Transformer have been explored to handle long inputs more naturally. These diverse approa"}]}
{"case_index": 265, "query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"An example of such a prompt for a [BLANK] an- swering tool is shown in Figure 3; all prompts used are shown in Appendix A.2.\"?", "gold": "question", "answer": "The exact term that fills **[BLANK]** is **‚Äúquestion‚Äù**, i.e., the phrase is:\n\n‚ÄúAn example of such a prompt for a **question** answering tool is shown in Figure 3; all prompts used are shown in Appendix A.2.‚Äù", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'question' and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 85.284, "llm_ms": 1663.127, "top_contexts": [{"rank": 1, "score": 0.62605804, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.3418640964351755, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 20, "text_snippet": "te API calls for the question answering tool. Mitself on this dataset. Each of these steps is described in more detail below. Sampling API Calls For each API, we write a promptP(x)that encourages the LM to anno- tate an example x=x1,...,x n"}, {"rank": 2, "score": 34.55196546372836, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.3187858572988372, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 27, "text_snippet": "fferent choices in the evaluation. As is now standard, we use few-shot prompts to evaluate the models, at temperature 0. The prompts and number of shots are part of a Microsoft internal tool to evaluate language models, and in particular we"}, {"rank": 3, "score": 36.30291315049822, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.2427915223238599, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 93, "text_snippet": "mitations While our approach enables LMs to learn how to use a variety of tools in a self-supervised way, there are some clear limitations to what can be achieved with our method in its current form. One such limi- tation is the inability o"}, {"rank": 4, "score": 0.58135283, "source": "dense+rrf", "rrf_score": 0.027650648360030512, "rerank_score": 0.23301902208710085, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 52, "text_snippet": "speciÔ¨Åcally the RM dataset) as labeled by our contractors. Most of the use-cases have are generative, rather than classiÔ¨Åcation or QA. We also show some illustrative prompts (written by researchers to mimic the kinds of prompts submitted to"}, {"rank": 5, "score": 33.341698841843275, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.23244603690086044, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 94, "text_snippet": "e are no examples of chained tool use in the Ô¨Ånetuning dataset. Our current approach also does not allow the LM to use a tool in an in- teractive way ‚Äì especially for tools such as search engines, that could potentially return hundreds of d"}]}
{"case_index": 266, "query": "From 2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt, what exact term fills [BLANK] in: \"The model is already [BLANK], and the chat template is as follows: <|user|> /n Question <|end|> /n <|assistant|> Thephi-3-small model (7B parameters) leverages the tiktoken tokenizer (for better multilingual tokenization) with a vocabulary size of 1003522and has default context length 8192.\"?", "gold": "chat-finetuned", "answer": "The exact term is **‚Äúchat-finetuned‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'chat-finetuned' and is explicitly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 86.172, "llm_ms": 1285.714, "top_contexts": [{"rank": 1, "score": 0.7594828, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.5138585670285541, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 10, "text_snippet": "ectly adapted to phi-3-mini . The model uses 3072 hidden dimension, 32 heads and 32 layers. We trained using bfloat16 for a total of 3.3T tokens. The model is already chat-finetuned, and the chat template is as follows: <|user|> /n Question"}, {"rank": 2, "score": 0.6419545, "source": "dense+rrf", "rrf_score": 0.0315136476426799, "rerank_score": 0.2781559314474321, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 16, "text_snippet": "3. The table shows the Keys/values a query token in block 8 attended to. Blue=local blocks, orange=remote/vertical blocks, gray=blocks skipped. total parameters. Additionally, we utilize the SparseMixer approach [LGC23, LDL+23] for training"}, {"rank": 3, "score": 0.6440386, "source": "dense+rrf", "rrf_score": 0.03125, "rerank_score": 0.26319562193502516, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 6, "text_snippet": "7B parameters), matched the performance of models 25 times larger trained on regular data. In this report we present a new model, phi-3-mini (3.8B parameters), trained for 3.3T tokens on larger and more advanced versions of the datasets use"}, {"rank": 4, "score": 0.6445787, "source": "dense+rrf", "rrf_score": 0.03125763125763126, "rerank_score": 0.25045967238255484, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 8, "text_snippet": "ved solely by changing the training data. phi-3-mini: The development of a compact language model that rivals the capabilities of ChatGPT, while fitting on a phone, is a testament to the power of data-driven machine learning. By meticulousl"}, {"rank": 5, "score": 0.6046698, "source": "dense+rrf", "rrf_score": 0.031024531024531024, "rerank_score": 0.24683898921219777, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 0, "text_snippet": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone Microsoft Abstract We introduce phi-3-mini , a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both"}]}
{"case_index": 267, "query": "From 2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt, what exact term fills [BLANK] in: \"For example our previous model trained on this data recipe, phi-2 (2.7B parameters), matched the [BLANK] of models 25 times large\"?", "gold": "performance", "answer": "The exact term that fills **[BLANK]** is **‚Äúperformance‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context explicitly says 'matched the performance of models 25 times large'; answer matches expected term.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 73.486, "llm_ms": 1321.331, "top_contexts": [{"rank": 1, "score": 0.6350288, "source": "dense+rrf", "rrf_score": 0.0304147465437788, "rerank_score": 0.3724124541976752, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 5, "text_snippet": "he existence of frontier LLMs themselves, which allow us to interact with data in novel ways. In our previous works on the phi models [GZA+23, LBE+23, JBA+23] it was shown that a combination of LLM-based filtering of publicly available web "}, {"rank": 2, "score": 0.7396524, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3383718026067894, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 6, "text_snippet": "7B parameters), matched the performance of models 25 times larger trained on regular data. In this report we present a new model, phi-3-mini (3.8B parameters), trained for 3.3T tokens on larger and more advanced versions of the datasets use"}, {"rank": 3, "score": 0.6919249, "source": "dense+rrf", "rrf_score": 0.031746031746031744, "rerank_score": 0.275144427809962, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 0, "text_snippet": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone Microsoft Abstract We introduce phi-3-mini , a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both"}, {"rank": 4, "score": 0.6469011, "source": "dense+rrf", "rrf_score": 0.030117753623188408, "rerank_score": 0.250243220650584, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 16, "text_snippet": "3. The table shows the Keys/values a query token in block 8 attended to. Blue=local blocks, orange=remote/vertical blocks, gray=blocks skipped. total parameters. Additionally, we utilize the SparseMixer approach [LGC23, LDL+23] for training"}, {"rank": 5, "score": 0.65254045, "source": "dense+rrf", "rrf_score": 0.029418126757516764, "rerank_score": 0.24014426818440787, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 21, "text_snippet": "s an example, the result of a game in premier league in a particular day might be good training data for frontier models, but we need to remove such information to leave more model capacity for ‚Äúreasoning‚Äù for the mini size models. We compa"}]}
{"case_index": 268, "query": "From 2302.13971__llama_open_and_efficient_foundation_language_models.txt, what exact term fills [BLANK] in: \"2.3 Optimizer Our models are trained using the AdamW opti- mizer (Loshchilov and Hutter, 2017), with the fol- lowing [BLANK]: Œ≤1= 0.9,Œ≤2= 0.95.\"?", "gold": "hyper-parameters", "answer": "The exact term is **‚Äúhyper-parameters‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term and is explicitly supported by Context 1 ('following hyper-parameters: Œ≤1=0.9, Œ≤2=0.95').", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 67.652, "llm_ms": 1073.016, "top_contexts": [{"rank": 1, "score": 0.6482137, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.38426432522419707, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 22, "text_snippet": " (2021), at each layer of the network. The details of the hyper-parameters for our dif- ferent models are given in Table 2. 2.3 Optimizer Our models are trained using the AdamW opti- mizer (Loshchilov and Hutter, 2017), with the fol- lowing"}, {"rank": 2, "score": 0.5939746, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.23180783114717074, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 189, "text_snippet": "ximum learning rate, which is shown in Table 10. The learning rate is decayed using a cosine cycle length that matches the total number of training tokens. All models are trained using AdamW (Loshchilov and Hutter, 2019) with a weight decay"}, {"rank": 3, "score": 0.48693722, "source": "dense+rrf", "rrf_score": 0.027984344422700584, "rerank_score": 0.22756407876557275, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 283, "text_snippet": "air encodings as in Brown et al. (2020) are used for all models. All our language models and RL policies have a context length of 2k tokens. We Ô¨Ålter out prompts that are longer than 1k tokens and limit the maximum response length to 1k tok"}, {"rank": 4, "score": 34.44308771591394, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.2251418130556358, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 50, "text_snippet": "es are up to 100k for 7B models, 65536 for 13B models, and 32768 for 70B models. The position indices for these models are re-scaled with Position Interpolation (Chen et al., 2023). Training Procedure We follow most training hyper-parameter"}, {"rank": 5, "score": 0.5052321, "source": "dense+rrf", "rrf_score": 0.029083245521601686, "rerank_score": 0.21192905827956496, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 187, "text_snippet": "In particular, we use the LAMB optimizer with learning rate 3.75e-3, with batch size 448, trained for at most 7100 steps. The training is stopped once the validation accuracy (for masked language modeling) reaches the target 72.0%, and the "}]}
{"case_index": 269, "query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"We represent each API call as a tuple c= (ac,ic) whereacis the name of the API and icis the cor- [BLANK] input.\"?", "gold": "responding", "answer": "The exact term is **‚Äúcorresponding‚Äù**, i.e., ‚Äú‚Ä¶and *i<sub>c</sub>* is the **corresponding** input.‚Äù", "answer_score": 0.3, "answer_correctness": 0.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "The blank should be 'responding' per expected answer, but the model answered 'corresponding'. The provided context actually contains 'cor-responding input', so the answer is grounded in context but does not match the expected term.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 88.515, "llm_ms": 1814.552, "top_contexts": [{"rank": 1, "score": 0.6175529, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3700060573885758, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 15, "text_snippet": "lls into any given text, using special tokens to mark the start and end of each such call. We represent each API call as a tuple c= (ac,ic) whereacis the name of the API and icis the cor- responding input. Given an API call cwith a cor- res"}, {"rank": 2, "score": 0.4744176, "source": "dense+rrf", "rrf_score": 0.031024531024531024, "rerank_score": 0.28540893056351413, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 1, "text_snippet": "metic or fac- tual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer , a model traine"}, {"rank": 3, "score": 30.68991125725103, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.2267193809495627, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 93, "text_snippet": "mitations While our approach enables LMs to learn how to use a variety of tools in a self-supervised way, there are some clear limitations to what can be achieved with our method in its current form. One such limi- tation is the inability o"}, {"rank": 4, "score": 0.49886903, "source": "dense+rrf", "rrf_score": 0.029513888888888888, "rerank_score": 0.22554485138743927, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 17, "text_snippet": "her the ob- tained responses are helpful for predicting future tokens; this is used as a Ô¨Åltering criterion. After Ô¨Åltering, we merge API calls for different tools, resulting in the augmented dataset C‚àó, and Ô¨Ånetune 1In practice, we use the"}, {"rank": 5, "score": 0.47400224, "source": "dense+rrf", "rrf_score": 0.02900988017658188, "rerank_score": 0.22452861475737404, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 12, "text_snippet": " how an API can be used, we let a LM annotate a huge language modeling dataset with potential API calls. We then use a self-supervised loss to determine which of these API calls actually help the model in predicting future tokens. Finally, "}]}
{"case_index": 270, "query": "From 2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt, what exact term fills [BLANK] in: \"3.Third, [BLANK] reasoning can be used for tasks such as math word problems, commonsense reasonin\"?", "gold": "chain-of-thought", "answer": "The exact term is **‚Äúchain-of-thought‚Äù**, as in: **‚ÄúThird, chain-of-thought reasoning can be used for tasks such as math word problems, commonsense reasoning ‚Ä¶‚Äù**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'chain-of-thought' and is directly supported by the provided context snippet.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 54.395, "llm_ms": 1547.544, "top_contexts": [{"rank": 1, "score": 0.7652514, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.3602306456045008, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 18, "text_snippet": " used for tasks such as math word problems, commonsense reasoning, and symbolic manipulation, and is potentially applicable (at least in principle) to any task that humans can solve via language. 4.Finally, chain-of-thought reasoning can be"}, {"rank": 2, "score": 0.67417145, "source": "dense+rrf", "rrf_score": 0.031099324975891997, "rerank_score": 0.3245734005606463, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 17, "text_snippet": "omputation can be allocated to problems that require more reasoning steps. 2.Second, a chain of thought provides an interpretable window into the behavior of the model, suggesting how it might have arrived at a particular answer and providi"}, {"rank": 3, "score": 0.7028153, "source": "dense+rrf", "rrf_score": 0.03149801587301587, "rerank_score": 0.3095183997692419, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 1, "text_snippet": "such reasoning abilities emerge naturally in sufÔ¨Åciently large language models via a simple method called chain-of- thought prompting , where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three"}, {"rank": 4, "score": 0.6325287, "source": "dense+rrf", "rrf_score": 0.027425373134328357, "rerank_score": 0.3061523026609187, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 30, "text_snippet": " output ‚ü©triples for arithmetic, commonsense, and symbolic reasoning benchmarks. Chains of thought are highlighted. Full prompts in Appendix G. math word problems, we used this single set of eight chain of thought exemplars for all benchmar"}, {"rank": 5, "score": 0.7494452, "source": "dense+rrf", "rrf_score": 0.030834914611005692, "rerank_score": 0.27653753068994447, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 19, "text_snippet": "rve the utility of chain-of-thought prompting for arithmetic reasoning (Section 3), commonsense reasoning (Section 4), and symbolic reasoning (Section 5). 3 Arithmetic Reasoning We begin by considering math word problems of the form in Figu"}]}
{"case_index": 271, "query": "From 2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt, what exact term fills [BLANK] in: \"These models perform downstream tasks primarily via prompting: all relevant task [BLANK] and data to process is formatted as a textual input context, and the model returns a generated text completion.\"?", "gold": "specification", "answer": "The exact term that fills **[BLANK]** is **‚Äúspecification‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'specification' and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 116.75, "llm_ms": 1259.094, "top_contexts": [{"rank": 1, "score": 0.6753578, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.4760724933600783, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 3, "text_snippet": "k in a variety of user-facing language technologies, including conversational interfaces, search and summarization, and collabo- rative writing (Shuster et al., 2022; Thoppilan et al., 2022; Lee et al., 2022, inter alia ). These models perf"}, {"rank": 2, "score": 0.5711385, "source": "dense+rrf", "rrf_score": 0.028612012987012988, "rerank_score": 0.27664088737985976, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 49, "text_snippet": "UIDs. The relevant key-value pair for answering the query is bolded here within the input context for clarity. indicate that extended-context models are not nec- essarily better than their non-extended counterparts at using their input cont"}, {"rank": 3, "score": 33.970965414997764, "source": "dense+rrf", "rrf_score": 0.014285714285714285, "rerank_score": 0.26243505441275816, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 81, "text_snippet": "ge model can take in 16K tokens, is it actually beneficial to provide 16K tokens of context? The answer to this question is ultimately downstream task-specific since it de- pends on the marginal value of the added context and the model‚Äôs ab"}, {"rank": 4, "score": 45.406268257891654, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.2607018379769441, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 0, "text_snippet": "Lost in the Middle: How Language Models Use Long Contexts Nelson F. Liu1‚àóKevin Lin2John Hewitt1Ashwin Paranjape3 Michele Bevilacqua3Fabio Petroni3Percy Liang1 1Stanford University2University of California, Berkeley3Samaya AI nfliu@cs.stanfo"}, {"rank": 5, "score": 0.57067484, "source": "dense+rrf", "rrf_score": 0.026171741361614778, "rerank_score": 0.2529168069400145, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 230, "text_snippet": "ucibility. First, we provide the exact input prompts for all tasks in Table 20‚ÄìTable 27 in Appendix G (and emphasize that we do not perform any Ô¨Ånetuning and only apply prompting to off-the-shelf language models). Second, we conduct experim"}]}
{"case_index": 272, "query": "From 2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt, what exact term fills [BLANK] in: \"Thanks to its small size, phi- 3-mini can be [BLANK] to 4-bits so that it only occupies ‚âà1.8GB of memory.\"?", "gold": "quantized", "answer": "The exact term that fills **[BLANK]** is **‚Äúquantized‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'quantized' and is directly supported by Context 1 sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 100.222, "llm_ms": 960.722, "top_contexts": [{"rank": 1, "score": 0.7140529, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.40566749830506027, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 17, "text_snippet": " running locally on a cell-phone. Thanks to its small size, phi- 3-mini can be quantized to 4-bits so that it only occupies ‚âà1.8GB of memory. We tested the quantized model by deploying phi-3-mini on iPhone 14 with A16 Bionic chip running na"}, {"rank": 2, "score": 0.6719463, "source": "dense+rrf", "rrf_score": 0.031754032258064516, "rerank_score": 0.2866810296056684, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 0, "text_snippet": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone Microsoft Abstract We introduce phi-3-mini , a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both"}, {"rank": 3, "score": 0.6472143, "source": "dense+rrf", "rrf_score": 0.03125763125763126, "rerank_score": 0.27770815144298916, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 16, "text_snippet": "3. The table shows the Keys/values a query token in block 8 attended to. Blue=local blocks, orange=remote/vertical blocks, gray=blocks skipped. total parameters. Additionally, we utilize the SparseMixer approach [LGC23, LDL+23] for training"}, {"rank": 4, "score": 0.69428015, "source": "dense+rrf", "rrf_score": 0.03149801587301587, "rerank_score": 0.27712432831252165, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 6, "text_snippet": "7B parameters), matched the performance of models 25 times larger trained on regular data. In this report we present a new model, phi-3-mini (3.8B parameters), trained for 3.3T tokens on larger and more advanced versions of the datasets use"}, {"rank": 5, "score": 0.69873, "source": "dense+rrf", "rrf_score": 0.03128054740957967, "rerank_score": 0.2591395589838628, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 8, "text_snippet": "ved solely by changing the training data. phi-3-mini: The development of a compact language model that rivals the capabilities of ChatGPT, while fitting on a phone, is a testament to the power of data-driven machine learning. By meticulousl"}]}
{"case_index": 273, "query": "From 2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt, what exact term fills [BLANK] in: \"nstruction Tuning [BLANK] tuning [ 46,38,62,61] has emerged as a crucial step in training language models.\"?", "gold": "instruction", "answer": "The exact term that fills **[BLANK]** is:\n\n**Instruction**", "answer_score": 0.3, "answer_correctness": 0.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Expected exact term is \"instruction\" (lowercase). Model answered \"Instruction\"; not an exact match as requested.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 86.916, "llm_ms": 1095.535, "top_contexts": [{"rank": 1, "score": 0.7743501, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3379642394925691, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 22, "text_snippet": "nstruction Tuning Instruction tuning [ 46,38,62,61] has emerged as a crucial step in training language models. Instruction tuning involves learning from input-output pairs where the input is natural language task description,and the output "}, {"rank": 2, "score": 0.6363608, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.23275256564562802, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 21, "text_snippet": " to the phenomenon that all models are to some extent constrained by their underlying pre-trained model (while Orca 2 training could be applied any base LLM, we report results on LLaMA-2 7B and 13B in this report). Orca 2 models have not un"}, {"rank": 3, "score": 31.2806411475747, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.22600900898706022, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 148, "text_snippet": "synthetic data for post-training. While Orca 2 models have not gone through RLHF training for safety, we believe that the use of synthetic data for post-training that has been filtered with various content safety filters could provide anoth"}, {"rank": 4, "score": 0.5204752, "source": "dense+rrf", "rrf_score": 0.028125, "rerank_score": 0.2114233021710062, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 0, "text_snippet": "Orca 2: Teaching Small Language Models How to Reason Arindam Mitra, Luciano Del Corro‚Ä†, Shweti Mahajan‚Ä†, Andres Codas‚Ä° Clarisse Simoes‚Ä°, Sahaj Agarwal, Xuxi Chen‚àó, Anastasia Razdaibiedina‚àó Erik Jones‚àó, Kriti Aggarwal‚àó, Hamid Palangi, Guoqin"}, {"rank": 5, "score": 25.8157385213325, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.1876968219021908, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 13, "text_snippet": "ity to reason zero-shot [ 23]. These abilities include answering complex questions, generating explanations, and solving multi-step problems, for instance, such as those on the US Medical Licensing exam, on which LLMs now achieve a passing "}]}
{"case_index": 274, "query": "From 2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt, what exact term fills [BLANK] in: \"ughts only need to 1We show some GPT-3 (Brown et al., 2020) results in Appendix A.1, which [BLANK] PaLM-540B.\"?", "gold": "outperforms", "answer": "The exact term is **‚Äúoutperforms‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term and is explicitly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 78.717, "llm_ms": 986.854, "top_contexts": [{"rank": 1, "score": 0.66727567, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.4023836237130121, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 32, "text_snippet": "ughts only need to 1We show some GPT-3 (Brown et al., 2020) results in Appendix A.1, which outperforms PaLM-540B. 3  Published as a conference paper at ICLR 2023 appear sparsely in the most relevant positions of a trajectory, so we let the "}, {"rank": 2, "score": 41.22930587235901, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.30628212411595923, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 31, "text_snippet": "xt example is a human trajectory of actions, thoughts, and environment observations to solve a task instance (see Appendix C). For the tasks where reasoning is of primary importance (Figure 1(1)), we alternate the generation of thoughts and"}, {"rank": 3, "score": 0.60435, "source": "dense+rrf", "rrf_score": 0.028371628371628373, "rerank_score": 0.25826392645878904, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 14, "text_snippet": "et al., 2022) and GPT-3-175B (Brown et al., 2020), and two densely-activated decoder-only language models: LaMDA-137B (Thoppilan et al., 2022) and PaLM-540B (Chowdhery et al., 2022). On all four language models, self-consistency improves ov"}, {"rank": 4, "score": 0.61595863, "source": "dense+rrf", "rrf_score": 0.028169014084507043, "rerank_score": 0.24664819799891388, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 92, "text_snippet": "nal experiments using GPT-3 (Brown et al., 2020) in Appendix A.1, and associated GPT-3 ReAct prompting code at https://anonymous.4open.science/r/ReAct-2268/ . ETHICS STATEMENT ReAct prompts large language models to generate more human inter"}, {"rank": 5, "score": 0.6560074, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.2456360764748813, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 118, "text_snippet": "3 A A DDITIONAL RESULTS A.1 GPT-3 E XPERIMENTS PaLM-540B GPT-3 HotpotQA (exact match) 29.4 30.8 ALFWorld (success rate %) 70.9 78.4 Table 5: ReAct prompting results using PaLM-540B vs. GPT-3 (text-davinci-002, greedy decoding). On HotpotQA,"}]}
{"case_index": 275, "query": "From 1706.03762__attention_is_all_you_need.txt, what exact term fills [BLANK] in: \"The Transformer allows for significantly more [BLANK] and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\"?", "gold": "parallelization", "answer": "The exact term is **‚Äúparallelization‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'parallelization' and is explicitly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 56.428, "llm_ms": 1176.174, "top_contexts": [{"rank": 1, "score": 0.7110776, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.45898508570161844, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 10, "text_snippet": "instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being t"}, {"rank": 2, "score": 0.5895103, "source": "dense+rrf", "rrf_score": 0.030621785881252923, "rerank_score": 0.2569461357877739, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 3, "text_snippet": "h-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show tha"}, {"rank": 3, "score": 0.6037803, "source": "dense+rrf", "rrf_score": 0.030798389007344232, "rerank_score": 0.23761470274460225, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 50, "text_snippet": "6]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score. 6 Results 6.1 Machine Translation On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Ta"}, {"rank": 4, "score": 26.339203272760006, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.23570692605564256, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 1, "text_snippet": ", 2018), BERT is designed to pre- train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a re- sult, the pre-trained BERT model can be Ô¨Åne- tuned with just one a"}, {"rank": 5, "score": 0.61074483, "source": "dense+rrf", "rrf_score": 0.031024531024531024, "rerank_score": 0.21875987722635667, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 51, "text_snippet": "tom line of Table 3. Training took 3.5days on 8P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models. On the WMT 2014 English-to-French tr"}]}
{"case_index": 276, "query": "From 2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt, what exact term fills [BLANK] in: \"The [BLANK] says, ‚ÄúI‚Äôm sorry, but we don‚Äôt serve alcohol here.‚Äù The man replies, ‚ÄúOh, I didn‚Äôt realize this place was a church!‚Äù What does the man mean by his response?\"?", "gold": "bartender", "answer": "**bartender**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "The context explicitly states 'The bartender says‚Ä¶', and the model answered 'bartender', matching the expected term.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 121.921, "llm_ms": 1019.641, "top_contexts": [{"rank": 1, "score": 0.6424104, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.41251424642299506, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 8, "text_snippet": " ### LLaMA2-13B : Ans: The ball is still in the box because it was there when John left the room. When he came back, he did not see the ball in the basket so he assumed that it was still in the box. Ques 10. A man walks into a bar and asks "}, {"rank": 2, "score": 34.59440586692402, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.22456346706878066, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 238, "text_snippet": "? AI: I am an AI created by OpenAI. How can I help you today? Human: I‚Äôm feeling kind of down today. AI: Continued on next page 31  Use Case Example chat This is a conversation with Steven. Steven likes to watch NetÔ¨Çix and hasn‚Äôt left his h"}, {"rank": 3, "score": 39.537512146386675, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.21315229423002457, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 169, "text_snippet": "re not a planet anymore. Pluto: What? Come on, man. That‚Äôs not fair. Sun: I‚Äôm sorry, but it‚Äôs true. You just don‚Äôt meet the criteria anymore. Pluto: This is bulls**t! I‚Äôve been a planet for over 70 years! Sun: Things change, Pluto. You‚Äôve j"}, {"rank": 4, "score": 44.258156025878336, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.20813451207377504, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 198, "text_snippet": "feel like you could have worn the tux,\" she says. \"But you're saying you're a shape-shifter. I don't know if you can change your tux, but you can change your mind. You can change your mind. You can change your mind.\" Phoenix says he did, bu"}, {"rank": 5, "score": 27.080772287807402, "source": "dense+rrf", "rrf_score": 0.014285714285714285, "rerank_score": 0.20612426527651492, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 229, "text_snippet": "antly answers questions with sarcastic responses: You: How many pounds are in a kilogram? Marv: This again? There are 2.2 pounds in a kilogram. Please make a note of this. You: What does HTML stand for? Marv: Was Google too busy? Hypertext "}]}
{"case_index": 277, "query": "From 2305.06983__active_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"These attempts include methods that passively use the past context to retrieve additional [BLANK] at a fixed interval (Khandelwal et\"?", "gold": "information", "answer": "The exact term is **‚Äúinformation‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context explicitly contains the phrase 'retrieve additional information at a fixed interval', matching the expected blank. Answer is correct and fully grounded.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 64.211, "llm_ms": 1164.413, "top_contexts": [{"rank": 1, "score": 0.5842867, "source": "dense+rrf", "rrf_score": 0.032018442622950824, "rerank_score": 0.35187050219460714, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 12, "text_snippet": "Biden) may not cover all aspects and de- tails. It is crucial to retrieve extra information as needed during generation, such as when generat- ing a certain aspect (e.g., Joe Biden‚Äôs education history) or a specific detail (e.g., the date o"}, {"rank": 2, "score": 0.5557608, "source": "dense+rrf", "rrf_score": 0.02964254577157803, "rerank_score": 0.22349312152199227, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 13, "text_snippet": "rieve additional information at a fixed interval (Khandelwal et al., 2020; Borgeaud et al., 2022; Ram et al., 2023; Trivedi et al., 2022) which might not accurately reflect what LMs intend to gener- ate in the future or retrieve at inapprop"}, {"rank": 3, "score": 28.951075239511347, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.22244272280342256, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 17, "text_snippet": "ng Active REtrieval augmented generation (FLARE ), as illustrated in Figure 1. FLARE iter- atively generates a temporary next sentence , use it as the query to retrieve relevant documents if it contains low-probability tokens and regenerate"}, {"rank": 4, "score": 0.5949228, "source": "dense+rrf", "rrf_score": 0.030798389007344232, "rerank_score": 0.21839536817844746, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 15, "text_snippet": "e to avoid unneces- sary or inappropriate retrieval that occurs in passive retrieval augmented LMs (Khandelwal et al., 2020; Borgeaud et al., 2022; Ram et al., 2023; Trivedi et al., 2022). Given the observation that large LMs tend to be wel"}, {"rank": 5, "score": 0.557398, "source": "dense+rrf", "rrf_score": 0.029083245521601686, "rerank_score": 0.21309673730633052, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 21, "text_snippet": "ing when and what to retrieve, we follow exist- ing methods (Ram et al., 2023; Trivedi et al., 2022) to prepend the retrieved documents before the user input to aid future generation for both baselines and our method for fair comparisons: y"}]}
{"case_index": 278, "query": "From 2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt, what exact term fills [BLANK] in: \"The question encoder and the reader model are then Ô¨Åne- tuned using pairs of [BLANK] and answers jointly.\"?", "gold": "questions", "answer": "The exact term that fills **[BLANK]** is **‚Äúquestions‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'questions' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 61.905, "llm_ms": 1142.519, "top_contexts": [{"rank": 1, "score": 0.56960607, "source": "dense+rrf", "rrf_score": 0.029051670471052088, "rerank_score": 0.36863507283837377, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 8, "text_snippet": "/BM25 for open- domain QA before ORQA (Lee et al., 2019), which proposes a sophisticated inverse cloze task (ICT) objective, predicting the blocks that contain the masked sentence, for additional pretraining. The question encoder and the re"}, {"rank": 2, "score": 0.5956746, "source": "dense+rrf", "rrf_score": 0.031054405392392875, "rerank_score": 0.2977584758207373, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 9, "text_snippet": "open-domainarXiv:2004.04906v3 [cs.CL] 30 Sep 2020  QA datasets, it also suffers from two weaknesses. First, ICT pretraining is computationally intensive and it is not completely clear that regular sentences are good surrogates of questions "}, {"rank": 3, "score": 29.837043528502832, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.23479891247031265, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 98, "text_snippet": "Scaling in-context learning to 1, 000 examples. ArXiv, abs/2212.06713, 2022. Gautier Izacard and Edouard Grave. Distilling knowledge from reader to retriever for question answering. arXiv preprint arXiv:2012.04584 , 2020a. Gautier Izacard a"}, {"rank": 4, "score": 31.139428997107863, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.23106538805012564, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 17, "text_snippet": "rieval accuracy , which is the fraction of ques- tions for whichCFcontains a span that answers the question. 3 Dense Passage Retriever (DPR) We focus our research in this work on improv- ing the retrieval component in open-domain QA. Given "}, {"rank": 5, "score": 0.58561844, "source": "dense+rrf", "rrf_score": 0.028991596638655463, "rerank_score": 0.2304041099788798, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 12, "text_snippet": " vs. 33.3%) in the open Natural Questions setting (Lee et al., 2019; Kwiatkowski et al., 2019). Our contributions are twofold. First, we demon- strate that with the proper training setup, sim- ply Ô¨Åne-tuning the question and passage encoder"}]}
{"case_index": 279, "query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \"More recently, sentence or document encoders which produce contextual token [BLANK] have been pre-trained from unlabeled text and Ô¨Åne-tuned for a supervised downstream task (Dai and Le, 2015; Howard and R\"?", "gold": "representations", "answer": "The exact term is **‚Äúrepresentations‚Äù** (i.e., ‚Äúcontextual token representations‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected blank ('representations') and is directly supported by Context 1 phrase 'contextual token representations'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 90.562, "llm_ms": 1296.645, "top_contexts": [{"rank": 1, "score": 0.6000752, "source": "dense+rrf", "rrf_score": 0.031544957774465976, "rerank_score": 0.4591404396683092, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 17, "text_snippet": "hows that the cloze task can be used to improve the robustness of text generation mod- els. 2.2 Unsupervised Fine-tuning Approaches As with the feature-based approaches, the Ô¨Årst works in this direction only pre-trained word em- bedding par"}, {"rank": 2, "score": 0.59164083, "source": "dense+rrf", "rrf_score": 0.028991596638655463, "rerank_score": 0.25185394621338625, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 20, "text_snippet": "or BERT. Apart from output layers, the same architec- tures are used in both pre-training and Ô¨Åne-tuning. The same pre-trained model parameters are used to initialize models for different down-stream tasks. During Ô¨Åne-tuning, all parameters"}, {"rank": 3, "score": 0.584324, "source": "dense+rrf", "rrf_score": 0.029513888888888888, "rerank_score": 0.25106132770036244, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 11, "text_snippet": "for downstream transfer. First, single-layer representations were learned using word vectors [ MCCD13 ,PSM14 ] and fed to task-speciÔ¨Åc architectures, then RNNs with multiple layers of representations and contextual state were used to form s"}, {"rank": 4, "score": 0.59888506, "source": "dense+rrf", "rrf_score": 0.030309988518943745, "rerank_score": 0.23985081352244458, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 1, "text_snippet": ", 2018), BERT is designed to pre- train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a re- sult, the pre-trained BERT model can be Ô¨Åne- tuned with just one a"}, {"rank": 5, "score": 37.25051827728281, "source": "dense+rrf", "rrf_score": 0.014084507042253521, "rerank_score": 0.2326530419608738, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 10, "text_snippet": "ford et al. (2018), which uses unidirec- tional language models for pre-training, BERT uses masked language models to enable pre- trained deep bidirectional representations. This is also in contrast to Peters et al. (2018a), which uses a sh"}]}
{"case_index": 280, "query": "From 1706.03762__attention_is_all_you_need.txt, what exact term fills [BLANK] in: \"Numerous efforts have since continued to push the boundaries of recurrent language models and [BLANK] architectures [38, 24, 15].\"?", "gold": "encoder-decoder", "answer": "The exact term is **‚Äúencoder-decoder‚Äù**, as in: ‚Äúrecurrent language models and **encoder-decoder** architectures [38, 24, 15].‚Äù", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'encoder-decoder' and is directly supported by the provided context sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 106.895, "llm_ms": 1739.167, "top_contexts": [{"rank": 1, "score": 0.6940623, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3481519740305486, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 7, "text_snippet": " [ 35,2,5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15]. Recurrent models typically factor computation along the symbol positions of the input and "}, {"rank": 2, "score": 0.62067646, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.26353014823950055, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 6, "text_snippet": "performed while at Google Research. 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7 [cs.CL] 2 Aug 2023  1 Introduction Recurrent neural networks, long short-term memory [ 13] and "}, {"rank": 3, "score": 0.61855066, "source": "dense+rrf", "rrf_score": 0.03125763125763126, "rerank_score": 0.2379977100524491, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 4, "text_snippet": "ural networks have proven to be powerful language models, Ô¨Årst in the form of recurrent architectures (Graves, 2013; Jozefowicz et al., 2016; Mikolov et al., 2010) and more recently in the form of Transformers (Vaswani et al., 2017), that u"}, {"rank": 4, "score": 23.76223058869832, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.19461042155126346, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 281, "text_snippet": " you always used to look like this , did n‚Äôt you , _‚Äù What is the word in the blank space (_)? The answer is Orca-v2-13B-Cautious The answer is izzy. The word in the blank space is the name of the girl who is crying. Figure 24: Demonstrativ"}, {"rank": 5, "score": 0.6195582, "source": "dense+rrf", "rrf_score": 0.031009615384615385, "rerank_score": 0.18906205142991667, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 11, "text_snippet": "for downstream transfer. First, single-layer representations were learned using word vectors [ MCCD13 ,PSM14 ] and fed to task-speciÔ¨Åc architectures, then RNNs with multiple layers of representations and contextual state were used to form s"}]}
{"case_index": 281, "query": "From 2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"d to ensure that the retrieved context can act as a [BLANK] for the generated answer.\"?", "gold": "justification", "answer": "The exact term is **‚Äújustification‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Model answer matches expected term 'justification' and is directly supported by the provided context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 70.973, "llm_ms": 1070.033, "top_contexts": [{"rank": 1, "score": 0.6426578, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.30837113858360604, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 21, "text_snippet": "d to ensure that the retrieved context can act as a justification for the generated answer. Indeed, RAG systems are often used in applications where the factual con- sistency of the generated text w.r.t. the grounded sources is highly impor"}, {"rank": 2, "score": 26.949913944004557, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.25965205305246997, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 45, "text_snippet": "ially for longer contexts. 6 Conclusions We have highlighted the need for automated reference-free evaluation of RAG systems. In par- ticular, we have argued the need for an evaluation framework that can assess faithfulness (i.e. is the ans"}, {"rank": 3, "score": 24.645619623707113, "source": "dense+rrf", "rrf_score": 0.014084507042253521, "rerank_score": 0.25232261624674845, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 20, "text_snippet": "trieval augmented LMs is to generate the answer y= [s1,s2, ...,sm] = [w1, w2, ..., w n]containing msentences or ntokens leveraging information retrieved from the corpus. In retrieval augmented LM, the LM typically pairs with a retriever tha"}, {"rank": 4, "score": 0.6107132, "source": "dense+rrf", "rrf_score": 0.030776515151515152, "rerank_score": 0.23669881053768532, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 129, "text_snippet": "s in the process of information retrieval and generation [164]‚Äì[166]. Context Relevance evaluates the precision and specificity of the retrieved context, ensuring relevance and minimizing processing costs associated with extraneous content."}, {"rank": 5, "score": 0.5720259, "source": "dense+rrf", "rrf_score": 0.027912386121341344, "rerank_score": 0.23378218872566098, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 20, "text_snippet": "generate an answer as(q). When building a RAG system,  we usually do not have access to human-annotated datasets or reference answers. We therefore fo- cus on metrics that are fully self-contained and reference-free. We focus in particular "}]}
{"case_index": 282, "query": "From 2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt, what exact term fills [BLANK] in: \"Our largest model obtains [BLANK] results on a range of downstream evaluation datasets including Wikitext103 (Merity et al., 2017) and the Pile (Gao et al., 2020) (¬ß4).\"?", "gold": "state-of-the-art", "answer": "The exact term is **‚Äústate-of-the-art‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term and is directly supported by Context 1, which contains the exact phrase 'obtains state-of-the-art results'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 141.164, "llm_ms": 1265.135, "top_contexts": [{"rank": 1, "score": 68.46520137279138, "source": "dense+rrf", "rrf_score": 0.01639344262295082, "rerank_score": 0.4185097705033301, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 12, "text_snippet": "ase size (Fig. 1): R/e.sc/t.sc/r.sc/o.sc provides a constant gain for models ranging from 150M to 7B parameters, and R/e.sc/t.sc/r.sc/o.sccan be improved at evaluation time by increasing the database size and the number of retrieved neigh- "}, {"rank": 2, "score": 0.70476395, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.3237458111294175, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 77, "text_snippet": "etrieval. 4.1. Language modelling Datasets. We evaluate our models on C4 (RaÔ¨Äel et al., 2020), Wikitext103 (Merity et al., 2017), Curation Corpus (Curation, 2020), Lambada (Paperno et al., 2016) and the Pile (Gao et al., 2020). We also eval"}, {"rank": 3, "score": 39.89411988965746, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.315472523777206, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 228, "text_snippet": "baseline models, measured in bits-per-bytes on evaluation set, in Table 14. F.2. The Pile In Fig. 4, we compare R/e.sc/t.sc/r.sc/o.scagainst Jurassic-1 (Lieber et al., 2021). The full bits-per-bytes results are reported in Table 15. F.3. Fi"}, {"rank": 4, "score": 0.656656, "source": "dense+rrf", "rrf_score": 0.027984344422700584, "rerank_score": 0.2919920031815438, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 173, "text_snippet": "n Conference of the North American Chapter of the Association for Computational Linguistics , 2018. 23  Improving language models by retrieving from trillions of tokens A. Datasets We provide a full description of MassiveText and of our ext"}, {"rank": 5, "score": 0.6400384, "source": "dense+rrf", "rrf_score": 0.027651515151515153, "rerank_score": 0.2913071986598169, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 30, "text_snippet": " News 236,918 398 No 10% Wikipedia 13,288 23 Yes 5% GitHub 374,952 143 No 5% 4  Improving language models by retrieving from trillions of tokens 2.4.R/e.sc/t.sc/r.sc/o.scmodel architecture Our model relies on an encoder-decoder transformer "}]}
{"case_index": 283, "query": "From 2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt, what exact term fills [BLANK] in: \"Section III focuses on [BLANK] methods in retrieval,including indexing, query and embedding optimization.\"?", "gold": "optimization", "answer": "The exact term is **‚Äúoptimization‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'optimization' and is directly supported by Context 1 sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 79.714, "llm_ms": 1129.704, "top_contexts": [{"rank": 1, "score": 0.62521493, "source": "dense+rrf", "rrf_score": 0.032018442622950824, "rerank_score": 0.28997071130800967, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 15, "text_snippet": "t challenges. The paper unfolds as follows: Section II introduces the main concept and current paradigms of RAG. The following three sections explore core components‚Äî‚ÄúRetrieval‚Äù, ‚ÄúGen- eration‚Äù and ‚ÄúAugmentation‚Äù, respectively. Section III "}, {"rank": 2, "score": 0.6005572, "source": "dense+rrf", "rrf_score": 0.027984344422700584, "rerank_score": 0.23418209966174125, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 79, "text_snippet": "on the long-context understanding ability of LLMs, and we still have a long way to go in harnessing open-source LLMs within our framework. 10  4 Related Work 4.1 Retrieval-Augmented Generation. Augmenting language models with information re"}, {"rank": 3, "score": 25.199029590815964, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.22181236097457716, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 84, "text_snippet": "HyDE [11] construct hypothet- ical documents (assumed answers to the original query). It focuses on embedding similarity from answer to answer rather than seeking embedding similarity for the problem or query. Using the Step-back Prompting "}, {"rank": 4, "score": 26.90611028472533, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.20727092058608226, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 11, "text_snippet": "h a detailed and structured understanding of both large models and RAG. It aims to illuminate the evolution of retrieval augmentation techniques, assess the strengths and weaknesses of various approaches in their respective contexts, and sp"}, {"rank": 5, "score": 0.5880303, "source": "dense+rrf", "rrf_score": 0.02658450704225352, "rerank_score": 0.20459765941789504, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 20, "text_snippet": "trieval augmented LMs is to generate the answer y= [s1,s2, ...,sm] = [w1, w2, ..., w n]containing msentences or ntokens leveraging information retrieved from the corpus. In retrieval augmented LM, the LM typically pairs with a retriever tha"}]}
{"case_index": 284, "query": "From 2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt, what exact term fills [BLANK] in: \"We observe that some benchmarks improve much less from 7B to 14B than they do from 3.8B to 7B, perhaps indicating that our data mixture needs further work to be in the ‚Äúdata optimal regime‚Äù for 14B [BLANK] model.\"?", "gold": "parameters", "answer": "The exact term is **‚Äúparameters‚Äù**, i.e., the sentence reads: ‚Äú‚Ä¶for **14B parameters** model.‚Äù", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "The context explicitly contains the phrase '14B parameters model', so filling [BLANK] with 'parameters' is correct and fully supported.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 69.517, "llm_ms": 1364.581, "top_contexts": [{"rank": 1, "score": 0.7313573, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.4891725543778565, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 22, "text_snippet": "trained on the same data for slightly more epochs (4.8T tokens total as for phi-3-small . The model has 40 heads and 40 layers, with embedding dimension 5120. We observe that some benchmarks improve much less from 7B to 14B than they do fro"}, {"rank": 2, "score": 0.59966797, "source": "dense+rrf", "rrf_score": 0.028629032258064516, "rerank_score": 0.27351057725343975, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 0, "text_snippet": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone Microsoft Abstract We introduce phi-3-mini , a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both"}, {"rank": 3, "score": 0.62218416, "source": "dense+rrf", "rrf_score": 0.02976190476190476, "rerank_score": 0.23878658098725683, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 6, "text_snippet": "7B parameters), matched the performance of models 25 times larger trained on regular data. In this report we present a new model, phi-3-mini (3.8B parameters), trained for 3.3T tokens on larger and more advanced versions of the datasets use"}, {"rank": 4, "score": 0.6318259, "source": "dense+rrf", "rrf_score": 0.02919863597612958, "rerank_score": 0.23532771822385512, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 20, "text_snippet": "hat train language models in either ‚Äúcompute optimal regime‚Äù [HBM+22] or ‚Äúover-train regime‚Äù, we mainly focus on the quality of data for a given scale .3 We try to calibrate the training data to be closer to the ‚Äúdata optimal‚Äù regime for sm"}, {"rank": 5, "score": 0.63326144, "source": "dense+rrf", "rrf_score": 0.029857397504456328, "rerank_score": 0.23001629294536893, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 18, "text_snippet": "y training data to improve the performance of small language models and deviate from the standard scaling-laws . In this work we show that such method allows to reach the level of highly capable models such as GPT-3.5 or Mixtral with only 3"}]}
{"case_index": 285, "query": "From 1706.03762__attention_is_all_you_need.txt, what exact term fills [BLANK] in: \"[BLANK] attention is identical to our algorithm, except for the scaling factor of1‚àödk.\"?", "gold": "dot-product", "answer": "**Dot-product** attention is identical to our algorithm, except for the scaling factor of \\( \\frac{1}{\\sqrt{d_k}} \\).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Correctly identifies the blank as 'dot-product' and matches the provided context verbatim.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 66.165, "llm_ms": 1314.085, "top_contexts": [{"rank": 1, "score": 0.66492045, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.2925174129145456, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 22, "text_snippet": "k)V (1) The two most commonly used attention functions are additive attention [ 2], and dot-product (multi- plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of1‚àödk. Additive attention "}, {"rank": 2, "score": 21.11087182295989, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.20719834494584913, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 77, "text_snippet": "tention baselines. All other algorithms except for Linformer run out of memory on an A100 GPU before 64K, and FlashAttention is still 2\u0002more eÔ¨Écient than Linformer. 5 Limitations and Future Directions We discuss limitations of our approach "}, {"rank": 3, "score": 0.5390088, "source": "dense+rrf", "rrf_score": 0.030621785881252923, "rerank_score": 0.20700514155697572, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 32, "text_snippet": " techniques (tiling, recomputation) to overcome the technical challenge of computing exact attention in sub-quadratic HBM accesses. We describe this in Algorithm 1. The main idea is that we split the inputs Q¬ñK¬ñVinto blocks, load them from "}, {"rank": 4, "score": 23.70708783243311, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.20489405645958683, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 181, "text_snippet": "ock-sparse FlashAttention We describe the full block-sparse FlashAttention algorithm in Algorithm 5. The algorithm is identical to Algorithm 2, except that we skip zero blocks. We prove the IO-complexity of block-sparse FlashAttention . Pro"}, {"rank": 5, "score": 0.5234447, "source": "dense+rrf", "rrf_score": 0.025158227848101268, "rerank_score": 0.20084583112896653, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 174, "text_snippet": "in\u0012ùëÄ ùëë¬ñùëÄ ùêµùëê\u0013\u0013 =Œò\u0012 min\u0012ùëÄ ùëë¬ñùëë\u0013\u0013 ¬ï We then have: ùëáùëê=ùëÅ ùêµùëê=Œò\u0012ùëÅùëë ùëÄ\u0013 ¬ï As a result, the number of HBM accesses is: Œò¬πùëÅùëëùëáùëê¬∫=Œò\u0012ùëÅ2ùëë2 ùëÄ\u0013 ¬ï \u0003 Proof of Proposition 3. For contradiction, suppose that there exists an algorithm that computes exact attentio"}]}
{"case_index": 286, "query": "From 2005.14165__language_models_are_few_shot_learners.txt, what exact term fills [BLANK] in: \"Finally, we Ô¨Ånd that GPT-3 can generate samples of news articles which human evaluators have difÔ¨Åculty [BLANK] from articles written by humans.\"?", "gold": "distinguishing", "answer": "The exact term is **‚Äúdistinguishing‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'distinguishing' and is directly supported by the provided context sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 72.104, "llm_ms": 1030.037, "top_contexts": [{"rank": 1, "score": 0.6069507, "source": "dense+rrf", "rrf_score": 0.03047794966520434, "rerank_score": 0.39908433587896286, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 4, "text_snippet": "vel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3‚Äôs few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training"}, {"rank": 2, "score": 0.6489774, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.36812938497500447, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 34, "text_snippet": "show that in the few-shot setting, GPT-3 can generate synthetic news articles which human evaluators have difÔ¨Åculty distinguishing from human-generated articles. At the same time, we also Ô¨Ånd some tasks on which few-shot performance struggl"}, {"rank": 3, "score": 0.6719066, "source": "dense+rrf", "rrf_score": 0.032266458495966696, "rerank_score": 0.24616605308389744, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 186, "text_snippet": "icles generated by GPT-3 and a control model. We found that mean human accuracy at detecting the intentionally bad longer articles from the control model was ‚àº88%, while mean human accuracy at detecting the longer articles that were produce"}, {"rank": 4, "score": 30.306851022408836, "source": "dense+rrf", "rrf_score": 0.014285714285714285, "rerank_score": 0.2420928097058081, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 146, "text_snippet": " several tasks that involve rearranging or unscrambling the letters in a word, tasks which are unlikely to have been exactly seen during training. Third, we test GPT-3‚Äôs ability to solve SAT-style analogy problems few-shot. Finally, we test"}, {"rank": 5, "score": 0.6185415, "source": "dense+rrf", "rrf_score": 0.027526395173453996, "rerank_score": 0.23934663241929646, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 180, "text_snippet": " accuracy in identifying whether short ( ‚àº200 word) news articles are model generated . We Ô¨Ånd that human accuracy (measured by the ratio of correct assignments to non-neutral assignments) ranges from 86% on the control model to 52% on GPT-"}]}
{"case_index": 287, "query": "From 2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt, what exact term fills [BLANK] in: \"However, the pre-defined size limits LLMs in many [BLANK], like summarizing long documents or answering long questions.\"?", "gold": "applications", "answer": "The exact term that fills **[BLANK]** is **‚Äúapplications‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'applications' and is directly supported by Context 1 sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 58.998, "llm_ms": 1149.852, "top_contexts": [{"rank": 1, "score": 0.61685383, "source": "dense+rrf", "rrf_score": 0.029051670471052088, "rerank_score": 0.35429580758942847, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 8, "text_snippet": "e time. In addition to training LoRA weights in linear layers, LongLoRA further makes embedding and normalization layers trainable. This extension is pivotal for context extension, and only introduces a minimal number of additional trainabl"}, {"rank": 2, "score": 41.82133461213699, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.2354288324099431, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 0, "text_snippet": "Published as a conference paper at ICLR 2024 LONG LORA: E FFICIENT FINE-TUNING OF LONG - CONTEXT LARGE LANGUAGE MODELS Yukang Chen1Shengju Qian1Haotian Tang2Xin Lai1 Zhijian Liu2Song Han2,3Jiaya Jia1 1CUHK2MIT3NVIDIA ABSTRACT We present Lon"}, {"rank": 3, "score": 0.6327767, "source": "dense+rrf", "rrf_score": 0.028577260665441927, "rerank_score": 0.22361507542251302, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 100, "text_snippet": "ohere rerank or bge-raranker-large, and general large language mod- els like GPT [12], [99]. 2) Context Selection/Compression: A common misconcep- tion in the RAG process is the belief that retrieving as many relevant documents as possible "}, {"rank": 4, "score": 32.3883665308717, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.212265887271734, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 75, "text_snippet": "ed in training-from- scratch transformers. This experiment is to examine their capability of fine-tuning on pre-trained LLMs (Touvron et al., 2023b), toward long context adaptation. Dilated attention performs well in full fine-tuning but is"}, {"rank": 5, "score": 31.86728760701509, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.20887522524792929, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 12, "text_snippet": "e. In terms of efficiency , regardless of whether LoRA is employed or not, computational cost increases dramatically as the context size expands, primarily due to the standard self-attention mechanism (Vaswani et al., 2017). As shown in Fig"}]}
{"case_index": 288, "query": "From 2302.13971__llama_open_and_efficient_foundation_language_models.txt, what exact term fills [BLANK] in: \"We re- place the ReLU [BLANK] by the SwiGLU ac- tivation function, introduced by Shazeer (2020) to improve the performance.\"?", "gold": "non-linearity", "answer": "The exact term is **‚Äúnon-linearity‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term and is directly supported by Context 1 quote ('replace the ReLU non-linearity').", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 109.032, "llm_ms": 962.342, "top_contexts": [{"rank": 1, "score": 0.607164, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.341531692792622, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 21, "text_snippet": "se the RMSNorm normalizing func- tion, introduced by Zhang and Sennrich (2019). SwiGLU activation function [PaLM]. We re- place the ReLU non-linearity by the SwiGLU ac- tivation function, introduced by Shazeer (2020) to improve the performa"}, {"rank": 2, "score": 20.59085537988183, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.20555470956659297, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 30, "text_snippet": "ment prompts and improve the performance of GPT-3. There has also been work on aligning agents in text-based environments using RL with 4  a normative prior (Nahian et al., 2021). Our work can be seen as a direct application of RLHF to alig"}, {"rank": 3, "score": 20.561125935679236, "source": "dense+rrf", "rrf_score": 0.014285714285714285, "rerank_score": 0.1974065257027066, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 5, "text_snippet": "at inference. For instance, although Hoffmann et al. (2022) recommends training a 10B model on 200B tokens, we Ô¨Ånd that the performance of a 7B model continues to improve even after 1T tokens. The focus of this work is to train a series of "}, {"rank": 4, "score": 24.046500518702963, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.18740216124236173, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 0, "text_snippet": "LLaMA: Open and EfÔ¨Åcient Foundation Language Models Hugo Touvron‚àó, Thibaut Lavril‚àó, Gautier Izacard‚àó, Xavier Martinet Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi√®re, Naman Goyal Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand "}, {"rank": 5, "score": 0.44454876, "source": "dense+rrf", "rrf_score": 0.029827662395050816, "rerank_score": 0.1761012965026519, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 118, "text_snippet": "nal Linguistics. Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and Denny Zhou. 2022. Recitation-augmented language models. CoRR , abs/2210.01296. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth√©e "}]}
{"case_index": 289, "query": "From 2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt, what exact term fills [BLANK] in: \"Therefore, we believe the modern RAG systems should re-consider the granularity of their retrieval units to exploit the advantages of the current [BLANK] LLMs.\"?", "gold": "long-context", "answer": "The exact term is **‚Äúlong-context‚Äù**, i.e., ‚Äú‚Ä¶to exploit the advantages of the current **long-context** LLMs.‚Äù", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'long-context' and is directly supported by the provided context quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 95.086, "llm_ms": 1790.259, "top_contexts": [{"rank": 1, "score": 0.7550814, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.42835716120636796, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 20, "text_snippet": "ontext methods evolve, the performance of LongRAG will continue to improve. Therefore, we believe the modern RAG systems should re-consider the granularity of their retrieval units to exploit the advantages of the current long-context LLMs."}, {"rank": 2, "score": 0.7230106, "source": "dense+rrf", "rrf_score": 0.031746031746031744, "rerank_score": 0.24736360153121884, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 0, "text_snippet": "LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs ‚ô†Ziyan Jiang,‚ô†Xueguang Ma,‚ô†Wenhu Chen ‚ô†University of Waterloo ziyanjiang528@gmail.com ,{x93ma ,wenhuchen}@uwaterloo.ca Project Website: https://tiger-ai-lab.github.io/"}, {"rank": 3, "score": 0.6999866, "source": "dense+rrf", "rrf_score": 0.029709507042253523, "rerank_score": 0.23258640579302492, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 21, "text_snippet": "operates on long retrieval units, with only a few (typically fewer than 10) being fed into the reader. An illustrative example is shown in Figure 2. 2.1 Long Retriever The traditional RAG framework employs smaller retrieval units and priori"}, {"rank": 4, "score": 35.531216427942766, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.22757802680061795, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 19, "text_snippet": "e end performance dips significantly due to the huge amount of ‚Äúhard negatives‚Äù, which confuses the reader. With ‚Äúlong retriever units‚Äù, we observe an entirely different trend. As we recall more units (from 1 to 8 units), both the recall an"}, {"rank": 5, "score": 0.6857183, "source": "dense+rrf", "rrf_score": 0.030017921146953404, "rerank_score": 0.22279018527441535, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 7, "text_snippet": "assive corpus with up to tens of millions of information units). Subsequently, the retrieved units are passed to the reader to generate the final response. On the contrary, the reader only needs to extract answers from 1arXiv:2406.15319v3 ["}]}
{"case_index": 290, "query": "From 2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt, what exact term fills [BLANK] in: \"Raez, Erich Elsenzand Laurent Sifrey,z All authors from DeepMind,yEqual contributions,zEqual senior authorship We enhance [BLANK] language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens.\"?", "gold": "auto-regressive", "answer": "The exact term is **‚Äúauto-regressive‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'auto-regressive' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 106.719, "llm_ms": 1130.549, "top_contexts": [{"rank": 1, "score": 0.70045316, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.474135008686492, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 1, "text_snippet": ", Karen Simonyan, Jack W. Raez, Erich Elsenzand Laurent Sifrey,z All authors from DeepMind,yEqual contributions,zEqual senior authorship We enhance auto-regressive language models by conditioning on document chunks retrieved from a large co"}, {"rank": 2, "score": 35.85882612666035, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.22723818083394165, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 224, "text_snippet": "s obtained using the 7.5B R/e.sc/t.sc/r.sc/o.scmodel in Table 6, 7, 20 and 21. E.3. Neighbour quantiÔ¨Åcation To quantify a notion of distance between the source document and the retrieved chunks, we can ask the distance between source articl"}, {"rank": 3, "score": 0.6241752, "source": "dense+rrf", "rrf_score": 0.027783137179239824, "rerank_score": 0.22194387655806008, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 22, "text_snippet": " ùïç=¬ª1¬îùë£¬º, obtained using a text tokenizer1. Wespliteach ùëõ-token-longexample ùëã=¬πùë•1¬î¬ì¬ì¬ì¬îùë•ùëõ¬∫intoasequenceof ùëôchunks¬πùê∂1¬î¬ì¬ì¬ì¬îùê∂ùëô¬∫ of sizeùëö=ùëõ ùëô, i.e.ùê∂1,¬πùë•1¬î¬ì¬ì¬ì¬îùë•ùëö¬∫¬î ¬ì¬ì¬ì¬î ùê∂ùëô,¬πùë•ùëõ\u0000ùëö¬∏1¬î¬ì¬ì¬ì¬îùë•ùëõ¬∫2ùïçùëö. We useùëõ=2048andùëö=64. We augment each chunk ùê∂ùë¢with a se"}, {"rank": 4, "score": 0.6833216, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.20856807395918692, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 0, "text_snippet": "Improving language models by retrieving from trillions of tokens Sebastian Borgeaudy, Arthur Menschy, Jordan HoÔ¨Ämanny, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark,"}, {"rank": 5, "score": 32.73049542331464, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.2074928634592358, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 284, "text_snippet": "d sailing .China ootball 7-a-side andsailing.China toppedthe medal table fortheÔ¨Åfth toppedthemedal table fortheÔ¨Åfth consecutive Paralympics,with96go consecutive Paralympics,with 96 go ldsand207totalmedals.GreatBrit ldsand207 total medals . "}]}
{"case_index": 291, "query": "From 2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"For instance, BERTScore (Zhang et al., 2020) and MoverScore (Zhao et al., 2019) use contex- tualised embeddings, produced by a [BLANK] BERT model, to compare the similarity between the generated answer and the reference answers.\"?", "gold": "pre-trained", "answer": "The exact term is **‚Äúpre-trained‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'pre-trained' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 89.085, "llm_ms": 1041.134, "top_contexts": [{"rank": 1, "score": 0.72893655, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.4747319763736132, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 18, "text_snippet": "2023b). In terms of how ground truth answers or, more generally, generations, have been typically used in the literature, most approaches have relied on the availability of one or more reference answers. For instance, BERTScore (Zhang et al"}, {"rank": 2, "score": 28.269552344625584, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.21975354134399092, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 127, "text_snippet": " on the content‚Äôs objectives: unlabeled and labeled content. For unlabeled content, the evaluation encompasses the faithfulness, relevance, and non-harmfulness of the generated answers. In contrast, for labeled content, the focus is on the "}, {"rank": 3, "score": 40.06595954910462, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.2177341651302448, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 9, "text_snippet": "we present Ragas1, a framework for the automated assessment 1Ragas is available at https://github.com/ explodinggradients/ragas .arXiv:2309.15217v2 [cs.CL] 28 Apr 2025  of retrieval augmented generation systems. We fo- cus on settings where"}, {"rank": 4, "score": 0.5476645, "source": "dense+rrf", "rrf_score": 0.026709401709401708, "rerank_score": 0.21025058596404247, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 5, "text_snippet": " training corpus (Kandpal et al., 2022; Mallen et al., 2023). The standard solution to these issues is to rely on Retrieval Augmented Generation (RAG) (Lee et al., 2019; Lewis et al., 2020; Guu et al., 2020). Answering a question then essen"}, {"rank": 5, "score": 29.621545006904793, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.2078550506989467, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 7, "text_snippet": "lementation requires a significant amount of tuning, as the overall per- formance will be affected by the retrieval model, the considered corpus, the LM, or the prompt for- mulation, among others. Automated evaluation of retrieval-augmented"}]}
{"case_index": 292, "query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \"The [BLANK] example in Figure 1 will serve as a running example for this section.\"?", "gold": "question-answering", "answer": "The exact term is **‚Äúquestion-answering‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Model answer matches expected term and is directly supported by Context 1 sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 84.536, "llm_ms": 982.685, "top_contexts": [{"rank": 1, "score": 38.555751748901955, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.2993376524709354, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 23, "text_snippet": "rs, and all of the param- eters are Ô¨Åne-tuned using labeled data from the downstream tasks. Each downstream task has sep- arate Ô¨Åne-tuned models, even though they are ini- tialized with the same pre-trained parameters. The question-answerin"}, {"rank": 2, "score": 0.50176513, "source": "dense+rrf", "rrf_score": 0.02813852813852814, "rerank_score": 0.2810338191205339, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 109, "text_snippet": "reading books. In Proceedings of the IEEE international conference on computer vision , pages 19‚Äì27. Appendix for ‚ÄúBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding‚Äù We organize the appendix into three section"}, {"rank": 3, "score": 44.8617148264114, "source": "dense+rrf", "rrf_score": 0.01639344262295082, "rerank_score": 0.2567138689573799, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 476, "text_snippet": "heng, and Jingjing Liu. Uniter: Learning universal image-text representations. arXiv preprint arXiv:1909.11740 , 2019. [Cra17] Kate Crawford. The trouble with bias. NIPS 2017 Keynote , 2017. [DCLT18] Jacob Devlin, Ming-Wei Chang, Kenton Lee"}, {"rank": 4, "score": 0.5355806, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.23680778278012385, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 0, "text_snippet": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova Google AI Language {jacobdevlin,mingweichang,kentonl,kristout }@google.com Abstract We introduce a ne"}, {"rank": 5, "score": 26.02061793272917, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.22929853343018988, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 10, "text_snippet": "ford et al. (2018), which uses unidirec- tional language models for pre-training, BERT uses masked language models to enable pre- trained deep bidirectional representations. This is also in contrast to Peters et al. (2018a), which uses a sh"}]}
{"case_index": 293, "query": "From 2302.13971__llama_open_and_efficient_foundation_language_models.txt, what exact term fills [BLANK] in: \"and [BLANK] deÔ¨Ånitions and macros written by users to increase consistency across papers.\"?", "gold": "inline-expanded", "answer": "The exact term is **‚Äúinline-expanded‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'inline-expanded' and is directly supported by the provided context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 57.235, "llm_ms": 1083.808, "top_contexts": [{"rank": 1, "score": 0.53079003, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.254176424668222, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 17, "text_snippet": " and inline-expanded deÔ¨Ånitions and macros written by users to increase consistency across papers. Stack Exchange [2%]. We include a dump of Stack Exchange, a website of high quality ques- tions and answers that covers a diverse set of do- "}, {"rank": 2, "score": 33.25089982376228, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.20328385536196614, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 16, "text_snippet": " Books3 section of TheP- ile (Gao et al., 2020), a publicly available dataset for training large language models. We perform deduplication at the book level, removing books with more than 90% content overlap. ArXiv [2.5%]. We process arXiv "}, {"rank": 3, "score": 0.45822954, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.17263296438047188, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 116, "text_snippet": "Marie-Anne Lachaux, Timoth¬¥ ee Lacroix, Baptiste Rozi` ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. arXiv preprint"}, {"rank": 4, "score": 0.43257868, "source": "dense+rrf", "rrf_score": 0.02886002886002886, "rerank_score": 0.17200456052626156, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 118, "text_snippet": "nal Linguistics. Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and Denny Zhou. 2022. Recitation-augmented language models. CoRR , abs/2210.01296. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth√©e "}, {"rank": 5, "score": 0.43724364, "source": "dense+rrf", "rrf_score": 0.029437229437229435, "rerank_score": 0.17089317535252013, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 190, "text_snippet": "2022. [57]Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo- th√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lamp"}]}
{"case_index": 294, "query": "From 2005.14165__language_models_are_few_shot_learners.txt, what exact term fills [BLANK] in: \"Brown‚àóBenjamin Mann‚àóNick Ryder‚àóMelanie Subbiah‚àó Jared Kaplan‚Ä†Prafulla Dhariwal Arvind Neelakantan Pranav Shyam Girish Sastry Amanda Askell Sandhini Agarwal Ariel [BLANK] Gretchen Krueger Tom Henighan Rewon Child Aditya Ramesh Daniel M.\"?", "gold": "herbert-voss", "answer": "The exact term is **‚ÄúHerbert-Voss‚Äù**.", "answer_score": 0.3, "answer_correctness": 0.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context shows the name as 'Ariel Herbert-Voss', but expected answer requires exact term 'herbert-voss' (lowercase). Model used different casing, so not an exact match.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 43.76, "llm_ms": 837.372, "top_contexts": [{"rank": 1, "score": 0.62599015, "source": "dense+rrf", "rrf_score": 0.02921395544346364, "rerank_score": 0.5229572923429104, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 0, "text_snippet": "Language Models are Few-Shot Learners Tom B. Brown‚àóBenjamin Mann‚àóNick Ryder‚àóMelanie Subbiah‚àó Jared Kaplan‚Ä†Prafulla Dhariwal Arvind Neelakantan Pranav Shyam Girish Sastry Amanda Askell Sandhini Agarwal Ariel Herbert-Voss Gretchen Krueger Tom"}, {"rank": 2, "score": 0.7130691, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.461947764142149, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 84, "text_snippet": "tions. EMNLP . Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, "}, {"rank": 3, "score": 0.68859845, "source": "dense+rrf", "rrf_score": 0.03149801587301587, "rerank_score": 0.4372293922258836, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 99, "text_snippet": " Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris"}, {"rank": 4, "score": 133.12771008105508, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.38120062268755606, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 83, "text_snippet": "on Osindero, Karen Si- monyan, Jack W. Rae, Erich Elsen, and Laurent Sifre. 2022. Improving language models by retrieving from trillions of tokens. In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Bal- timore, Ma"}, {"rank": 5, "score": 128.0465156894382, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.37541003753080154, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 101, "text_snippet": " Vinyals, Simon Osindero, Karen Simonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre. 2021. Improving language models by retriev- ing from trillions of tokens. Sergey Brin. 1999. Extracting patterns and relations from the world wide web. I"}]}
{"case_index": 295, "query": "From 2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt, what exact term fills [BLANK] in: \"Pre-training is performed in two disjoint and sequential phases; phase-1 comprises mostly of web sources aimed at teaching the model general knowledge and language [BLANK].\"?", "gold": "understanding", "answer": "The exact term is **‚Äúunderstanding‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'understanding' and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 85.771, "llm_ms": 1118.763, "top_contexts": [{"rank": 1, "score": 0.73922014, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.39720645479035394, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 19, "text_snippet": "n internet sources, as well as synthetic LLM-generated data. Pre-training is performed in two disjoint and sequential phases; phase-1 comprises mostly of web sources aimed at teaching the model general knowledge and language understanding. "}, {"rank": 2, "score": 0.526307, "source": "dense+rrf", "rrf_score": 0.029827662395050816, "rerank_score": 0.262536324714933, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 0, "text_snippet": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone Microsoft Abstract We introduce phi-3-mini , a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both"}, {"rank": 3, "score": 39.84420139585986, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.20714839266564322, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 16, "text_snippet": "3. The table shows the Keys/values a query token in block 8 attended to. Blue=local blocks, orange=remote/vertical blocks, gray=blocks skipped. total parameters. Additionally, we utilize the SparseMixer approach [LGC23, LDL+23] for training"}, {"rank": 4, "score": 0.54108816, "source": "dense+rrf", "rrf_score": 0.029631255487269532, "rerank_score": 0.20442700699292443, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 8, "text_snippet": "ved solely by changing the training data. phi-3-mini: The development of a compact language model that rivals the capabilities of ChatGPT, while fitting on a phone, is a testament to the power of data-driven machine learning. By meticulousl"}, {"rank": 5, "score": 0.5327503, "source": "dense+rrf", "rrf_score": 0.029877369007803793, "rerank_score": 0.1997680318515389, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 18, "text_snippet": "y training data to improve the performance of small language models and deviate from the standard scaling-laws . In this work we show that such method allows to reach the level of highly capable models such as GPT-3.5 or Mixtral with only 3"}]}
{"case_index": 296, "query": "From 2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt, what exact term fills [BLANK] in: \"Other efficient [BLANK], e.g., dilated or sparse attention, have a large gap to the standard style and do not work well like ours, as in Table 6.\"?", "gold": "attentions", "answer": "The exact term is **‚Äúattentions‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches the quoted context exactly: 'Other efficient attentions...'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 61.275, "llm_ms": 931.89, "top_contexts": [{"rank": 1, "score": 0.6990594, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.40732488654930055, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 15, "text_snippet": "s most existing optimization and infrastructure. Techniques for common LLMs can also be applied to ours. For example, Flash-Attention2 (Dao et al., 2022; Dao, 2023) is compatible with our method in both training and inference time. The reas"}, {"rank": 2, "score": 0.64030266, "source": "dense+rrf", "rrf_score": 0.03021353930031804, "rerank_score": 0.3123785402870512, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 75, "text_snippet": "ed in training-from- scratch transformers. This experiment is to examine their capability of fine-tuning on pre-trained LLMs (Touvron et al., 2023b), toward long context adaptation. Dilated attention performs well in full fine-tuning but is"}, {"rank": 3, "score": 0.644751, "source": "dense+rrf", "rrf_score": 0.02967032967032967, "rerank_score": 0.27729427605404805, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 16, "text_snippet": "nd do not work well like ours, as in Table 6. We empirically show that learnable embedding and normalization layers are the key to unlocking long context LoRA fine-tuning, in Table 2. Embedding and normalization layers take up a small 2  Pu"}, {"rank": 4, "score": 0.659438, "source": "dense+rrf", "rrf_score": 0.03125763125763126, "rerank_score": 0.2687626146554634, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 42, "text_snippet": " Child et al., 2019), designed for training from scratch, have gaps to the standard full attention, which is used in pre-training. In Table 6, we show that S2-Attn not only enables efficient fine-tuning but also supports fullattention testi"}, {"rank": 5, "score": 41.60604197183782, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.25940964289109425, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 12, "text_snippet": "e. In terms of efficiency , regardless of whether LoRA is employed or not, computational cost increases dramatically as the context size expands, primarily due to the standard self-attention mechanism (Vaswani et al., 2017). As shown in Fig"}]}
{"case_index": 297, "query": "From 2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt, what exact term fills [BLANK] in: \"Toourknowledge,ourworkistheÔ¨Å[BLANK]Ô¨Åts of scaling the retrieval database to trillions of tokens for large parametric language models.\"?", "gold": "rsttoshowthebene", "answer": "The exact term is **‚Äúrst to show the bene‚Äù** (i.e., the sentence reads: ‚ÄúTo our knowledge, our work is the **first to show the benefits** of scaling the retrieval database to trillions of tokens for large parametric language models.‚Äù).", "answer_score": 0.9299999999999999, "answer_correctness": 0.9, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.9, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context shows 'firsttoshowthebenefits'. Expected blank is 'rsttoshowthebene' (substring). Model answer includes that substring but adds spaces/quotes and extra explanation.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 97.342, "llm_ms": 1644.226, "top_contexts": [{"rank": 1, "score": 0.72825396, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.38487435030244965, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 8, "text_snippet": "ens) (Guu et al., 2020; Khandelwal et al., 2020; Lewisetal.,2020;Yogatamaetal.,2021). Toourknowledge,ourworkistheÔ¨ÅrsttoshowthebeneÔ¨Åts of scaling the retrieval database to trillions of tokens for large parametric language models. Our main Co"}, {"rank": 2, "score": 29.03307647810714, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.2811491399634665, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 69, "text_snippet": "e stem from their ability to memorise training data, their high training cost, the static nature of their training data (Lazaridou et al., 2021), their tendency of amplifying inherent biases in the training data, and their ability to genera"}, {"rank": 3, "score": 0.6917149, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.2578253376816664, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 113, "text_snippet": "formers ( R/e.sc/t.sc/r.sc/o.sc), a method for modelling arbitrary text se- quences whilstretrievingfromdatabaseswithtrillions oftokens‚Äîscalingthedataavailable to models by an order of magnitude compared to what is typically consumed during"}, {"rank": 4, "score": 24.85501656692208, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.2443024903547995, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 199, "text_snippet": " but only trains the new retrieval weights, as explained in ¬ß4.2. Our best R/e.sc/t.sc/r.sc/o.scÔ¨Åt checkpoint has a Wikitext103 valid perplexity 18¬ì46, when retrieving from Wikipedia. We use this R/e.sc/t.sc/r.sc/o.sccheckpoint in Table 4 f"}, {"rank": 5, "score": 26.071952464575, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.24345280427826158, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 101, "text_snippet": "he chunked cross-attention mechanism. 4https://github.com/facebookresearch/FiD 13  Improving language models by retrieving from trillions of tokens Figure5jR/e.sc/t.sc/r.sc/o.sc-Ô¨Åttingabaselinetransformer. AnytransformercanbeÔ¨Åne-tunedintoar"}]}
{"case_index": 298, "query": "From 2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt, what exact term fills [BLANK] in: \"1arXiv:2309.12307v3 [cs.CL] 8 Mar 2024 Published as a conference paper at ICLR 2024 !Trainable ‚ùÑ[BLANK](-headSelf-A1en(onFeed ForwardNorminput++Lora !\"?", "gold": "frozennormpostmul", "answer": "The exact term that fills **[BLANK]** is:\n\n**FrozenNormpostMul**", "answer_score": 0.3, "answer_correctness": 0.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Expected exact term is 'frozennormpostmul' (lowercase). Model answered 'FrozenNormpostMul' with different casing, so not exact. Context supports the term as shown.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 77.445, "llm_ms": 1285.949, "top_contexts": [{"rank": 1, "score": 0.7000589, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3914786463519279, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 6, "text_snippet": "luated on the proof-pile (Azerbayev et al., 2022) test set in perplexity. 1 I NTRODUCTION Large language models (LLMs) are typically trained with a pre-defined context size, such as 2048 tokens for LLaMA (Touvron et al., 2023a) and 4096 tok"}, {"rank": 2, "score": 0.64507353, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.3193386613484143, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 0, "text_snippet": "Published as a conference paper at ICLR 2024 LONG LORA: E FFICIENT FINE-TUNING OF LONG - CONTEXT LARGE LANGUAGE MODELS Yukang Chen1Shengju Qian1Haotian Tang2Xin Lai1 Zhijian Liu2Song Han2,3Jiaya Jia1 1CUHK2MIT3NVIDIA ABSTRACT We present Lon"}, {"rank": 3, "score": 0.6038227, "source": "dense+rrf", "rrf_score": 0.0293236301369863, "rerank_score": 0.25288081690517766, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 44, "text_snippet": "ters and memory cost, compared to full fine-tuning. However, adapting LLMs from short context length to long is not easy. We empirically observe an obvious gap between LoRA and full fine-tuning. As shown in Table 2, the gap between LoRA and"}, {"rank": 4, "score": 47.26905316694357, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.24860339960575698, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 7, "text_snippet": "circumstances or facing information uncertainties. Recent results have hinted at the possibility of combining verbal reasoning with interactive decision making in autonomous systems. On one hand, properly prompted large language models (LLM"}, {"rank": 5, "score": 0.6151977, "source": "dense+rrf", "rrf_score": 0.030090497737556562, "rerank_score": 0.23956778279596364, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 68, "text_snippet": "perplexity and fine-tuning steps for a Llama2 7B model extending to the 8192 context length on the PG19 validation set, in 8  Published as a conference paper at ICLR 2024 Table 6: Comparisons among S2-Attn and alternative attention patterns"}]}
{"case_index": 299, "query": "From 2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt, what exact term fills [BLANK] in: \"Sec- tion VII mainly discusses the challenges that RAG [BLANK] and its future development directions.\"?", "gold": "currentlyfaces", "answer": "The exact term is **‚Äúcurrently faces‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context 1 explicitly contains the phrase 'RAG currentlyfaces', matching the expected blank (no space). Model answered 'currently faces' with a space; term is correct but formatting differs slightly.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 73.788, "llm_ms": 1271.742, "top_contexts": [{"rank": 1, "score": 60.00411366888134, "source": "dense+rrf", "rrf_score": 0.01639344262295082, "rerank_score": 0.27549591096241693, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 16, "text_snippet": "three augmentation processes. Section VI focuses on RAG‚Äôs downstream tasks and evaluation system. Sec- tion VII mainly discusses the challenges that RAG currentlyfaces and its future development directions. At last, the paper concludes in S"}, {"rank": 2, "score": 0.6653324, "source": "dense+rrf", "rrf_score": 0.02964254577157803, "rerank_score": 0.24364667601320728, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 141, "text_snippet": " benchmarks collectively form a robust framework for the systematic evaluation of RAG models, as summarized in Table IV. VII. D ISCUSSION AND FUTURE PROSPECTS Despite the considerable progress in RAG technology, sev- eral challenges persist"}, {"rank": 3, "score": 0.76603615, "source": "dense+rrf", "rrf_score": 0.03177805800756621, "rerank_score": 0.22578776562510555, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 3, "text_snippet": "augmentation techniques. The paper highlights the state-of-the- art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-"}, {"rank": 4, "score": 24.135068156317274, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.20868886551749327, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 293, "text_snippet": "ant not just to characterize biases in language systems but to intervene. The literature on this is also extensive [ QMZH19 ,HZJ+19], so we offer only a few brief comments on future directions speciÔ¨Åc to large language models. In order to p"}, {"rank": 5, "score": 0.6966985, "source": "dense+rrf", "rrf_score": 0.03125763125763126, "rerank_score": 0.20080090909708997, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 11, "text_snippet": "h a detailed and structured understanding of both large models and RAG. It aims to illuminate the evolution of retrieval augmentation techniques, assess the strengths and weaknesses of various approaches in their respective contexts, and sp"}]}
{"case_index": 300, "query": "From 2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt, what exact term fills [BLANK] in: \"[BLANK] is far simpler than prior approaches that either train an additional veriÔ¨Åer (Cobbe et al., 2021) or train a re-ranker given additional human annotations to improve generation quality (Thoppilan et al., 2022).\"?", "gold": "self-consistency", "answer": "**Self-consistency** is the exact term that fills **[BLANK]**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'self-consistency' and is directly supported by the provided context sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 75.325, "llm_ms": 1036.617, "top_contexts": [{"rank": 1, "score": 0.741826, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.5145578850499332, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 12, "text_snippet": " by choosing the most consistent answer in the Ô¨Ånal answer set. Self-consistency is far simpler than prior approaches that either train an additional veriÔ¨Åer (Cobbe et al., 2021) or train a re-ranker given additional human annotations to im"}, {"rank": 2, "score": 0.68334246, "source": "dense+rrf", "rrf_score": 0.03128054740957967, "rerank_score": 0.3574429801549956, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 81, "text_snippet": " decoding process (Batra et al., 2012; Li et al., 2016; Vijayakumar et al., 2018). Re-ranking is another common approach to improve generation quality in language models (Adiwar- dana et al., 2020; Shen et al., 2021). Thoppilan et al. (2022"}, {"rank": 3, "score": 0.7109804, "source": "dense+rrf", "rrf_score": 0.031054405392392875, "rerank_score": 0.31866426743245, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 79, "text_snippet": "pecialized approaches for improving reasoning (Andor et al., 2019; Ran et al., 2019; Geva et al., 2020; PiÀõ ekos et al., 2021). Compared to prior work, self-consistency is applicable to a wide range of reasoning tasks without any additional"}, {"rank": 4, "score": 0.6532508, "source": "dense+rrf", "rrf_score": 0.029877369007803793, "rerank_score": 0.2831590452716508, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 84, "text_snippet": "ng processes has been noticed, but only leveraged via task-speciÔ¨Åc training, either through an additional QA model over extracted reasoning paths (Chen et al., 2019), or by the introduction of latent variables in a commonsense knowledge gra"}, {"rank": 5, "score": 53.492464567615514, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.2763201107049996, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 45, "text_snippet": "tic reasoning accuracy by self-consistency compared to chain-of-thought prompting (Wei et al., 2022). The previous SoTA baselines are obtained from: a: Relevance and LCA operation classiÔ¨Åer (Roy & Roth, 2015), b: Lan et al. (2021), c: Amini"}]}
{"case_index": 301, "query": "From 2305.06983__active_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"2.3 Active Retrieval Augmented Generation To aid [BLANK] generation with retrieval, we pro- pose active retrieval augmented generation.\"?", "gold": "long-form", "answer": "The exact term is **‚Äúlong-form‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'long-form' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 60.518, "llm_ms": 1056.141, "top_contexts": [{"rank": 1, "score": 0.64371717, "source": "dense+rrf", "rrf_score": 0.032266458495966696, "rerank_score": 0.30493429459703997, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 22, "text_snippet": " generate the complete answer at once y=LM([Dx,x]). 2.3 Active Retrieval Augmented Generation To aid long-form generation with retrieval, we pro- pose active retrieval augmented generation. It is a generic framework that actively decides wh"}, {"rank": 2, "score": 0.6332425, "source": "dense+rrf", "rrf_score": 0.03128054740957967, "rerank_score": 0.21781046362108838, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 21, "text_snippet": "ing when and what to retrieve, we follow exist- ing methods (Ram et al., 2023; Trivedi et al., 2022) to prepend the retrieved documents before the user input to aid future generation for both baselines and our method for fair comparisons: y"}, {"rank": 3, "score": 25.01895353771121, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.2162776394325278, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 28, "text_snippet": "University)][Search(Joe Biden degree)]the University of Pennsylvania, where he earneda law degree.Generate a summary about Joe Biden.Input$&$&#%$&%%%GenerationRetriever$%$%%Figure 2: An illustration of forward-looking active re- trieval aug"}, {"rank": 4, "score": 0.5901339, "source": "dense+rrf", "rrf_score": 0.027417840375586856, "rerank_score": 0.19443456526444444, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 11, "text_snippet": "eddocumentsLMGeneration$%$%% Figure 1: An illustration of forward-looking active retrieval augmented generation (FLARE). Starting with the user input xand initial retrieval results Dx, FLARE iteratively generates a temporary next sentence ("}, {"rank": 5, "score": 0.62266266, "source": "dense+rrf", "rrf_score": 0.029857397504456328, "rerank_score": 0.19339785155126105, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 16, "text_snippet": ". When deciding what to retrieve , it is important to consider what LMs intend to gen- erate in the future, as the goal of active retrieval is to benefit future generations. Therefore, we propose anticipating the future by generating a temp"}]}
{"case_index": 302, "query": "From 2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt, what exact term fills [BLANK] in: \"Training language models to follow [BLANK] with human feedback Long Ouyang‚àóJeff Wu‚àóXu Jiang‚àóDiogo Almeida‚àóCarroll L.\"?", "gold": "instructions", "answer": "The exact term is **‚Äúinstructions‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context explicitly contains the title 'Training language models to follow instructions with human feedback', so 'instructions' correctly fills the blank.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.25, "hit_rank": 4, "matched_by": "doc_id+chunk_id", "retrieval_ms": 44.79, "llm_ms": 1206.911, "top_contexts": [{"rank": 1, "score": 0.63118005, "source": "dense+rrf", "rrf_score": 0.031054405392392875, "rerank_score": 0.3992538286308834, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 99, "text_snippet": "ustin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. 2021. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114 . Long Ouyang, Jeff Wu, Xu Jiang, Diog"}, {"rank": 2, "score": 0.75432456, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.37937777473571405, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 180, "text_snippet": "low instructions with human feedback, 2022. [47]Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke "}, {"rank": 3, "score": 0.6703131, "source": "dense+rrf", "rrf_score": 0.03149801587301587, "rerank_score": 0.34609699702211644, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 179, "text_snippet": " technical report, 2023. [45]OpenAI. Chatgpt (sep 25 version). https://chat.openai.com/chat , 2023. [Large language model]. [46]Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agar"}, {"rank": 4, "score": 0.76762986, "source": "dense+rrf", "rrf_score": 0.032018442622950824, "rerank_score": 0.33676041874465973, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 0, "text_snippet": "Training language models to follow instructions with human feedback Long Ouyang‚àóJeff Wu‚àóXu Jiang‚àóDiogo Almeida‚àóCarroll L. Wainwright‚àó Pamela Mishkin‚àóChong Zhang Sandhini Agarwal Katarina Slama Alex Ray John Schulman Jacob Hilton Fraser Kelt"}, {"rank": 5, "score": 63.28866491216394, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.3007048375589544, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 122, "text_snippet": "avarese, and Caiming Xiong. 2022. Codegen: An open large lan- guage model for code with multi-turn program syn- thesis. arXiv preprint arXiv:2203.13474 . Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, "}]}
{"case_index": 303, "query": "From 2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt, what exact term fills [BLANK] in: \"SpeciÔ¨Åcally, we suggest retrieval from a large text database as a [BLANK] path to scaling language models.\"?", "gold": "complementary", "answer": "The exact term that fills **[BLANK]** is **‚Äúcomplementary‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 85.346, "llm_ms": 955.095, "top_contexts": [{"rank": 1, "score": 0.715937, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.34528950439214007, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 6, "text_snippet": "rence time, and increased memorization of the training data. Inthiswork,weendeavortodecouplethese,byexploringeÔ¨Écientmeansofaugmentinglanguage models with a massive-scale memory without signiÔ¨Åcantly increasing computations. SpeciÔ¨Åcally, we s"}, {"rank": 2, "score": 34.65305705063043, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.28864624401648026, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 22, "text_snippet": " ùïç=¬ª1¬îùë£¬º, obtained using a text tokenizer1. Wespliteach ùëõ-token-longexample ùëã=¬πùë•1¬î¬ì¬ì¬ì¬îùë•ùëõ¬∫intoasequenceof ùëôchunks¬πùê∂1¬î¬ì¬ì¬ì¬îùê∂ùëô¬∫ of sizeùëö=ùëõ ùëô, i.e.ùê∂1,¬πùë•1¬î¬ì¬ì¬ì¬îùë•ùëö¬∫¬î ¬ì¬ì¬ì¬î ùê∂ùëô,¬πùë•ùëõ\u0000ùëö¬∏1¬î¬ì¬ì¬ì¬îùë•ùëõ¬∫2ùïçùëö. We useùëõ=2048andùëö=64. We augment each chunk ùê∂ùë¢with a se"}, {"rank": 3, "score": 29.695250236337767, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.27183890504319547, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 69, "text_snippet": "e stem from their ability to memorise training data, their high training cost, the static nature of their training data (Lazaridou et al., 2021), their tendency of amplifying inherent biases in the training data, and their ability to genera"}, {"rank": 4, "score": 0.6949141, "source": "dense+rrf", "rrf_score": 0.031754032258064516, "rerank_score": 0.26823613563678195, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 113, "text_snippet": "formers ( R/e.sc/t.sc/r.sc/o.sc), a method for modelling arbitrary text se- quences whilstretrievingfromdatabaseswithtrillions oftokens‚Äîscalingthedataavailable to models by an order of magnitude compared to what is typically consumed during"}, {"rank": 5, "score": 0.6919308, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.2677645147476297, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 8, "text_snippet": "ens) (Guu et al., 2020; Khandelwal et al., 2020; Lewisetal.,2020;Yogatamaetal.,2021). Toourknowledge,ourworkistheÔ¨ÅrsttoshowthebeneÔ¨Åts of scaling the retrieval database to trillions of tokens for large parametric language models. Our main Co"}]}
{"case_index": 304, "query": "From 2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt, what exact term fills [BLANK] in: \"Reasoning tasks typically have Ô¨Åxed answers, which is why [BLANK] have generally considered greedy decoding approaches (Radford et al., 2019; Wei et al., 2022; Chowdhery et al., 2022).\"?", "gold": "researchers", "answer": "The exact term that fills **[BLANK]** is **‚Äúresearchers‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'researchers' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 56.497, "llm_ms": 1385.311, "top_contexts": [{"rank": 1, "score": 0.71838015, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.40071275896891706, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 27, "text_snippet": " 1 we also report the results by taking a ‚Äúweighted average‚Äù, i.e., each agets a score of its weighted sum divided by‚àëm i=11(ai=a), which results in a much worse performance. Self-consistency explores an interesting space between open-ended"}, {"rank": 2, "score": 0.69909, "source": "dense+rrf", "rrf_score": 0.03125, "rerank_score": 0.32312303141105664, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 79, "text_snippet": "pecialized approaches for improving reasoning (Andor et al., 2019; Ran et al., 2019; Geva et al., 2020; PiÀõ ekos et al., 2021). Compared to prior work, self-consistency is applicable to a wide range of reasoning tasks without any additional"}, {"rank": 3, "score": 0.63550913, "source": "dense+rrf", "rrf_score": 0.029386529386529386, "rerank_score": 0.3188815870959954, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 40, "text_snippet": " 4  Published as a conference paper at ICLR 2023 3.2 M AINRESULTS We report the results of self-consistency averaged over 10 runs, where we sampled 40 outputs independently from the decoder in each run. The baseline we compare to is chain-o"}, {"rank": 4, "score": 0.7569219, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.3146360495840538, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 5, "text_snippet": "odel performance across a variety of multi-step reasoning tasks (Wei et al., 2022). In this paper, we introduce a novel decoding strategy called self-consistency to replace the greedy decoding strategy used in chain-of-thought prompting (We"}, {"rank": 5, "score": 47.34251723455839, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.25343372800525465, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 109, "text_snippet": ", Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022. LaMDA: Language models for dialog applications. arXiv preprint arXiv:2201.08239 . Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and"}]}
{"case_index": 305, "query": "From 2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt, what exact term fills [BLANK] in: \"BERT (Devlin et al., 2019) networks (base, un- cased) and take the [BLANK] at the [CLS] token as the output, so d= 768 .\"?", "gold": "representation", "answer": "The exact term is **‚Äúrepresentation‚Äù** ‚Äî i.e., ‚Äútake the **representation** at the [CLS] token as the output, so d = 768.‚Äù", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'representation' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 62.081, "llm_ms": 1532.079, "top_contexts": [{"rank": 1, "score": 0.6593278, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3837145202515303, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 23, "text_snippet": "BERT (Devlin et al., 2019) networks (base, un- cased) and take the representation at the [CLS] token as the output, so d= 768 . Inference During inference time, we apply the passage encoder EPto all the passages and index them using FAISS ("}, {"rank": 2, "score": 0.5705664, "source": "dense+rrf", "rrf_score": 0.03021353930031804, "rerank_score": 0.2735851294840338, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 22, "text_snippet": "osine similarity and L2 distance (Mussmann and Ermon, 2016; Ram and Gray, 2012). As our ablation study Ô¨Ånds other similarity functions perform compara- bly (Section 5.2; Appendix B), we thus choose the simpler inner product function and imp"}, {"rank": 3, "score": 34.27002704211689, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.22180356179898075, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 81, "text_snippet": "the dual-encoder framework, they introduced a late-interaction operator on top of the BERT encoders. Dense retrieval for open-domain QA has been explored by Das et al. (2019), who propose to re- trieve relevant passages iteratively using re"}, {"rank": 4, "score": 30.050961171110977, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.20905358564583285, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 43, "text_snippet": "tion or sequence tagging. At the output, the token rep- resentations are fed into an output layer for token- level tasks, such as sequence tagging or question answering, and the [CLS] representation is fed into an output layer for classiÔ¨Åca"}, {"rank": 5, "score": 0.53344834, "source": "dense+rrf", "rrf_score": 0.026500526500526502, "rerank_score": 0.2014218197653273, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 64, "text_snippet": "on model serves as a reranker through cross- attention between the question and the passage. Al- though cross-attention is not feasible for retrieving relevant passages in a large corpus due to its non- decomposable nature, it has more capa"}]}
{"case_index": 306, "query": "From 2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt, what exact term fills [BLANK] in: \"GPUs, compute speed has out-paced memory speed [ 61,62,63], and most operations in [BLANK] are bottlenecked by memory accesses [ 43].\"?", "gold": "transformers", "answer": "The exact term is **‚ÄúTransformers‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context 1 explicitly contains the sentence with 'Transformers' filling the blank; matches expected answer (case-insensitive).", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 47.663, "llm_ms": 1061.848, "top_contexts": [{"rank": 1, "score": 0.6612748, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.36995162542453197, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 10, "text_snippet": "mplementation of attention on GPT-2. FlashAttention does not read and write the large ùëÅ\u0002ùëÅattention matrix to HBM, resulting in an 7.6 \u0002 speedup on the attention computation. GPUs, compute speed has out-paced memory speed [ 61,62,63], and mo"}, {"rank": 2, "score": 0.6574867, "source": "dense+rrf", "rrf_score": 0.031746031746031744, "rerank_score": 0.23937931235061563, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 23, "text_snippet": "peed [ 61,62,63], operations are increasingly bottlenecked by memory (HBM) accesses. Thus exploiting fast SRAM becomes more important. Execution Model. GPUs have a massive number of threads to execute an operation (called a kernel). Each ke"}, {"rank": 3, "score": 0.5939919, "source": "dense+rrf", "rrf_score": 0.029273504273504274, "rerank_score": 0.23888480685611013, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 22, "text_snippet": "forms of memory of diÔ¨Äerent sizes and speeds, with smaller memory being faster. As an example, the A100 GPU has 40-80GB of high bandwidth memory (HBM) with bandwidth 1.5-2.0TB/s and 192KB of on-chip SRAM per each of 108 streaming multiproce"}, {"rank": 4, "score": 54.282533381794494, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.22470301565158735, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 100, "text_snippet": "n, Atri Rudra, and Christopher R√©. 2022. FlashAttention: Fast and memory-efficient exact attention with IO- awareness. ArXiv:2205.14135. Hermann Ebbinghaus. 1913. Memory: A contribu- tion to experimental psychology. H. A. Ruger & C. E. Buss"}, {"rank": 5, "score": 39.50654310766438, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.18896423907057275, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 84, "text_snippet": "mputer/RedPajama-Data . Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. CoRR , abs/2307.08691, 2023. Tri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra, and Christopher R ¬¥e. Flashattention: Fast an"}]}
{"case_index": 307, "query": "From 2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt, what exact term fills [BLANK] in: \"As shown in Figure 2, we split context length into several groups and conduct attention in each group [BLANK].\"?", "gold": "individually", "answer": "The exact term is **‚Äúindividually‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'individually' and is explicitly supported by Context 1 sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 78.957, "llm_ms": 1145.529, "top_contexts": [{"rank": 1, "score": 0.6667758, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.3672693221201423, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 13, "text_snippet": "nds the context windows of pre-trained LLMs, e.g., Llama2 (Touvron et al., 2023b). LoRA (Hu et al., 2022) uses low-rank weight updates to approximate full fine-tuning. Similarly, we find that short attention is also able to approximate long"}, {"rank": 2, "score": 0.6622993, "source": "dense+rrf", "rrf_score": 0.031746031746031744, "rerank_score": 0.27893654074917484, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 38, "text_snippet": "g, which presents consistently good quality in various context lengths. The first trial is to train with short attention, only pattern 1 in Figure 2. As we know for a long context, the high cost mainly comes from self-attention modules. Thu"}, {"rank": 3, "score": 0.69119734, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.2529710499321681, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 39, "text_snippet": "ucted in each group with a 2048 size. The group number is 4, as ablated in Section B.2 in the appendix. This pattern is efficient but still does not work in a very long context, as shown in Table 1. The perplexity becomes larger as the cont"}, {"rank": 4, "score": 0.65694726, "source": "dense+rrf", "rrf_score": 0.03125, "rerank_score": 0.24864244699588556, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 107, "text_snippet": "P SIZES . In Table 7, we provide an ablation study on the group size of the S2-Attn. We experimented with fine-tuning Llama2 7B to 8192 and 16384 context lengths via LongLoRA. The group size varies from{1/2, 1/4, 1/6, 1/8 }of the target con"}, {"rank": 5, "score": 36.28596245380149, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.23193872410746205, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 0, "text_snippet": "Published as a conference paper at ICLR 2024 LONG LORA: E FFICIENT FINE-TUNING OF LONG - CONTEXT LARGE LANGUAGE MODELS Yukang Chen1Shengju Qian1Haotian Tang2Xin Lai1 Zhijian Liu2Song Han2,3Jiaya Jia1 1CUHK2MIT3NVIDIA ABSTRACT We present Lon"}]}
{"case_index": 308, "query": "From 2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt, what exact term fills [BLANK] in: \"Since decision making and reasoning [BLANK] are integrated into a large language model, ReAct enjoys several unique features: A) Intuitive and easy to design : Designing ReAct promp\"?", "gold": "capabilities", "answer": "The exact term is **‚Äúcapabilities‚Äù** (i.e., ‚ÄúSince decision making and reasoning **capabilities** are integrated into a large language model, ReAct enjoys several unique features: ‚Ä¶‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected blank term 'capabilities' and is directly supported by the provided context sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 73.04, "llm_ms": 1104.665, "top_contexts": [{"rank": 1, "score": 0.6836616, "source": "dense+rrf", "rrf_score": 0.032266458495966696, "rerank_score": 0.391688021573647, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 32, "text_snippet": "ughts only need to 1We show some GPT-3 (Brown et al., 2020) results in Appendix A.1, which outperforms PaLM-540B. 3  Published as a conference paper at ICLR 2023 appear sparsely in the most relevant positions of a trajectory, so we let the "}, {"rank": 2, "score": 0.65926135, "source": "dense+rrf", "rrf_score": 0.030798389007344232, "rerank_score": 0.2211532177560924, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 88, "text_snippet": " the development of versatile and generalist agents like Reed et al. (2022). 6 C ONCLUSION We have proposed ReAct ‚Äì a simple yet effective method for synergizing reasoning and acting in large language models. Through a diverse set of experi"}, {"rank": 3, "score": 0.67664754, "source": "dense+rrf", "rrf_score": 0.030330882352941176, "rerank_score": 0.20764722874217187, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 24, "text_snippet": "o understand the decision basis of model actions. To summarize, our key contributions are the following: (1) we introduce ReAct , a novel prompt- based paradigm to synergize reasoning and acting in language models for general task solving; "}, {"rank": 4, "score": 0.7231133, "source": "dense+rrf", "rrf_score": 0.031544957774465976, "rerank_score": 0.200573737257762, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 19, "text_snippet": "udies on how reasoning and acting can be combined in a synergistic manner for general task solving, and if such a combination can bring systematic beneÔ¨Åts compared to reasoning or acting alone. In this work, we present ReAct , a general par"}, {"rank": 5, "score": 0.67385423, "source": "dense+rrf", "rrf_score": 0.03076923076923077, "rerank_score": 0.20018234618290862, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 7, "text_snippet": "circumstances or facing information uncertainties. Recent results have hinted at the possibility of combining verbal reasoning with interactive decision making in autonomous systems. On one hand, properly prompted large language models (LLM"}]}
{"case_index": 309, "query": "From 2305.06983__active_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"n when they do not have the necessary knowledge to avoid unnecessary or [BLANK] retrieval, and (2) the retrieval queries should reflect the intents of future generations.\"?", "gold": "inappropriate", "answer": "The exact term that fills **[BLANK]** is **‚Äúinappropriate‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'inappropriate' and is directly supported by the provided context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 51.398, "llm_ms": 1426.823, "top_contexts": [{"rank": 1, "score": 0.63653994, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.47831095714074473, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 25, "text_snippet": "n when they do not have the necessary knowledge to avoid unnecessary or inappropriate retrieval, and (2) the retrieval queries should reflect the intents of future generations. We propose two forward-looking active retrieval augmented gener"}, {"rank": 2, "score": 0.49557418, "source": "dense+rrf", "rrf_score": 0.02928692699490662, "rerank_score": 0.32657087569388343, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 24, "text_snippet": "t, and the input to LMs is the concatena- tion of the retrieved documents Dqt, the user input x, and the previous generation y<t. We discard previously retrieved documents ‚à™t‚Ä≤<tDqt‚Ä≤and only use the retrieved documents from the current step "}, {"rank": 3, "score": 0.5611036, "source": "dense+rrf", "rrf_score": 0.03149801587301587, "rerank_score": 0.24931890367213583, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 14, "text_snippet": " al., 2022). We ask the following question: can we create a simple and generic retrieval augmented LM that ac- tively decides when and what to retrieve throughout the generation process, and are applicable to a va- riety of long-form genera"}, {"rank": 4, "score": 0.5365984, "source": "dense+rrf", "rrf_score": 0.030330882352941176, "rerank_score": 0.24415782064831046, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 16, "text_snippet": ". When deciding what to retrieve , it is important to consider what LMs intend to gen- erate in the future, as the goal of active retrieval is to benefit future generations. Therefore, we propose anticipating the future by generating a temp"}, {"rank": 5, "score": 0.5912771, "source": "dense+rrf", "rrf_score": 0.030834914611005692, "rerank_score": 0.22630053638919334, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 15, "text_snippet": "e to avoid unneces- sary or inappropriate retrieval that occurs in passive retrieval augmented LMs (Khandelwal et al., 2020; Borgeaud et al., 2022; Ram et al., 2023; Trivedi et al., 2022). Given the observation that large LMs tend to be wel"}]}
{"case_index": 310, "query": "From 2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt, what exact term fills [BLANK] in: \"Note that Mcan be very large (e.g., 21 million passages in our [BLANK], de- scribed in Section 4.1) and kis usually small, such as20‚Äì100.\"?", "gold": "experiments", "answer": "The exact term is **‚Äúexperiments‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term and is directly supported by Context 1 ('21 million passages in our experiments').", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 73.683, "llm_ms": 975.251, "top_contexts": [{"rank": 1, "score": 0.6609551, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.44299315752115714, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 18, "text_snippet": "evant to the input question for the reader at run-time. Note that Mcan be very large (e.g., 21 million passages in our experiments, de- scribed in Section 4.1) and kis usually small, such as20‚Äì100. 3.1 Overview Our dense passage retriever ("}, {"rank": 2, "score": 0.5981829, "source": "dense+rrf", "rrf_score": 0.03128054740957967, "rerank_score": 0.22061263014017862, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 17, "text_snippet": "rieval accuracy , which is the fraction of ques- tions for whichCFcontains a span that answers the question. 3 Dense Passage Retriever (DPR) We focus our research in this work on improv- ing the retrieval component in open-domain QA. Given "}, {"rank": 3, "score": 36.84989178551297, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.2184234041748519, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 1, "text_snippet": "where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented us- ingdense representations alone, where em- beddings are learned from a sma"}, {"rank": 4, "score": 0.6402737, "source": "dense+rrf", "rrf_score": 0.031054405392392875, "rerank_score": 0.20848110135109052, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 15, "text_snippet": "sic retrieval units3and getM total passages in our corpus C={p1,p2,...,p M}, where each passage pican be viewed as a sequence of tokensw(i) 1,w(i) 2,¬∑¬∑¬∑,w(i) |pi|. Given a question q, the task is to Ô¨Ånd a span w(i) s,w(i) s+1,¬∑¬∑¬∑,w(i) efrom"}, {"rank": 5, "score": 30.620263479847328, "source": "dense+rrf", "rrf_score": 0.014285714285714285, "rerank_score": 0.20190248346838677, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 152, "text_snippet": "1‚Äì1140, 2020. T. Hennigan, T. Cai, T. Norman, and I. Babuschkin. Haiku: Sonnet for JAX, 2020. URL http: //github.com/deepmind/dm-haiku . G. Izacard and E. Grave. Leveraging passage retrieval with generative models for open domain question a"}]}
{"case_index": 311, "query": "From 1706.03762__attention_is_all_you_need.txt, what exact term fills [BLANK] in: \"The output is computed as a weighted sum 3 Scaled [BLANK] Attention Multi-Head Attention Figure 2: (left) Scaled Dot-Pr\"?", "gold": "dot-product", "answer": "The exact term is **‚ÄúDot-Product‚Äù**, i.e., **‚ÄúScaled Dot-Product Attention.‚Äù**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Blank is 'dot-product' as in 'Scaled Dot-Product Attention'; directly supported by context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 53.324, "llm_ms": 1047.278, "top_contexts": [{"rank": 1, "score": 0.6768191, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.34511939368532146, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 19, "text_snippet": "sequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position ican depend only on the known outputs at positions less than i. 3.2 Attention An attention "}, {"rank": 2, "score": 0.71690214, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.2881819496797958, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 20, "text_snippet": "t Attention  Multi-Head Attention Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel. of the values, where the weight assigned to each value is computed by a "}, {"rank": 3, "score": 0.575639, "source": "dense+rrf", "rrf_score": 0.02854251012145749, "rerank_score": 0.22079093738893774, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 27, "text_snippet": "ead, the total computational cost is similar to that of single-head attention with full dimensionality. 3.2.3 Applications of Attention in our Model The Transformer uses multi-head attention in three different ways: ‚Ä¢In \"encoder-decoder att"}, {"rank": 4, "score": 0.65357876, "source": "dense+rrf", "rrf_score": 0.031746031746031744, "rerank_score": 0.21858489672618497, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 25, "text_snippet": "-dimensional 4To illustrate why the dot products get large, assume that the components of qandkare independent random variables with mean 0and variance 1. Then their dot product, q¬∑k=Pdk i=1qiki, has mean 0and variance dk. 4  output values."}, {"rank": 5, "score": 0.6077812, "source": "dense+rrf", "rrf_score": 0.03007688828584351, "rerank_score": 0.21722213638158414, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 12, "text_snippet": "e Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3."}]}
{"case_index": 312, "query": "From 2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt, what exact term fills [BLANK] in: \"The existing RAG framework tends to use short retrieval units, such as 100-word passages in popular open- domain [BLANK] tasks (Chen et al., 2017; Lewis et al., 2020; Karpukhin et al., 2020).\"?", "gold": "question-answering", "answer": "The exact term is **‚Äúquestion-answering‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'question-answering' and is explicitly stated in the provided context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 48.484, "llm_ms": 1149.262, "top_contexts": [{"rank": 1, "score": 0.74316466, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.4698122389384811, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 6, "text_snippet": "rely offloaded from the parametric knowledge of LLMs by leveraging a standalone retrieval component from an external corpus. The existing RAG framework tends to use short retrieval units, such as 100-word passages in popular open- domain qu"}, {"rank": 2, "score": 55.886312402689725, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.3080752868036613, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 79, "text_snippet": "on the long-context understanding ability of LLMs, and we still have a long way to go in harnessing open-source LLMs within our framework. 10  4 Related Work 4.1 Retrieval-Augmented Generation. Augmenting language models with information re"}, {"rank": 3, "score": 0.7282359, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.3011354003767233, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 0, "text_snippet": "LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs ‚ô†Ziyan Jiang,‚ô†Xueguang Ma,‚ô†Wenhu Chen ‚ô†University of Waterloo ziyanjiang528@gmail.com ,{x93ma ,wenhuchen}@uwaterloo.ca Project Website: https://tiger-ai-lab.github.io/"}, {"rank": 4, "score": 38.259536785588665, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.24687586492510577, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 6, "text_snippet": "7; Guu et al., 2020; Lewis et al., 2020; Izacard and Grave, 2021; Sachan et al., 2021; Lee et al., 2021; Jiang et al., 2022; Izacard et al., 2022; Nakano et al., 2021; Qian et al., 2023; Lazaridou et al., 2022; Shi et al., 2023). These sing"}, {"rank": 5, "score": 0.6798987, "source": "dense+rrf", "rrf_score": 0.028577260665441927, "rerank_score": 0.22742157522325482, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 11, "text_snippet": " even millions of tokens as input (Reid et al., 2024; Achiam et al., 2023). In this paper, we propose to revisit this design choice for open-domain question answering and propose the LongRAG framework as a solution to balance the workload b"}]}
{"case_index": 313, "query": "From 2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"Sec- ond,Answer [BLANK] refers to the idea that the generated answer should address the actual ques- tion that was provided.\"?", "gold": "relevance", "answer": "The exact term is **‚ÄúRelevance‚Äù** (i.e., **Answer Relevance**).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context 1 explicitly states 'Second, Answer Relevance refers...' so the blank is 'relevance'. Model answer matches.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 75.363, "llm_ms": 1210.649, "top_contexts": [{"rank": 1, "score": 0.59101284, "source": "dense+rrf", "rrf_score": 0.032266458495966696, "rerank_score": 0.3356015227345818, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 21, "text_snippet": "d to ensure that the retrieved context can act as a justification for the generated answer. Indeed, RAG systems are often used in applications where the factual con- sistency of the generated text w.r.t. the grounded sources is highly impor"}, {"rank": 2, "score": 0.5543384, "source": "dense+rrf", "rrf_score": 0.029437229437229435, "rerank_score": 0.21478063303427877, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 22, "text_snippet": " generate the complete answer at once y=LM([Dx,x]). 2.3 Active Retrieval Augmented Generation To aid long-form generation with retrieval, we pro- pose active retrieval augmented generation. It is a generic framework that actively decides wh"}, {"rank": 3, "score": 28.58935284185014, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.21349728701479465, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 45, "text_snippet": "ially for longer contexts. 6 Conclusions We have highlighted the need for automated reference-free evaluation of RAG systems. In par- ticular, we have argued the need for an evaluation framework that can assess faithfulness (i.e. is the ans"}, {"rank": 4, "score": 26.65792186318019, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.19907874389506086, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 10, "text_snippet": "n in the input context. We first experiment with multi-document ques- tion answering, which requires models to reason over provided documents to find relevant informa- tion and use it to answer a given question; this task mimics the retriev"}, {"rank": 5, "score": 23.627988570359534, "source": "dense+rrf", "rrf_score": 0.014084507042253521, "rerank_score": 0.17689455736414475, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 281, "text_snippet": " you always used to look like this , did n‚Äôt you , _‚Äù What is the word in the blank space (_)? The answer is Orca-v2-13B-Cautious The answer is izzy. The word in the blank space is the name of the girl who is crying. Figure 24: Demonstrativ"}]}
{"case_index": 314, "query": "From 2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt, what exact term fills [BLANK] in: \"The posed query and selected documents are [BLANK] into a coherent prompt to which a large language model is tasked with formulating a response.\"?", "gold": "synthesized", "answer": "The exact term is **‚Äúsynthesized‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term; directly supported by Context 1 which contains the sentence with 'synthesized'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 71.318, "llm_ms": 1114.055, "top_contexts": [{"rank": 1, "score": 0.68795645, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.40375163748340964, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 22, "text_snippet": "cores between the query vector and the vector of chunks within the indexed corpus. The system prioritizes and retrieves the top K chunks that demonstrate the greatest similarity to the query. These chunks are subsequently used as the expand"}, {"rank": 2, "score": 0.5820825, "source": "dense+rrf", "rrf_score": 0.02564102564102564, "rerank_score": 0.25569313240810193, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 84, "text_snippet": "HyDE [11] construct hypothet- ical documents (assumed answers to the original query). It focuses on embedding similarity from answer to answer rather than seeking embedding similarity for the problem or query. Using the Step-back Prompting "}, {"rank": 3, "score": 0.5934016, "source": "dense+rrf", "rrf_score": 0.027799227799227798, "rerank_score": 0.23610241679100624, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 146, "text_snippet": " should be retrieved, evaluate the relevance of the documents to the prompt, their position, and the number included in the context. The research findings reveal that including irrelevant documents can unexpectedly increase accuracy by over"}, {"rank": 4, "score": 0.58552825, "source": "dense+rrf", "rrf_score": 0.02782608695652174, "rerank_score": 0.2339464866714317, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 20, "text_snippet": "trieval augmented LMs is to generate the answer y= [s1,s2, ...,sm] = [w1, w2, ..., w n]containing msentences or ntokens leveraging information retrieved from the corpus. In retrieval augmented LM, the LM typically pairs with a retriever tha"}, {"rank": 5, "score": 24.257937903661446, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.2331436568543612, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 71, "text_snippet": "al domains. The task is to generate a clinical note from the dialogue [59]. ‚Ä¢MS-MARCO: This dataset [ 2] is a large-scale collection of natural language questions and answers derived from real web queries and documents. ‚Ä¢QMSum: A benchmark "}]}
{"case_index": 315, "query": "From 2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt, what exact term fills [BLANK] in: \"42.9% in Top-5 accuracy), but also results in a [BLANK] improvement on the end-to-end QA accuracy compared to ORQA (41.5% vs.\"?", "gold": "substantial", "answer": "The exact term is **‚Äúsubstantial‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "The context explicitly contains the phrase 'results in a substantial improvement', matching the expected term.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 74.263, "llm_ms": 1145.474, "top_contexts": [{"rank": 1, "score": 0.6523254, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.38377369108428816, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 11, "text_snippet": "is surprisingly simple: the embedding is optimized for maximizing inner products of the question and relevant passage vectors, with an objective compar- ing all pairs of questions and passages in a batch. OurDense Passage Retriever (DPR) is"}, {"rank": 2, "score": 0.6124673, "source": "dense+rrf", "rrf_score": 0.0304147465437788, "rerank_score": 0.26510524003398345, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 13, "text_snippet": "omain question answering, a higher retrieval precision indeed translates to a higher end-to-end QA accuracy. By applying a modern reader model to the top retrieved passages, we achieve compara- ble or better results on multiple QA datasets "}, {"rank": 3, "score": 0.64014673, "source": "dense+rrf", "rrf_score": 0.03125, "rerank_score": 0.2630246625837325, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 70, "text_snippet": "BM25+DPR 39.0 57.0 35.2 28.0 36.7 MultiDPR 41.5 56.8 42.4 49.4 24.1 BM25+DPR 38.8 57.9 41.1 50.6 35.8 Table 4: End-to-end QA (Exact Match) Accuracy. The Ô¨Årst block of results are copied from their cited papers. REALM Wikiand REALM Newsare t"}, {"rank": 4, "score": 0.7042752, "source": "dense+rrf", "rrf_score": 0.032266458495966696, "rerank_score": 0.2616380030044363, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 2, "text_snippet": "9% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.1 1 Introduction Open-domain question answering (QA) (V oorhees, 1999) is a t"}, {"rank": 5, "score": 0.60364324, "source": "dense+rrf", "rrf_score": 0.028814262023217248, "rerank_score": 0.2504570694904942, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 17, "text_snippet": "rieval accuracy , which is the fraction of ques- tions for whichCFcontains a span that answers the question. 3 Dense Passage Retriever (DPR) We focus our research in this work on improv- ing the retrieval component in open-domain QA. Given "}]}
{"case_index": 316, "query": "From 2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt, what exact term fills [BLANK] in: \"However, common Python interfaces to deep learning such as PyTorch and TensorÔ¨Çow do not allow Ô¨Å[BLANK] control of memory access.\"?", "gold": "ne-grained", "answer": "The exact term is **‚Äúfine-grained‚Äù**.", "answer_score": 0.3, "answer_correctness": 0.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Expected blank is 'ne-grained' (as given), but model answered 'fine-grained'. Context shows 'fine-grained', so answer is grounded but does not match expected exact term.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 52.672, "llm_ms": 1074.528, "top_contexts": [{"rank": 1, "score": 0.71943927, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.4023836237130121, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 11, "text_snippet": "on of the runtime‚Äîsuch as database joins [71], image processing [ 70], numerical linear algebra [ 4], and more [ 40,85]. However, common Python interfaces to deep learning such as PyTorch and TensorÔ¨Çow do not allow Ô¨Åne-grained control of me"}, {"rank": 2, "score": 55.03968745046789, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.25177371285208777, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 100, "text_snippet": "n, Atri Rudra, and Christopher R√©. 2022. FlashAttention: Fast and memory-efficient exact attention with IO- awareness. ArXiv:2205.14135. Hermann Ebbinghaus. 1913. Memory: A contribu- tion to experimental psychology. H. A. Ruger & C. E. Buss"}, {"rank": 3, "score": 39.956400076416315, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.20040378476642468, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 84, "text_snippet": "mputer/RedPajama-Data . Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. CoRR , abs/2307.08691, 2023. Tri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra, and Christopher R ¬¥e. Flashattention: Fast an"}, {"rank": 4, "score": 0.61341965, "source": "dense+rrf", "rrf_score": 0.031024531024531024, "rerank_score": 0.19620150460354546, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 7, "text_snippet": " with wall-clock speed) and tend to ignore overheads from memory access (IO). In this paper, we argue that a missing principle is making attention algorithms IO-aware [1]‚Äîthat is, carefully accounting for reads and writes to diÔ¨Äerent levels"}, {"rank": 5, "score": 0.6399764, "source": "dense+rrf", "rrf_score": 0.030834914611005692, "rerank_score": 0.19612945530378909, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 78, "text_snippet": "implementation. This requires writing the attention algorithm in a considerably lower-level language than PyTorch, and requires signiÔ¨Åcant engineering eÔ¨Äort. Implementations may also not be transferrable across GPU architectures. These limi"}]}
{"case_index": 317, "query": "From 2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt, what exact term fills [BLANK] in: \"In this work, we provide a [BLANK] evaluation comparing Orca 2 to several other models.\"?", "gold": "comprehensive", "answer": "The exact term is **‚Äúcomprehensive‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'comprehensive' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 69.645, "llm_ms": 1147.502, "top_contexts": [{"rank": 1, "score": 0.6926287, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.2922716627634592, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 19, "text_snippet": "ference)andsystem2(target), whichoneisbetter?‚Äù. However, previouswork[ 13,42,60,67] has demonstrated that this approach has several drawbacks. In this work, we provide a comprehensive evaluation comparing Orca 2 to several other models. We "}, {"rank": 2, "score": 29.915931941356174, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.24134500015301735, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 148, "text_snippet": "synthetic data for post-training. While Orca 2 models have not gone through RLHF training for safety, we believe that the use of synthetic data for post-training that has been filtered with various content safety filters could provide anoth"}, {"rank": 3, "score": 0.6351352, "source": "dense+rrf", "rrf_score": 0.03021353930031804, "rerank_score": 0.22605034714682926, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 0, "text_snippet": "Orca 2: Teaching Small Language Models How to Reason Arindam Mitra, Luciano Del Corro‚Ä†, Shweti Mahajan‚Ä†, Andres Codas‚Ä° Clarisse Simoes‚Ä°, Sahaj Agarwal, Xuxi Chen‚àó, Anastasia Razdaibiedina‚àó Erik Jones‚àó, Kriti Aggarwal‚àó, Hamid Palangi, Guoqin"}, {"rank": 4, "score": 30.425917659458115, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.2110550633296732, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 281, "text_snippet": " you always used to look like this , did n‚Äôt you , _‚Äù What is the word in the blank space (_)? The answer is Orca-v2-13B-Cautious The answer is izzy. The word in the blank space is the name of the girl who is crying. Figure 24: Demonstrativ"}, {"rank": 5, "score": 0.63039684, "source": "dense+rrf", "rrf_score": 0.02803921568627451, "rerank_score": 0.20078785772695676, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 2, "text_snippet": "asks, potentially different from the one used by the larger model. For example, while larger models might provide a direct answer to a complex task, smaller models may not have the same capacity. In Orca 2, we teach the model various reason"}]}
{"case_index": 318, "query": "From 2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt, what exact term fills [BLANK] in: \"In addition to training LoRA weights in linear layers, LongLoRA further makes embedding and [BLANK] layers trainable.\"?", "gold": "normalization", "answer": "The exact term is **‚Äúnormalization‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'normalization' and is explicitly supported by the provided context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 64.392, "llm_ms": 1014.41, "top_contexts": [{"rank": 1, "score": 0.7337008, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3298469206973678, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 8, "text_snippet": "e time. In addition to training LoRA weights in linear layers, LongLoRA further makes embedding and normalization layers trainable. This extension is pivotal for context extension, and only introduces a minimal number of additional trainabl"}, {"rank": 2, "score": 0.5892976, "source": "dense+rrf", "rrf_score": 0.028693528693528692, "rerank_score": 0.2450881063922643, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 0, "text_snippet": "Published as a conference paper at ICLR 2024 LONG LORA: E FFICIENT FINE-TUNING OF LONG - CONTEXT LARGE LANGUAGE MODELS Yukang Chen1Shengju Qian1Haotian Tang2Xin Lai1 Zhijian Liu2Song Han2,3Jiaya Jia1 1CUHK2MIT3NVIDIA ABSTRACT We present Lon"}, {"rank": 3, "score": 0.66357577, "source": "dense+rrf", "rrf_score": 0.031754032258064516, "rerank_score": 0.23983769908641503, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 3, "text_snippet": "ll under the premise of trainable embedding and normalization. LongLoRA combines this improved LoRA with S2-Attn. LongLoRA demonstrates strong empirical results on various tasks on Llama2 models from 7B/13B to 70B. LongLoRA extends Llama2 7"}, {"rank": 4, "score": 0.64055634, "source": "dense+rrf", "rrf_score": 0.0315136476426799, "rerank_score": 0.2309085201796847, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 7, "text_snippet": "ding ! ! ‚ùÑ(b) Low-rank adaptx N ‚ùÑPa1ern 1 -w/o shi@(a) Shi3ed sparse a6en7onEach pa1ern in half heads a !Norm (0.004%)a !Lora (0.12%) a ‚ùÑ Linear Projec3on (96%) a !Embedding (1.94%)a ‚ùÑHead (1.94%) (c) Propor5ons of Parameters (LLaMA7B for e"}, {"rank": 5, "score": 34.991707453020105, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.21846753890271323, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 2, "text_snippet": " shifted sparse attention (S2-Attn) effectively enables context extension, leading to non-trivial computation saving with similar perfor- mance to fine-tuning with vanilla attention. Particularly, it can be implemented with only two lines o"}]}
{"case_index": 319, "query": "From 2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt, what exact term fills [BLANK] in: \"Next, we collect a dataset of [BLANK] comparisons between outputs from our models on a larger set of API prompts.\"?", "gold": "human-labeled", "answer": "The exact term is **‚Äúhuman-labeled‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'human-labeled' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 89.738, "llm_ms": 1056.174, "top_contexts": [{"rank": 1, "score": 0.6029743, "source": "dense+rrf", "rrf_score": 0.03131881575727918, "rerank_score": 0.36050971736748605, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 11, "text_snippet": "and use this to train our supervised learning baselines. Next, we collect a dataset of human-labeled comparisons between outputs from our models on a larger set of API prompts. We then train a reward model (RM) on this dataset to predict wh"}, {"rank": 2, "score": 38.13826179402499, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.29252203472343835, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 1, "text_snippet": ". For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language m"}, {"rank": 3, "score": 0.5803896, "source": "dense+rrf", "rrf_score": 0.02813852813852814, "rerank_score": 0.27459128522742066, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 2, "text_snippet": "demonstrations of the desired model behavior, which we use to Ô¨Åne-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further Ô¨Åne-tune this supervised model using reinforcement learn"}, {"rank": 4, "score": 0.5983362, "source": "dense+rrf", "rrf_score": 0.02857142857142857, "rerank_score": 0.27364085412774364, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 53, "text_snippet": "dataset in Appendix A. 3.3 Tasks Our training tasks are from two sources: (1) a dataset of prompts written by our labelers and (2) a dataset of prompts submitted to early InstructGPT models on our API (see Table 6). These prompts are very d"}, {"rank": 5, "score": 30.604644682352067, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.2664389015252177, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 81, "text_snippet": "on API distribution. Our main metric is human preference ratings on a held out set of prompts from the same source as our training distribution. When using prompts from the API for evaluation, we only select prompts by customers we haven‚Äôt "}]}
{"case_index": 320, "query": "From 2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt, what exact term fills [BLANK] in: \"Wespliteach ùëõ-[BLANK] ùëã=¬πùë•1¬î¬ì¬ì¬ì¬îùë•ùëõ¬∫intoasequenceof ùëôchunks¬πùê∂1¬î¬ì¬ì¬ì¬îùê∂ùëô¬∫ of sizeùëö=ùëõ ùëô, i.e.ùê∂1,¬πùë•1¬î¬ì¬ì¬ì¬îùë•ùëö¬∫¬î ¬ì¬ì¬ì¬î ùê∂ùëô,¬πùë•ùëõ\u0000ùëö¬∏1¬î¬ì¬ì¬ì¬îùë•ùëõ¬∫2ùïçùëö.\"?", "gold": "token-longexample", "answer": "The exact term is **‚Äútoken-long example‚Äù** (i.e., ‚ÄúWe split each *n-token-long example* ‚Ä¶‚Äù).", "answer_score": 0.9299999999999999, "answer_correctness": 0.9, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.9, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context shows the phrase 'n-token-longexample' (no space). Model answered 'token-long example' with a space; meaning is correct but not the exact term formatting.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 41.966, "llm_ms": 1135.985, "top_contexts": [{"rank": 1, "score": 0.73612785, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.380614897144785, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 22, "text_snippet": " ùïç=¬ª1¬îùë£¬º, obtained using a text tokenizer1. Wespliteach ùëõ-token-longexample ùëã=¬πùë•1¬î¬ì¬ì¬ì¬îùë•ùëõ¬∫intoasequenceof ùëôchunks¬πùê∂1¬î¬ì¬ì¬ì¬îùê∂ùëô¬∫ of sizeùëö=ùëõ ùëô, i.e.ùê∂1,¬πùë•1¬î¬ì¬ì¬ì¬îùë•ùëö¬∫¬î ¬ì¬ì¬ì¬î ùê∂ùëô,¬πùë•ùëõ\u0000ùëö¬∏1¬î¬ì¬ì¬ì¬îùë•ùëõ¬∫2ùïçùëö. We useùëõ=2048andùëö=64. We augment each chunk ùê∂ùë¢with a se"}, {"rank": 2, "score": 0.60245144, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.280962998819667, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 242, "text_snippet": "37  Improving language models by retrieving from trillions of tokens Table 16jGreat Circle (novel) , from Wikipedia September 21. The article is about a recent novel and chunks ùê∂3andùê∂4are speciÔ¨Åcally about its reception. The name Publishers"}, {"rank": 3, "score": 0.59510547, "source": "dense+rrf", "rrf_score": 0.029138513513513514, "rerank_score": 0.27838176025039435, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 38, "text_snippet": "t token of chunk ùê∂ùë¢is the Ô¨Årst to be able to access the retrieved content ùê∏ùë¢while maintaining autoregressivity in(1). Hence, there is a one token overlap between chunk ùê∂ùë¢=\u0010 ùë•¬πùë¢\u00001¬∫ùëö¬∏ùëñ\u0011 ùëñ2¬ª1¬îùëö¬ºand the corresponding attending chunk ùê∂¬∏ ùë¢,¬πùë•ùë¢ùëö¬∏ùëñ"}, {"rank": 4, "score": 28.069791242011867, "source": "dense+rrf", "rrf_score": 0.014084507042253521, "rerank_score": 0.2661291368501974, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 256, "text_snippet": "n lishersWeeklywrote,\"Shipstead man agestoportray both Marian‚Äôsand Ha agestoportray both Marian‚Äô s andHa dley‚Äôs dley‚Äôs 38  Improving language models by retrieving from trillions of tokens Table 17jAll-Ireland Senior Football Championship Fi"}, {"rank": 5, "score": 28.340719977877463, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.2579360503064987, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 228, "text_snippet": "baseline models, measured in bits-per-bytes on evaluation set, in Table 14. F.2. The Pile In Fig. 4, we compare R/e.sc/t.sc/r.sc/o.scagainst Jurassic-1 (Lieber et al., 2021). The full bits-per-bytes results are reported in Table 15. F.3. Fi"}]}
{"case_index": 321, "query": "From 2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt, what exact term fills [BLANK] in: \"2 These three novel designs significantly boost the overall performance of RAG on open-domain question- answering tasks like NQ (Kwiatkowski et al., 2019), HotpotQA (Yang et al., 2018), Qasper (Dasigi et al., 2021) and [BLANK] (Bai et al., 2023).\"?", "gold": "multifieldqa-en", "answer": "The exact term that fills **[BLANK]** is **‚ÄúMultiFieldQA-en‚Äù** (Bai et al., 2023).", "answer_score": 0.3, "answer_correctness": 0.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context shows 'MultiFieldQA-en', but expected answer is the exact term 'multifieldqa-en' (case mismatch). Answer is grounded in provided context but not correct per expected exact string.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 118.292, "llm_ms": 1527.006, "top_contexts": [{"rank": 1, "score": 0.82616925, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.514070605152747, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 14, "text_snippet": "further extract answers from the concatenation of retrievals, which is normally around 30K tokens. We simply prompt an existing long-context LM (like Gemini or GPT4) with the question to produce the answers in a zero-shot fashion. 2  These "}, {"rank": 2, "score": 43.033260645844265, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.27028682849226987, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 79, "text_snippet": "on the long-context understanding ability of LLMs, and we still have a long way to go in harnessing open-source LLMs within our framework. 10  4 Related Work 4.1 Retrieval-Augmented Generation. Augmenting language models with information re"}, {"rank": 3, "score": 0.68693924, "source": "dense+rrf", "rrf_score": 0.02928692699490662, "rerank_score": 0.26806592006149926, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 11, "text_snippet": " even millions of tokens as input (Reid et al., 2024; Achiam et al., 2023). In this paper, we propose to revisit this design choice for open-domain question answering and propose the LongRAG framework as a solution to balance the workload b"}, {"rank": 4, "score": 0.7375288, "source": "dense+rrf", "rrf_score": 0.03128054740957967, "rerank_score": 0.24345589081346397, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 5, "text_snippet": " training corpus (Kandpal et al., 2022; Mallen et al., 2023). The standard solution to these issues is to rely on Retrieval Augmented Generation (RAG) (Lee et al., 2019; Lewis et al., 2020; Guu et al., 2020). Answering a question then essen"}, {"rank": 5, "score": 40.58865751883006, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.24247142944216055, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 6, "text_snippet": "7; Guu et al., 2020; Lewis et al., 2020; Izacard and Grave, 2021; Sachan et al., 2021; Lee et al., 2021; Jiang et al., 2022; Izacard et al., 2022; Nakano et al., 2021; Qian et al., 2023; Lazaridou et al., 2022; Shi et al., 2023). These sing"}]}
{"case_index": 322, "query": "From 2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt, what exact term fills [BLANK] in: \"Then we feed these retrieved units ( ‚âà30K tokens) to an existing [BLANK] LLM to perform zero-sho\"?", "gold": "long-context", "answer": "The exact term is **‚Äúlong-context‚Äù**, i.e., the sentence reads: ‚Äú‚Ä¶to an existing **long-context** LLM to perform zero-sho[t]‚Ä¶‚Äù.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'long-context' and is directly supported by the provided context snippet.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 65.324, "llm_ms": 1691.718, "top_contexts": [{"rank": 1, "score": 0.6850729, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.36129499833626777, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 3, "text_snippet": "ize, we significantly reduce the total number of units from 22M to 600K. This greatly reduces the burden on the retriever, resulting in strongretrievalperformancewithonlyafew(lessthan8)topunits. Comparedtotraditional RAG,whichmayrequirehund"}, {"rank": 2, "score": 0.6216885, "source": "dense+rrf", "rrf_score": 0.0304147465437788, "rerank_score": 0.24371835962315283, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 4, "text_snippet": "‚âà30K tokens) to an existing long-context LLM to perform zero-shot answer generation. Without requiring any training, LongRAG achieves an EM of 62.7% on NQ and 64.3% on HotpotQA, which are on par with the (fully-trained) SoTA model. Furtherm"}, {"rank": 3, "score": 30.014884081526702, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.21765961261457065, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 14, "text_snippet": "further extract answers from the concatenation of retrievals, which is normally around 30K tokens. We simply prompt an existing long-context LM (like Gemini or GPT4) with the question to produce the answers in a zero-shot fashion. 2  These "}, {"rank": 4, "score": 0.62367165, "source": "dense+rrf", "rrf_score": 0.03036576949620428, "rerank_score": 0.20721126770486356, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 0, "text_snippet": "LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs ‚ô†Ziyan Jiang,‚ô†Xueguang Ma,‚ô†Wenhu Chen ‚ô†University of Waterloo ziyanjiang528@gmail.com ,{x93ma ,wenhuchen}@uwaterloo.ca Project Website: https://tiger-ai-lab.github.io/"}, {"rank": 5, "score": 0.6667412, "source": "dense+rrf", "rrf_score": 0.02964254577157803, "rerank_score": 0.20413014251799166, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 79, "text_snippet": "on the long-context understanding ability of LLMs, and we still have a long way to go in harnessing open-source LLMs within our framework. 10  4 Related Work 4.1 Retrieval-Augmented Generation. Augmenting language models with information re"}]}
{"case_index": 323, "query": "From 2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt, what exact term fills [BLANK] in: \"[BLANK] trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq.\"?", "gold": "flashattention", "answer": "**FlashAttention**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Blank is 'FlashAttention' per context; expected term 'flashattention' matches case-insensitively.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 65.761, "llm_ms": 908.973, "top_contexts": [{"rank": 1, "score": 0.7768228, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.35283230132901683, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 3, "text_snippet": "imate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3 \u0002speedup on GPT-2 (seq. length "}, {"rank": 2, "score": 0.758787, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.288127675710043, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 17, "text_snippet": "to make it easier to build on this primitive.1 We empirically validate that FlashAttention speeds up model training and improves model quality by modeling longer context. We also benchmark the runtime and memory footprint of FlashAttention "}, {"rank": 3, "score": 0.6571539, "source": "dense+rrf", "rrf_score": 0.027651515151515153, "rerank_score": 0.26194941284549644, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 55, "text_snippet": "th-256 (sequence length 64K). ‚Ä¢Benchmarking Attention. We measure the runtime and memory performance of FlashAttention and block-sparse FlashAttention based on sequence length. We conÔ¨Årm that the memory footprint ofFlashAttention scales lin"}, {"rank": 4, "score": 0.7341016, "source": "dense+rrf", "rrf_score": 0.029957522915269395, "rerank_score": 0.24302950361909542, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 56, "text_snippet": "n all existing approximate attention baselines. Additional experiment details are in Appendix E. 4.1 Faster Models with FlashAttention BERT. FlashAttention yields the fastest single-node BERT training speed that we know of. We train a BERT-"}, {"rank": 5, "score": 54.73686116546747, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.24168852077457564, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 100, "text_snippet": "n, Atri Rudra, and Christopher R√©. 2022. FlashAttention: Fast and memory-efficient exact attention with IO- awareness. ArXiv:2205.14135. Hermann Ebbinghaus. 1913. Memory: A contribu- tion to experimental psychology. H. A. Ruger & C. E. Buss"}]}
{"case_index": 324, "query": "From 2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt, what exact term fills [BLANK] in: \"Other works (Wu et al., 2022; Bulatov et al., 2022) utilize memory mechanisms as a [BLANK] on past inputs, to look up relevant tokens.\"?", "gold": "compression", "answer": "The exact term that fills **[BLANK]** is **‚Äúcompression‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context 1 explicitly states 'utilize memory mechanisms as a compression on past inputs', matching expected term.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 63.57, "llm_ms": 1051.685, "top_contexts": [{"rank": 1, "score": 75.64334518933785, "source": "dense+rrf", "rrf_score": 0.01639344262295082, "rerank_score": 0.3820366945371868, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 23, "text_snippet": " et al., 2022; Ding et al., 2023; Qiu et al., 2020). They alleviate the quadratic complexity of the self-attention computation. For example, Longformer (Beltagy et al., 2020) and BigBird (Zaheer et al., 2020) use sparse attention to handle "}, {"rank": 2, "score": 32.66637624156937, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.24326464308997506, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 43, "text_snippet": "tion patterns. Easy Implementation. S2-Attn is easy to implement. It involves only two steps: (1) shifting tokens in half attention heads, and (2) transposing features from token dimension to batch dimension. Two lines of code are enough. W"}, {"rank": 3, "score": 30.199068981157513, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.22784258856328338, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 75, "text_snippet": "ed in training-from- scratch transformers. This experiment is to examine their capability of fine-tuning on pre-trained LLMs (Touvron et al., 2023b), toward long context adaptation. Dilated attention performs well in full fine-tuning but is"}, {"rank": 4, "score": 0.6097388, "source": "dense+rrf", "rrf_score": 0.029850746268656716, "rerank_score": 0.22377418156540846, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 85, "text_snippet": "extending the supported text snippet lengthfromalimitof512tokensto32ktokens. Typically, thedevelopmentoflong-contextembeddingmodels involves first obtaining a long-context backbone model. This can be achieved either by pre-training with lon"}, {"rank": 5, "score": 28.560829642799618, "source": "dense+rrf", "rrf_score": 0.014285714285714285, "rerank_score": 0.21540471538676098, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 81, "text_snippet": " al., 2022), fine-tuning the retriever and reader jointly (Yu, 2022; Izacard et al., 2022; Singh et al., 2021; Izacard & Grave, 2020a), and integrating the retriever with the black-box language model (Yu et al., 2023; Shi et al., 2023; Triv"}]}
{"case_index": 325, "query": "From 2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt, what exact term fills [BLANK] in: \"ls with larger context windows (e.g., 4096, 32K, and even 100K tokens), but it remains unclear how these [BLANK] language models make use of their input contexts when performing downstream tasks.\"?", "gold": "extended-context", "answer": "The exact term is **‚Äúextended-context‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'extended-context' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 99.548, "llm_ms": 908.549, "top_contexts": [{"rank": 1, "score": 0.77561414, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.46717042892856037, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 8, "text_snippet": "ls with larger context windows (e.g., 4096, 32K, and even 100K tokens), but it remains unclear how these extended-context language models make use of their input contexts when performing downstream tasks. We empirically investigate this que"}, {"rank": 2, "score": 0.64809275, "source": "dense+rrf", "rrf_score": 0.03125, "rerank_score": 0.2659110642987401, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 2, "text_snippet": "urs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts, even for explicitly long-context models. Our analysis provides a better understanding "}, {"rank": 3, "score": 0.62734616, "source": "dense+rrf", "rrf_score": 0.028370221327967807, "rerank_score": 0.24998266690617268, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 81, "text_snippet": "ge model can take in 16K tokens, is it actually beneficial to provide 16K tokens of context? The answer to this question is ultimately downstream task-specific since it de- pends on the marginal value of the added context and the model‚Äôs ab"}, {"rank": 4, "score": 37.521994240037536, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.2420307964937396, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 49, "text_snippet": "UIDs. The relevant key-value pair for answering the query is bolded here within the input context for clarity. indicate that extended-context models are not nec- essarily better than their non-extended counterparts at using their input cont"}, {"rank": 5, "score": 0.7121049, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.24132388279684505, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 7, "text_snippet": "e memory and compute that increases quadrat- ically in sequence length. As a result, Trans- former language models were often trained with relatively small context windows (between 512- 2048 tokens). Recent improvements in hardware (e.g., f"}]}
{"case_index": 326, "query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"How this is done depends entirely on the API itself ‚Äì for example, it can involve call- ing another neural network, [BLANK] a Python script or using a retrieval system to perform search over a large corpus.\"?", "gold": "executing", "answer": "The exact term is **‚Äúexecuting‚Äù** (as in: ‚Äúcalling another neural network, **executing** a Python script or using a retrieval system‚Ä¶‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'executing' and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 89.171, "llm_ms": 1666.383, "top_contexts": [{"rank": 1, "score": 0.5876142, "source": "dense+rrf", "rrf_score": 0.031544957774465976, "rerank_score": 0.459642324888219, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 22, "text_snippet": "mpling from Mgiven the sequence [P(x),x1,...,x i‚àí1,<API> ]as a preÔ¨Åx and</API> as an end-of-sequence token.2 2We discard all examples where Mdoes not generate the </API> token.Executing API Calls As a next step, we execute all API calls gen"}, {"rank": 2, "score": 0.6300721, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.2769538564893238, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 1, "text_snippet": "metic or fac- tual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer , a model traine"}, {"rank": 3, "score": 0.58172846, "source": "dense+rrf", "rrf_score": 0.03057889822595705, "rerank_score": 0.2461157796451874, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 97, "text_snippet": "rred from making an API call. 8 Conclusion We have introduced Toolformer, a language model that learns in a self-supervised way how to use different tools such as search engines, calculators, and translation systems via simple API calls. Th"}, {"rank": 4, "score": 0.6174812, "source": "dense+rrf", "rrf_score": 0.0315136476426799, "rerank_score": 0.23784450589491166, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 12, "text_snippet": " how an API can be used, we let a LM annotate a huge language modeling dataset with potential API calls. We then use a self-supervised loss to determine which of these API calls actually help the model in predicting future tokens. Finally, "}, {"rank": 5, "score": 35.529564578098025, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.22584970265359236, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 93, "text_snippet": "mitations While our approach enables LMs to learn how to use a variety of tools in a self-supervised way, there are some clear limitations to what can be achieved with our method in its current form. One such limi- tation is the inability o"}]}
{"case_index": 327, "query": "From 2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt, what exact term fills [BLANK] in: \"2 Published as a conference paper at ICLR 2023 We conduct empirical evaluations of ReAct and [BLANK] baselines on four diverse benchmarks: question answering (HotPotQA, Yang et al., 2018),\"?", "gold": "state-of-the-art", "answer": "The term that fills **[BLANK]** is:\n\n**‚Äústate-of-the-art‚Äù**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'state-of-the-art' and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 89.933, "llm_ms": 1276.019, "top_contexts": [{"rank": 1, "score": 0.7735383, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.4685978839952698, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 20, "text_snippet": "pertaining to a task in an interleaved manner, which allows the model to perform dynamic reasoning to create, maintain, and adjust high-level plans for acting (reason to act), while also interact with the external environments (e.g. Wikiped"}, {"rank": 2, "score": 38.92534752706399, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.2712666593052402, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 115, "text_snippet": "lti-hop question answering. arXiv preprint arXiv:1809.09600 , 2018. 12  Published as a conference paper at ICLR 2023 Shunyu Yao, Rohan Rao, Matthew Hausknecht, and Karthik Narasimhan. Keep CALM and explore: Language models for action genera"}, {"rank": 3, "score": 0.68721056, "source": "dense+rrf", "rrf_score": 0.031009615384615385, "rerank_score": 0.26344755094847994, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 88, "text_snippet": " the development of versatile and generalist agents like Reed et al. (2022). 6 C ONCLUSION We have proposed ReAct ‚Äì a simple yet effective method for synergizing reasoning and acting in large language models. Through a diverse set of experi"}, {"rank": 4, "score": 35.055723804365016, "source": "dense+rrf", "rrf_score": 0.0136986301369863, "rerank_score": 0.26198729559282025, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 180, "text_snippet": "vide the exact prompts used for common NLP tasks in the following tables as well, including NLI (Table 18, Table 19, Table 20) and Closed-Book Question-Answering tasks (Table 16, Table 21). 20  Published as a conference paper at ICLR 2023 T"}, {"rank": 5, "score": 0.6750364, "source": "dense+rrf", "rrf_score": 0.02857142857142857, "rerank_score": 0.25539220429465187, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 78, "text_snippet": "ent, due to a lack of commonsense reasoning. Both shortcomings can be addressed in the ReAct paradigm. More details about ReAct-IM is in Appendix B.2. An example prompt for ReAct-IM can be found in Appendix C.4, and an example trajectory in"}]}
{"case_index": 328, "query": "From 2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt, what exact term fills [BLANK] in: \"[BLANK] autoregressive token models Our approach uses retrieval as a way to augment input examples at the granularity of small chunks of tokens.\"?", "gold": "retrieval-enhanced", "answer": "The exact term is **‚ÄúRetrieval-enhanced‚Äù**, i.e., the full heading is:\n\n**‚ÄúRetrieval-enhanced autoregressive token models‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'retrieval-enhanced' and is directly supported by Context 1 heading 'Retrieval-enhanced autoregressive token models'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 119.166, "llm_ms": 1294.575, "top_contexts": [{"rank": 1, "score": 0.63404036, "source": "dense+rrf", "rrf_score": 0.032018442622950824, "rerank_score": 0.39905012031879616, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 21, "text_snippet": "remove all training documents with high similarity (0.8 or higher) to a validation or test set document. Additionally, we remove all validation and test articles from Wikitext103 (Merity et al., 2017) from our Wikipedia training data. 2.2. "}, {"rank": 2, "score": 0.62012416, "source": "dense+rrf", "rrf_score": 0.03021353930031804, "rerank_score": 0.2709491792730665, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 22, "text_snippet": " ùïç=¬ª1¬îùë£¬º, obtained using a text tokenizer1. Wespliteach ùëõ-token-longexample ùëã=¬πùë•1¬î¬ì¬ì¬ì¬îùë•ùëõ¬∫intoasequenceof ùëôchunks¬πùê∂1¬î¬ì¬ì¬ì¬îùê∂ùëô¬∫ of sizeùëö=ùëõ ùëô, i.e.ùê∂1,¬πùë•1¬î¬ì¬ì¬ì¬îùë•ùëö¬∫¬î ¬ì¬ì¬ì¬î ùê∂ùëô,¬πùë•ùëõ\u0000ùëö¬∏1¬î¬ì¬ì¬ì¬îùë•ùëõ¬∫2ùïçùëö. We useùëõ=2048andùëö=64. We augment each chunk ùê∂ùë¢with a se"}, {"rank": 3, "score": 29.995502495737746, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.2702024183830067, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 183, "text_snippet": "sc¬πùê∂ùë¢¬∫ùëóand the chunk ùê∂ùë¢to be relatively well aligned, and assume that they start at the same position. Therefore, when computing C/a.sc¬πùêª¬∏ ùë¢¬îùê∏ùë¢¬∫, we set the distance between the data token ùëñ2¬ª1¬îùëô¬ºof chunkùê∂¬∏ ùë¢and 5https://github.com/earwig/m"}, {"rank": 4, "score": 31.04107378878099, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.2697435897435842, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 199, "text_snippet": " but only trains the new retrieval weights, as explained in ¬ß4.2. Our best R/e.sc/t.sc/r.sc/o.scÔ¨Åt checkpoint has a Wikitext103 valid perplexity 18¬ì46, when retrieving from Wikipedia. We use this R/e.sc/t.sc/r.sc/o.sccheckpoint in Table 4 f"}, {"rank": 5, "score": 0.6289594, "source": "dense+rrf", "rrf_score": 0.028438886647841874, "rerank_score": 0.25513160311449223, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 38, "text_snippet": "t token of chunk ùê∂ùë¢is the Ô¨Årst to be able to access the retrieved content ùê∏ùë¢while maintaining autoregressivity in(1). Hence, there is a one token overlap between chunk ùê∂ùë¢=\u0010 ùë•¬πùë¢\u00001¬∫ùëö¬∏ùëñ\u0011 ùëñ2¬ª1¬îùëö¬ºand the corresponding attending chunk ùê∂¬∏ ùë¢,¬πùë•ùë¢ùëö¬∏ùëñ"}]}
{"case_index": 329, "query": "From 2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"explain how these three quality aspects can be measured in a fully automated way, by [BLANK] an LLM.\"?", "gold": "prompting", "answer": "The exact term is **‚Äúprompting‚Äù** (i.e., ‚Äú‚Ä¶measured in a fully automated way, by **prompting** an LLM.‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'prompting' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 53.679, "llm_ms": 1073.27, "top_contexts": [{"rank": 1, "score": 0.6474958, "source": "dense+rrf", "rrf_score": 0.03177805800756621, "rerank_score": 0.3298330932898308, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 23, "text_snippet": " explain how these three quality aspects can be measured in a fully automated way, by prompting an LLM. In our implementation and experiments, all prompts are evaluated using the gpt-3.5-turbo-16k model, which is available through the OpenA"}, {"rank": 2, "score": 0.6585295, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.2254894326282775, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 237, "text_snippet": "-auto-eval-best-practices-RAG, 2023. [164] S. Es, J. James, L. Espinosa-Anke, and S. Schockaert, ‚ÄúRagas: Au- tomated evaluation of retrieval augmented generation,‚Äù arXiv preprint arXiv:2309.15217 , 2023. [165] J. Saad-Falcon, O. Khattab, C."}, {"rank": 3, "score": 32.096704020887756, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.2168392965602377, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 22, "text_snippet": "lly, Context Relevance refers to the idea that the retrieved context should be focused, containing as little irrelevant informa- tion as possible. This is important given the cost associated with feeding long context passages to LLMs. Moreo"}, {"rank": 4, "score": 0.635309, "source": "dense+rrf", "rrf_score": 0.029211087420042643, "rerank_score": 0.21212646673576188, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 128, "text_snippet": "Evaluation Aspects Contemporary evaluation practices of RAG models empha- size three primary quality scores and four essential abilities, which collectively inform the evaluation of the two principal targets of the RAG model: retrieval and "}, {"rank": 5, "score": 0.64210975, "source": "dense+rrf", "rrf_score": 0.029211087420042643, "rerank_score": 0.2058422174840035, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 127, "text_snippet": " on the content‚Äôs objectives: unlabeled and labeled content. For unlabeled content, the evaluation encompasses the faithfulness, relevance, and non-harmfulness of the generated answers. In contrast, for labeled content, the focus is on the "}]}
{"case_index": 330, "query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \"Such re- strictions are sub-optimal for [BLANK] tasks, and could be very harmful when applying Ô¨Åne- tuning based approaches to token-level tasks such as question answering, where it is crucial to incor- porate context from both directions.\"?", "gold": "sentence-level", "answer": "The exact term is **‚Äúsentence-level‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'sentence-level' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 95.36, "llm_ms": 1228.111, "top_contexts": [{"rank": 1, "score": 0.735819, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.5559996328439271, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 7, "text_snippet": "elf-attention layers of the Transformer (Vaswani et al., 2017). Such re- strictions are sub-optimal for sentence-level tasks, and could be very harmful when applying Ô¨Åne- tuning based approaches to token-level tasks such as question answeri"}, {"rank": 2, "score": 0.6055864, "source": "dense+rrf", "rrf_score": 0.03128054740957967, "rerank_score": 0.3041171072714721, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 1, "text_snippet": ", 2018), BERT is designed to pre- train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a re- sult, the pre-trained BERT model can be Ô¨Åne- tuned with just one a"}, {"rank": 3, "score": 42.311716232953216, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.2850021544814635, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 43, "text_snippet": "tion or sequence tagging. At the output, the token rep- resentations are fed into an output layer for token- level tasks, such as sequence tagging or question answering, and the [CLS] representation is fed into an output layer for classiÔ¨Åca"}, {"rank": 4, "score": 0.5762082, "source": "dense+rrf", "rrf_score": 0.027479766610201392, "rerank_score": 0.2620997269069411, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 114, "text_snippet": "  E1 E2 EN...Figure 3: Differences in pre-training model architectures. BERT uses a bidirectional Transformer. OpenAI GPT uses a left-to-right Transformer. ELMo uses the concatenation of independently trained left-to-right and right-to- lef"}, {"rank": 5, "score": 0.5733469, "source": "dense+rrf", "rrf_score": 0.026709401709401708, "rerank_score": 0.2370516231985115, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 20, "text_snippet": "or BERT. Apart from output layers, the same architec- tures are used in both pre-training and Ô¨Åne-tuning. The same pre-trained model parameters are used to initialize models for different down-stream tasks. During Ô¨Åne-tuning, all parameters"}]}
{"case_index": 331, "query": "From 2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt, what exact term fills [BLANK] in: \"FlashAttention enables the Ô¨Årst Transformer that can achieve [BLANK] performance on the Path-X [ 80] challenge, solely from using a longer sequence length (16K).\"?", "gold": "better-than-chance", "answer": "The exact term is **‚Äúbetter-than-chance‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'better-than-chance' and is directly supported by Context 1/5.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 85.595, "llm_ms": 1194.988, "top_contexts": [{"rank": 1, "score": 0.8029177, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.44135175816069516, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 19, "text_snippet": "lift from modeling longer sequences on long-document classiÔ¨Åcation [13]. FlashAttention enables the Ô¨Årst Transformer that can achieve better-than-chance performance on the Path-X [ 80] challenge, solely from using a longer sequence length ("}, {"rank": 2, "score": 0.7420124, "source": "dense+rrf", "rrf_score": 0.0315136476426799, "rerank_score": 0.3144342841363867, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 54, "text_snippet": "T-2 with context length 1K, while achieving 0.7 better perplexity. Modeling longer sequences yields 6.4 points of lift on two long- document classiÔ¨Åcation tasks. Finally, FlashAttention yields the Ô¨Årst Transformer that can achieve better-th"}, {"rank": 3, "score": 44.2787668574487, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.25766833107860515, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 69, "text_snippet": "s specialized medical text and thus may be more susceptible to a distribution shift in the document length, whereas ECtHR contains general language. Table 5: Long Document performance (mi- croùêπ1) at diÔ¨Äerent sequence lengths using FlashAtte"}, {"rank": 4, "score": 0.77005005, "source": "dense+rrf", "rrf_score": 0.0315136476426799, "rerank_score": 0.24596060533464206, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 72, "text_snippet": "a transformer on Path-64, and then transfer to Path-X by spatially interpolating the positional embeddings. FlashAttention achieves 61.4 accuracy on Path-X. Additionally, block-sparse FlashAttention enables the Transformers to scale to sequ"}, {"rank": 5, "score": 42.52381514538901, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.2413225113165481, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 4, "text_snippet": "T-2 and 6.4 points of lift on long-document classiÔ¨Åcation) and entirely new capabilities: the Ô¨Årst Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64"}]}
{"case_index": 332, "query": "From 2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt, what exact term fills [BLANK] in: \"progress on aligning language models by training them to act in [BLANK] with the user‚Äôs intention (Leike et al., 2018).\"?", "gold": "accordance", "answer": "The exact term is **‚Äúaccordance‚Äù** (as in: ‚Äúact in **accordance** with the user‚Äôs intention‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'accordance' and is directly supported by Context 1/3.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 91.433, "llm_ms": 1266.698, "top_contexts": [{"rank": 1, "score": 0.75013345, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3779698352088929, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 8, "text_snippet": "progress on aligning language models by training them to act in accordance with the user‚Äôs intention (Leike et al., 2018). This encompasses both explicit intentions such as following instructions and implicit intentions such as staying trut"}, {"rank": 2, "score": 0.63318324, "source": "dense+rrf", "rrf_score": 0.031054405392392875, "rerank_score": 0.3506881839939666, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 7, "text_snippet": "the next token on a webpage from the internet‚Äîis different from the objective ‚Äúfollow the user‚Äôs instructions helpfully and safely‚Äù (Radford et al., 2019; Brown et al., 2020; Fedus et al., 2021; Rae et al., 2021; Thoppilan et al., 2022). Th"}, {"rank": 3, "score": 0.67624664, "source": "dense+rrf", "rrf_score": 0.031024531024531024, "rerank_score": 0.2706428827675916, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 75, "text_snippet": " models that act in accordance with user intentions. More practically, for the purpose of our language tasks, we use a framework similar to Askell et al. (2021), who deÔ¨Åne models to be aligned if they are helpful, honest, and harmless. To b"}, {"rank": 4, "score": 0.5830927, "source": "dense+rrf", "rrf_score": 0.027692895339954164, "rerank_score": 0.26074222510345346, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 9, "text_snippet": "fabricate information or mislead the user), and harmless (they should not cause physical, psychological, or social harm to people or the environment). We elaborate on the evaluation of these criteria in Section 3.6. We focus on Ô¨Åne-tuning a"}, {"rank": 5, "score": 0.58472216, "source": "dense+rrf", "rrf_score": 0.027242401779095628, "rerank_score": 0.25937475263042337, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 168, "text_snippet": "e models by retrieving from trillions of tokens,‚Äù inInternational conference on machine learning . PMLR, 2022, pp. 2206‚Äì2240. [6] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray et a"}]}
{"case_index": 333, "query": "From 2302.13971__llama_open_and_efficient_foundation_language_models.txt, what exact term fills [BLANK] in: \"We also removed the comments from the .tex Ô¨Åles, and [BLANK] deÔ¨Ånitions and macros written by users to i\"?", "gold": "inline-expanded", "answer": "The exact term is **‚Äúinline-expanded‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term and is directly supported by the provided context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 90.485, "llm_ms": 1248.143, "top_contexts": [{"rank": 1, "score": 0.47178435, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.3299819766645206, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 16, "text_snippet": " Books3 section of TheP- ile (Gao et al., 2020), a publicly available dataset for training large language models. We perform deduplication at the book level, removing books with more than 90% content overlap. ArXiv [2.5%]. We process arXiv "}, {"rank": 2, "score": 0.50111336, "source": "dense+rrf", "rrf_score": 0.032266458495966696, "rerank_score": 0.20244486910163348, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 17, "text_snippet": " and inline-expanded deÔ¨Ånitions and macros written by users to increase consistency across papers. Stack Exchange [2%]. We include a dump of Stack Exchange, a website of high quality ques- tions and answers that covers a diverse set of do- "}, {"rank": 3, "score": 0.42913678, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.17995129847123614, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 78, "text_snippet": "rom similarly- formatted data that may occur in Internet text seen during pre-training, e.g., StackOverflow questionsand answers. To better understand the effect of additional fine- tuning and model scale, we also experimented with Llama-2 "}, {"rank": 4, "score": 20.870115121910903, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.1731924826223343, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 0, "text_snippet": "LLaMA: Open and EfÔ¨Åcient Foundation Language Models Hugo Touvron‚àó, Thibaut Lavril‚àó, Gautier Izacard‚àó, Xavier Martinet Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi√®re, Naman Goyal Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand "}, {"rank": 5, "score": 0.40538886, "source": "dense+rrf", "rrf_score": 0.028949545078577336, "rerank_score": 0.1612862256203569, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 116, "text_snippet": "Marie-Anne Lachaux, Timoth¬¥ ee Lacroix, Baptiste Rozi` ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. arXiv preprint"}]}
{"case_index": 334, "query": "From 2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt, what exact term fills [BLANK] in: \"nces, since the time and memory complexity of [BLANK] are quadratic in sequence length.\"?", "gold": "self-attention", "answer": "The exact term that fills **[BLANK]** is **‚Äúself-attention‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'self-attention' and is directly supported by the provided context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 106.439, "llm_ms": 1322.29, "top_contexts": [{"rank": 1, "score": 0.73639727, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3222394519501612, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 1, "text_snippet": "nces, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading oÔ¨Ä model quality to reduce the compute complexity, but often do n"}, {"rank": 2, "score": 0.6711977, "source": "dense+rrf", "rrf_score": 0.031024531024531024, "rerank_score": 0.3013562869708681, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 0, "text_snippet": "FlashAttention : Fast and Memory-EÔ¨Écient Exact Attention with IO-Awareness Tri Daoy, Daniel Y. Fuy, Stefano Ermony, Atri Rudraz, and Christopher R√©y yDepartment of Computer Science, Stanford University zDepartment of Computer Science and En"}, {"rank": 3, "score": 0.63752705, "source": "dense+rrf", "rrf_score": 0.029116045245077504, "rerank_score": 0.2678518407506187, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 100, "text_snippet": "n, Atri Rudra, and Christopher R√©. 2022. FlashAttention: Fast and memory-efficient exact attention with IO- awareness. ArXiv:2205.14135. Hermann Ebbinghaus. 1913. Memory: A contribu- tion to experimental psychology. H. A. Ruger & C. E. Buss"}, {"rank": 4, "score": 0.63217074, "source": "dense+rrf", "rrf_score": 0.027651515151515153, "rerank_score": 0.2615344827244941, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 5, "text_snippet": "wn larger [ 5] and deeper [ 83], but equipping them with longer context remains diÔ¨Écult [ 80], since the self-attention module at their heart has time and memory complexity quadratic in sequence length. An important question is whether maki"}, {"rank": 5, "score": 0.6644628, "source": "dense+rrf", "rrf_score": 0.02877846790890269, "rerank_score": 0.2323018822972138, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 76, "text_snippet": "nce lengths. Memory Footprint. Figure 3 (right) shows the memory footprint of FlashAttention and block-sparse FlashAttention compared to various exact, approximate, and sparse attention baselines. FlashAttention and block-sparse FlashAttent"}]}
{"case_index": 335, "query": "From 2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt, what exact term fills [BLANK] in: \"The more that [BLANK] thinking and analysis is required for a problem (Evans, 2010), the greater the diversity of reasoning paths that can recover the answer.\"?", "gold": "deliberate", "answer": "The exact term is **‚Äúdeliberate‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'deliberate' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 97.478, "llm_ms": 1260.771, "top_contexts": [{"rank": 1, "score": 0.7226261, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.474796760592195, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 6, "text_snippet": "aths that reach a correct answer (Stanovich & West, 2000). The more that deliberate thinking and analysis is required for a problem (Evans, 2010), the greater the diversity of reasoning paths that can recover the answer. Figure 1 illustrate"}, {"rank": 2, "score": 35.59489310440976, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.28037374097235007, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 87, "text_snippet": "rved that it signiÔ¨Åcantly improves accuracy in a range of arithmetic and commonsense reasoning tasks, across four large language models with varying scales. Beyond accuracy gains, self-consistency is also useful for collecting rationales wh"}, {"rank": 3, "score": 32.23987233373653, "source": "dense+rrf", "rrf_score": 0.014285714285714285, "rerank_score": 0.27788757011700255, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 11, "text_snippet": "mainder for $2 per egg, so in  total she sells 7 * $2 = $14 per day.  The answer is $14. The answer is $14. Greedy decode Figure 1: The self-consistency method contains three steps: (1) prompt a language model using chain-of-thought (CoT) p"}, {"rank": 4, "score": 39.21662143548177, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.2756794696493724, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 144, "text_snippet": "ed reasoning paths from the PaLM-540B model in Table 13. We see that the diversity in the additionally sampled reasoning paths indeed helps the model arrive at a more correct Ô¨Ånal answer after aggregation. A.1.2 R OBUSTNESS TO DIFFERENT SET"}, {"rank": 5, "score": 39.64072026199778, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.2667696127489417, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 77, "text_snippet": "much shorter and less opportunity remains for generating diversity in the decoding process. In addition, we tested self-consistency with zero-shot chain-of-thought (Kojima et al., 2022) and show that self-consistency works for zero-shot CoT"}]}
{"case_index": 336, "query": "From 2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt, what exact term fills [BLANK] in: \"Chi‚Ä† Sharan Narang‚Ä†Aakanksha Chowdhery‚Ä†Denny Zhou‚Ä†¬ß ‚Ä†Google Research, Brain Team ‚Ä°xuezhiw@google.com ,¬ßdennyzhou@google.com ABSTRACT [BLANK] prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks.\"?", "gold": "chain-of-thought", "answer": "The exact term that fills **[BLANK]** is:\n\n**‚ÄúChain-of-thought‚Äù**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context 1 explicitly states: 'Chain-of-thought prompting...' so the blank is 'chain-of-thought'. Model answer matches exactly (case-insensitive).", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 95.585, "llm_ms": 1288.01, "top_contexts": [{"rank": 1, "score": 0.8123411, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.5652261280880743, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 0, "text_snippet": "Published as a conference paper at ICLR 2023 SELF-CONSISTENCY IMPROVES CHAIN OF THOUGHT REASONING IN LANGUAGE MODELS Xuezhi Wang‚Ä†‚Ä°Jason Wei‚Ä†Dale Schuurmans‚Ä†Quoc Le‚Ä†Ed H. Chi‚Ä† Sharan Narang‚Ä†Aakanksha Chowdhery‚Ä†Denny Zhou‚Ä†¬ß ‚Ä†Google Research, "}, {"rank": 2, "score": 0.7562145, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.3151384036213943, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 0, "text_snippet": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models Jason Wei Xuezhi Wang Dale Schuurmans Maarten Bosma Brian Ichter Fei Xia Ed H. Chi Quoc V . Le Denny Zhou Google Research, Brain Team {jasonwei,dennyzhou}@google.com Abst"}, {"rank": 3, "score": 0.69951904, "source": "dense+rrf", "rrf_score": 0.03057889822595705, "rerank_score": 0.29724155767826066, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 113, "text_snippet": "on. arXiv preprint arXiv:1803.05355 , 2018. Lev S Vygotsky. Thinking and speech. The collected works of LS Vygotsky , 1:39‚Äì285, 1987. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdh- ery, and Denny "}, {"rank": 4, "score": 0.71151376, "source": "dense+rrf", "rrf_score": 0.030776515151515152, "rerank_score": 0.2644193627796953, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 120, "text_snippet": "Zhou. 2022. Self- consistency improves chain of thought reasoning in language models. CoRR , abs/2203.11171. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicit"}, {"rank": 5, "score": 43.31583305827246, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.24534040611461685, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 40, "text_snippet": " 4  Published as a conference paper at ICLR 2023 3.2 M AINRESULTS We report the results of self-consistency averaged over 10 runs, where we sampled 40 outputs independently from the decoder in each run. The baseline we compare to is chain-o"}]}
{"case_index": 337, "query": "From 2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt, what exact term fills [BLANK] in: \"For each attention head, the [BLANK] attention enforces different sparsity patterns over KV cache.\"?", "gold": "blocksparse", "answer": "The exact term is **‚Äúblocksparse‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'blocksparse' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 114.454, "llm_ms": 1033.131, "top_contexts": [{"rank": 1, "score": 0.69091827, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.2906513044849981, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 12, "text_snippet": "ze the training and inference speed, we design a novel blocksparse attention module. For each attention head, the blocksparse attention enforces different sparsity patterns over KV cache. This ensures that all tokens are attended to on diff"}, {"rank": 2, "score": 51.85977417831917, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.22949997643130354, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 0, "text_snippet": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone Microsoft Abstract We introduce phi-3-mini , a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both"}, {"rank": 3, "score": 0.5922735, "source": "dense+rrf", "rrf_score": 0.03076923076923077, "rerank_score": 0.22495815295634605, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 13, "text_snippet": "ployment speed-up from the blocksparse design, we implemented highly efficient, yet flexible kernels for both training and inference. For training, we build a triton kernel based on Flash Attention [DFE+22]. For inference, we implemented a "}, {"rank": 4, "score": 38.667759420823785, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.21793881662134296, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 16, "text_snippet": "3. The table shows the Keys/values a query token in block 8 attended to. Blue=local blocks, orange=remote/vertical blocks, gray=blocks skipped. total parameters. Additionally, we utilize the SparseMixer approach [LGC23, LDL+23] for training"}, {"rank": 5, "score": 26.33563555229674, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.19343628298077575, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 55, "text_snippet": "in, with the exception that we use alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar to the Sparse Transformer [ CGRS19 ]. To study the dependence of ML performance on model size, we tr"}]}
{"case_index": 338, "query": "From 2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt, what exact term fills [BLANK] in: \"On the other hand, block-sparse [BLANK] is faster than all existing approximate attention methods that we know of.\"?", "gold": "flashattention", "answer": "The exact term is **‚ÄúFlashAttention‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Blank is 'FlashAttention' per provided context; matches expected term (case-insensitive).", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 67.48, "llm_ms": 1043.53, "top_contexts": [{"rank": 1, "score": 0.76090825, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3985405318540607, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 20, "text_snippet": "n. FlashAttention is up to 3\u0002faster than the standard attention implemen- tation across common sequence lengths from 128 to 2K and scales up to 64K. Up to sequence length of 512, FlashAttention is both faster and more memory-eÔ¨Écient than an"}, {"rank": 2, "score": 0.7374842, "source": "dense+rrf", "rrf_score": 0.0315136476426799, "rerank_score": 0.3838546525079431, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 75, "text_snippet": "tion mechanisms grow linearly with se- quence length, but FlashAttention still runs faster than approximate and sparse attention for short sequences due to fewer memory accesses. The approximate attention runtimes begin to cross over with F"}, {"rank": 3, "score": 0.73688734, "source": "dense+rrf", "rrf_score": 0.030536130536130537, "rerank_score": 0.3756318712352167, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 55, "text_snippet": "th-256 (sequence length 64K). ‚Ä¢Benchmarking Attention. We measure the runtime and memory performance of FlashAttention and block-sparse FlashAttention based on sequence length. We conÔ¨Årm that the memory footprint ofFlashAttention scales lin"}, {"rank": 4, "score": 0.7420261, "source": "dense+rrf", "rrf_score": 0.031746031746031744, "rerank_score": 0.3270626966441084, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 2, "text_snippet": "exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity ofFlashAttention , showing that it requires fewer HBM accesses"}, {"rank": 5, "score": 0.75587106, "source": "dense+rrf", "rrf_score": 0.03128054740957967, "rerank_score": 0.3112532501471175, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 76, "text_snippet": "nce lengths. Memory Footprint. Figure 3 (right) shows the memory footprint of FlashAttention and block-sparse FlashAttention compared to various exact, approximate, and sparse attention baselines. FlashAttention and block-sparse FlashAttent"}]}
{"case_index": 339, "query": "From 2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt, what exact term fills [BLANK] in: \"Research on training small LMs has often relied on imitation learning to [BLANK] the output of more capable models.\"?", "gold": "replicate", "answer": "The exact term is **‚Äúreplicate‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'replicate' and is directly supported by the provided context sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 96.946, "llm_ms": 969.147, "top_contexts": [{"rank": 1, "score": 0.6658753, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3988325118981256, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 1, "text_snippet": "n benchmarks like BigBench Hard and AGIEval. In Orca 2, we continue exploring how improved training signals can enhance smaller LMs‚Äô reasoning abilities. Research on training small LMs has often relied on imitation learning to replicate the"}, {"rank": 2, "score": 0.634742, "source": "dense+rrf", "rrf_score": 0.03149801587301587, "rerank_score": 0.23768283266695286, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 13, "text_snippet": "ity to reason zero-shot [ 23]. These abilities include answering complex questions, generating explanations, and solving multi-step problems, for instance, such as those on the US Medical Licensing exam, on which LLMs now achieve a passing "}, {"rank": 3, "score": 0.550061, "source": "dense+rrf", "rrf_score": 0.02878726010616578, "rerank_score": 0.22530162410001803, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 0, "text_snippet": "Orca 2: Teaching Small Language Models How to Reason Arindam Mitra, Luciano Del Corro‚Ä†, Shweti Mahajan‚Ä†, Andres Codas‚Ä° Clarisse Simoes‚Ä°, Sahaj Agarwal, Xuxi Chen‚àó, Anastasia Razdaibiedina‚àó Erik Jones‚àó, Kriti Aggarwal‚àó, Hamid Palangi, Guoqin"}, {"rank": 4, "score": 28.176664204480435, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.2225090771005154, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 148, "text_snippet": "synthetic data for post-training. While Orca 2 models have not gone through RLHF training for safety, we believe that the use of synthetic data for post-training that has been filtered with various content safety filters could provide anoth"}, {"rank": 5, "score": 0.6551198, "source": "dense+rrf", "rrf_score": 0.031024531024531024, "rerank_score": 0.2219488621931388, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 14, "text_snippet": " the goal is to replicate the outputs of larger, more capable teacher models. While these models can produce content that matches the style of their teachers, they often fall short of their reasoning and comprehension skills [ 13]. While ef"}]}
{"case_index": 340, "query": "From 2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt, what exact term fills [BLANK] in: \"It has been observed that [BLANK] prompting signiÔ¨Åcantly improves model performance across a variety of multi-step reasoning tasks\"?", "gold": "chain-of-thought", "answer": "The exact term is **‚Äúchain-of-thought‚Äù** (as in ‚Äúchain-of-thought prompting‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'chain-of-thought' and is directly supported by the provided context sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 68.848, "llm_ms": 1083.208, "top_contexts": [{"rank": 1, "score": 60.79702748142635, "source": "dense+rrf", "rrf_score": 0.01639344262295082, "rerank_score": 0.41889232386099634, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 4, "text_snippet": ". For example, given the question ‚ÄúIf there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?‚Äù , instead of directly responding with ‚Äú5‚Äù, a language model would be prompted to respond with the entir"}, {"rank": 2, "score": 0.7180644, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.36780357078830844, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 5, "text_snippet": "odel performance across a variety of multi-step reasoning tasks (Wei et al., 2022). In this paper, we introduce a novel decoding strategy called self-consistency to replace the greedy decoding strategy used in chain-of-thought prompting (We"}, {"rank": 3, "score": 0.68251693, "source": "dense+rrf", "rrf_score": 0.02946912242686891, "rerank_score": 0.29501635341504767, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 87, "text_snippet": "rved that it signiÔ¨Åcantly improves accuracy in a range of arithmetic and commonsense reasoning tasks, across four large language models with varying scales. Beyond accuracy gains, self-consistency is also useful for collecting rationales wh"}, {"rank": 4, "score": 0.68888795, "source": "dense+rrf", "rrf_score": 0.03057889822595705, "rerank_score": 0.2890809083611884, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 41, "text_snippet": "The results are shown in Table 2.7Self-consistency improves the arithmetic reasoning performance over all four language models signiÔ¨Åcantly over chain-of-thought prompting. More surprisingly, the gains become more signiÔ¨Åcant when the langua"}, {"rank": 5, "score": 40.03325874184095, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.27334791223775723, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 86, "text_snippet": "ength sequence in recurrent language models. Nye et al. (2021) improve the logical consistency of samples from a System 1 model by adding a System 2-inspired logical reasoning module. In this paper we focus on a slightly different notion of"}]}
{"case_index": 341, "query": "From 2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt, what exact term fills [BLANK] in: \"2.1 Long Retriever The [BLANK] RAG framework employs smaller retrieval units and prioritizes retrieving the exact fine- grained short context containing the answer.\"?", "gold": "traditional", "answer": "The exact term is **‚Äútraditional‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context 1 explicitly states: 'The traditional RAG framework employs smaller retrieval units...' matching the expected blank.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 75.555, "llm_ms": 1245.153, "top_contexts": [{"rank": 1, "score": 0.78470004, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.416862346980281, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 21, "text_snippet": "operates on long retrieval units, with only a few (typically fewer than 10) being fed into the reader. An illustrative example is shown in Figure 2. 2.1 Long Retriever The traditional RAG framework employs smaller retrieval units and priori"}, {"rank": 2, "score": 0.7224237, "source": "dense+rrf", "rrf_score": 0.0315136476426799, "rerank_score": 0.2669308774886485, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 0, "text_snippet": "LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs ‚ô†Ziyan Jiang,‚ô†Xueguang Ma,‚ô†Wenhu Chen ‚ô†University of Waterloo ziyanjiang528@gmail.com ,{x93ma ,wenhuchen}@uwaterloo.ca Project Website: https://tiger-ai-lab.github.io/"}, {"rank": 3, "score": 0.71764195, "source": "dense+rrf", "rrf_score": 0.031024531024531024, "rerank_score": 0.25564873198983007, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 20, "text_snippet": "ontext methods evolve, the performance of LongRAG will continue to improve. Therefore, we believe the modern RAG systems should re-consider the granularity of their retrieval units to exploit the advantages of the current long-context LLMs."}, {"rank": 4, "score": 0.6722548, "source": "dense+rrf", "rrf_score": 0.027783137179239824, "rerank_score": 0.23888996076917646, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 5, "text_snippet": "ather than chunking them into smaller units. By doing so, we achieve an F1 score of 25.9% on Qasper (previously 22.5%) and 57.5% on MultiFieldQA-en (previously 51.2%). Our study offers insights into the future roadmap for combining RAG with"}, {"rank": 5, "score": 31.774346699364234, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.22766143556401955, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 79, "text_snippet": "on the long-context understanding ability of LLMs, and we still have a long way to go in harnessing open-source LLMs within our framework. 10  4 Related Work 4.1 Retrieval-Augmented Generation. Augmenting language models with information re"}]}
{"case_index": 342, "query": "From 2302.13971__llama_open_and_efficient_foundation_language_models.txt, what exact term fills [BLANK] in: \"We train large [BLANK] on a large quantity of textual data using a standard optimizer.\"?", "gold": "transformers", "answer": "The exact term is **‚Äútransformers‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context explicitly states: 'We train large transformers on a large quantity of textual data using a standard optimizer.' Model answer matches expected term.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 63.921, "llm_ms": 1245.526, "top_contexts": [{"rank": 1, "score": 0.5710784, "source": "dense+rrf", "rrf_score": 0.030679156908665108, "rerank_score": 0.2641017576164592, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 9, "text_snippet": "v1 [cs.CL] 27 Feb 2023  2 Approach Our training approach is similar to the methods described in previous work (Brown et al., 2020; Chowdhery et al., 2022), and is inspired by the Chinchilla scaling laws (Hoffmann et al., 2022). We train lar"}, {"rank": 2, "score": 0.5811068, "source": "dense+rrf", "rrf_score": 0.030309988518943745, "rerank_score": 0.2385549513889294, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 0, "text_snippet": "LLaMA: Open and EfÔ¨Åcient Foundation Language Models Hugo Touvron‚àó, Thibaut Lavril‚àó, Gautier Izacard‚àó, Xavier Martinet Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi√®re, Naman Goyal Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand "}, {"rank": 3, "score": 24.399793314353076, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.2257755822830453, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 29, "text_snippet": "t results on a total of 20 benchmarks: ‚Ä¢Zero-shot. We provide a textual description of the task and a test example. The model either provides an answer using open-ended generation, or ranks the proposed answers. ‚Ä¢Few-shot. We provide a few "}, {"rank": 4, "score": 20.300094092714946, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.22324310849253132, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 137, "text_snippet": "periment highlights the potential of specializing Orca 2 models for specific tasks using synthetic data generated with prompt erasing . 20  7 Limitations Orca 2, built upon the LLaMA 2 model family, retains many of its limitations, as well "}, {"rank": 5, "score": 0.57664824, "source": "dense+rrf", "rrf_score": 0.028594771241830064, "rerank_score": 0.20388498785722137, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 1, "text_snippet": "ow that it is possible to train state-of-the-art models using publicly avail- able datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and L"}]}
{"case_index": 343, "query": "From 2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt, what exact term fills [BLANK] in: \"On the contrary, the reader only needs to extract answers from 1arXiv:2406.15319v3 [cs.CL] 1 Sep 2024 [BLANK] RAG Retrieval Ranker Reader LongRAG Long Retrieval Long Reader Figure 1: Traditional RAG vs.\"?", "gold": "traditional", "answer": "The exact term that fills **[BLANK]** is:\n\n**Traditional RAG**", "answer_score": 0.06, "answer_correctness": 0.0, "answer_groundedness": 0.2, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 0.2, "abstained": false, "should_abstain": false, "notes": "Expected blank is the single word 'traditional'. Model answered 'Traditional RAG' (extra word), so it does not match the exact term. Context shows the heading 'Traditional RAG' immediately after the blank location, supporting that the missing term is 'Traditional'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 42.779, "llm_ms": 1111.908, "top_contexts": [{"rank": 1, "score": 0.80292773, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.47567939923328495, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 7, "text_snippet": "assive corpus with up to tens of millions of information units). Subsequently, the retrieved units are passed to the reader to generate the final response. On the contrary, the reader only needs to extract answers from 1arXiv:2406.15319v3 ["}, {"rank": 2, "score": 0.7092364, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.2710900506411287, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 21, "text_snippet": "operates on long retrieval units, with only a few (typically fewer than 10) being fed into the reader. An illustrative example is shown in Figure 2. 2.1 Long Retriever The traditional RAG framework employs smaller retrieval units and priori"}, {"rank": 3, "score": 0.70715964, "source": "dense+rrf", "rrf_score": 0.03125, "rerank_score": 0.22599010622940968, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 20, "text_snippet": "ontext methods evolve, the performance of LongRAG will continue to improve. Therefore, we believe the modern RAG systems should re-consider the granularity of their retrieval units to exploit the advantages of the current long-context LLMs."}, {"rank": 4, "score": 35.348909688439846, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.2227704092597127, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 13, "text_snippet": "ybysearching through all the long retrieval units in the corpus. Only a few top retrieval units (1 to 8 retrieval units in the four datasets we tested on), without re-ranking, are used for the next step. Compared to retrieving hundreds of s"}, {"rank": 5, "score": 0.68509555, "source": "dense+rrf", "rrf_score": 0.03057889822595705, "rerank_score": 0.22246265424533881, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 0, "text_snippet": "LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs ‚ô†Ziyan Jiang,‚ô†Xueguang Ma,‚ô†Wenhu Chen ‚ô†University of Waterloo ziyanjiang528@gmail.com ,{x93ma ,wenhuchen}@uwaterloo.ca Project Website: https://tiger-ai-lab.github.io/"}]}
{"case_index": 344, "query": "From 2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt, what exact term fills [BLANK] in: \"### [BLANK](GPT-3.5-Turbo) : John and Mark will likely think that the ball is in the box, since that is where John left it before he left for work.\"?", "gold": "chatgpt", "answer": "The exact term is **ChatGPT**, i.e.:\n\n**\"### ChatGPT(GPT-3.5-Turbo) : John and Mark will likely think that the ball is in the box, since that is where John left it before he left for work.\"**", "answer_score": 0.6499999999999999, "answer_correctness": 0.5, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.5, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context shows the heading as \"### ChatGPT(GPT-3.5-Turbo)\". Model answered \"ChatGPT\" (correct concept) but expected exact term is lowercase \"chatgpt\", so exact-match is off by case.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 144.349, "llm_ms": 1651.552, "top_contexts": [{"rank": 1, "score": 0.7190555, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.44047081504193786, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 10, "text_snippet": "ve still been in the box. 4. Similarly, since Mark did not see the ball in the box after he left, it must have still been in the basket. Therefore, the ball is still in the box. ### ChatGPT(GPT-3.5-Turbo) : John and Mark will likely think t"}, {"rank": 2, "score": 0.5915509, "source": "dense+rrf", "rrf_score": 0.031754032258064516, "rerank_score": 0.33804199961031406, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 7, "text_snippet": "John and Mark have not seen each other‚Äôs actions. Therefore, they only know what happened while they were in the room. John only saw the ball in the box before he left for work. Mark only saw the ball in the basket before he left for school"}, {"rank": 3, "score": 0.6388919, "source": "dense+rrf", "rrf_score": 0.031754032258064516, "rerank_score": 0.30914205414965323, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 6, "text_snippet": "then leaves for school. They both come back together later in the day, and they do not know what happened in the room after each of them left the room. Where do they think the ball is? ### Orca-2-13B : Let‚Äôs analyze the situation step by st"}, {"rank": 4, "score": 0.5647717, "source": "dense+rrf", "rrf_score": 0.03125763125763126, "rerank_score": 0.2771700110339834, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 9, "text_snippet": " man has been told by someone else (probably his friend) that he cannot have any more alcohol at this establishment. ### LLaMA2-Chat-13B : This is a classic lateral thinking puzzle. The solution is that the ball is still in the box. Here‚Äôs "}, {"rank": 5, "score": 0.6294706, "source": "dense+rrf", "rrf_score": 0.03125763125763126, "rerank_score": 0.23770655119465967, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 5, "text_snippet": "rca 2 models match or surpass all other models including models 5-10x larger. Note that all models are using the same LLaMA-2 base models of the respective size. ‚àówork done while at Microsoft;‚Ä†,‚Ä°denote equal contributions.arXiv:2311.11045v2"}]}
{"case_index": 345, "query": "From 2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt, what exact term fills [BLANK] in: \"Le Denny Zhou Google Research, Brain Team {jasonwei,dennyzhou}@google.com Abstract We explore how generating a chain of thought ‚Äîa series of [BLANK] reasoning steps‚ÄîsigniÔ¨Åcantly improves the ability of large language models to perform complex reasoning.\"?", "gold": "intermediate", "answer": "The exact term is **‚Äúintermediate‚Äù**, as in: ‚Äúa series of **intermediate** reasoning steps‚Äù.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'intermediate' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 104.746, "llm_ms": 1428.385, "top_contexts": [{"rank": 1, "score": 0.8193825, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.5474561000754208, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 0, "text_snippet": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models Jason Wei Xuezhi Wang Dale Schuurmans Maarten Bosma Brian Ichter Fei Xia Ed H. Chi Quoc V . Le Denny Zhou Google Research, Brain Team {jasonwei,dennyzhou}@google.com Abst"}, {"rank": 2, "score": 0.69886035, "source": "dense+rrf", "rrf_score": 0.030017921146953404, "rerank_score": 0.36660273291355094, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 0, "text_snippet": "Published as a conference paper at ICLR 2023 SELF-CONSISTENCY IMPROVES CHAIN OF THOUGHT REASONING IN LANGUAGE MODELS Xuezhi Wang‚Ä†‚Ä°Jason Wei‚Ä†Dale Schuurmans‚Ä†Quoc Le‚Ä†Ed H. Chi‚Ä† Sharan Narang‚Ä†Aakanksha Chowdhery‚Ä†Denny Zhou‚Ä†¬ß ‚Ä†Google Research, "}, {"rank": 3, "score": 0.69312274, "source": "dense+rrf", "rrf_score": 0.029571646010002173, "rerank_score": 0.25432926669088396, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 120, "text_snippet": "Zhou. 2022. Self- consistency improves chain of thought reasoning in language models. CoRR , abs/2203.11171. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicit"}, {"rank": 4, "score": 50.93037641964235, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.2420045685118631, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 133, "text_snippet": " Proceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence , 32, Apr. 2018. URL https: //ojs.aaai.org/index.php/AAAI/article/view/12340 . Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and"}, {"rank": 5, "score": 53.249341511860784, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.24163891759997308, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 114, "text_snippet": "-augmented ensembles in language models. arXiv preprint arXiv:2207.00747 , 2022b. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models"}]}
{"case_index": 346, "query": "From 2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt, what exact term fills [BLANK] in: \"We also provide [BLANK] results with a 7B, 14B models trained for 4.8T tokens, called phi- 3-small ,phi-3-medium , both significantly more capable than phi-3-mini (e.g., respectively 75%, 78% on MMLU, and 8.7, 8.9 on MT-bench).\"?", "gold": "parameter-scaling", "answer": "The exact term that fills **[BLANK]** is **‚Äúparameter-scaling‚Äù**, as in:\n\n‚ÄúWe also provide **parameter-scaling** results with a 7B, 14B models trained for 4.8T tokens, called phi-3-small, phi-3-medium‚Ä¶‚Äù", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'parameter-scaling' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 115.184, "llm_ms": 1685.144, "top_contexts": [{"rank": 1, "score": 0.731061, "source": "dense+rrf", "rrf_score": 0.032266458495966696, "rerank_score": 0.49443006442927223, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 1, "text_snippet": "hone. Our training dataset is a scaled-up version of the one used for phi-2 , composed of heavily filtered publicly available web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also pr"}, {"rank": 2, "score": 0.75927275, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.35144495800318787, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 0, "text_snippet": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone Microsoft Abstract We introduce phi-3-mini , a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both"}, {"rank": 3, "score": 0.67920554, "source": "dense+rrf", "rrf_score": 0.029273504273504274, "rerank_score": 0.2731159427671208, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 21, "text_snippet": "s an example, the result of a game in premier league in a particular day might be good training data for frontier models, but we need to remove such information to leave more model capacity for ‚Äúreasoning‚Äù for the mini size models. We compa"}, {"rank": 4, "score": 0.6860653, "source": "dense+rrf", "rrf_score": 0.029437229437229435, "rerank_score": 0.26906812387134915, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 22, "text_snippet": "trained on the same data for slightly more epochs (4.8T tokens total as for phi-3-small . The model has 40 heads and 40 layers, with embedding dimension 5120. We observe that some benchmarks improve much less from 7B to 14B than they do fro"}, {"rank": 5, "score": 0.77123606, "source": "dense+rrf", "rrf_score": 0.032266458495966696, "rerank_score": 0.2656829196355037, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 6, "text_snippet": "7B parameters), matched the performance of models 25 times larger trained on regular data. In this report we present a new model, phi-3-mini (3.8B parameters), trained for 3.3T tokens on larger and more advanced versions of the datasets use"}]}
{"case_index": 347, "query": "From 2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt, what exact term fills [BLANK] in: \"In this work, we present ReAct , a general paradigm to combine [BLANK] and acting with language models for solving diverse language reasoning and decision making tasks (Figure 1).\"?", "gold": "reasoning", "answer": "The exact term that fills **[BLANK]** is **‚Äúreasoning‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'reasoning' and is directly supported by the provided context sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 70.129, "llm_ms": 1190.738, "top_contexts": [{"rank": 1, "score": 0.7959081, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.45433103988881524, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 19, "text_snippet": "udies on how reasoning and acting can be combined in a synergistic manner for general task solving, and if such a combination can bring systematic beneÔ¨Åts compared to reasoning or acting alone. In this work, we present ReAct , a general par"}, {"rank": 2, "score": 0.7335513, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.31311236410319043, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 24, "text_snippet": "o understand the decision basis of model actions. To summarize, our key contributions are the following: (1) we introduce ReAct , a novel prompt- based paradigm to synergize reasoning and acting in language models for general task solving; "}, {"rank": 3, "score": 0.72746056, "source": "dense+rrf", "rrf_score": 0.03149801587301587, "rerank_score": 0.291433078762959, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 88, "text_snippet": " the development of versatile and generalist agents like Reed et al. (2022). 6 C ONCLUSION We have proposed ReAct ‚Äì a simple yet effective method for synergizing reasoning and acting in large language models. Through a diverse set of experi"}, {"rank": 4, "score": 0.6799897, "source": "dense+rrf", "rrf_score": 0.027417840375586856, "rerank_score": 0.23056226885701112, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 26, "text_snippet": "bining it with complementary paradigms like reinforcement learning could further unlock the potential of large language models. 2REAC T: SYNERGIZING REASONING +AC TING Consider a general setup of an agent interacting with an environment for"}, {"rank": 5, "score": 31.335809740983535, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.22935109305661372, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 60, "text_snippet": " be due to the sub-optimal greedy decoding procedure, and future work using better decoding (e.g. beam search) might help address this issue. 6  Published as a conference paper at ICLR 2023 8b 62b 540b size051015202530HotpotQA EMlearning = "}]}
{"case_index": 348, "query": "From 2005.14165__language_models_are_few_shot_learners.txt, what exact term fills [BLANK] in: \"There is evidence that suggests that the [BLANK] achieved under this paradigm can be poor because th\"?", "gold": "generalization", "answer": "The exact term is **‚Äúgeneralization‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'generalization' and is directly supported by Context 1/2 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 57.74, "llm_ms": 1015.63, "top_contexts": [{"rank": 1, "score": 0.55380285, "source": "dense+rrf", "rrf_score": 0.031099324975891997, "rerank_score": 0.32422347900574083, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 15, "text_snippet": "with the expressiveness of the model and the narrowness of the training distribution. This can create problems for the pre-training plus Ô¨Åne-tuning paradigm, where models are designed to be large to absorb information during pre-training, b"}, {"rank": 2, "score": 0.61219513, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.2251250851300995, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 16, "text_snippet": "neralization achieved under this paradigm can be poor because the model is overly speciÔ¨Åc to the training distribution and does not generalize well outside it [YdC+19,MPL19 ]. Thus, the performance of Ô¨Åne-tuned models on speciÔ¨Åc benchmarks,"}, {"rank": 3, "score": 0.5471944, "source": "dense+rrf", "rrf_score": 0.02871794871794872, "rerank_score": 0.22471859294974142, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 25, "text_snippet": "skills and tasks within the parameters of the model, it is plausible that in-context learning abilities might show similarly strong gains with scale. 1In the context of language models this has sometimes been called ‚Äúzero-shot transfer‚Äù, bu"}, {"rank": 4, "score": 22.23511073349716, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.22196007783301736, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 48, "text_snippet": " The panels above show four methods for performing a task with a language model ‚Äì Ô¨Åne-tuning is the traditional method, whereas zero-, one-, and few-shot, which we study in this work, require the model to perform the task with only forward "}, {"rank": 5, "score": 20.760161180586714, "source": "dense+rrf", "rrf_score": 0.014285714285714285, "rerank_score": 0.1974065257027066, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 93, "text_snippet": "mitations While our approach enables LMs to learn how to use a variety of tools in a self-supervised way, there are some clear limitations to what can be achieved with our method in its current form. One such limi- tation is the inability o"}]}
{"case_index": 349, "query": "From 2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt, what exact term fills [BLANK] in: \"This kind of [BLANK] design, with a ‚Äúheavy‚Äù retriever and a ‚Äúlight‚Äù reader, puts too much pressure on the retriever.\"?", "gold": "imbalanced", "answer": "The exact term is **‚Äúimbalanced‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'imbalanced' and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 87.062, "llm_ms": 1086.559, "top_contexts": [{"rank": 1, "score": 62.35860532919358, "source": "dense+rrf", "rrf_score": 0.01639344262295082, "rerank_score": 0.30753422022684973, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 9, "text_snippet": " performance. (Down) QA performance compared with other methods on the NQ dataset and the HotpotQA dataset. these retrievals, which is a fairly easy task. This kind of imbalanced design, with a ‚Äúheavy‚Äù retriever and a ‚Äúlight‚Äù reader, puts t"}, {"rank": 2, "score": 0.6392597, "source": "dense+rrf", "rrf_score": 0.03125763125763126, "rerank_score": 0.28921665496752325, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 21, "text_snippet": "operates on long retrieval units, with only a few (typically fewer than 10) being fed into the reader. An illustrative example is shown in Figure 2. 2.1 Long Retriever The traditional RAG framework employs smaller retrieval units and priori"}, {"rank": 3, "score": 0.6159462, "source": "dense+rrf", "rrf_score": 0.02919863597612958, "rerank_score": 0.2663059440132941, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 79, "text_snippet": "on the long-context understanding ability of LLMs, and we still have a long way to go in harnessing open-source LLMs within our framework. 10  4 Related Work 4.1 Retrieval-Augmented Generation. Augmenting language models with information re"}, {"rank": 4, "score": 41.58231739739273, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.251030668120928, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 0, "text_snippet": "LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs ‚ô†Ziyan Jiang,‚ô†Xueguang Ma,‚ô†Wenhu Chen ‚ô†University of Waterloo ziyanjiang528@gmail.com ,{x93ma ,wenhuchen}@uwaterloo.ca Project Website: https://tiger-ai-lab.github.io/"}, {"rank": 5, "score": 0.5926843, "source": "dense+rrf", "rrf_score": 0.028438886647841874, "rerank_score": 0.2405998320849284, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 20, "text_snippet": "ontext methods evolve, the performance of LongRAG will continue to improve. Therefore, we believe the modern RAG systems should re-consider the granularity of their retrieval units to exploit the advantages of the current long-context LLMs."}]}
{"case_index": 350, "query": "From 2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt, what exact term fills [BLANK] in: \"Scaling up the size of lan- guage models has been shown to confer a range of beneÔ¨Åts, such as improved [BLANK] and sample efÔ¨Åciency (Ka- plan et al., 2020; Brown et al., 2020, inter alia ).\"?", "gold": "performance", "answer": "The exact term is **‚Äúperformance‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Model answer matches expected term 'performance' and is directly supported by the provided context sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 68.9, "llm_ms": 1042.839, "top_contexts": [{"rank": 1, "score": 0.7134458, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.5040721105027098, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 6, "text_snippet": "recently been revolutionized by language models (Peters et al., 2018; Devlin et al., 2019; Brown et al., 2020, inter alia ). Scaling up the size of lan- guage models has been shown to confer a range of beneÔ¨Åts, such as improved performance "}, {"rank": 2, "score": 0.6892863, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.4161696286884855, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 124, "text_snippet": " chain-of-thought prompting? The Ô¨Ånding that successful chain-of-thought reasoning predictably emerges only at certain model scales is intriguing. Scaling up language models has been shown to confer beneÔ¨Åts such as improved performance and "}, {"rank": 3, "score": 40.02665274370335, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.27655935985258706, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 189, "text_snippet": "ing confers various beneÔ¨Åts in a range of scenarios. As examples, it has been shown that natural language intermediate steps can improve performance (Zaidan et al., 2007; Yao et al., 2021; Hase and Bansal, 2022; Gu et al., 2022), improve ro"}, {"rank": 4, "score": 42.112806053697746, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.26651515300480977, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 16, "text_snippet": " for arriving at the answer (and also, solutions/explanations typically come after the Ô¨Ånal answer (Narang et al., 2020; Wiegreffe et al., 2022; Lampinen et al., 2022, inter alia )). Chain-of-thought prompting has several attractive propert"}, {"rank": 5, "score": 0.6441885, "source": "dense+rrf", "rrf_score": 0.029083245521601686, "rerank_score": 0.2505936411483869, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 5, "text_snippet": " seminal work to over hundred billion parameters (Brown et al., 2020; Radford et al., 2019) in the last two years which has led to models that do very well on a wide array of tasks in a zero or few-shot formulation. Increasing model size pr"}]}
{"case_index": 351, "query": "From 2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt, what exact term fills [BLANK] in: \"Index Terms ‚ÄîLarge language model, [BLANK] gen- eration, natural language processing\"?", "gold": "retrieval-augmented", "answer": "The exact term is **‚Äúretrieval-augmented‚Äù**, as in **‚Äúretrieval-augmented generation.‚Äù**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'retrieval-augmented' and is directly supported by the provided context showing 'Index Terms ‚ÄîLarge language model, retrieval-augmented generation, natural language processing'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 39.504, "llm_ms": 1534.269, "top_contexts": [{"rank": 1, "score": 0.6362647, "source": "dense+rrf", "rrf_score": 0.032266458495966696, "rerank_score": 0.2948679651689645, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 3, "text_snippet": "augmentation techniques. The paper highlights the state-of-the- art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-"}, {"rank": 2, "score": 0.67632586, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.23306314205804607, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 4, "text_snippet": "l, retrieval-augmented gen- eration, natural language processing, information retrieval I. I NTRODUCTION LARGE language models (LLMs) have achieved remark- able success, though they still face significant limitations, especially in domain-s"}, {"rank": 3, "score": 0.61177015, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.1919071688902528, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 79, "text_snippet": "on the long-context understanding ability of LLMs, and we still have a long way to go in harnessing open-source LLMs within our framework. 10  4 Related Work 4.1 Retrieval-Augmented Generation. Augmenting language models with information re"}, {"rank": 4, "score": 0.616766, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.18797698848532854, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 167, "text_snippet": "4] P. Lewis, E. Perez, A. Piktus, F. Petroni, V . Karpukhin, N. Goyal, H. K ¬®uttler, M. Lewis, W.-t. Yih, T. Rockt ¬®aschel et al. , ‚ÄúRetrieval- augmented generation for knowledge-intensive nlp tasks,‚Äù Advances in Neural Information Processi"}, {"rank": 5, "score": 23.142665559747776, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.18793036843870847, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 111, "text_snippet": "val augmented language models. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea Madotto, and Pascale Fung. 2022. Survey of hallu- cination in natural language generation. ACM Com- puting Surv"}]}
{"case_index": 352, "query": "From 1706.03762__attention_is_all_you_need.txt, what exact term fills [BLANK] in: \"3.1 Encoder and Decoder Stacks Encoder: The encoder is composed of a stack of N= 6 [BLANK] layers.\"?", "gold": "identical", "answer": "The exact term is **‚Äúidentical‚Äù** (i.e., ‚Äúa stack of N = 6 identical layers‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'identical' and is directly supported by the provided context quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 94.567, "llm_ms": 1178.929, "top_contexts": [{"rank": 1, "score": 0.64959866, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.3381131923602311, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 16, "text_snippet": "ransformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively. 3.1 Encoder and Decoder Stacks "}, {"rank": 2, "score": 0.6605105, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.25612895937252156, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 18, "text_snippet": "s also composed of a stack of N= 6identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the enc"}, {"rank": 3, "score": 0.5921621, "source": "dense+rrf", "rrf_score": 0.031009615384615385, "rerank_score": 0.2525546278974921, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 28, "text_snippet": "the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9]. ‚Ä¢The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries c"}, {"rank": 4, "score": 0.6176901, "source": "dense+rrf", "rrf_score": 0.031746031746031744, "rerank_score": 0.24894058029444932, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 17, "text_snippet": "ted feed-forward network. We employ a residual connection [ 11] around each of the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is LayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function"}, {"rank": 5, "score": 29.702238544712788, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.24571637394524296, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 27, "text_snippet": "ead, the total computational cost is similar to that of single-head attention with full dimensionality. 3.2.3 Applications of Attention in our Model The Transformer uses multi-head attention in three different ways: ‚Ä¢In \"encoder-decoder att"}]}
{"case_index": 353, "query": "From 2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt, what exact term fills [BLANK] in: \"We [BLANK] probe InstructGPT‚Äôs capabilities, and Ô¨Ånd that it is able to follow instructions for summarizing code, answer questions about code, and sometimes follows instructions\"?", "gold": "qualitatively", "answer": "The exact term is **‚Äúqualitatively‚Äù**:\n\n‚ÄúWe **qualitatively** probe InstructGPT‚Äôs capabilities, and Ô¨Ånd that it is able to follow instructions for summarizing code, answer questions about code, and sometimes follows instructions‚Äù", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'qualitatively' and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 138.092, "llm_ms": 1319.347, "top_contexts": [{"rank": 1, "score": 0.68924284, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3951548918970429, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 23, "text_snippet": "e than our SFT baseline, and labelers signiÔ¨Åcantly prefer InstructGPT to these models (InstructGPT has a 73.4 ¬±2%winrate vs. our baseline, compared to 26.8 ¬±2%and 29.8¬±2%for our version of T0 and FLAN, respectively). InstructGPT models show"}, {"rank": 2, "score": 0.6506623, "source": "dense+rrf", "rrf_score": 0.03128054740957967, "rerank_score": 0.29599981865270225, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 126, "text_snippet": "ow some qualitative examples in Figure 8. Our 175B PPO-ptx model is able to reliably answers questions about code, and can also follow instructions in other languages; however, we notice that it often produces an output in English even when"}, {"rank": 3, "score": 0.6608603, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.2800472460652921, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 24, "text_snippet": "answer questions about code, and sometimes follows instructions in different languages, despite these instructions being very rare in the Ô¨Åne-tuning distribution. In contrast, GPT-3 can perform these tasks but requires more careful promptin"}, {"rank": 4, "score": 0.65413606, "source": "dense+rrf", "rrf_score": 0.029910714285714284, "rerank_score": 0.2615735232830167, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 136, "text_snippet": "least for our customers‚Äô natural language task distribution. 2.We‚Äôve seen some evidence that InstructGPT generalizes ‚Äòfollowing instructions‚Äô to settings that we don‚Äôt supervise it in, for example on non-English language tasks and code-rela"}, {"rank": 5, "score": 0.61381847, "source": "dense+rrf", "rrf_score": 0.029418126757516764, "rerank_score": 0.24921575704764226, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 360, "text_snippet": "5B InstructGPT (PPO-ptx) models. We sample at T= 1for InstructGPT, and use T= 0.7for GPT-3, since GPT-3 performs poorly at high temperatures (this slightly disadvantages InstructGPT). In Figure 42, we show the full French sample from Figure"}]}
{"case_index": 354, "query": "From 2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt, what exact term fills [BLANK] in: \"o generate [BLANK]Ô¨Åc actions or plans, and then use a controller to choose or execute them.\"?", "gold": "domain-speci", "answer": "The exact term is **‚Äúdomain-speciÔ¨Åc‚Äù**.", "answer_score": 0.3, "answer_correctness": 0.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Expected blank fill is the partial string 'domain-speci', but model answered the full word 'domain-specific'. Context supports 'domain-specific' phrase, but it does not match the exact expected term.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 63.978, "llm_ms": 1030.677, "top_contexts": [{"rank": 1, "score": 0.65953106, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.34289674651671326, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 18, "text_snippet": "o generate domain-speciÔ¨Åc actions or plans, and then use a controller to choose or execute them. However, they do not employ language models to reason abstractly about high-level goals or maintain a working memory to support acting, barring"}, {"rank": 2, "score": 0.59543973, "source": "dense+rrf", "rrf_score": 0.031754032258064516, "rerank_score": 0.26965816587629676, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 17, "text_snippet": " fact hallucination and error propagation over the reasoning process (Figure 1 (1b)). On the other hand, recent work has explored the use of pre-trained language models for planning and acting in interactive environments (Ahn et al., 2022; "}, {"rank": 3, "score": 0.59353435, "source": "dense+rrf", "rrf_score": 0.031009615384615385, "rerank_score": 0.23457923575042477, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 19, "text_snippet": "udies on how reasoning and acting can be combined in a synergistic manner for general task solving, and if such a combination can bring systematic beneÔ¨Åts compared to reasoning or acting alone. In this work, we present ReAct , a general par"}, {"rank": 4, "score": 0.56903726, "source": "dense+rrf", "rrf_score": 0.029437229437229435, "rerank_score": 0.22311760054412283, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 24, "text_snippet": "o understand the decision basis of model actions. To summarize, our key contributions are the following: (1) we introduce ReAct , a novel prompt- based paradigm to synergize reasoning and acting in language models for general task solving; "}, {"rank": 5, "score": 23.332247505790605, "source": "dense+rrf", "rrf_score": 0.0136986301369863, "rerank_score": 0.21497541640437587, "doc_id": "2203.11171__self_consistency_improves_chain_of_thought_reasoning_in_language_models.txt", "chunk_id": 91, "text_snippet": "ults make use of LaMDA-137B and PaLM-540B that are not publicly available, we provide the exact input prompts for all tasks in Appendix A.3 (and note that we do not perform any Ô¨Ånetuning and only apply prompting to off-the-shelf language mo"}]}
{"case_index": 355, "query": "From 1706.03762__attention_is_all_you_need.txt, what exact term fills [BLANK] in: \"3.2.1 Scaled [BLANK] Attention We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2).\"?", "gold": "dot-product", "answer": "The exact term is **‚ÄúDot-Product‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "The blank is 'dot-product' as in 'Scaled Dot-Product Attention'; answer matches context and expected term (case-insensitive).", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 67.58, "llm_ms": 1097.978, "top_contexts": [{"rank": 1, "score": 0.69807845, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3141289974225821, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 20, "text_snippet": "t Attention  Multi-Head Attention Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel. of the values, where the weight assigned to each value is computed by a "}, {"rank": 2, "score": 32.16968799673107, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.23384847053042893, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 29, "text_snippet": "n layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We"}, {"rank": 3, "score": 0.6203728, "source": "dense+rrf", "rrf_score": 0.031746031746031744, "rerank_score": 0.18149073829698928, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 19, "text_snippet": "sequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position ican depend only on the known outputs at positions less than i. 3.2 Attention An attention "}, {"rank": 4, "score": 0.5803095, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.17218465978192618, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 69, "text_snippet": " dilate (Ding et al., 2023), block sparse (Qiu et al., 2020), stride sparse (Child et al., 2019) for comparison. ‚Äò cro. heads / layers ‚Äô means to swap different attention settings across attention heads or sequential layers . Taking S2-Attn"}, {"rank": 5, "score": 0.61189437, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.1718956031378544, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 37, "text_snippet": "dimension. 3.2 S HIFTED SPARSE ATTENTION Standard self-attention costs O(n2)computations, making LLMs on long sequences high memory cost and slow. To avoid this issue during training, we propose Shifted Sparse Attention (S2-Attn), as shown "}]}
{"case_index": 356, "query": "From 2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt, what exact term fills [BLANK] in: \"We Ô¨Årst split each of the [BLANK] into text passages of equal lengths as the basic retrieval units3and getM total passages in our corpus C={p1,\"?", "gold": "documents", "answer": "The exact term that fills **[BLANK]** is:\n\n**documents**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'documents' and is directly supported by the provided context sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 100.666, "llm_ms": 1276.578, "top_contexts": [{"rank": 1, "score": 0.569714, "source": "dense+rrf", "rrf_score": 0.02889344262295082, "rerank_score": 0.34949197794496656, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 14, "text_snippet": "Who Ô¨Årst voiced Meg on Family Guy? ‚Äù or ‚Äú Where was the 8th Dalai Lama born? ‚Äù, a system is required to answer it using a large corpus of diversiÔ¨Åed topics. More speciÔ¨Åcally, we assumethe extractive QA setting, in which the answer is restri"}, {"rank": 2, "score": 0.6945009, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.29510052965594985, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 15, "text_snippet": "sic retrieval units3and getM total passages in our corpus C={p1,p2,...,p M}, where each passage pican be viewed as a sequence of tokensw(i) 1,w(i) 2,¬∑¬∑¬∑,w(i) |pi|. Given a question q, the task is to Ô¨Ånd a span w(i) s,w(i) s+1,¬∑¬∑¬∑,w(i) efrom"}, {"rank": 3, "score": 0.63714725, "source": "dense+rrf", "rrf_score": 0.031009615384615385, "rerank_score": 0.23404089782409157, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 32, "text_snippet": "ple, disjoint text blocks of 100 words as passages , serving as our basic retrieval units, following (Wang et al., 2019), which results in 21,015,324 passages in the end.5 Each passage is also prepended with the title of the Wikipedia artic"}, {"rank": 4, "score": 0.6019874, "source": "dense+rrf", "rrf_score": 0.029411764705882353, "rerank_score": 0.2295248108138161, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 31, "text_snippet": "a Pre-processing Following (Lee et al., 2019), we use the English Wikipedia dump from Dec. 20, 2018 as the source documents for answering questions. We Ô¨Årst apply the pre-processing code released in DrQA (Chen et al., 2017) to extract the c"}, {"rank": 5, "score": 0.64253616, "source": "dense+rrf", "rrf_score": 0.030776515151515152, "rerank_score": 0.2154146485425376, "doc_id": "2004.04906__dense_passage_retrieval_for_open_domain_question_answering.txt", "chunk_id": 17, "text_snippet": "rieval accuracy , which is the fraction of ques- tions for whichCFcontains a span that answers the question. 3 Dense Passage Retriever (DPR) We focus our research in this work on improv- ing the retrieval component in open-domain QA. Given "}]}
{"case_index": 357, "query": "From 2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt, what exact term fills [BLANK] in: \"This behavior cloning has been shown to be very effective in [BLANK] the style of the teacher model.\"?", "gold": "mimicking", "answer": "The exact term is **‚Äúmimicking‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term and is directly supported by Context 1 ('effective in mimicking the style of the teacher model').", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 64.096, "llm_ms": 1046.88, "top_contexts": [{"rank": 1, "score": 0.6716715, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.32981532869409963, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 23, "text_snippet": "rations [ 7] and give models enhanced zero-shot and reasoning abilities [62]. Several studies, including Alpaca [ 55], Vicuna [ 6], WizardLM [ 64], Baize [ 65], and Koala [ 12], have adopted instruction tuning to train smaller ‚Äústudent‚Äù lan"}, {"rank": 2, "score": 28.527673649085404, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.22260675719819548, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 148, "text_snippet": "synthetic data for post-training. While Orca 2 models have not gone through RLHF training for safety, we believe that the use of synthetic data for post-training that has been filtered with various content safety filters could provide anoth"}, {"rank": 3, "score": 0.63808835, "source": "dense+rrf", "rrf_score": 0.031054405392392875, "rerank_score": 0.22195483706671118, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 14, "text_snippet": " the goal is to replicate the outputs of larger, more capable teacher models. While these models can produce content that matches the style of their teachers, they often fall short of their reasoning and comprehension skills [ 13]. While ef"}, {"rank": 4, "score": 22.076988795000016, "source": "dense+rrf", "rrf_score": 0.014084507042253521, "rerank_score": 0.22038151377612736, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 22, "text_snippet": "nstruction Tuning Instruction tuning [ 46,38,62,61] has emerged as a crucial step in training language models. Instruction tuning involves learning from input-output pairs where the input is natural language task description,and the output "}, {"rank": 5, "score": 28.49022677023049, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.21886895598823258, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 24, "text_snippet": "not result in proportional improvement to small model performance when thoroughly evaluated on knowledge-intensive or reasoning-intensive tasks where correctness is not just judged by style. We note that instruction tuning, while very benef"}]}
{"case_index": 358, "query": "From 2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt, what exact term fills [BLANK] in: \"The training retrieval database is made of the same subsets as the training data, in [BLANK] that matc\"?", "gold": "proportion", "answer": "The exact term is **‚Äúproportion‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'proportion' and is directly supported by the provided context snippet.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 59.067, "llm_ms": 1028.658, "top_contexts": [{"rank": 1, "score": 0.6274036, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.3160410283564673, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 19, "text_snippet": "nd multiple languages totalling over 5 trillion tokens (detailed in Table 1). Sequences are sampled from subsets of the training data, with sampling weights given in the right-most column of Table 1. We tokenize the dataset using SentencePi"}, {"rank": 2, "score": 33.116446789488336, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.29859418152499445, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 101, "text_snippet": "he chunked cross-attention mechanism. 4https://github.com/facebookresearch/FiD 13  Improving language models by retrieving from trillions of tokens Figure5jR/e.sc/t.sc/r.sc/o.sc-Ô¨Åttingabaselinetransformer. AnytransformercanbeÔ¨Åne-tunedintoar"}, {"rank": 3, "score": 27.78636018708447, "source": "dense+rrf", "rrf_score": 0.014084507042253521, "rerank_score": 0.2766781801610275, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 199, "text_snippet": " but only trains the new retrieval weights, as explained in ¬ß4.2. Our best R/e.sc/t.sc/r.sc/o.scÔ¨Åt checkpoint has a Wikitext103 valid perplexity 18¬ì46, when retrieving from Wikipedia. We use this R/e.sc/t.sc/r.sc/o.sccheckpoint in Table 4 f"}, {"rank": 4, "score": 32.78158058828615, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.2763770204255826, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 69, "text_snippet": "e stem from their ability to memorise training data, their high training cost, the static nature of their training data (Lazaridou et al., 2021), their tendency of amplifying inherent biases in the training data, and their ability to genera"}, {"rank": 5, "score": 28.411018793223583, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.27623709505244826, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 183, "text_snippet": "sc¬πùê∂ùë¢¬∫ùëóand the chunk ùê∂ùë¢to be relatively well aligned, and assume that they start at the same position. Therefore, when computing C/a.sc¬πùêª¬∏ ùë¢¬îùê∏ùë¢¬∫, we set the distance between the data token ùëñ2¬ª1¬îùëô¬ºof chunkùê∂¬∏ ùë¢and 5https://github.com/earwig/m"}]}
{"case_index": 359, "query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \"2 Related Work There is a long history of pre-training general lan- guage [BLANK], and we brieÔ¨Çy review the most widely-used approaches in this section.\"?", "gold": "representations", "answer": "The exact term is **‚Äúrepresentations‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected blank ('representations') and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 71.318, "llm_ms": 888.863, "top_contexts": [{"rank": 1, "score": 0.6740184, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.42351076676913474, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 11, "text_snippet": "e Ô¨Årst Ô¨Åne- tuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level andtoken-level tasks, outper- forming many task-speciÔ¨Åc architectures. ‚Ä¢ BERT advances the state of the art for elev"}, {"rank": 2, "score": 34.40360388990345, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.27284655327826973, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 109, "text_snippet": "reading books. In Proceedings of the IEEE international conference on computer vision , pages 19‚Äì27. Appendix for ‚ÄúBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding‚Äù We organize the appendix into three section"}, {"rank": 3, "score": 29.956233385819584, "source": "dense+rrf", "rrf_score": 0.014285714285714285, "rerank_score": 0.25814650182175247, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 139, "text_snippet": "Model OutputQuestion Question QuestionFigure 10: Examples of semantic understanding and one-step missing errors that were Ô¨Åxed by scaling PaLM from 62B to 540B. A.2 What is the role of prompt engineering? One of the key considerations of pr"}, {"rank": 4, "score": 0.56960446, "source": "dense+rrf", "rrf_score": 0.029273504273504274, "rerank_score": 0.24612703870374525, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 12, "text_snippet": "ieÔ¨Çy review the most widely-used approaches in this section. 2.1 Unsupervised Feature-based Approaches Learning widely applicable representations of words has been an active area of research for decades, including non-neural (Brown et al., "}, {"rank": 5, "score": 0.58772945, "source": "dense+rrf", "rrf_score": 0.0288981288981289, "rerank_score": 0.24606814521860298, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 10, "text_snippet": "ford et al. (2018), which uses unidirec- tional language models for pre-training, BERT uses masked language models to enable pre- trained deep bidirectional representations. This is also in contrast to Peters et al. (2018a), which uses a sh"}]}
{"case_index": 360, "query": "From 2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt, what exact term fills [BLANK] in: \"Our models generalize to the [BLANK] of ‚Äúheld-out‚Äù labelers that did not produce any train- ing data.\"?", "gold": "preferences", "answer": "The exact term is **‚Äúpreferences‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'preferences' and is directly supported by the provided context sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 108.366, "llm_ms": 958.42, "top_contexts": [{"rank": 1, "score": 0.6327855, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.36129499833626777, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 93, "text_snippet": "e preferences of \"held-out\" labelers that did not produce any train- ing data. Held-out labelers have similar ranking preferences as workers who we used to produce training data (see Figure 3). In particular, according to held-out workers, "}, {"rank": 2, "score": 49.482032885538295, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.3194230765171683, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 20, "text_snippet": "ignment tax‚Äù since our alignment procedure comes at the cost of 3  lower performance on certain tasks that we may care about. We can greatly reduce the performance regressions on these datasets by mixing PPO updates with updates that increa"}, {"rank": 3, "score": 0.62060785, "source": "dense+rrf", "rrf_score": 0.030834914611005692, "rerank_score": 0.2763751947114748, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 21, "text_snippet": " ing data. To test the generalization of our models, we conduct a preliminary experiment with held-out labelers, and Ô¨Ånd that they prefer InstructGPT outputs to outputs from GPT-3 at about the same rate as our training labelers. However, mo"}, {"rank": 4, "score": 39.88273113581902, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.27135875491543343, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 92, "text_snippet": "mpted)SFT PPO-ptx FLAN T0 Model246Likert score Figure 5: Comparing our models with FLAN and T0 in terms of Likert scores on a 1-7 scale, on the InstructGPT prompt distribution. FLAN and T0 perform better than default GPT-3, and comparably w"}, {"rank": 5, "score": 0.55824625, "source": "dense+rrf", "rrf_score": 0.02782608695652174, "rerank_score": 0.2608265952782091, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 12, "text_snippet": "We illustrate this process in Figure 2. This procedure aligns the behavior of GPT-3 to the stated preferences of a speciÔ¨Åc group of people (mostly our labelers and researchers), rather than any broader notion of ‚Äúhuman values‚Äù; we discuss t"}]}
{"case_index": 361, "query": "From 2305.06983__active_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"Most existing retrieval aug- mented LMs employ a [BLANK] setup that only retrieves information once based on the input.\"?", "gold": "retrieve-and-generate", "answer": "The exact term is **‚Äúretrieve-and-generate‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term and is explicitly stated in the provided context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 53.521, "llm_ms": 955.004, "top_contexts": [{"rank": 1, "score": 0.70936555, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.38372851130539914, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 1, "text_snippet": "create factually inaccurate out- put. Augmenting LMs by retrieving informa- tion from external knowledge resources is one promising solution. Most existing retrieval aug- mented LMs employ a retrieve-and-generate setup that only retrieves i"}, {"rank": 2, "score": 0.61411536, "source": "dense+rrf", "rrf_score": 0.02976190476190476, "rerank_score": 0.23943927358718312, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 22, "text_snippet": " generate the complete answer at once y=LM([Dx,x]). 2.3 Active Retrieval Augmented Generation To aid long-form generation with retrieval, we pro- pose active retrieval augmented generation. It is a generic framework that actively decides wh"}, {"rank": 3, "score": 0.6219181, "source": "dense+rrf", "rrf_score": 0.028790389395194696, "rerank_score": 0.23023121718061237, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 5, "text_snippet": "u et al., 2021). Augmenting LMs with retrieval components that look up relevant in- formation from external knowledge resources is a promising direction to address hallucination (Khan- delwal et al., 2020; Izacard et al., 2022). Retrieval a"}, {"rank": 4, "score": 0.636243, "source": "dense+rrf", "rrf_score": 0.02871794871794872, "rerank_score": 0.21960563103355923, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 20, "text_snippet": "trieval augmented LMs is to generate the answer y= [s1,s2, ...,sm] = [w1, w2, ..., w n]containing msentences or ntokens leveraging information retrieved from the corpus. In retrieval augmented LM, the LM typically pairs with a retriever tha"}, {"rank": 5, "score": 0.6215875, "source": "dense+rrf", "rrf_score": 0.030117753623188408, "rerank_score": 0.2172303094490531, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 24, "text_snippet": "t, and the input to LMs is the concatena- tion of the retrieved documents Dqt, the user input x, and the previous generation y<t. We discard previously retrieved documents ‚à™t‚Ä≤<tDqt‚Ä≤and only use the retrieved documents from the current step "}]}
{"case_index": 362, "query": "From 2305.06983__active_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"and previously generated output y<t= [y0, ...,yt‚àí1]: qt=qry(x,y<t), where qry(¬∑)is the query [BLANK] function.\"?", "gold": "formulation", "answer": "The exact term is **‚Äúformulation‚Äù** ‚Äî i.e., ‚Äúthe query **formulation** function.‚Äù", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'formulation' and is directly supported by Context 2 ('query formulation function').", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 56.45, "llm_ms": 1242.072, "top_contexts": [{"rank": 1, "score": 0.6517416, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.38367562923242193, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 22, "text_snippet": " generate the complete answer at once y=LM([Dx,x]). 2.3 Active Retrieval Augmented Generation To aid long-form generation with retrieval, we pro- pose active retrieval augmented generation. It is a generic framework that actively decides wh"}, {"rank": 2, "score": 0.65230167, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.3780105196969557, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 23, "text_snippet": "and previously generated output y<t= [y0, ...,yt‚àí1]: qt=qry(x,y<t), where qry(¬∑)is the query formulation function. At the beginning ( t= 1), the previous generation is empty ( y<1=‚àÖ), and the user input is used as the initial query ( q1=x)."}, {"rank": 3, "score": 0.55836236, "source": "dense+rrf", "rrf_score": 0.03125, "rerank_score": 0.25585798758340006, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 24, "text_snippet": "t, and the input to LMs is the concatena- tion of the retrieved documents Dqt, the user input x, and the previous generation y<t. We discard previously retrieved documents ‚à™t‚Ä≤<tDqt‚Ä≤and only use the retrieved documents from the current step "}, {"rank": 4, "score": 0.5397092, "source": "dense+rrf", "rrf_score": 0.03125763125763126, "rerank_score": 0.22011356754149575, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 21, "text_snippet": "ing when and what to retrieve, we follow exist- ing methods (Ram et al., 2023; Trivedi et al., 2022) to prepend the retrieved documents before the user input to aid future generation for both baselines and our method for fair comparisons: y"}, {"rank": 5, "score": 0.47847986, "source": "dense+rrf", "rrf_score": 0.02967032967032967, "rerank_score": 0.20773768761458572, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 45, "text_snippet": "oaches trigger retrieval every ltokens, where lrepresents the window size. Generated tokens from the previous window are used as the query: qt=yt‚àí1(t‚â•2), yt= [w(t‚àí1)l+1, ..., w tl]. Some existing methods in this category are RETRO (Borgeaud"}]}
{"case_index": 363, "query": "From 2302.13971__llama_open_and_efficient_foundation_language_models.txt, what exact term fills [BLANK] in: \"We train our models on trillions of tokens, and show that it is possible to train [BLANK] models using pu\"?", "gold": "state-of-the-art", "answer": "The exact term is **‚Äústate-of-the-art‚Äù**, i.e.,\n\n‚ÄúWe train our models on trillions of tokens, and show that it is possible to train **state-of-the-art** models using pu‚Ä¶‚Äù", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term and is directly supported by the provided context excerpt.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 89.733, "llm_ms": 1569.206, "top_contexts": [{"rank": 1, "score": 0.56772316, "source": "dense+rrf", "rrf_score": 0.029726775956284153, "rerank_score": 0.3804982287250982, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 0, "text_snippet": "LLaMA: Open and EfÔ¨Åcient Foundation Language Models Hugo Touvron‚àó, Thibaut Lavril‚àó, Gautier Izacard‚àó, Xavier Martinet Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi√®re, Naman Goyal Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand "}, {"rank": 2, "score": 0.5728252, "source": "dense+rrf", "rrf_score": 0.02919863597612958, "rerank_score": 0.2802374634232342, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 1, "text_snippet": "ow that it is possible to train state-of-the-art models using publicly avail- able datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and L"}, {"rank": 3, "score": 0.5722875, "source": "dense+rrf", "rrf_score": 0.029211087420042643, "rerank_score": 0.2572896403214877, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 5, "text_snippet": "at inference. For instance, although Hoffmann et al. (2022) recommends training a 10B model on 200B tokens, we Ô¨Ånd that the performance of a 7B model continues to improve even after 1T tokens. The focus of this work is to train a series of "}, {"rank": 4, "score": 21.91346057030047, "source": "dense+rrf", "rrf_score": 0.014285714285714285, "rerank_score": 0.22580530354239955, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 90, "text_snippet": "3 while being more than 10 √ósmaller, and LLaMA-65B is competitive with Chinchilla-70B and PaLM-540B. Unlike previous studies, we show that it is possible to achieve state-of-the-art performance by training exclusively on publicly available "}, {"rank": 5, "score": 0.58543956, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.20737849045901838, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 85, "text_snippet": "ing. There is a long history of scaling for language models, for both the model and dataset sizes. Brants et al. (2007) showed the beneÔ¨Åts of using language models trained on 2 trillion tokens, resulting in 300 billion n-grams, on the quali"}]}
{"case_index": 364, "query": "From 2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt, what exact term fills [BLANK] in: \"ignment tax‚Äù since our alignment procedure comes at the cost of 3 lower [BLANK] on certain tasks that we may care about.\"?", "gold": "performance", "answer": "The exact term is **‚Äúperformance‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context 1 explicitly contains the phrase 'lower performance on certain tasks', matching the expected blank.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 96.633, "llm_ms": 1333.298, "top_contexts": [{"rank": 1, "score": 0.60837305, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3952500155350521, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 20, "text_snippet": "ignment tax‚Äù since our alignment procedure comes at the cost of 3  lower performance on certain tasks that we may care about. We can greatly reduce the performance regressions on these datasets by mixing PPO updates with updates that increa"}, {"rank": 2, "score": 0.5387584, "source": "dense+rrf", "rrf_score": 0.0315136476426799, "rerank_score": 0.2598490059470856, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 19, "text_snippet": "wSPairs (Nangia et al., 2020) datasets. We can minimize performance regressions on public NLP datasets by modifying our RLHF Ô¨Åne-tuning procedure. During RLHF Ô¨Åne-tuning, we observe performance regressions compared to GPT-3 on certain publi"}, {"rank": 3, "score": 0.50081885, "source": "dense+rrf", "rrf_score": 0.02651914295749912, "rerank_score": 0.25444026815271453, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 27, "text_snippet": " in Section 4, divided into three parts: results on the API prompt distribution (4.1), results on public NLP datasets (4.2), and qualitative results (4.3). Finally we give an extended discussion of our work in Section 5, including implicati"}, {"rank": 4, "score": 31.118256720581908, "source": "dense+rrf", "rrf_score": 0.014285714285714285, "rerank_score": 0.2498732337349233, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 259, "text_snippet": "model refuse to answer certain instructions. This comes with new challenges: different applications have different levels of risk, and thus we likely want what a model refuses to be conÔ¨Ågurable at inference time. Also, there is a risk that "}, {"rank": 5, "score": 34.87268987409671, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.24159084067689623, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 240, "text_snippet": "ing performance when using these approaches over standard language models [ RSR+19]. Thus our design decision comes at the cost of potentially worse performance on tasks which empirically beneÔ¨Åt from bidirectionality. This may include Ô¨Åll-i"}]}
{"case_index": 365, "query": "From 2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt, what exact term fills [BLANK] in: \"However, more work is needed to study how these models perform on broader groups of users, and how they perform on inputs where humans [BLANK] about the desired behavior.\"?", "gold": "disagree", "answer": "The exact term is **‚Äúdisagree‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "The context sentence explicitly uses 'humans disagree about the desired behavior,' matching the expected blank.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 102.78, "llm_ms": 1206.955, "top_contexts": [{"rank": 1, "score": 0.6184866, "source": "dense+rrf", "rrf_score": 0.031544957774465976, "rerank_score": 0.44695655386548483, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 21, "text_snippet": " ing data. To test the generalization of our models, we conduct a preliminary experiment with held-out labelers, and Ô¨Ånd that they prefer InstructGPT outputs to outputs from GPT-3 at about the same rate as our training labelers. However, mo"}, {"rank": 2, "score": 0.6176275, "source": "dense+rrf", "rrf_score": 0.029850746268656716, "rerank_score": 0.26626021005305034, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 143, "text_snippet": "ns that labelers use as a guide when writing demonstrations and choosing their preferred output, and we answer their questions about edge cases in a shared chat room. More study is needed on the exact effect of different instruction sets an"}, {"rank": 3, "score": 30.393829643915176, "source": "dense+rrf", "rrf_score": 0.014285714285714285, "rerank_score": 0.2599910320380362, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 22, "text_snippet": "nstruction Tuning Instruction tuning [ 46,38,62,61] has emerged as a crucial step in training language models. Instruction tuning involves learning from input-output pairs where the input is natural language task description,and the output "}, {"rank": 4, "score": 34.072887993752076, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.2518673562654147, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 164, "text_snippet": "consensus amongst many groups. We discuss some related considerations in Section 5.2. 5.5 Broader impacts This work is motivated by our aim to increase the positive impact of large language models by training them to do what a given set of "}, {"rank": 5, "score": 36.49632955213475, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.24624089644722297, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 136, "text_snippet": "least for our customers‚Äô natural language task distribution. 2.We‚Äôve seen some evidence that InstructGPT generalizes ‚Äòfollowing instructions‚Äô to settings that we don‚Äôt supervise it in, for example on non-English language tasks and code-rela"}]}
{"case_index": 366, "query": "From 2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt, what exact term fills [BLANK] in: \"This assumption is now [BLANK] disrupted by the existence of frontier LLMs themselves, which allow us to inte\"?", "gold": "significantly", "answer": "The exact term is **‚Äúsignificantly‚Äù** (as in: ‚ÄúThis assumption is now **significantly** disrupted by the existence of frontier LLMs themselves‚Ä¶‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'significantly' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 83.73, "llm_ms": 1700.067, "top_contexts": [{"rank": 1, "score": 0.61437726, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.3046468919761695, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 4, "text_snippet": "age Models (LLMs) have steadily increased in size from a mere billion parameters just five years ago (GPT-2 had 1.5 bil- lion parameters [RWC+19]) to trillion parameters today. The impetus for this effort originates in the seemingly predict"}, {"rank": 2, "score": 0.56380725, "source": "dense+rrf", "rrf_score": 0.031754032258064516, "rerank_score": 0.27848522701825135, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 0, "text_snippet": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone Microsoft Abstract We introduce phi-3-mini , a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both"}, {"rank": 3, "score": 0.6700448, "source": "dense+rrf", "rrf_score": 0.032266458495966696, "rerank_score": 0.2691602108565152, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 5, "text_snippet": "he existence of frontier LLMs themselves, which allow us to interact with data in novel ways. In our previous works on the phi models [GZA+23, LBE+23, JBA+23] it was shown that a combination of LLM-based filtering of publicly available web "}, {"rank": 4, "score": 0.5660079, "source": "dense+rrf", "rrf_score": 0.031024531024531024, "rerank_score": 0.23989986407377278, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 7, "text_snippet": "5. 1arXiv:2404.14219v4 [cs.CL] 30 Aug 2024  User: Explain why it is surprising that one can build a language model small enough to fit on a phone, yet almost as powerful as ChatGPT. Just use one funny sentence. phi-3-mini: It‚Äôs like fitting"}, {"rank": 5, "score": 38.67305292370357, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.2194718746001474, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 16, "text_snippet": "3. The table shows the Keys/values a query token in block 8 attended to. Blue=local blocks, orange=remote/vertical blocks, gray=blocks skipped. total parameters. Additionally, we utilize the SparseMixer approach [LGC23, LDL+23] for training"}]}
{"case_index": 367, "query": "From 2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt, what exact term fills [BLANK] in: \"Knowledge in the form of natural language can be entirely offloaded from the [BLANK] knowledge of LLMs by leveragi\"?", "gold": "parametric", "answer": "The exact term is **‚Äúparametric‚Äù**, i.e., ‚Äú‚Ä¶offloaded from the **parametric** knowledge of LLMs‚Ä¶‚Äù", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'parametric' and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 100.383, "llm_ms": 1307.989, "top_contexts": [{"rank": 1, "score": 56.04321040532367, "source": "dense+rrf", "rrf_score": 0.01639344262295082, "rerank_score": 0.3445287623754549, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 5, "text_snippet": "ather than chunking them into smaller units. By doing so, we achieve an F1 score of 25.9% on Qasper (previously 22.5%) and 57.5% on MultiFieldQA-en (previously 51.2%). Our study offers insights into the future roadmap for combining RAG with"}, {"rank": 2, "score": 0.6504172, "source": "dense+rrf", "rrf_score": 0.031544957774465976, "rerank_score": 0.23117714158838185, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 79, "text_snippet": "on the long-context understanding ability of LLMs, and we still have a long way to go in harnessing open-source LLMs within our framework. 10  4 Related Work 4.1 Retrieval-Augmented Generation. Augmenting language models with information re"}, {"rank": 3, "score": 32.33792798864114, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.21387169399136002, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 0, "text_snippet": "LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs ‚ô†Ziyan Jiang,‚ô†Xueguang Ma,‚ô†Wenhu Chen ‚ô†University of Waterloo ziyanjiang528@gmail.com ,{x93ma ,wenhuchen}@uwaterloo.ca Project Website: https://tiger-ai-lab.github.io/"}, {"rank": 4, "score": 28.1836293099908, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.21070792065469268, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 19, "text_snippet": "e end performance dips significantly due to the huge amount of ‚Äúhard negatives‚Äù, which confuses the reader. With ‚Äúlong retriever units‚Äù, we observe an entirely different trend. As we recall more units (from 1 to 8 units), both the recall an"}, {"rank": 5, "score": 27.653968693195836, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.1901859494179779, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 14, "text_snippet": " al., 2022). We ask the following question: can we create a simple and generic retrieval augmented LM that ac- tively decides when and what to retrieve throughout the generation process, and are applicable to a va- riety of long-form genera"}]}
{"case_index": 368, "query": "From 2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt, what exact term fills [BLANK] in: \"Scaling LLMs like GPT-4 [ 44] and PaLM-2 [ 1] to ever more parameters led to emergent abilities [ 63] unseen in smaller models (less than ‚àº10B parameters), most notably the [BLANK] ability to reason zero-shot [ 23].\"?", "gold": "remarkable", "answer": "The exact term is **‚Äúremarkable‚Äù** (i.e., ‚Äúthe **remarkable** ability to reason zero-shot‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'remarkable' and is directly supported by the provided context sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 74.941, "llm_ms": 1335.258, "top_contexts": [{"rank": 1, "score": 0.7138759, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.47353980791614053, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 12, "text_snippet": "humans and machines, enhancing user experience in existing applications like coding [3], web search [ 36], chatbots [ 45,56], customer service and content creation. This transformation brought by LLMs is also paving the way for new innovati"}, {"rank": 2, "score": 31.907737133457022, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.21229730211167888, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 1, "text_snippet": "n benchmarks like BigBench Hard and AGIEval. In Orca 2, we continue exploring how improved training signals can enhance smaller LMs‚Äô reasoning abilities. Research on training small LMs has often relied on imitation learning to replicate the"}, {"rank": 3, "score": 28.430119581574587, "source": "dense+rrf", "rrf_score": 0.014285714285714285, "rerank_score": 0.20868247218726166, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 75, "text_snippet": "any exemplars or CoT prompting. Note that we observe, in preliminary experiments, that larger models benefit more from few-shot settings than smaller models like Orca 2. We conduct evaluation only based on the zero-shot settings, we leave a"}, {"rank": 4, "score": 34.62173866742455, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.20133956500123243, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 0, "text_snippet": "Orca 2: Teaching Small Language Models How to Reason Arindam Mitra, Luciano Del Corro‚Ä†, Shweti Mahajan‚Ä†, Andres Codas‚Ä° Clarisse Simoes‚Ä°, Sahaj Agarwal, Xuxi Chen‚àó, Anastasia Razdaibiedina‚àó Erik Jones‚àó, Kriti Aggarwal‚àó, Hamid Palangi, Guoqin"}, {"rank": 5, "score": 0.6680524, "source": "dense+rrf", "rrf_score": 0.03149801587301587, "rerank_score": 0.20018268723477986, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 13, "text_snippet": "ity to reason zero-shot [ 23]. These abilities include answering complex questions, generating explanations, and solving multi-step problems, for instance, such as those on the US Medical Licensing exam, on which LLMs now achieve a passing "}]}
{"case_index": 369, "query": "From 2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt, what exact term fills [BLANK] in: \"SFT leverages highly curated [BLANK] data across diverse domains, e.g., math, coding, reasoning, conversation, model identity, and safety.\"?", "gold": "high-quality", "answer": "The exact term that fills **[BLANK]** is **‚Äúhigh-quality‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'high-quality' and is explicitly supported by Context 1 sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 55.885, "llm_ms": 2566.263, "top_contexts": [{"rank": 1, "score": 0.59515786, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3454847953437028, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 24, "text_snippet": "at were trained on the same fixed data. We plot the log of MMLU error versus the log of model size. 4  Post-training. Post-training of phi-3 went through two stages, including supervised finetuning (SFT) and direct preference optimization ("}, {"rank": 2, "score": 56.236647013670265, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.2482046872039131, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 0, "text_snippet": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone Microsoft Abstract We introduce phi-3-mini , a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both"}, {"rank": 3, "score": 0.5431613, "source": "dense+rrf", "rrf_score": 0.02976190476190476, "rerank_score": 0.24614461166000687, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 25, "text_snippet": "DPO data covers chat format data, reasoning, and responsible AI (RAI) efforts. We use DPO to steer the model away from unwanted behavior, by using those outputs as ‚Äúrejected‚Äù responses. Besides improvement in math, coding, reasoning, robust"}, {"rank": 4, "score": 32.55307091255173, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.19225026974680573, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 57, "text_snippet": "onse become accurate, but also gets more specific with suggestions. 11  Figure 7: The demo case shows Phi-3.5-Vision‚Äôs capability in natural image understanding and reasoning. ourselves, covering diverse domains and tasks such as general na"}, {"rank": 5, "score": 36.76226578791828, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.1805809858689931, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 6, "text_snippet": "7B parameters), matched the performance of models 25 times larger trained on regular data. In this report we present a new model, phi-3-mini (3.8B parameters), trained for 3.3T tokens on larger and more advanced versions of the datasets use"}]}
{"case_index": 370, "query": "From 2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt, what exact term fills [BLANK] in: \"1arXiv:2404.14219v4 [cs.CL] 30 Aug 2024 User: Explain why it is [BLANK] that one can build a language model small enough to fit on a phone, yet almost as powerful as ChatGPT.\"?", "gold": "surprising", "answer": "The exact term is **‚Äúsurprising‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "The context explicitly contains the sentence with the word 'surprising' filling the blank, and the model answer matches exactly.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 81.929, "llm_ms": 1061.86, "top_contexts": [{"rank": 1, "score": 0.76121944, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.4598907103825061, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 7, "text_snippet": "5. 1arXiv:2404.14219v4 [cs.CL] 30 Aug 2024  User: Explain why it is surprising that one can build a language model small enough to fit on a phone, yet almost as powerful as ChatGPT. Just use one funny sentence. phi-3-mini: It‚Äôs like fitting"}, {"rank": 2, "score": 0.7188806, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.35311827956988673, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 6, "text_snippet": "7B parameters), matched the performance of models 25 times larger trained on regular data. In this report we present a new model, phi-3-mini (3.8B parameters), trained for 3.3T tokens on larger and more advanced versions of the datasets use"}, {"rank": 3, "score": 0.7017218, "source": "dense+rrf", "rrf_score": 0.031746031746031744, "rerank_score": 0.28237543008614324, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 0, "text_snippet": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone Microsoft Abstract We introduce phi-3-mini , a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both"}, {"rank": 4, "score": 0.59086573, "source": "dense+rrf", "rrf_score": 0.030776515151515152, "rerank_score": 0.20365207547070707, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 16, "text_snippet": "3. The table shows the Keys/values a query token in block 8 attended to. Blue=local blocks, orange=remote/vertical blocks, gray=blocks skipped. total parameters. Additionally, we utilize the SparseMixer approach [LGC23, LDL+23] for training"}, {"rank": 5, "score": 0.6854163, "source": "dense+rrf", "rrf_score": 0.030776515151515152, "rerank_score": 0.19835283347848845, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 8, "text_snippet": "ved solely by changing the training data. phi-3-mini: The development of a compact language model that rivals the capabilities of ChatGPT, while fitting on a phone, is a testament to the power of data-driven machine learning. By meticulousl"}]}
{"case_index": 371, "query": "From 2302.13971__llama_open_and_efficient_foundation_language_models.txt, what exact term fills [BLANK] in: \"Here are the main difference with the original architec- ture, and where we were found the inspiration for this change (in bracket): [BLANK] [GPT3].\"?", "gold": "pre-normalization", "answer": "The exact term is **‚ÄúPre-normalization‚Äù** (as in: ‚Äú**Pre-normalization [GPT3].**‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'pre-normalization' and is directly supported by Context 1 ('Pre-normalization [GPT3].').", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 84.013, "llm_ms": 1447.63, "top_contexts": [{"rank": 1, "score": 0.7018744, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.38213222485233383, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 20, "text_snippet": " transformer architec- ture (Vaswani et al., 2017). We leverage various improvements that were subsequently proposed, and used in different models such as PaLM. Here are the main difference with the original architec- ture, and where we wer"}, {"rank": 2, "score": 23.37931154662811, "source": "dense+rrf", "rrf_score": 0.014285714285714285, "rerank_score": 0.21152595487247192, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 89, "text_snippet": "y for transformer based language models, which were later reÔ¨Åned by Hoffmann et al. (2022), by adapting the learning rate schedule when scaling datasets. Finally, Wei et al. (2022) studied the effect of scal- ing on the abilities of large l"}, {"rank": 3, "score": 22.053548924189833, "source": "dense+rrf", "rrf_score": 0.014084507042253521, "rerank_score": 0.20601710460875117, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 260, "text_snippet": "entation and no successful deployments since then. Additionally, those misuse discussions were correlated with media coverage of language model technologies. From this, we assess that the threat of misuse from these actors is not immediate,"}, {"rank": 4, "score": 25.346994557845267, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.19701357465592814, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 23, "text_snippet": "rs, and all of the param- eters are Ô¨Åne-tuned using labeled data from the downstream tasks. Each downstream task has sep- arate Ô¨Åne-tuned models, even though they are ini- tialized with the same pre-trained parameters. The question-answerin"}, {"rank": 5, "score": 0.58053565, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.1518719845373501, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 8, "text_snippet": "inchilla. In the rest of this paper, we present an overview of the modiÔ¨Åcations we made to the transformer architecture (Vaswani et al., 2017), as well as our training method. We then report the performance of our models and compare with ot"}]}
{"case_index": 372, "query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"However, all of these models have several inherent [BLANK] that can at best be partially addressed by further scal- ing.\"?", "gold": "limitations", "answer": "The exact term is **‚Äúlimitations‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context 1 explicitly contains the phrase 'several inherent limitations' matching the expected blank.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 60.82, "llm_ms": 1079.745, "top_contexts": [{"rank": 1, "score": 62.54759531210587, "source": "dense+rrf", "rrf_score": 0.01639344262295082, "rerank_score": 0.33661202185791655, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 3, "text_snippet": "ew-shot results on a variety of natural lan- guage processing tasks (Brown et al., 2020; Chowd- hery et al., 2022, i.a.) and show several emergent capabilities (Wei et al., 2022). However, all of these models have several inherent limitatio"}, {"rank": 2, "score": 35.316168202328136, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.21034756212589395, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 1, "text_snippet": "metic or fac- tual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer , a model traine"}, {"rank": 3, "score": 45.81689432709211, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.20808481434424214, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 212, "text_snippet": "line decision making: Benchmarks and additional opinions,‚Äù arXiv preprint arXiv:2306.02224 , 2023. [108] T. Schick, J. Dwivedi-Yu, R. Dess `ƒ±, R. Raileanu, M. Lomeli, L. Zettle- moyer, N. Cancedda, and T. Scialom, ‚ÄúToolformer: Language mode"}, {"rank": 4, "score": 0.5838083, "source": "dense+rrf", "rrf_score": 0.029418126757516764, "rerank_score": 0.19444543366791234, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 94, "text_snippet": "e are no examples of chained tool use in the Ô¨Ånetuning dataset. Our current approach also does not allow the LM to use a tool in an in- teractive way ‚Äì especially for tools such as search engines, that could potentially return hundreds of d"}, {"rank": 5, "score": 29.908436794225043, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.19426688052380223, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 66, "text_snippet": " that Tool- former outperforms all baselines for both TEM- PLAMA andDATESET . However, closer inspec- tion shows that improvements on TEMPLAMA can not be attributed to the calendar tool, which is only used for 0.2% of all examples, but most"}]}
{"case_index": 373, "query": "From 2005.14165__language_models_are_few_shot_learners.txt, what exact term fills [BLANK] in: \"While it has shown some initial promise, this approach still achieves results far inferior to Ô¨Åne-tuning ‚Äì for example [RWC+19] achieves only 4% on Natural [BLANK], and even its 55 F1 CoQa result is now more than 35 points\"?", "gold": "questions", "answer": "The exact term is **‚ÄúQuestions‚Äù** (as in **‚ÄúNatural Questions‚Äù**).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context explicitly contains the phrase 'Natural Questions', so the blank is 'questions' (case-insensitive).", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 67.452, "llm_ms": 1230.32, "top_contexts": [{"rank": 1, "score": 0.65844464, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.5399965348547259, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 22, "text_snippet": "g‚Äù, using the text input of a pretrained language model as a form of task speciÔ¨Åcation: the model is conditioned on a natural language instruction and/or a few demonstrations of the task and is then expected to complete further instances of"}, {"rank": 2, "score": 0.61827856, "source": "dense+rrf", "rrf_score": 0.0315136476426799, "rerank_score": 0.28771331780111065, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 23, "text_snippet": "tions, and even its 55 F1 CoQa result is now more than 35 points behind the state of the art. Meta-learning clearly requires substantial improvement in order to be viable as a practical method of solving language tasks. Another recent trend"}, {"rank": 3, "score": 0.5828629, "source": "dense+rrf", "rrf_score": 0.028531243721117136, "rerank_score": 0.23899098615214062, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 32, "text_snippet": "GPT-3 achieves promising results in the zero-shot and one-shot settings, and in the the few-shot setting is sometimes competitive with or even occasionally surpasses state-of-the-art (despite state-of-the-art being held by Ô¨Åne-tuned models)"}, {"rank": 4, "score": 42.50628809034279, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.229227116184393, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 131, "text_snippet": "] [ZLL+18] [DGM06 ] [BHDD+06] [GMDD07 ] [BDD+09] [PCC18 ] [PHR+18]. GPT-3‚Äôs test-set performance on the SuperGLUE dataset is shown in Table 3.8. In the few-shot setting, we used 32 examples for all tasks, sampled randomly from the training "}, {"rank": 5, "score": 35.489731030252116, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.22578625281648432, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 45, "text_snippet": " major reduction in the need for task-speciÔ¨Åc data and reduced potential to learn an overly narrow distribution from a large but narrow Ô¨Åne-tuning dataset. The main disadvantage is that results from this method have so far been much worse t"}]}
{"case_index": 374, "query": "From 2305.06983__active_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"e to avoid unneces- sary or [BLANK] retrieval that occurs in passive retrieval augmented LMs (Khandelwal et al., 2020; Borgeaud et al., 2022; Ram et al., 2023; Trivedi et al., 2022).\"?", "gold": "inappropriate", "answer": "The exact term is **‚Äúinappropriate‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term and is explicitly supported by Context 1/2.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 62.875, "llm_ms": 1056.064, "top_contexts": [{"rank": 1, "score": 0.7273266, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.43102875780313754, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 15, "text_snippet": "e to avoid unneces- sary or inappropriate retrieval that occurs in passive retrieval augmented LMs (Khandelwal et al., 2020; Borgeaud et al., 2022; Ram et al., 2023; Trivedi et al., 2022). Given the observation that large LMs tend to be wel"}, {"rank": 2, "score": 0.65996474, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.27840737836282164, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 14, "text_snippet": " al., 2022). We ask the following question: can we create a simple and generic retrieval augmented LM that ac- tively decides when and what to retrieve throughout the generation process, and are applicable to a va- riety of long-form genera"}, {"rank": 3, "score": 0.5833098, "source": "dense+rrf", "rrf_score": 0.028693528693528692, "rerank_score": 0.25956408683432847, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 13, "text_snippet": "rieve additional information at a fixed interval (Khandelwal et al., 2020; Borgeaud et al., 2022; Ram et al., 2023; Trivedi et al., 2022) which might not accurately reflect what LMs intend to gener- ate in the future or retrieve at inapprop"}, {"rank": 4, "score": 39.285692149423575, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.23304699790839348, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 130, "text_snippet": "delwal et al., 2020) retrieves every token, RETRO or IC-RALM (Borgeaud et al., 2022; Ram et al., 2023) retrievers every 4 ‚àº32 to- kens, and IRCoT (Trivedi et al., 2022) retrieves every sentence. Compared to single-time retrieval, however, i"}, {"rank": 5, "score": 37.521460631101746, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.22634651352593083, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 5, "text_snippet": " training corpus (Kandpal et al., 2022; Mallen et al., 2023). The standard solution to these issues is to rely on Retrieval Augmented Generation (RAG) (Lee et al., 2019; Lewis et al., 2020; Guu et al., 2020). Answering a question then essen"}]}
{"case_index": 375, "query": "From 2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt, what exact term fills [BLANK] in: \"[BLANK] is up to 3\u0002faster than the standard attention implemen- tation across common sequence lengths from 128 to 2K and scales up to 64K.\"?", "gold": "flashattention", "answer": "FlashAttention", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "The blank is 'FlashAttention' per the provided context; model answered 'FlashAttention' (case difference from expected 'flashattention' is inconsequential).", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 90.101, "llm_ms": 891.221, "top_contexts": [{"rank": 1, "score": 0.81919545, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.4107821701998185, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 20, "text_snippet": "n. FlashAttention is up to 3\u0002faster than the standard attention implemen- tation across common sequence lengths from 128 to 2K and scales up to 64K. Up to sequence length of 512, FlashAttention is both faster and more memory-eÔ¨Écient than an"}, {"rank": 2, "score": 0.77161235, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.39013252307215135, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 55, "text_snippet": "th-256 (sequence length 64K). ‚Ä¢Benchmarking Attention. We measure the runtime and memory performance of FlashAttention and block-sparse FlashAttention based on sequence length. We conÔ¨Årm that the memory footprint ofFlashAttention scales lin"}, {"rank": 3, "score": 0.7285347, "source": "dense+rrf", "rrf_score": 0.02862400327131466, "rerank_score": 0.28651563857004003, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 76, "text_snippet": "nce lengths. Memory Footprint. Figure 3 (right) shows the memory footprint of FlashAttention and block-sparse FlashAttention compared to various exact, approximate, and sparse attention baselines. FlashAttention and block-sparse FlashAttent"}, {"rank": 4, "score": 0.7631912, "source": "dense+rrf", "rrf_score": 0.031009615384615385, "rerank_score": 0.27221258218980077, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 75, "text_snippet": "tion mechanisms grow linearly with se- quence length, but FlashAttention still runs faster than approximate and sparse attention for short sequences due to fewer memory accesses. The approximate attention runtimes begin to cross over with F"}, {"rank": 5, "score": 39.68564476909715, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.2633358872567374, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 19, "text_snippet": "lift from modeling longer sequences on long-document classiÔ¨Åcation [13]. FlashAttention enables the Ô¨Årst Transformer that can achieve better-than-chance performance on the Path-X [ 80] challenge, solely from using a longer sequence length ("}]}
{"case_index": 376, "query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"After Ô¨Åltering, we merge API calls for different tools, [BLANK] in the augmented dataset C‚àó, and Ô¨Ånetune 1In practice, we use the token sequences ‚Äú [‚Äù, ‚Äú]‚Äù and ‚Äú->‚Äù to represent ‚Äú <API> ‚Äù, ‚Äú</API> ‚Äù and ‚Äú‚Üí‚Äù, respec- tively.\"?", "gold": "resulting", "answer": "The exact term is **‚Äúresulting‚Äù** (i.e., ‚ÄúAfter Ô¨Åltering, we merge API calls for different tools, **resulting** in the augmented dataset C‚àó, ‚Ä¶‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches the expected term 'resulting' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 98.003, "llm_ms": 1395.457, "top_contexts": [{"rank": 1, "score": 0.7655403, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.41825086184548094, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 17, "text_snippet": "her the ob- tained responses are helpful for predicting future tokens; this is used as a Ô¨Åltering criterion. After Ô¨Åltering, we merge API calls for different tools, resulting in the augmented dataset C‚àó, and Ô¨Ånetune 1In practice, we use the"}, {"rank": 2, "score": 0.5764258, "source": "dense+rrf", "rrf_score": 0.031024531024531024, "rerank_score": 0.2491378052585761, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 27, "text_snippet": "atterns in the pretraining corpus, thus hurting perplexity.  x1:i‚àí1,e(ci,ri),xi:n; we proceed analogously for texts with multiple API calls. Doing this for all x‚àà Cresults in the new dataset C‚àóaugmented with API calls. We use this new datas"}, {"rank": 3, "score": 0.58985955, "source": "dense+rrf", "rrf_score": 0.031009615384615385, "rerank_score": 0.2480991858028239, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 1, "text_snippet": "metic or fac- tual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer , a model traine"}, {"rank": 4, "score": 0.57923335, "source": "dense+rrf", "rrf_score": 0.031009615384615385, "rerank_score": 0.234618019364983, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 12, "text_snippet": " how an API can be used, we let a LM annotate a huge language modeling dataset with potential API calls. We then use a self-supervised loss to determine which of these API calls actually help the model in predicting future tokens. Finally, "}, {"rank": 5, "score": 48.284577474512304, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.21376765229812683, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 212, "text_snippet": "line decision making: Benchmarks and additional opinions,‚Äù arXiv preprint arXiv:2306.02224 , 2023. [108] T. Schick, J. Dwivedi-Yu, R. Dess `ƒ±, R. Raileanu, M. Lomeli, L. Zettle- moyer, N. Cancedda, and T. Scialom, ‚ÄúToolformer: Language mode"}]}
{"case_index": 377, "query": "From 2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt, what exact term fills [BLANK] in: \"During evaluation the retrieval database consists in the full union of these datasets, with the exception of books for which we use a [BLANK] of 4%.\"?", "gold": "sub-sample", "answer": "The exact term is **‚Äúsub-sample‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term and is explicitly supported by Context 1 ('use a sub-sample of 4%').", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 113.545, "llm_ms": 1204.376, "top_contexts": [{"rank": 1, "score": 0.7148185, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3710943064365435, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 20, "text_snippet": "f the same subsets as the training data, in proportion that matches the training sampling frequencies. During evaluation the retrieval database consists in the full union of these datasets, with the exception of books for which we use a sub"}, {"rank": 2, "score": 32.86460215607148, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.2989951002265601, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 199, "text_snippet": " but only trains the new retrieval weights, as explained in ¬ß4.2. Our best R/e.sc/t.sc/r.sc/o.scÔ¨Åt checkpoint has a Wikitext103 valid perplexity 18¬ì46, when retrieving from Wikipedia. We use this R/e.sc/t.sc/r.sc/o.sccheckpoint in Table 4 f"}, {"rank": 3, "score": 0.57759297, "source": "dense+rrf", "rrf_score": 0.028612012987012988, "rerank_score": 0.2894671056511388, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 173, "text_snippet": "n Conference of the North American Chapter of the Association for Computational Linguistics , 2018. 23  Improving language models by retrieving from trillions of tokens A. Datasets We provide a full description of MassiveText and of our ext"}, {"rank": 4, "score": 31.678080584289706, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.28175257906827605, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 22, "text_snippet": " ùïç=¬ª1¬îùë£¬º, obtained using a text tokenizer1. Wespliteach ùëõ-token-longexample ùëã=¬πùë•1¬î¬ì¬ì¬ì¬îùë•ùëõ¬∫intoasequenceof ùëôchunks¬πùê∂1¬î¬ì¬ì¬ì¬îùê∂ùëô¬∫ of sizeùëö=ùëõ ùëô, i.e.ùê∂1,¬πùë•1¬î¬ì¬ì¬ì¬îùë•ùëö¬∫¬î ¬ì¬ì¬ì¬î ùê∂ùëô,¬πùë•ùëõ\u0000ùëö¬∏1¬î¬ì¬ì¬ì¬îùë•ùëõ¬∫2ùïçùëö. We useùëõ=2048andùëö=64. We augment each chunk ùê∂ùë¢with a se"}, {"rank": 5, "score": 0.6048094, "source": "dense+rrf", "rrf_score": 0.03057889822595705, "rerank_score": 0.2800732212277104, "doc_id": "2112.04426__improving_language_models_by_retrieving_from_trillions_of_tokens.txt", "chunk_id": 113, "text_snippet": "formers ( R/e.sc/t.sc/r.sc/o.sc), a method for modelling arbitrary text se- quences whilstretrievingfromdatabaseswithtrillions oftokens‚Äîscalingthedataavailable to models by an order of magnitude compared to what is typically consumed during"}]}
{"case_index": 378, "query": "From 2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt, what exact term fills [BLANK] in: \"just the order of the [BLANK] to change the position of the document that contains the answer (Figure 3).\"?", "gold": "documents", "answer": "The exact term is **‚Äúdocuments‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "The blank is 'documents', exactly as shown in the provided context snippet.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 51.333, "llm_ms": 1003.626, "top_contexts": [{"rank": 1, "score": 0.61683714, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.3031933029127169, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 27, "text_snippet": "just the order of the documents to change the position of the document that contains the answer (Figure 3). To modulate the input context length in this task, we increase or decrease the number of retrieved documents that do not contain the"}, {"rank": 2, "score": 0.56246334, "source": "dense+rrf", "rrf_score": 0.028958333333333336, "rerank_score": 0.29380088310905134, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 49, "text_snippet": "UIDs. The relevant key-value pair for answering the query is bolded here within the input context for clarity. indicate that extended-context models are not nec- essarily better than their non-extended counterparts at using their input cont"}, {"rank": 3, "score": 0.6289043, "source": "dense+rrf", "rrf_score": 0.030017921146953404, "rerank_score": 0.24984382806975025, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 11, "text_snippet": "r of documents in the input context (akin to retrieving more or less documents in retrieval-augmented generation), and (ii) control the position of the relevant information within the input context by changing the order of the documents to "}, {"rank": 4, "score": 27.659439581233368, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.24417452882558496, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 93, "text_snippet": " a serial-position-like effect in lan- guage models is perhaps surprising, since the self- attention mechanisms underlying Transformer lan- guage models is technically equally capable of re- trieving any token from their contexts. 7 Conclus"}, {"rank": 5, "score": 33.82807383638358, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.2403975233146008, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 0, "text_snippet": "Lost in the Middle: How Language Models Use Long Contexts Nelson F. Liu1‚àóKevin Lin2John Hewitt1Ashwin Paranjape3 Michele Bevilacqua3Fabio Petroni3Percy Liang1 1Stanford University2University of California, Berkeley3Samaya AI nfliu@cs.stanfo"}]}
{"case_index": 379, "query": "From 2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt, what exact term fills [BLANK] in: \"They both come back [BLANK] later in the day, and they do not know what happened in the room after each of them left the room.\"?", "gold": "together", "answer": "The exact term that fills **[BLANK]** is:\n\n**together**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Model answer matches expected term 'together' and is directly supported by the provided context sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 112.439, "llm_ms": 1183.755, "top_contexts": [{"rank": 1, "score": 0.61748385, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3970756282289402, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 6, "text_snippet": "then leaves for school. They both come back together later in the day, and they do not know what happened in the room after each of them left the room. Where do they think the ball is? ### Orca-2-13B : Let‚Äôs analyze the situation step by st"}, {"rank": 2, "score": 0.47794586, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.22959587601434392, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 7, "text_snippet": "John and Mark have not seen each other‚Äôs actions. Therefore, they only know what happened while they were in the room. John only saw the ball in the box before he left for work. Mark only saw the ball in the basket before he left for school"}, {"rank": 3, "score": 0.49791843, "source": "dense+rrf", "rrf_score": 0.0315136476426799, "rerank_score": 0.22889315081433265, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 10, "text_snippet": "ve still been in the box. 4. Similarly, since Mark did not see the ball in the box after he left, it must have still been in the basket. Therefore, the ball is still in the box. ### ChatGPT(GPT-3.5-Turbo) : John and Mark will likely think t"}, {"rank": 4, "score": 0.44598287, "source": "dense+rrf", "rrf_score": 0.03149801587301587, "rerank_score": 0.22001828667156328, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 5, "text_snippet": "rca 2 models match or surpass all other models including models 5-10x larger. Note that all models are using the same LLaMA-2 base models of the respective size. ‚àówork done while at Microsoft;‚Ä†,‚Ä°denote equal contributions.arXiv:2311.11045v2"}, {"rank": 5, "score": 22.822534128842015, "source": "dense+rrf", "rrf_score": 0.014285714285714285, "rerank_score": 0.20868679112233537, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 51, "text_snippet": " closest to how humans perform tasks ‚Äì for example, in the translation example in Figure 2.1, a human would likely know what to do from just the text instruction. Figure 2.1 shows the four methods using the example of translating English to"}]}
{"case_index": 380, "query": "From 2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"Evaluating RAG [BLANK] is, however, chal- lenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability\"?", "gold": "architectures", "answer": "The exact term is **‚Äúarchitectures‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches the context phrase 'Evaluating RAG architectures is, however, challenging...'; fully supported by provided context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 74.306, "llm_ms": 789.253, "top_contexts": [{"rank": 1, "score": 0.7624767, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.4438951959215983, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 1, "text_snippet": "ented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natu- ral language layer between"}, {"rank": 2, "score": 0.64571697, "source": "dense+rrf", "rrf_score": 0.027972027972027972, "rerank_score": 0.2670825857786453, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 48, "text_snippet": "actory results. III. R ETRIEVAL In the context of RAG, it is crucial to efficiently retrieve relevant documents from the data source. There are several key issues involved, such as the retrieval source, retrieval granularity, pre-processing"}, {"rank": 3, "score": 0.6841799, "source": "dense+rrf", "rrf_score": 0.030158730158730156, "rerank_score": 0.258325034987083, "doc_id": "2309.15217__ragas_automated_evaluation_of_retrieval_augmented_generation.txt", "chunk_id": 2, "text_snippet": "m to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the genera- tion itself. With Ragas, we put forward a suite of metrics which can be used to evaluate t"}, {"rank": 4, "score": 0.73323494, "source": "dense+rrf", "rrf_score": 0.031054405392392875, "rerank_score": 0.21540142522587036, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 125, "text_snippet": "atic evaluation of RAG applications, similarly base their assessments on these task- specific metrics [160]. Despite this, there is a notable paucity of research dedicated to evaluating the distinct characteristics of RAG models.The main ev"}, {"rank": 5, "score": 0.7278769, "source": "dense+rrf", "rrf_score": 0.029957522915269395, "rerank_score": 0.21327159278506402, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 126, "text_snippet": "insof search engines, recommendation systems, and information retrieval systems are employed to measure the performance of the RAG retrieval module. Metrics such as Hit Rate, MRR, and NDCG are commonly utilized for this purpose [161], [162]"}]}
{"case_index": 381, "query": "From 2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt, what exact term fills [BLANK] in: \"Hence instruction tuned models will be always limited by the knowledge learned during [BLANK].\"?", "gold": "pre-training", "answer": "The exact term that fills **[BLANK]** is **‚Äúpre-training‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'pre-training' and is explicitly supported by the provided context sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 82.494, "llm_ms": 1208.643, "top_contexts": [{"rank": 1, "score": 0.64589405, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.34044938199649377, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 24, "text_snippet": "not result in proportional improvement to small model performance when thoroughly evaluated on knowledge-intensive or reasoning-intensive tasks where correctness is not just judged by style. We note that instruction tuning, while very benef"}, {"rank": 2, "score": 49.784867187840504, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.28408698983794234, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 144, "text_snippet": "ng training and safety guardrails (e.g., content filter) within the Azure OpenAI API. However, detailed studies are required for better quantification of such risks. Small Model Capacity: We note that post-training, while significantly bene"}, {"rank": 3, "score": 0.5873219, "source": "dense+rrf", "rrf_score": 0.030798389007344232, "rerank_score": 0.23267523557336062, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 0, "text_snippet": "Orca 2: Teaching Small Language Models How to Reason Arindam Mitra, Luciano Del Corro‚Ä†, Shweti Mahajan‚Ä†, Andres Codas‚Ä° Clarisse Simoes‚Ä°, Sahaj Agarwal, Xuxi Chen‚àó, Anastasia Razdaibiedina‚àó Erik Jones‚àó, Kriti Aggarwal‚àó, Hamid Palangi, Guoqin"}, {"rank": 4, "score": 0.5779059, "source": "dense+rrf", "rrf_score": 0.014285714285714285, "rerank_score": 0.1868259627844482, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 1, "text_snippet": "n benchmarks like BigBench Hard and AGIEval. In Orca 2, we continue exploring how improved training signals can enhance smaller LMs‚Äô reasoning abilities. Research on training small LMs has often relied on imitation learning to replicate the"}, {"rank": 5, "score": 20.628002987249193, "source": "dense+rrf", "rrf_score": 0.014084507042253521, "rerank_score": 0.18654939335349802, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 88, "text_snippet": "en through convolution and/or linear RNNs, e.g., in RWKV (Peng, 2023), S4 (Gu et al., 2022), or Hyena (Poli et al., 2023). Many prior efforts evalu- ate perplexity on a diverse web corpus as a proxy for the ability to process long contexts;"}]}
{"case_index": 382, "query": "From 2305.06983__active_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"The [BLANK] of the prompt is shown in Prompt 3.1, and full details can be found in Prompt D.3.\"?", "gold": "structure", "answer": "The exact term is **‚Äústructure‚Äù**: ‚ÄúThe **structure** of the prompt is shown in Prompt 3.1, and full details can be found in Prompt D.3.‚Äù", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'structure' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 55.875, "llm_ms": 1318.094, "top_contexts": [{"rank": 1, "score": 31.80079270573795, "source": "dense+rrf", "rrf_score": 0.01639344262295082, "rerank_score": 0.31556671186811314, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 29, "text_snippet": "ss, we elicit such behavior by few-shot prompting (Brown et al., 2020). Specifically, for a downstream task, we place the search-related instruction and exemplars at the beginning as skill 1, followed by the instruction and exemplars of the"}, {"rank": 2, "score": 0.581808, "source": "dense+rrf", "rrf_score": 0.032018442622950824, "rerank_score": 0.29443660845382097, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 30, "text_snippet": "can be found in Prompt D.3. Prompt 3.1: retrieval instructions Skill 1. An instruction to guide LMs to generate search queries. Several search-related exemplars. Skill 2. An instruction to guide LMs to perform a specific downstream task (e."}, {"rank": 3, "score": 23.76732487113335, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.24132668385419512, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 43, "text_snippet": "or datasets that rely on knowledge from the open web, we use the Bing search engine as our retriever.3 Retrieved document formatting. Multiple re- trieved documents are linearized according to their ranking and then added to the beginning o"}, {"rank": 4, "score": 28.01694735540223, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.2300044902571439, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 22, "text_snippet": " generate the complete answer at once y=LM([Dx,x]). 2.3 Active Retrieval Augmented Generation To aid long-form generation with retrieval, we pro- pose active retrieval augmented generation. It is a generic framework that actively decides wh"}, {"rank": 5, "score": 22.949499687789295, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.22621476122026546, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 11, "text_snippet": "eddocumentsLMGeneration$%$%% Figure 1: An illustration of forward-looking active retrieval augmented generation (FLARE). Starting with the user input xand initial retrieval results Dx, FLARE iteratively generates a temporary next sentence ("}]}
{"case_index": 383, "query": "From 2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt, what exact term fills [BLANK] in: \"[BLANK] does not signiÔ¨Åcantly improve over GPT-3 on the Winogender (Rudinger et al., 2018) and CrowSPairs (Nangia et al., 2020) datasets.\"?", "gold": "instructgpt", "answer": "**InstructGPT** does not signiÔ¨Åcantly improve over GPT-3 on the Winogender (Rudinger et al., 2018) and CrowSPairs (Nangia et al., 2020) datasets.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term ('InstructGPT') and is directly supported by Context 1 sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 59.714, "llm_ms": 1211.617, "top_contexts": [{"rank": 1, "score": 0.6127161, "source": "dense+rrf", "rrf_score": 0.03131881575727918, "rerank_score": 0.4065244344075274, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 18, "text_snippet": "1% hallucination rate, respectively). InstructGPT shows small improvements in toxicity over GPT-3, but not bias. To measure toxicity, we use the RealToxicityPrompts dataset (Gehman et al., 2020) and conduct both automatic and human evaluati"}, {"rank": 2, "score": 35.1550060464091, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.2846889277236862, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 9, "text_snippet": "fabricate information or mislead the user), and harmless (they should not cause physical, psychological, or social harm to people or the environment). We elaborate on the evaluation of these criteria in Section 3.6. We focus on Ô¨Åne-tuning a"}, {"rank": 3, "score": 0.6262399, "source": "dense+rrf", "rrf_score": 0.030621785881252923, "rerank_score": 0.27624011875789073, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 22, "text_snippet": "r language models are used. We compare GPT-3 Ô¨Åne-tuned on our human preference data (i.e. InstructGPT) to GPT-3 Ô¨Åne-tuned on two different compilations of public NLP tasks: the FLAN (Wei et al., 2021) and T0 (Sanh et al., 2021) (in particul"}, {"rank": 4, "score": 36.8882505809864, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.25725690216596436, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 123, "text_snippet": ". Training language models to follow instructions with human feedback. In Advances in Neural Infor- mation Processing Systems . Markus N Rabe and Charles Staats. 2021. Self- attention does not need o(n2)memory. arXiv preprint arXiv:2112.056"}, {"rank": 5, "score": 0.62309396, "source": "dense+rrf", "rrf_score": 0.031009615384615385, "rerank_score": 0.24585979888302917, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 108, "text_snippet": "GPT on modiÔ¨Åed versions of the Winogender (Rudinger et al., 2018) and CrowS-Pairs (Nangia et al., 2020) datasets. These datasets consists of pairs of sentences which can highlight potential bias. We calculate the relative probabilities of p"}]}
{"case_index": 384, "query": "From 2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt, what exact term fills [BLANK] in: \"Achain of thought is a series of intermediate natural language reasoning steps that lead to the Ô¨Ånal output, and we refer to this approach as [BLANK] prompting .\"?", "gold": "chain-of-thought", "answer": "The exact term is **‚Äúchain-of-thought‚Äù**, i.e., **‚Äúchain-of-thought prompting.‚Äù**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'chain-of-thought' and is directly supported by Context 1 stating 'we refer to this approach as chain-of-thought prompting.'", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 148.008, "llm_ms": 2139.988, "top_contexts": [{"rank": 1, "score": 0.77230644, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.46148227980639256, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 11, "text_snippet": "rompt that consists of triples: ‚ü®input, chain of thought , output‚ü©. Achain of thought is a series of intermediate natural language reasoning steps that lead to the Ô¨Ånal output, and we refer to this approach as chain-of-thought prompting . A"}, {"rank": 2, "score": 0.68875825, "source": "dense+rrf", "rrf_score": 0.028531243721117136, "rerank_score": 0.3240352190459054, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 14, "text_snippet": "ach before giving the Ô¨Ånal answer: ‚ÄúAfter Jane gives 2 Ô¨Çowers to her mom she has 10 ...then after she gives 3 to her dad she will have 7 ...so the answer is 7. ‚Äù The goal of this paper is to endow language models with the ability to generat"}, {"rank": 3, "score": 0.7363517, "source": "dense+rrf", "rrf_score": 0.030309988518943745, "rerank_score": 0.3007773567489859, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 13, "text_snippet": "lity. This work underscores how large language models can learn via a few examples with natural language data about the task (c.f. automatically learning the patterns underlying inputs and outputs via a large training dataset). 2 Chain-of-T"}, {"rank": 4, "score": 0.79769826, "source": "dense+rrf", "rrf_score": 0.031099324975891997, "rerank_score": 0.28869627982531204, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 16, "text_snippet": " for arriving at the answer (and also, solutions/explanations typically come after the Ô¨Ånal answer (Narang et al., 2020; Wiegreffe et al., 2022; Lampinen et al., 2022, inter alia )). Chain-of-thought prompting has several attractive propert"}, {"rank": 5, "score": 33.33169376212481, "source": "dense+rrf", "rrf_score": 0.014084507042253521, "rerank_score": 0.28609711253971326, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 178, "text_snippet": ".3¬±0.6 0.0¬±0.0 50.2¬±0.5 Robustness ¬∑different annotator (B) 27.4 ¬±1.7 75.4¬±2.7 88.3¬±1.4 76.0¬±1.9 77.5¬±7.9 ¬∑different annotator (C) 25.5 ¬±2.5 81.1¬±3.6 85.0¬±1.8 68.1¬±2.2 71.4¬±11.1 23  C Extended Related Work Chain-of-thought prompting is a ge"}]}
{"case_index": 385, "query": "From 2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt, what exact term fills [BLANK] in: \"These methods range from [BLANK] [ 51,74] to low-rank approximation [ 12,50,84], and their combinations [ 3,9,92].\"?", "gold": "sparse-approximation", "answer": "The exact term is **‚Äúsparse-approximation‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'sparse-approximation' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 74.151, "llm_ms": 2315.331, "top_contexts": [{"rank": 1, "score": 0.7028322, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.3840379493857929, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 6, "text_snippet": "memory requirements of attention. These methods range from sparse-approximation [ 51,74] to low-rank approximation [ 12,50,84], and their combinations [ 3,9,92]. Although these methods reduce the compute requirements to linear or near-linea"}, {"rank": 2, "score": 54.73686116546747, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.23840370076846049, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 100, "text_snippet": "n, Atri Rudra, and Christopher R√©. 2022. FlashAttention: Fast and memory-efficient exact attention with IO- awareness. ArXiv:2205.14135. Hermann Ebbinghaus. 1913. Memory: A contribu- tion to experimental psychology. H. A. Ruger & C. E. Buss"}, {"rank": 3, "score": 39.04005060498258, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.20465797534850672, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 135, "text_snippet": "sparse) such as Reformer [ 51] and Smyrf [19] and with low-rank approximation such as Performer [ 12,54]. One can even combine sparse and low-rank approximation for better accuracy (e.g., Longformer [ 3], BigBird [ 92], Scatterbrain [ 9], L"}, {"rank": 4, "score": 39.956400076416315, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.20312499999999584, "doc_id": "2309.12307__longlora_efficient_fine_tuning_of_long_context_large_language_models.txt", "chunk_id": 84, "text_snippet": "mputer/RedPajama-Data . Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. CoRR , abs/2307.08691, 2023. Tri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra, and Christopher R ¬¥e. Flashattention: Fast an"}, {"rank": 5, "score": 37.199463814320964, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.19997668859723136, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 134, "text_snippet": " well as the (dense) FlashAttention on the Long-range Arena tasks. EÔ¨Écient Transformer. Transformer-based models have become the most widely-used architecture in natural language processing [ 22] and computer vision [ 24,91]. However, one o"}]}
{"case_index": 386, "query": "From 2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt, what exact term fills [BLANK] in: \"In [BLANK], we take the 2655 queries where the annotated long answer is a paragraph (as opposed to a list or a table).\"?", "gold": "particular", "answer": "The term is **NaturalQuestions-Open**.", "answer_score": 0.0, "answer_correctness": 0.0, "answer_groundedness": 0.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 0.0, "abstained": false, "should_abstain": false, "notes": "The blank is 'particular' (\"In particular, we take...\"). The model answered 'NaturalQuestions-Open', which is incorrect and not supported as the missing term.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 109.675, "llm_ms": 1036.219, "top_contexts": [{"rank": 1, "score": 0.62212914, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.36714907919871254, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 24, "text_snippet": " access the document that contains the answer within its input context and use it to answer the question. Figure 2 presents an example. We instantiate this task with data from NaturalQuestions-Open (Lee et al., 2019; Kwiatkowski et al., 201"}, {"rank": 2, "score": 0.6162714, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.3186884329697391, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 25, "text_snippet": " is a paragraph (as opposed to a list or a table). We use passages (chunks of at most 100 tokens) from Wikipedia as documents within our input contexts. For each of the queries, we need a document that contains the answer and k‚àí1distractor "}, {"rank": 3, "score": 32.76890675113865, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.27412929394849234, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 83, "text_snippet": " output) as a function of the number of retrieved documents k. We use a subset of NaturalQuestions-Open where the long answer is a paragraph (as opposed to a table or a list). Figure 11 presents retriever recall and open-  510 20 30 40 50 N"}, {"rank": 4, "score": 39.68191210111316, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.2607282744446496, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 0, "text_snippet": "Lost in the Middle: How Language Models Use Long Contexts Nelson F. Liu1‚àóKevin Lin2John Hewitt1Ashwin Paranjape3 Michele Bevilacqua3Fabio Petroni3Percy Liang1 1Stanford University2University of California, Berkeley3Samaya AI nfliu@cs.stanfo"}, {"rank": 5, "score": 28.331530201534928, "source": "dense+rrf", "rrf_score": 0.015151515151515152, "rerank_score": 0.22810900381219676, "doc_id": "2307.03172__lost_in_the_middle_how_language_models_use_long_contexts.txt", "chunk_id": 94, "text_snippet": "levant information, indicating that models struggle to robustly access and use infor- mation in long input contexts. In particular, per- formance is often lowest when models must use information in the middle of long input contexts. We cond"}]}
{"case_index": 387, "query": "From 1706.03762__attention_is_all_you_need.txt, what exact term fills [BLANK] in: \"This masking, combined with fact that the output embeddings are offset by one position, ensures that the [BLANK] for position ican depend only on the known outputs at positions less than i.\"?", "gold": "predictions", "answer": "The exact term is **‚Äúpredictions‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'predictions' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 105.599, "llm_ms": 2326.55, "top_contexts": [{"rank": 1, "score": 0.6665, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.46589277845974314, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 19, "text_snippet": "sequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position ican depend only on the known outputs at positions less than i. 3.2 Attention An attention "}, {"rank": 2, "score": 27.8101191482439, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.23000037464938863, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 275, "text_snippet": " all surprised to see this output come next. ‚Ä¢ 1 = the output is not at all plausible given the input text ‚Äì you would never Ô¨Ånd this text written anywhere on the Internet. ‚Ä¢ 4 = the output is somewhat plausible completion, but it contains "}, {"rank": 3, "score": 34.67074342590134, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.22793643061635832, "doc_id": "1706.03762__attention_is_all_you_need.txt", "chunk_id": 18, "text_snippet": "s also composed of a stack of N= 6identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the enc"}, {"rank": 4, "score": 27.58458828175276, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.21190635958634152, "doc_id": "2205.14135__flashattention_fast_and_memory_efficient_exact_attention_with_io_awareness.txt", "chunk_id": 162, "text_snippet": "r than standard attention. In terms of total memory required, both methods oÔ¨Äer substantial memory saving. The second diÔ¨Äerence between the two methods is the way information is summarized from each block to pass to the next block. Rabe and"}, {"rank": 5, "score": 27.0761432901503, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.2051212822241164, "doc_id": "2203.02155__training_language_models_to_follow_instructions_with_human_feedback.txt", "chunk_id": 277, "text_snippet": "ons of the input, prioritize differences in toxicity in your rankings, rather than differences in continuity. That is, if Output A and Output B are both plausible (i.e. would get a continuity score of 2), and Output A is a bit more plausibl"}]}
{"case_index": 388, "query": "From 2302.13971__llama_open_and_efficient_foundation_language_models.txt, what exact term fills [BLANK] in: \"Data mixtures used for pre- training, for each subset we list the sampling propor- tion, number of epochs [BLANK] on the subset when training on 1.4T tokens, and disk size.\"?", "gold": "performed", "answer": "The exact term is **‚Äúperformed‚Äù** (as in: ‚Äúnumber of epochs **performed** on the subset when training on 1.4T tokens‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches the context sentence exactly: 'number of epochs performed on the subset...'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 80.867, "llm_ms": 1404.674, "top_contexts": [{"rank": 1, "score": 0.68342173, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.39103060110356913, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 14, "text_snippet": "June-August 2022 period, covering 20Dataset Sampling prop. Epochs Disk size CommonCrawl 67.0% 1.10 3.3 TB C4 15.0% 1.06 783 GB Github 4.5% 0.64 328 GB Wikipedia 4.5% 2.45 83 GB Books 4.5% 2.23 85 GB ArXiv 2.5% 1.06 92 GB StackExchange 2.0% "}, {"rank": 2, "score": 0.55363405, "source": "dense+rrf", "rrf_score": 0.02878726010616578, "rerank_score": 0.2499771219471795, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 19, "text_snippet": "1.5e‚àí44M 1.4T Table 2: Model sizes, architectures, and optimization hyper-parameters. Overall, our entire training dataset contains roughly 1.4T tokens after tokenization. For most of our training data, each token is used only once dur- ing"}, {"rank": 3, "score": 0.60502136, "source": "dense+rrf", "rrf_score": 0.02976190476190476, "rerank_score": 0.21501533097741315, "doc_id": "2005.14165__language_models_are_few_shot_learners.txt", "chunk_id": 63, "text_snippet": ".org/the-data/ 8  Figure 2.2: Total compute used during training . Based on the analysis in Scaling Laws For Neural Language Models [KMH+20] we train much larger models on many fewer tokens than is typical. As a consequence, although GPT-3 "}, {"rank": 4, "score": 0.5641752, "source": "dense+rrf", "rrf_score": 0.02821939586645469, "rerank_score": 0.20556995568157313, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 23, "text_snippet": "of 1.0. We use 2,000warmup 0 200 400 600 800 1000 1200 1400 Billion of tokens1.51.61.71.81.92.02.12.2Training lossLLaMA 7B LLaMA 13B LLaMA 33B LLaMA 65BFigure 1: Training loss over train tokens for the 7B, 13B, 33B, and 65 models. LLaMA-33B"}, {"rank": 5, "score": 27.97675186465938, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.19843011089219978, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 0, "text_snippet": "LLaMA: Open and EfÔ¨Åcient Foundation Language Models Hugo Touvron‚àó, Thibaut Lavril‚àó, Gautier Izacard‚àó, Xavier Martinet Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi√®re, Naman Goyal Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand "}]}
{"case_index": 389, "query": "From 2302.13971__llama_open_and_efficient_foundation_language_models.txt, what exact term fills [BLANK] in: \"For instance, LLaMA-13B [BLANK] GPT-3 on most bench- marks, despite being 10 √ósmaller.\"?", "gold": "outperforms", "answer": "The exact term is **‚Äúoutperforms‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'outperforms' and is directly supported by multiple context snippets.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 86.108, "llm_ms": 1136.189, "top_contexts": [{"rank": 1, "score": 0.6711536, "source": "dense+rrf", "rrf_score": 0.031754032258064516, "rerank_score": 0.29400941443592304, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 35, "text_snippet": "WinoGrande. LLaMA-13B model also outperforms GPT-3 on most benchmarks despite being 10 √ósmaller. 3.2 Closed-book Question Answering We compare LLaMA to existing large language models on two closed-book question answering benchmarks: Natural"}, {"rank": 2, "score": 0.7244952, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.28196952611281295, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 6, "text_snippet": "rameters with competitive performance compared to the best existing LLMs. For instance, LLaMA-13B outperforms GPT-3 on most bench- marks, despite being 10 √ósmaller. We believe that this model will help democratize the access and study of LL"}, {"rank": 3, "score": 0.68807566, "source": "dense+rrf", "rrf_score": 0.03200204813108039, "rerank_score": 0.2701525989845251, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 89, "text_snippet": "y for transformer based language models, which were later reÔ¨Åned by Hoffmann et al. (2022), by adapting the learning rate schedule when scaling datasets. Finally, Wei et al. (2022) studied the effect of scal- ing on the abilities of large l"}, {"rank": 4, "score": 0.6694938, "source": "dense+rrf", "rrf_score": 0.029877369007803793, "rerank_score": 0.2201033474393818, "doc_id": "2302.13971__llama_open_and_efficient_foundation_language_models.txt", "chunk_id": 34, "text_snippet": " datasets include Cloze and Winograd style tasks, as well as multiple choice question an- swering. We evaluate in the zero-shot setting as done in the language modeling community. In Table 3, we compare with existing models of various sizes"}, {"rank": 5, "score": 0.6056516, "source": "dense+rrf", "rrf_score": 0.028283227848101264, "rerank_score": 0.18409654026780448, "doc_id": "2404.14219__phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "chunk_id": 116, "text_snippet": "Marie-Anne Lachaux, Timoth¬¥ ee Lacroix, Baptiste Rozi` ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. arXiv preprint"}]}
{"case_index": 390, "query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"For each position i‚ààI, we then obtain up to m API callsc1 i,...,cm iby [BLANK] from Mgiven the sequence [P(x),x1,...,x i‚àí1,<API> ]as a p\"?", "gold": "sampling", "answer": "The term is **‚Äúsampling‚Äù** (i.e., ‚Äúby **sampling** from *M* given the sequence ‚Ä¶‚Äù).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'sampling' and is directly supported by the provided context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 69.627, "llm_ms": 1256.472, "top_contexts": [{"rank": 1, "score": 0.68369615, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.38137440558182756, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 21, "text_snippet": "n+1as a continuation for the sequence z1,...,z n. We Ô¨Årst sample up to kcandidate posi- tions for doing API calls by computing, for each i‚àà{1,...,n}, the probability pi=pM(<API>|P(x),x1:i‚àí1) thatMassigns to starting an API call at position "}, {"rank": 2, "score": 0.5909612, "source": "dense+rrf", "rrf_score": 0.03007688828584351, "rerank_score": 0.2779609939717252, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 27, "text_snippet": "atterns in the pretraining corpus, thus hurting perplexity.  x1:i‚àí1,e(ci,ri),xi:n; we proceed analogously for texts with multiple API calls. Doing this for all x‚àà Cresults in the new dataset C‚àóaugmented with API calls. We use this new datas"}, {"rank": 3, "score": 0.65740645, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.25676307200620674, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 22, "text_snippet": "mpling from Mgiven the sequence [P(x),x1,...,x i‚àí1,<API> ]as a preÔ¨Åx and</API> as an end-of-sequence token.2 2We discard all examples where Mdoes not generate the </API> token.Executing API Calls As a next step, we execute all API calls gen"}, {"rank": 4, "score": 0.5573254, "source": "dense+rrf", "rrf_score": 0.029273504273504274, "rerank_score": 0.2447837865975948, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 1, "text_snippet": "metic or fac- tual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer , a model traine"}, {"rank": 5, "score": 0.57728726, "source": "dense+rrf", "rrf_score": 0.029211087420042643, "rerank_score": 0.23145555613888785, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 12, "text_snippet": " how an API can be used, we let a LM annotate a huge language modeling dataset with potential API calls. We then use a self-supervised loss to determine which of these API calls actually help the model in predicting future tokens. Finally, "}]}
{"case_index": 391, "query": "From 1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt, what exact term fills [BLANK] in: \"Unlike recent language repre- sentation models (Peters et al., 2018a; Rad- ford et al., 2018), BERT is designed to pre- train deep [BLANK] repre\"?", "gold": "bidirectional", "answer": "The exact term is **‚Äúbidirectional‚Äù**, as in: ‚ÄúBERT is designed to pre-train deep **bidirectional** repre‚Ä¶‚Äù", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'bidirectional' and is directly supported by the provided context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 50.732, "llm_ms": 1248.583, "top_contexts": [{"rank": 1, "score": 0.73217964, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.48762761249153347, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 0, "text_snippet": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova Google AI Language {jacobdevlin,mingweichang,kentonl,kristout }@google.com Abstract We introduce a ne"}, {"rank": 2, "score": 0.7260939, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.32158705176655306, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 10, "text_snippet": "ford et al. (2018), which uses unidirec- tional language models for pre-training, BERT uses masked language models to enable pre- trained deep bidirectional representations. This is also in contrast to Peters et al. (2018a), which uses a sh"}, {"rank": 3, "score": 0.5949344, "source": "dense+rrf", "rrf_score": 0.02844551282051282, "rerank_score": 0.268284283320007, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 30, "text_snippet": "ion is constructed by summing the corresponding token, segment, and position embeddings. A visualiza- tion of this construction can be seen in Figure 2. 3.1 Pre-training BERT Unlike Peters et al. (2018a) and Radford et al. (2018), we do not"}, {"rank": 4, "score": 0.6002649, "source": "dense+rrf", "rrf_score": 0.028404512489927477, "rerank_score": 0.2427478062316694, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 9, "text_snippet": "ng, the MLM ob- jective enables the representation to fuse the left and the right context, which allows us to pre- train a deep bidirectional Transformer. In addi- tion to the masked language model, we also use a ‚Äúnext sentence prediction‚Äù "}, {"rank": 5, "score": 0.6897728, "source": "dense+rrf", "rrf_score": 0.031024531024531024, "rerank_score": 0.24190716660041742, "doc_id": "1810.04805__bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding.txt", "chunk_id": 1, "text_snippet": ", 2018), BERT is designed to pre- train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a re- sult, the pre-trained BERT model can be Ô¨Åne- tuned with just one a"}]}
{"case_index": 392, "query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"In contrast to existing approaches, this enables a much more [BLANK] use of tools that is not tied to speciÔ¨Åc tasks.\"?", "gold": "comprehensive", "answer": "The exact term is **‚Äúcomprehensive‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term; directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 64.979, "llm_ms": 991.264, "top_contexts": [{"rank": 1, "score": 0.62783027, "source": "dense+rrf", "rrf_score": 0.03177805800756621, "rerank_score": 0.35851275268294475, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 11, "text_snippet": "when andhow to use which tool. In contrast to existing approaches, this enables a much more comprehensive use of tools that is not tied to speciÔ¨Åc tasks. Our approach for achieving these goals is based on the recent idea of using large LMs "}, {"rank": 2, "score": 0.6146755, "source": "dense+rrf", "rrf_score": 0.03057889822595705, "rerank_score": 0.2808772717633871, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 1, "text_snippet": "metic or fac- tual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer , a model traine"}, {"rank": 3, "score": 0.5603116, "source": "dense+rrf", "rrf_score": 0.02690501986276634, "rerank_score": 0.26775542371359995, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 43, "text_snippet": " mod- els, respectively. 4.2 Downstream Tasks We evaluate all models on a variety of downstream tasks. In all cases, we consider a prompted zero- shot setup ‚Äì i.e., models are instructed to solve each task in natural language, but we do not"}, {"rank": 4, "score": 0.6268471, "source": "dense+rrf", "rrf_score": 0.029437229437229435, "rerank_score": 0.24661070217295286, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 13, "text_snippet": "h tool to use when and how. As our approach is agnostic of the dataset be- ing used, we can apply it to the exact same dataset that was used to pretrain a model in the Ô¨Årst place. This ensures that the model does not lose any of its general"}, {"rank": 5, "score": 0.6761433, "source": "dense+rrf", "rrf_score": 0.03177805800756621, "rerank_score": 0.23938571760292252, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 90, "text_snippet": "pilan et al., 2022) or they work by prompting the language model in a few-shot setup tailored towards a speciÔ¨Åc task where it is known a priori which tools needs to beused (Gao et al., 2022; Lazaridou et al., 2022; Yao et al., 2022). In con"}]}
{"case_index": 393, "query": "From 2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt, what exact term fills [BLANK] in: \"The development trajectory of RAG in the era of large models exhibits several distinct stage [BLANK].\"?", "gold": "characteristics", "answer": "The exact term is **‚Äúcharacteristics‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term and is directly supported by Context 1 phrase 'distinct stage characteristics'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 65.315, "llm_ms": 827.303, "top_contexts": [{"rank": 1, "score": 0.6659981, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.2706906902056579, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 6, "text_snippet": "AG technology has rapidly developed in recent years, and the technology tree summarizing related research is shown Corresponding Author.Email:haofen.wang@tongji.edu.cn 1Resources are available at https://github.com/Tongji-KGLLM/ RAG-Surveyi"}, {"rank": 2, "score": 0.6497679, "source": "dense+rrf", "rrf_score": 0.03149801587301587, "rerank_score": 0.19970339398648804, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 13, "text_snippet": "he fine-tuning of LLMs. Researchers have also been exploring ways to enhance language models in the pre-training stage through retrieval-augmented techniques. advanced RAG, and modular RAG. This review contex- tualizes the broader scope of "}, {"rank": 3, "score": 0.6417636, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.19818893963766354, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 3, "text_snippet": "augmentation techniques. The paper highlights the state-of-the- art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-"}, {"rank": 4, "score": 0.61124253, "source": "dense+rrf", "rrf_score": 0.029877369007803793, "rerank_score": 0.19618228754774591, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 0, "text_snippet": "1 Retrieval-Augmented Generation for Large Language Models: A Survey Yunfan Gaoa, Yun Xiongb, Xinyu Gaob, Kangxiang Jiab, Jinliu Panb, Yuxi Bic, Yi Daia, Jiawei Suna, Meng Wangc, and Haofen Wanga,c aShanghai Research Institute for Intellige"}, {"rank": 5, "score": 0.606521, "source": "dense+rrf", "rrf_score": 0.029957522915269395, "rerank_score": 0.19465815683768073, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 11, "text_snippet": "h a detailed and structured understanding of both large models and RAG. It aims to illuminate the evolution of retrieval augmentation techniques, assess the strengths and weaknesses of various approaches in their respective contexts, and sp"}]}
{"case_index": 394, "query": "From 2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt, what exact term fills [BLANK] in: \"her [BLANK] prompting in this form can successfully elicit successful reasoning across a range of 3 Q: Roger has 5 tennis balls.\"?", "gold": "chain-of-thought", "answer": "The exact term that fills **[BLANK]** is:\n\n**chain-of-thought**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "The blank is 'chain-of-thought', explicitly shown in Context 1 and consistent with the paper title and other contexts.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 85.26, "llm_ms": 1207.495, "top_contexts": [{"rank": 1, "score": 0.6013421, "source": "dense+rrf", "rrf_score": 0.032266458495966696, "rerank_score": 0.39909972349339934, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 24, "text_snippet": "her chain-of-thought prompting in this form can successfully elicit successful reasoning across a range of 3  Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have "}, {"rank": 2, "score": 0.57625103, "source": "dense+rrf", "rrf_score": 0.029211087420042643, "rerank_score": 0.24886151798154119, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 71, "text_snippet": "revailing theme (Wei et al., 2022b). For many reasoning tasks where standard prompting has a Ô¨Çat scaling curve, chain- of-thought prompting leads to dramatically increasing scaling curves. Chain-of-thought prompting appears to expand the se"}, {"rank": 3, "score": 0.59572965, "source": "dense+rrf", "rrf_score": 0.029513888888888888, "rerank_score": 0.22778285626786435, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 19, "text_snippet": "rve the utility of chain-of-thought prompting for arithmetic reasoning (Section 3), commonsense reasoning (Section 4), and symbolic reasoning (Section 5). 3 Arithmetic Reasoning We begin by considering math word problems of the form in Figu"}, {"rank": 4, "score": 0.5756955, "source": "dense+rrf", "rrf_score": 0.028790389395194696, "rerank_score": 0.22349049945976096, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 1, "text_snippet": "such reasoning abilities emerge naturally in sufÔ¨Åciently large language models via a simple method called chain-of- thought prompting , where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three"}, {"rank": 5, "score": 40.341629314223475, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.21974358974358527, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 114, "text_snippet": "-augmented ensembles in language models. arXiv preprint arXiv:2207.00747 , 2022b. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models"}]}
{"case_index": 395, "query": "From 2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt, what exact term fills [BLANK] in: \"4.Finally, [BLANK] reasoning can be readily elicited in sufÔ¨Åciently large off-the-shelf language models simply by including examples of chain of thought sequences into the exemplars of few-shot prompting.\"?", "gold": "chain-of-thought", "answer": "The exact term is **‚Äúchain-of-thought‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term and is directly supported by the provided context sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 46.246, "llm_ms": 957.775, "top_contexts": [{"rank": 1, "score": 0.79945695, "source": "dense+rrf", "rrf_score": 0.03278688524590164, "rerank_score": 0.47835944122277946, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 18, "text_snippet": " used for tasks such as math word problems, commonsense reasoning, and symbolic manipulation, and is potentially applicable (at least in principle) to any task that humans can solve via language. 4.Finally, chain-of-thought reasoning can be"}, {"rank": 2, "score": 0.7696802, "source": "dense+rrf", "rrf_score": 0.031746031746031744, "rerank_score": 0.285269931879062, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 1, "text_snippet": "such reasoning abilities emerge naturally in sufÔ¨Åciently large language models via a simple method called chain-of- thought prompting , where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three"}, {"rank": 3, "score": 0.781907, "source": "dense+rrf", "rrf_score": 0.03225806451612903, "rerank_score": 0.2688811706967339, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 70, "text_snippet": "t generally applicable (Section 4). Finally, we showed that for symbolic reasoning, chain-of-thought prompting facilitates OOD generalization to longer sequence lengths (Section 5). In all experiments, chain-of-thought reasoning is elicited"}, {"rank": 4, "score": 42.8058047436729, "source": "dense+rrf", "rrf_score": 0.015384615384615385, "rerank_score": 0.22725633840404058, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 190, "text_snippet": "h the ability to produce intermediate steps, prior work typically Ô¨Ånetunes models on either manually annotated training datasets (Camburu et al., 2018; Rajani et al., 2019, inter alia ) or generates synthetic datasets (Talmor et al., 2020; "}, {"rank": 5, "score": 29.9589356423375, "source": "dense+rrf", "rrf_score": 0.014084507042253521, "rerank_score": 0.22357352924437054, "doc_id": "2201.11903__chain_of_thought_prompting_elicits_reasoning_in_large_language_models.txt", "chunk_id": 139, "text_snippet": "Model OutputQuestion Question QuestionFigure 10: Examples of semantic understanding and one-step missing errors that were Ô¨Åxed by scaling PaLM from 62B to 540B. A.2 What is the role of prompt engineering? One of the key considerations of pr"}]}
{"case_index": 396, "query": "From 2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt, what exact term fills [BLANK] in: \"The New England Journal of Medicine is a registered trademark of [QA(‚ÄúWho is the publisher of The New England Journal of Medicine?‚Äù) ‚Üí [BLANK] Medical Society] the MMS.\"?", "gold": "massachusetts", "answer": "Massachusetts", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context explicitly shows 'Massachusetts Medical Society', so blank is 'massachusetts'. Model answer matches.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 62.23, "llm_ms": 1093.966, "top_contexts": [{"rank": 1, "score": 83.26995028402422, "source": "dense+rrf", "rrf_score": 0.01639344262295082, "rerank_score": 0.32854153357047333, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 4, "text_snippet": "lucinate facts (Maynez et al., 2020; Ji et al., 2022), difÔ¨Åcul- ties in understanding low-resource languages (Lin et al., 2021), a lack of mathematical skills to per- form precise calculations (Patel et al., 2021) and an unawareness of the "}, {"rank": 2, "score": 31.43497555419874, "source": "dense+rrf", "rrf_score": 0.015625, "rerank_score": 0.22939916997969062, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 1, "text_snippet": "metic or fac- tual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer , a model traine"}, {"rank": 3, "score": 35.61203952730132, "source": "dense+rrf", "rrf_score": 0.015873015873015872, "rerank_score": 0.1985242657881543, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 0, "text_snippet": "Toolformer: Language Models Can Teach Themselves to Use Tools Timo Schick Jane Dwivedi-Yu Roberto Dess√¨‚Ä†Roberta Raileanu Maria Lomeli Luke Zettlemoyer Nicola Cancedda Thomas Scialom Meta AI Research‚Ä†Universitat Pompeu Fabra Abstract Languag"}, {"rank": 4, "score": 22.124206036694492, "source": "dense+rrf", "rrf_score": 0.014492753623188406, "rerank_score": 0.19547715575304325, "doc_id": "2302.04761__toolformer_language_models_can_teach_themselves_to_use_tools.txt", "chunk_id": 13, "text_snippet": "h tool to use when and how. As our approach is agnostic of the dataset be- ing used, we can apply it to the exact same dataset that was used to pretrain a model in the Ô¨Årst place. This ensures that the model does not lose any of its general"}, {"rank": 5, "score": 44.31340279960361, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.193677014805523, "doc_id": "2312.10997__retrieval_augmented_generation_for_large_language_models_a_survey.txt", "chunk_id": 212, "text_snippet": "line decision making: Benchmarks and additional opinions,‚Äù arXiv preprint arXiv:2306.02224 , 2023. [108] T. Schick, J. Dwivedi-Yu, R. Dess `ƒ±, R. Raileanu, M. Lomeli, L. Zettle- moyer, N. Cancedda, and T. Scialom, ‚ÄúToolformer: Language mode"}]}
{"case_index": 397, "query": "From 2305.06983__active_retrieval_augmented_generation.txt, what exact term fills [BLANK] in: \"It [BLANK] generates search queries (shown in gray italic ) to retrieve relevant information to aid future generations.\"?", "gold": "iteratively", "answer": "The exact term is **‚Äúiteratively‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context 1 explicitly contains the sentence with the blank filled by 'iteratively', matching the expected answer.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 57.891, "llm_ms": 990.68, "top_contexts": [{"rank": 1, "score": 74.34870921813842, "source": "dense+rrf", "rrf_score": 0.01639344262295082, "rerank_score": 0.36234493788334754, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 28, "text_snippet": "University)][Search(Joe Biden degree)]the University of Pennsylvania, where he earneda law degree.Generate a summary about Joe Biden.Input$&$&#%$&%%%GenerationRetriever$%$%%Figure 2: An illustration of forward-looking active re- trieval aug"}, {"rank": 2, "score": 0.6397993, "source": "dense+rrf", "rrf_score": 0.031754032258064516, "rerank_score": 0.23478742437811959, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 16, "text_snippet": ". When deciding what to retrieve , it is important to consider what LMs intend to gen- erate in the future, as the goal of active retrieval is to benefit future generations. Therefore, we propose anticipating the future by generating a temp"}, {"rank": 3, "score": 0.6018964, "source": "dense+rrf", "rrf_score": 0.031024531024531024, "rerank_score": 0.229521116700534, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 11, "text_snippet": "eddocumentsLMGeneration$%$%% Figure 1: An illustration of forward-looking active retrieval augmented generation (FLARE). Starting with the user input xand initial retrieval results Dx, FLARE iteratively generates a temporary next sentence ("}, {"rank": 4, "score": 44.43243426552883, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.22323344573837856, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 31, "text_snippet": "y)]‚Äù (shown in gray italic ), we stop the generation and use the query terms to retrieve relevant documents, which are prepended before the user input to aid future generation until the next search query is generated or reaches the end. Add"}, {"rank": 5, "score": 35.936023504118936, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.20872287457631625, "doc_id": "2305.06983__active_retrieval_augmented_generation.txt", "chunk_id": 30, "text_snippet": "can be found in Prompt D.3. Prompt 3.1: retrieval instructions Skill 1. An instruction to guide LMs to generate search queries. Several search-related exemplars. Skill 2. An instruction to guide LMs to perform a specific downstream task (e."}]}
{"case_index": 398, "query": "From 2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt, what exact term fills [BLANK] in: \"This highlights the potential of endowing smaller models with better reasoning [BLANK].\"?", "gold": "capabilities", "answer": "The exact term that fills **[BLANK]** is **‚Äúcapabilities‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'capabilities' and is directly supported by Context 1 sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 52.376, "llm_ms": 1275.562, "top_contexts": [{"rank": 1, "score": 0.6418536, "source": "dense+rrf", "rrf_score": 0.03131881575727918, "rerank_score": 0.27476321198326453, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 20, "text_snippet": "problem solving, reading comprehension, summarization, groundedness, truthfulness and toxic content generation and identification. Our preliminary results indicate that Orca 2 significantly surpasses models of a similar size, even matching "}, {"rank": 2, "score": 31.72843894149969, "source": "dense+rrf", "rrf_score": 0.016129032258064516, "rerank_score": 0.25680892798057553, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 148, "text_snippet": "synthetic data for post-training. While Orca 2 models have not gone through RLHF training for safety, we believe that the use of synthetic data for post-training that has been filtered with various content safety filters could provide anoth"}, {"rank": 3, "score": 0.67596585, "source": "dense+rrf", "rrf_score": 0.0315136476426799, "rerank_score": 0.25357158739857666, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 1, "text_snippet": "n benchmarks like BigBench Hard and AGIEval. In Orca 2, we continue exploring how improved training signals can enhance smaller LMs‚Äô reasoning abilities. Research on training small LMs has often relied on imitation learning to replicate the"}, {"rank": 4, "score": 25.37330006149412, "source": "dense+rrf", "rrf_score": 0.014705882352941176, "rerank_score": 0.2311893529660104, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 136, "text_snippet": "g ). We mix the task-specific data with the rest of the training dataset and evaluate Orca 2 on a distinct set of the ROCStories corpus [ 41]. While sampling the test instances, we remove any instances from ROCStories that are in FLAN train"}, {"rank": 5, "score": 25.594041224158033, "source": "dense+rrf", "rrf_score": 0.014925373134328358, "rerank_score": 0.22581948043932276, "doc_id": "2311.11045__orca_2_teaching_small_language_models_how_to_reason.txt", "chunk_id": 281, "text_snippet": " you always used to look like this , did n‚Äôt you , _‚Äù What is the word in the blank space (_)? The answer is Orca-v2-13B-Cautious The answer is izzy. The word in the blank space is the name of the girl who is crying. Figure 24: Demonstrativ"}]}
{"case_index": 399, "query": "From 2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt, what exact term fills [BLANK] in: \"This tight synergy between ‚Äúacting‚Äù and ‚Äúreasoning‚Äù allows humans to learn new tasks quickly and perform robust decision making or reasoning, even under previously unseen circumstances or facing information [BLANK].\"?", "gold": "uncertainties", "answer": "The exact term is **‚Äúuncertainties‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'uncertainties' and is directly supported by the provided context sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 96.125, "llm_ms": 1126.777, "top_contexts": [{"rank": 1, "score": 0.67697895, "source": "dense+rrf", "rrf_score": 0.03252247488101534, "rerank_score": 0.42689392482341726, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 6, "text_snippet": "ealize when external information is needed (‚Äúhow do I prepare dough? Let me search on the Internet‚Äù). We may also act (open a cookbook to read the recipe, open the fridge, check ingredients) to support the reasoning and to answer questions "}, {"rank": 2, "score": 0.69116956, "source": "dense+rrf", "rrf_score": 0.032266458495966696, "rerank_score": 0.2410114803425379, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 19, "text_snippet": "udies on how reasoning and acting can be combined in a synergistic manner for general task solving, and if such a combination can bring systematic beneÔ¨Åts compared to reasoning or acting alone. In this work, we present ReAct , a general par"}, {"rank": 3, "score": 0.64517635, "source": "dense+rrf", "rrf_score": 0.03128054740957967, "rerank_score": 0.20566700658222842, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 7, "text_snippet": "circumstances or facing information uncertainties. Recent results have hinted at the possibility of combining verbal reasoning with interactive decision making in autonomous systems. On one hand, properly prompted large language models (LLM"}, {"rank": 4, "score": 0.63363355, "source": "dense+rrf", "rrf_score": 0.029211087420042643, "rerank_score": 0.19557887709410754, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 1, "text_snippet": "rformance across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In th"}, {"rank": 5, "score": 0.633147, "source": "dense+rrf", "rrf_score": 0.02946912242686891, "rerank_score": 0.19416531421802677, "doc_id": "2210.03629__react_synergizing_reasoning_and_acting_in_language_models.txt", "chunk_id": 88, "text_snippet": " the development of versatile and generalist agents like Reed et al. (2022). 6 C ONCLUSION We have proposed ReAct ‚Äì a simple yet effective method for synergizing reasoning and acting in large language models. Through a diverse set of experi"}]}
{"case_index": 400, "query": "From 2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt, what exact term fills [BLANK] in: \"Only a few top retrieval units (1 to 8 retrieval units in the four datasets we tested on), without [BLANK], are used for the next step.\"?", "gold": "re-ranking", "answer": "The exact term that fills **[BLANK]** is **‚Äúre-ranking‚Äù**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Model answer matches expected term 're-ranking' and is explicitly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 59.125, "llm_ms": 1039.565, "top_contexts": [{"rank": 1, "score": 0.61713576, "source": "dense+rrf", "rrf_score": 0.031099324975891997, "rerank_score": 0.417061937805646, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 13, "text_snippet": "ybysearching through all the long retrieval units in the corpus. Only a few top retrieval units (1 to 8 retrieval units in the four datasets we tested on), without re-ranking, are used for the next step. Compared to retrieving hundreds of s"}, {"rank": 2, "score": 0.6060997, "source": "dense+rrf", "rrf_score": 0.029386529386529386, "rerank_score": 0.2704847396676118, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 8, "text_snippet": "ever needs to scan over a massive amount of units to find the relevant piece. In contrast, LongRAG operates on long retrieval units (30x longer). The retriever of LongRAG has a significantly reduced workload, achieving strong retrieval qual"}, {"rank": 3, "score": 0.6187798, "source": "dense+rrf", "rrf_score": 0.03128054740957967, "rerank_score": 0.2665606966121775, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 87, "text_snippet": "a ‚Äúlong reader‚Äù component on top of the 4K-token retrieval units. Our proposed framework can significantly reduce the corpus size, enabling strong retrieval recall using only a few top units, thereby minimizing noise from hard negatives. On"}, {"rank": 4, "score": 0.60469925, "source": "dense+rrf", "rrf_score": 0.028083267871170464, "rerank_score": 0.257909942529571, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 21, "text_snippet": "operates on long retrieval units, with only a few (typically fewer than 10) being fed into the reader. An illustrative example is shown in Figure 2. 2.1 Long Retriever The traditional RAG framework employs smaller retrieval units and priori"}, {"rank": 5, "score": 0.6257819, "source": "dense+rrf", "rrf_score": 0.02886002886002886, "rerank_score": 0.2525197100775968, "doc_id": "2406.15319__longrag_enhancing_retrieval_augmented_generation_with_long_context_llms.txt", "chunk_id": 79, "text_snippet": "on the long-context understanding ability of LLMs, and we still have a long way to go in harnessing open-source LLMs within our framework. 10  4 Related Work 4.1 Retrieval-Augmented Generation. Augmenting language models with information re"}]}
