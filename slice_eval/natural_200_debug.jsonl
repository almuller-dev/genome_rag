{"case_index": 1, "query": "According to the paper 'refrag rethinking rag based decoding', what exact term completes this statement: \"with many retrieved passages being [BLANK] and reused across multiple inferences.\"?", "gold": "uninformative", "answer": "The missing term is **“uninformative”** — i.e., “with many retrieved passages being **uninformative** and reused across multiple inferences.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'uninformative' and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 9.613, "llm_ms": 2049.668, "top_contexts": [{"rank": 1, "score": 0.71116275, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 11, "text_snippet": "with many retrieved passages being uninformative and reused across multiple inferences. Allocating memory/computation for all the tokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The ret"}, {"rank": 2, "score": 0.66728914, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 2, "text_snippet": "ated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns th"}, {"rank": 3, "score": 0.6286733, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 13, "text_snippet": "nd memory usage during decoding, allwithout requiring modificationsto the LLM architecture or introducing new decoder parameters. REFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved passag"}, {"rank": 4, "score": 0.61777127, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 12, "text_snippet": "is information is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-a"}, {"rank": 5, "score": 0.6069732, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 29, "text_snippet": "teness. In contrast, common RAG approaches retrieve passages indiscriminately, without ensuring complete support from cited sources. 3.1 P ROBLEM FORMALIZATION AND OVERVIEW Formally, given input x, we train Mto sequentially generate textual"}]}
{"case_index": 2, "query": "According to the paper 'retrieval augmented generation for knowledge intensive nlp tasks', what exact term completes this statement: \"In contrast, we explore a setting where both parametric and [BLANK] memory components are pre-trained and pre-loaded with extensive knowledge.\"?", "gold": "non-parametric", "answer": "The missing term is **“non-parametric”**, completing the statement as:\n\n“In contrast, we explore a setting where both parametric and **non-parametric** memory components are pre-trained and pre-loaded with extensive knowledge.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'non-parametric' and is directly supported by Context 1/4 quoting the exact sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.201, "llm_ms": 1742.298, "top_contexts": [{"rank": 1, "score": 0.7448431, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 12, "text_snippet": "ck- augmented networks [ 25] and memory layers [ 30]. In contrast, we explore a setting where both parametric and non-parametric memory components are pre-trained and pre-loaded with extensive knowledge. Crucially, by using pre-trained acce"}, {"rank": 2, "score": 0.68796873, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 6, "text_snippet": "eval-augmented architecture. These models employ a non-parametric memory, e.g. a neural retriever over a large, external, potentially non-static knowledge source to enhance a parametric language model. In addition to their memorisation abil"}, {"rank": 3, "score": 0.6641427, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 1, "text_snippet": "ameters, and achieve state-of-the-art results when ﬁne-tuned on down- stream NLP tasks. However, their ability to access and precisely manipulate knowl- edge is still limited, and hence on knowledge-intensive tasks, their performance lags b"}, {"rank": 4, "score": 0.6323246, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 9, "text_snippet": "y to the “workhorse of NLP,” i.e. sequence-to-sequence (seq2seq) models. We endow pre-trained, parametric-memory generation models with a non-parametric memory through a general-purpose ﬁne-tuning approach which we refer to as retrieval-aug"}, {"rank": 5, "score": 0.62784266, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 3, "text_snippet": "itly in their parameters. However, they are also prone to hallucination and cannot represent the full long tail of knowledge from the training corpus. Retrieval-augmented language models (Khandelwal et al., 2020; Borgeaud et al., 2022; Izac"}]}
{"case_index": 3, "query": "According to the paper 'refrag rethinking rag based decoding', what exact term completes this statement: \"In addition, our optimization framework for large context [BLANK] extend the context size of LLMs by16 ×.\"?", "gold": "enablesrefragto", "answer": "The missing term is **“enables”**:\n\n“In addition, our optimization framework for large context **enables** REFRAG to extend the context size of LLMs by 16 ×.”", "answer_score": 0.3, "answer_correctness": 0.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context shows the full phrase is \"enablesREFRAGto\" (i.e., enables REFRAG to). Model answered only \"enables\", which does not match the expected exact term.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.266, "llm_ms": 1319.585, "top_contexts": [{"rank": 1, "score": 0.71388215, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 3, "text_snippet": " on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30 .85×the time-t"}, {"rank": 2, "score": 0.66439605, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 95, "text_snippet": "oducedREFRAG, a novel and efficient decoding framework tailored for RAG applications. By leveraging the inherent sparsity and block-diagonal attention patterns present in RAG contexts,REFRAG compresses, senses, and expands context represent"}, {"rank": 3, "score": 0.6643262, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 42, "text_snippet": "xt lengths s∈ { 4096,8192,16384}. Although our model is trained on s+o= 6144, both REFRAG 8andREFRAG 16maintain superior performance at longer contexts. The original Llama-2-7B supports only a4k context window, whereas our approach enables "}, {"rank": 4, "score": 0.66399395, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 0, "text_snippet": "REFRAG: Rethinking RAG based Decoding Xiaoqiang Lin1,2,∗,Aritra Ghosh1,Bryan Kian Hsiang Low2,Anshumali Shrivastava1,3,Vijai Mohan1 1Meta Superintelligence Labs,2National University of Singapore,3Rice University ∗Work done at Meta Large Lan"}, {"rank": 5, "score": 0.6610678, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 39, "text_snippet": "MA-2-7B evaluated on xs+1:s+owith only output tokens as input.LLaMA-Full Context: LLaMA-2-7B evaluated on xs+1:s+owith the full sequence x1:Tas input.CEPE: Memory-efficient long-context model (Yen et al., 2024) a previous SOTA model which s"}]}
{"case_index": 4, "query": "According to the paper 'refrag rethinking rag based decoding', what exact term completes this statement: \"igure 1) while preserving the [BLANK] nature of the decoder, thereby supporting multi-turn and agentic applications.\"?", "gold": "autoregressive", "answer": "The missing term is **“autoregressive”**, as in: “while preserving the **autoregressive** nature of the decoder, thereby supporting multi-turn and agentic applications.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Model answer matches expected term 'autoregressive' and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 17.574, "llm_ms": 2949.121, "top_contexts": [{"rank": 1, "score": 0.70578134, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 15, "text_snippet": "igure 1) while preserving the autoregressive nature of the decoder, thereby supporting multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight reinforcement learning (RL) policy that sel"}, {"rank": 2, "score": 0.6404817, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 12, "text_snippet": "is information is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-a"}, {"rank": 3, "score": 0.62969196, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 95, "text_snippet": "oducedREFRAG, a novel and efficient decoding framework tailored for RAG applications. By leveraging the inherent sparsity and block-diagonal attention patterns present in RAG contexts,REFRAG compresses, senses, and expands context represent"}, {"rank": 4, "score": 0.62696797, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 17, "text_snippet": "FRAG achieves better performance than LLaMA without incurring higher latency in the downstream applications. 2 Model Architecture We denote the decoder model as Mdecand the encoder model as Menc. Given an input with Ttokens x1, x2, . . . , "}, {"rank": 5, "score": 0.6250446, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 3, "text_snippet": " on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30 .85×the time-t"}]}
{"case_index": 5, "query": "According to the paper 'atlas few shot learning with retrieval augmented language models', what exact term completes this statement: \"Unfortunately, this approach does not scale with the number of documents, since the [BLANK] in the encoder results in a quadratic complexity with respect to the number of documents.\"?", "gold": "self-attention", "answer": "The missing term is **“self-attention”** (i.e., “since the self-attention in the encoder results in a quadratic complexity with respect to the number of documents.”).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'self-attention' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.811, "llm_ms": 1468.899, "top_contexts": [{"rank": 1, "score": 0.62645066, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 22, "text_snippet": "ther way to process the retrieved documents in the language model would be to concatenate the query and all the documents, and to use this long sequence as input of the model. Unfortunately, this approach does not scale with the number of d"}, {"rank": 2, "score": 0.61932427, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 49, "text_snippet": "ead. As we will see in practice, the impact of ﬁxing the documents encoder varies greatly for diﬀerent tasks when a large training dataset is available. For most of the few-shot settings that we consider, query-side ﬁnetuning does not have "}, {"rank": 3, "score": 0.60236526, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 59, "text_snippet": " memory by leveraging approximate nearest neighbors search (Grave et al., 2017a). The related kNN-LM model (Khandelwal et al., 2020) replaced LSTMs by transformer networks, and scaled the memory to billions of tokens, leading to strong perf"}, {"rank": 4, "score": 0.6016489, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 43, "text_snippet": "computation model may seem naive, the main assumption is that document sizes are constant.1Since we split long documents into passages with similar number of words, and use padding when processing documents of diﬀerent sizes, this assumptio"}, {"rank": 5, "score": 0.58855855, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 40, "text_snippet": "ent retriever ﬁne-tuning Retrieval is facilitated by using a document index, which is a pre-computed collection of the document embeddings for all the documents in the retrieval corpus. When jointly training the retriever and language model"}]}
{"case_index": 6, "query": "According to the paper 'refrag rethinking rag based decoding', what exact term completes this statement: \"3 [BLANK] To align the encoder and decoder, we follow the work of Yen et al.\"?", "gold": "methodology", "answer": "The missing term is **“Positional Encoding”** — i.e., the statement is:\n\n“**3 Positional Encoding** To align the encoder and decoder, we follow the work of Yen et al.”", "answer_score": 0.0, "answer_correctness": 0.0, "answer_groundedness": 0.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 0.0, "abstained": false, "should_abstain": false, "notes": "Expected term is \"methodology\" but the model answered \"Positional Encoding\". The provided context does not contain support for either term in the quoted statement.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 13.436, "llm_ms": 1360.603, "top_contexts": [{"rank": 1, "score": 0.66362405, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 12, "text_snippet": "is information is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-a"}, {"rank": 2, "score": 0.6476555, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 3, "text_snippet": " on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30 .85×the time-t"}, {"rank": 3, "score": 0.63741004, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 95, "text_snippet": "oducedREFRAG, a novel and efficient decoding framework tailored for RAG applications. By leveraging the inherent sparsity and block-diagonal attention patterns present in RAG contexts,REFRAG compresses, senses, and expands context represent"}, {"rank": 4, "score": 0.6279387, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 21, "text_snippet": "uery  Encoder Figure 1The main design ofREFRAG. The input context is chunked and processed by the light-weight encoder to produce chunk embeddings, which are precomputable for efficient reuse. A light-weight RL policy decide few chunks to e"}, {"rank": 5, "score": 0.6270313, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 0, "text_snippet": "REFRAG: Rethinking RAG based Decoding Xiaoqiang Lin1,2,∗,Aritra Ghosh1,Bryan Kian Hsiang Low2,Anshumali Shrivastava1,3,Vijai Mohan1 1Meta Superintelligence Labs,2National University of Singapore,3Rice University ∗Work done at Meta Large Lan"}]}
{"case_index": 7, "query": "According to the paper 'leveraging passage retrieval with generative models for open domain question answering', what exact term completes this statement: \"Then, a [BLANK] model generates the answer, taking as input the re- trieved passages in addition to the question.\"?", "gold": "sequence-to-sequence", "answer": "The missing term is **“generative”** — i.e., “Then, a **generative** model generates the answer, taking as input the retrieved passages in addition to the question.”", "answer_score": 0.06, "answer_correctness": 0.0, "answer_groundedness": 0.2, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 0.2, "abstained": false, "should_abstain": false, "notes": "Expected term is \"sequence-to-sequence\"; model answered \"generative\". While the paper discusses generative models, the exact quoted blank is not supported by the provided context and is incorrect.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 8.134, "llm_ms": 1372.553, "top_contexts": [{"rank": 1, "score": 0.6655046, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 21, "text_snippet": "Raffel et al., 2019; Lewis et al., 2019). The model takes as input the question, as well as the support passages, and generates the answer. More precisely, each retrieved passage and its title are concatenated with the question, and process"}, {"rank": 2, "score": 0.65569484, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}, {"rank": 3, "score": 0.6535494, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 0, "text_snippet": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering Gautier Izacard1,2,3Edouard Grave1 1Facebook AI Research, Paris 2ENS, PSL University, Paris 3Inria, Paris gizacard|egrave@fb.com Abstract Generative mode"}, {"rank": 4, "score": 0.65180564, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 31, "text_snippet": "tion answering. We observe that while conceptu- ally simple, this method outperforms existing work on the NaturalQuestion and TriviaQA benchmarks. In particular, generative models seem to perform well when evidence from multiple passages ne"}, {"rank": 5, "score": 0.63624215, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 16, "text_snippet": "las (Izacard et al., 2022), which fine-tunes an encoder- decoder model in conjunction with the retriever; REALM (Guu et al., 2020), a bidirectional, masked LM fine-tuned for open-domain question answering; and RAG (Retrieval-Augmented Gener"}]}
{"case_index": 8, "query": "According to the paper 'corrective retrieval augmented generation', what exact term completes this statement: \"The examples show that a low-quality retriever is prone to introducing a [BLANK] amount of irrelevant information, impeding the generators from acquiring accurate knowledge and potentially misleading them.\"?", "gold": "substantial", "answer": "The missing term is **“substantial”**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'substantial' and is directly supported by the provided context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.739, "llm_ms": 1291.725, "top_contexts": [{"rank": 1, "score": 0.7395158, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 6, "text_snippet": " The examples show that a low-quality retriever is prone to introducing a substantial amount of irrelevant information, impeding the generators from acquiring accurate knowledge and potentially misleading them. the parametric knowledge they"}, {"rank": 2, "score": 0.67567897, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 8, "text_snippet": "wledge raises significant concerns about the model’s behavior and performance in scenarios where retrieval may fail or return inaccu- rate results (Shi et al., 2023). As Figure 1 shows that a low-quality retriever is prone to introducingarX"}, {"rank": 3, "score": 0.60641384, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 18, "text_snippet": "pact the LLMs and exacerbate the problems. Thus, it is obvious that the lack of accurate and specific knowledge can lead to misleading or even inaccurate generation, which will severely hurt the experience of users in most practical applica"}, {"rank": 4, "score": 0.6044447, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 10, "text_snippet": " should not have been equally referred to and involved in RAG. On account of the above issues, this paper particularly studies the scenarios where the retriever returns inaccurate results. A method named Corrective Retrieval- Augmented Gene"}, {"rank": 5, "score": 0.58892775, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 1, "text_snippet": "evitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Al- though retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heav"}]}
{"case_index": 9, "query": "According to the paper 'raptor recursive abstractive processing for tree organized retrieval', what exact term completes this statement: \"The overall probability [BLANK] is a weighted combination P(x) =PK k=1πkN(x;µk,Σk), where πksignifies the mixture weight for the kthGaussian distribution.\"?", "gold": "distribution", "answer": "The missing term is **“distribution”**, completing the sentence as:\n\n“The overall probability **distribution** is a weighted combination \\(P(x) = \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(x; \\mu_k, \\Sigma_k)\\), where \\(\\pi_k\\) signifies the mixture weight for the \\(k\\)th Gaussian distribution.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer correctly fills the blank with 'distribution' and directly matches Context 1 wording.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 12.061, "llm_ms": 2241.465, "top_contexts": [{"rank": 1, "score": 0.5456979, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 28, "text_snippet": " Published as a conference paper at ICLR 2024 Given a set of Ntext segments, each represented as a d-dimensional dense vector embedding, the likelihood of a text vector, x, given its membership in the kthGaussian distribution, is denoted by"}, {"rank": 2, "score": 0.52347815, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 31, "text_snippet": "′⊂ D with the highest simi- larity scores from a corpus Dgiven an input context x, as described in §3.1. We then compute the retrieval likelihood of each retrieved document d: PR(d|x) =es(d,x)/γ P d∈D′es(d,x)/γ where γis a hyperparameter th"}, {"rank": 3, "score": 0.49896774, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 33, "text_snippet": "istribution according to the language model, using a uniform prior: pk∝pLM(a|dk,q). Using the Softmax operator, we have that pk=exp(logpLM(a|dk,q))∑K i=1exp(logpLM(a|di,q)). Leave-one-out Perplexity Distillation (LOOP). Finally, we propose "}, {"rank": 4, "score": 0.49864495, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 19, "text_snippet": "r produces the output sequence probability for each document, which are then marginalized, pRAG-Sequence (y|x)≈∑ z∈top-k(p(·|x))pη(z|x)pθ(y|x,z) =∑ z∈top-k(p(·|x))pη(z|x)N∏ ipθ(yi|x,z,y 1:i−1) RAG-Token Model In the RAG-Token model we can d"}, {"rank": 5, "score": 0.49208325, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 25, "text_snippet": "rator G. The retriever Raims to retrieve the top- Kdocuments D={dr1, ..., d rk}that are relevant to the input Xfrom the corpus C. Based on the input Xand the retrieved results D, the generator Gis responsible for generating the output Y. Th"}]}
{"case_index": 10, "query": "According to the paper 'from local to global a graph rag approach to query focused summarization', what exact term completes this statement: \"RAG is ideal when the total number of [BLANK] in a data source is too large to include in a single prompt to the LLM, i.e.\"?", "gold": "records", "answer": "The missing term is **“records”**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "The context explicitly states: 'RAG is ideal when the total number of records in a data source is too large...' and the model answer matches exactly.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.639, "llm_ms": 1354.076, "top_contexts": [{"rank": 1, "score": 0.69863784, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 29, "text_snippet": "lements (nodes, edges, covariates) that the LLM can summarize in parallel at both indexing time and query time. The “global answer” to a given query is produced using a final round of query-focused summarization over all community summaries"}, {"rank": 2, "score": 0.69537663, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 14, "text_snippet": "es, whereupon this information is incorporated into the generation of a response to the query by an LLM (or other generative AI model, such as a multi-media model). The query and retrieved records populate a prompt template, which is then p"}, {"rank": 3, "score": 0.6722971, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 8, "text_snippet": "nce analysis (Ranade and Joshi, 2023). Given a sensemaking query and a text with an implicit and interconnected set of concepts, an LLM can generate a summary that answers the query. The challenge, however, arises when the volume of data re"}, {"rank": 4, "score": 0.6682799, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 3, "text_snippet": "wo stages: first, to derive an entity knowledge graph from the source documents, then to pre- generate community summaries for all groups of closely related entities. Given a question, each community summary is used to generate a partial re"}, {"rank": 5, "score": 0.66597, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 96, "text_snippet": "tigate these downstream risks for questions of a global nature, which might otherwise be answered by samples of retrieved facts falsely presented as global summaries. 7 Conclusion We have presented GraphRAG, a RAG approach that combines kno"}]}
{"case_index": 11, "query": "According to the paper 'self rag learning to retrieve generate and critique through self reflection', what exact term completes this statement: \"Reflection tokens, inspired by reward models used in [BLANK] learning (Ziegler et al., 2019; Ouyang et al., 2022), are inserted offline into the original corpus by a trained critic model.\"?", "gold": "reinforcement", "answer": "The missing term is **“reinforcement”**, as in **“reinforcement learning”**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer correctly fills blank with 'reinforcement' and is directly supported by context stating 'reinforcement learning'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.562, "llm_ms": 1230.576, "top_contexts": [{"rank": 1, "score": 0.75172496, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 14, "text_snippet": "t with reflection tokens by unifying them as the next token prediction from the expanded model vocabulary. We train our generator LM on a diverse collection of text interleaved with reflection tokens and retrieved passages. Reflection token"}, {"rank": 2, "score": 0.7491361, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 15, "text_snippet": "st a critic model during training, reducing overhead. The critic model, in part, is supervised on a dataset of input, output, and corresponding reflection tokens collected by prompting a propriety LM (i.e., GPT-4; OpenAI 2023). While we dra"}, {"rank": 3, "score": 0.70864284, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 36, "text_snippet": "text with reflection tokens by unifying them as next token predictions from the expanded model vocabulary (i.e., the original vocabulary plus reflection tokens). Specifically, we train the generator model Mon a curated corpus with interleav"}, {"rank": 4, "score": 0.6942171, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 37, "text_snippet": "e quality of a given task output (Section 3.2.1). Using the critic model, we update the training corpus by inserting reflection tokens into task outputs offline. Subsequently, we train the final generator model ( M) using the conventional L"}, {"rank": 5, "score": 0.6910056, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 7, "text_snippet": "rmittent special tokens (i.e., reflection tokens ). Reflection tokens are categorized into retrieval andcritique tokens to indicate the need for retrieval and its generation quality respectively (Figure 1 right). In particular, given an inp"}]}
{"case_index": 12, "query": "According to the paper 'from local to global a graph rag approach to query focused summarization', what exact term completes this statement: \"This approach first uses one LLM to generate a diverse set of global [BLANK] questions based on corpus-sp\"?", "gold": "sensemaking", "answer": "The missing term is **“sensemaking”** — i.e., “a diverse set of global **sensemaking** questions based on corpus-sp…”.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'sensemaking' and is directly supported by context stating 'global sensemaking questions'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.25, "hit_rank": 4, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.987, "llm_ms": 1340.062, "top_contexts": [{"rank": 1, "score": 0.6819262, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 29, "text_snippet": "lements (nodes, edges, covariates) that the LLM can summarize in parallel at both indexing time and query time. The “global answer” to a given query is produced using a final round of query-focused summarization over all community summaries"}, {"rank": 2, "score": 0.6817231, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 1, "text_snippet": "ibuted equally to this work Abstract The use of retrieval-augmented generation (RAG) to retrieve relevant informa- tion from an external knowledge source enables large language models (LLMs) to answer questions over private and/or previousl"}, {"rank": 3, "score": 0.67291504, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 23, "text_snippet": " this work, we propose an approach for generating a set of questions for evaluating global sensemaking over the entirety of the corpus. Our approach is related to LLM methods that use a corpus to generate questions whose answers would be su"}, {"rank": 4, "score": 0.669351, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 11, "text_snippet": "swers are combined and used to generate a final global answer. The GraphRAG method and its ability to perform global sensemaking over an entire corpus form the main contribution of this work. To demonstrate this ability, we developed a nove"}, {"rank": 5, "score": 0.6675085, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 50, "text_snippet": "il the token limit is reached. This final context is used to generate the global answer returned to the user. 3.2 Global Sensemaking Question Generation To evaluate the effectiveness of RAG systems for global sensemaking tasks, we use an LL"}]}
{"case_index": 13, "query": "According to the paper 'refrag rethinking rag based decoding', what exact term completes this statement: \"However, most existing methods target generic LLM tasks with long context and are largely [BLANK] to our work.\"?", "gold": "orthogonal", "answer": "The missing term is **“orthogonal”** (“…and are largely **orthogonal** to our work.”).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'orthogonal' and is directly supported by Context 3 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.3333333333333333, "hit_rank": 3, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.585, "llm_ms": 1061.022, "top_contexts": [{"rank": 1, "score": 0.62553024, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 12, "text_snippet": "is information is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-a"}, {"rank": 2, "score": 0.6198537, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 3, "text_snippet": " on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30 .85×the time-t"}, {"rank": 3, "score": 0.614212, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 9, "text_snippet": "f research, with approaches 1arXiv:2509.01092v2 [cs.CL] 12 Oct 2025  ranging from modifying the attention mechanism’s complexity (Beltagy et al., 2020) to sparsifying attention and context (Child et al., 2019; Xiao et al., 2024; Jiang et al"}, {"rank": 4, "score": 0.61321527, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 94, "text_snippet": "Complementary approaches rank or prune context by estimated informativeness, e.g.,Selective Contextuses self-information to drop low-value tokens, and sentence-level methods learn context-aware encoders for question-specific compression and"}, {"rank": 5, "score": 0.5993551, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 95, "text_snippet": "oducedREFRAG, a novel and efficient decoding framework tailored for RAG applications. By leveraging the inherent sparsity and block-diagonal attention patterns present in RAG contexts,REFRAG compresses, senses, and expands context represent"}]}
{"case_index": 14, "query": "According to the paper 'leveraging passage retrieval with generative models for open domain question answering', what exact term completes this statement: \"(2019): •[BLANK] (Kwiatkowski et al., 2019) contains questions corresponding to Google search queries.\"?", "gold": "naturalquestions", "answer": "The missing term is **“Natural Questions”** — i.e., **Natural Questions (Kwiatkowski et al., 2019)** contains questions corresponding to Google search queries.", "answer_score": 0.76, "answer_correctness": 1.0, "answer_groundedness": 0.2, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 0.2, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term (naturalquestions/Natural Questions). However, provided context snippets do not include the quoted statement or explicitly show the term, so support from context is weak.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 10.565, "llm_ms": 1304.276, "top_contexts": [{"rank": 1, "score": 0.6914209, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 175, "text_snippet": "ext generation with relevance sampling. ArXiv, abs/2207.03030, 2022. 7, 15 Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry Heck. Learning deep structured semantic models for web search using clickthrough data. In Pro"}, {"rank": 2, "score": 0.6814287, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 0, "text_snippet": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering Gautier Izacard1,2,3Edouard Grave1 1Facebook AI Research, Paris 2ENS, PSL University, Paris 3Inria, Paris gizacard|egrave@fb.com Abstract Generative mode"}, {"rank": 3, "score": 0.6696491, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 101, "text_snippet": "ov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906 , 2020. URL https://arxiv.org/abs/2004.04906 . [27] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, "}, {"rank": 4, "score": 0.66246605, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}, {"rank": 5, "score": 0.66178596, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 93, "text_snippet": "Transactions of the Association for Computa- tional Linguistics , 7:452–466, 2019. doi: 10.1162/tacl_a_ 00276. URL https://aclanthology.org/Q19-1026 . Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V ., Goyal, N., Küttler, H., Le"}]}
{"case_index": 15, "query": "According to the paper 'replug retrieval augmented black box language models', what exact term completes this statement: \"ting the LM’s [BLANK], REPLUG treats the language model as a black box and augments it with a frozen or tunable retriever.\"?", "gold": "parameters", "answer": "The missing term is **“parameters”** — i.e., “fine-tuning the LM’s **parameters**, REPLUG treats the language model as a black box and augments it with a frozen or tunable retriever.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'parameters' and is directly supported by Context 1 ('ting the LM’s parameters, REPLUG treats...').", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 16.74, "llm_ms": 1550.042, "top_contexts": [{"rank": 1, "score": 0.80305356, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 5, "text_snippet": "ting the LM’s parameters, REPLUG treats the language model as a black box and augments it with a frozen or tunable retriever. This black-box assumption makes REPLUG applicable to large LMs (i.e., >100B parameters), which are often served vi"}, {"rank": 2, "score": 0.7813581, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 8, "text_snippet": "Black-Box Language Models Figure 1, REPLUG is extremely flexible and can be used with any existing black-box LM and retrieval model. We also introduce REPLUG LSR (REPLUG with LM- Supervised Retrieval), a training scheme that can further imp"}, {"rank": 3, "score": 0.7756068, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 71, "text_snippet": "a black box and augments it with a tuneable retrieval model. Our evaluation shows that REPLUG can be integrated with any existing language model to improve their performance  REPLUG: Retrieval-Augmented Black-Box Language Models on language"}, {"rank": 4, "score": 0.7667548, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 6, "text_snippet": "PIs. Internal representations of such models are not exposed and fine-tuning is not supported. In this work, we introduce REPLUG (Retrieve and Plug ), a new retrieval-augmented LM framework where the lan- guage model is viewed as a black bo"}, {"rank": 5, "score": 0.75147635, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 1, "text_snippet": "mechanisms to en- code the retrieved text, REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design can be eas- ily applied to any existing retrieval and language models. Furthermore, we show t"}]}
{"case_index": 16, "query": "According to the paper 'self rag learning to retrieve generate and critique through self reflection', what exact term completes this statement: \"of retrieved passages prepended to input, or pre-train a retriever and LM jointly, followed by few- shot [BLANK] on task datasets (Izacard et al., 2022b).\"?", "gold": "fine-tuning", "answer": "The missing term is **“fine-tuning”**, i.e., the phrase is **“few-shot fine-tuning on task datasets (Izacard et al., 2022b).”**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'fine-tuning' and is directly supported by Context 2 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.46, "llm_ms": 1364.48, "top_contexts": [{"rank": 1, "score": 0.7213162, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 22, "text_snippet": " the retriever and LM on instruction-tuning datasets in two steps. While we also train our model on diverse instruction-following datasets, SELF-RAGenables retrieval on demand and selection of the best possible model output via fine-grained"}, {"rank": 2, "score": 0.71698236, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 19, "text_snippet": "rieved text passages (Guu et al., 2020; Lewis et al., 2020), leading to large improvements in knowledge-intensive tasks after fine-tuning or used with off-the-shelf LMs (Ram et al., 2023). A more recent work (Luo et al., 2023) instruction-t"}, {"rank": 3, "score": 0.6979927, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 6, "text_snippet": "y trained to leverage and follow facts from provided passages. This work introduces Self-Reflective Retrieval-augmented Generation ( SELF-RAG)to improve an LLM’s generation quality, including its factual accuracy without hurting its versati"}, {"rank": 4, "score": 0.69386035, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 2, "text_snippet": "ew framework called Self-Reflective Retrieval-Augmented Gen- eration ( SELF-RAG)that enhances an LM’s quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passage"}, {"rank": 5, "score": 0.69122756, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 41, "text_snippet": "lding are unforgettable experience.No RetrievalNo Retrieval Retriever Figure 2: SELF-RAGtraining examples. The left example does not require retrieval while the right one requires retrieval; thus, passages are inserted. More examples are in"}]}
{"case_index": 17, "query": "According to the paper 'atlas few shot learning with retrieval augmented language models', what exact term completes this statement: \"3 2.2 Training [BLANK] for the retriever In this section, we discuss four diﬀerent loss functions to train the retriever jointly with the langu\"?", "gold": "objectives", "answer": "The missing term is **“objectives”**, i.e., the section title is:\n\n**“2.2 Training objectives for the retriever”**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer correctly fills the blank with 'objectives' and is directly supported by Context 1 showing the section title '2.2 Training objectives for the retriever'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 15.078, "llm_ms": 1339.355, "top_contexts": [{"rank": 1, "score": 0.71326995, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 22, "text_snippet": "ther way to process the retrieved documents in the language model would be to concatenate the query and all the documents, and to use this long sequence as input of the model. Unfortunately, this approach does not scale with the number of d"}, {"rank": 2, "score": 0.7008028, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 23, "text_snippet": "ent loss functions to train the retriever jointly with the language model. We consider loss functions that leverage the language model to provide supervisory signal to train the retriever. In other words, if the language model ﬁnds a docume"}, {"rank": 3, "score": 0.62029254, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 29, "text_snippet": "triever inREPLUG by using the LM itself to provide supervision about which documents should be retrieved. Inspired by Sachan et al. (2022), our approach can be seen as adjusting the probabilities of the retrieved documents to match the prob"}, {"rank": 4, "score": 0.61839795, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 29, "text_snippet": "om the retriever deﬁned in Equation 1: KL(pattn∥pretr) =K∑ k=1pattn(dk) log(pattn(dk) pretr(dk)) . Here, this loss is only used to optimize the parameters of the retriever, and not the language model. When using recent deep learning framewo"}, {"rank": 5, "score": 0.6165079, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 12, "text_snippet": "ntributions: •A thorough study on how to design and train retrieval-augmented language models, with a focus on downstream few-shot learning and sample eﬃciency. •The ﬁndings of this study lead to a retrieval-augmented language model, called"}]}
{"case_index": 18, "query": "According to the paper 'atlas few shot learning with retrieval augmented language models', what exact term completes this statement: \"We then minimize the [BLANK] between pattn(dk), and the distribution pretrfrom the retriever deﬁned in Equation 1: KL(pattn∥pretr) =K∑ k=1pa\"?", "gold": "kl-divergence", "answer": "The missing term is **“KL-divergence”** (i.e., *Kullback–Leibler divergence*):  \n\n“We then minimize the **KL-divergence** between \\(p_{\\text{attn}}(d_k)\\) and the distribution \\(p_{\\text{retr}}\\) from the retriever…”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "The blank is 'KL-divergence' (kl-divergence). This is explicitly stated in the provided context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.3333333333333333, "hit_rank": 3, "matched_by": "doc_id+chunk_id", "retrieval_ms": 11.397, "llm_ms": 2095.647, "top_contexts": [{"rank": 1, "score": 0.65762687, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 29, "text_snippet": "om the retriever deﬁned in Equation 1: KL(pattn∥pretr) =K∑ k=1pattn(dk) log(pattn(dk) pretr(dk)) . Here, this loss is only used to optimize the parameters of the retriever, and not the language model. When using recent deep learning framewo"}, {"rank": 2, "score": 0.6291433, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 32, "text_snippet": "ler loss function which is loosely inspired by the objectives from the attention distillation and EMDR2methods (Izacard & Grave, 2021; Sachan et al., 2021). More precisely, we want to train the retriever to predict how much each document wo"}, {"rank": 3, "score": 0.6234872, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 28, "text_snippet": "e value vninto account. Hence, we use the quantity αn∥vn∥2as the measure of relevance for token n. Following Izacard & Grave (2021), we average these scores over all attention heads, layers, and tokens to obtain a score for each document. W"}, {"rank": 4, "score": 0.61132824, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 34, "text_snippet": " documents, and use the negative value as relevance score for each document. Following the previous loss function, we use the softmax operator to obtain a probability distribution over documents: ploop(dk) =exp(−logpLM(a|DK\\{dk},q))∑K i=1ex"}, {"rank": 5, "score": 0.58832854, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 33, "text_snippet": "istribution according to the language model, using a uniform prior: pk∝pLM(a|dk,q). Using the Softmax operator, we have that pk=exp(logpLM(a|dk,q))∑K i=1exp(logpLM(a|di,q)). Leave-one-out Perplexity Distillation (LOOP). Finally, we propose "}]}
{"case_index": 19, "query": "According to the paper 'recomp improving retrieval augmented lms with compression and selective augmentation', what exact term completes this statement: \"(3) Faithful :sshould be a faithful and [BLANK] summary of the input document set (i.e., smust be entailed by the input document set ( [d1, d2, ...dN])).\"?", "gold": "interpretable", "answer": "The missing term is **“interpretable”**:\n\n“(3) Faithful: *s* should be a faithful and **interpretable** summary of the input document set (i.e., *s* must be entailed by the input document set ([d1, d2, … dN])).”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'interpretable' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 11.407, "llm_ms": 1617.047, "top_contexts": [{"rank": 1, "score": 0.71166444, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 18, "text_snippet": "cy. If the retrieved documents do not contain relevant information or retrieval augmentation is not necessary, scan be an empty sequence. (2) Effecive : when sis prepended to input sequence xand provided to LM Mas a prompt, LM should genera"}, {"rank": 2, "score": 0.58223057, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 17, "text_snippet": "is work, we assume a blackbox LM and train the compressor. Given the set of retrieved Ndocuments ( [d1, d2, ...dN]) and the input sequence x, a compressor returns a token sequence s. We design our compressor to be substantially smaller than"}, {"rank": 3, "score": 0.57851595, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 72, "text_snippet": "nerates sentences that seemingly contains the answer. Our compressor successfully reduce such erroneous behavior to 39%. Is generated summary faithful and comprehensive? We (the authors) manually evaluate outputs of the abstractive compress"}, {"rank": 4, "score": 0.5669881, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 6, "text_snippet": "further confuse LMs with irrelevant information, degrading model performances (Mallen et al., 2022; Shi et al., 2023a). To overcome such limitations, we propose RECOMP (Retrieve, Com press, Prepend), an inter- mediate step for RALMs which c"}, {"rank": 5, "score": 0.5668493, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 16, "text_snippet": "oth its strength and weaknesses, thereby building foundation for future work. 2 P ROBLEM FORMULATION :RECOMP Given an input sequence x, a target output sequence yand a set of Nretrieved documents D ([d1, d2, ...dN]),2RECOMP compresses retri"}]}
{"case_index": 20, "query": "According to the paper 'self rag learning to retrieve generate and critique through self reflection', what exact term completes this statement: \"We introduce a method to train an arbitrary LM to learn to use retrieval on-demand for diverse [BLANK] queries and introduce controlled generation guided by reflections tokens to further improve generation quality and attributions.\"?", "gold": "instruction-following", "answer": "The missing term is **“instruction-following”**, i.e., the full phrase is **“diverse instruction-following queries.”**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'instruction-following' and is directly supported by Context 5.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.2, "hit_rank": 5, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.461, "llm_ms": 1362.54, "top_contexts": [{"rank": 1, "score": 0.77425027, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 2, "text_snippet": "ew framework called Self-Reflective Retrieval-Augmented Gen- eration ( SELF-RAG)that enhances an LM’s quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passage"}, {"rank": 2, "score": 0.77342254, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 6, "text_snippet": "y trained to leverage and follow facts from provided passages. This work introduces Self-Reflective Retrieval-augmented Generation ( SELF-RAG)to improve an LLM’s generation quality, including its factual accuracy without hurting its versati"}, {"rank": 3, "score": 0.73686475, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 96, "text_snippet": "ains an LM to learn to retrieve, generate, and critique text passages and its own generation by predicting the next tokens from its original vocabulary as well as newly added special tokens, called reflection tokens. SELF-RAGfurther enables"}, {"rank": 4, "score": 0.7260352, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 26, "text_snippet": "able generation at inference, while RLHF focuses on human preference alignment during training. Other works use general control tokens to guide LM generation (Lu et al., 2022; Korbak et al., 2023), while SELF-RAGuses reflection tokens to de"}, {"rank": 5, "score": 0.72489774, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 21, "text_snippet": " al., 2023a; Gao et al., 2023). We introduce a method to train an arbitrary LM to learn to use retrieval on-demand for diverse instruction-following queries and introduce controlled generation guided by reflections tokens to further improve"}]}
{"case_index": 21, "query": "According to the paper 'atlas few shot learning with retrieval augmented language models', what exact term completes this statement: \"For example, in the case of question answering, the query [BLANK] to the question and the model needs to generate the answer.\"?", "gold": "corresponds", "answer": "The missing term is **“corresponds”** — i.e., “the query **corresponds** to the question and the model needs to generate the answer.”", "answer_score": 0.76, "answer_correctness": 1.0, "answer_groundedness": 0.2, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 0.2, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'corresponds'. However, the provided context snippets do not contain the quoted sentence or the word, so support from context is weak.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 7.997, "llm_ms": 1527.604, "top_contexts": [{"rank": 1, "score": 0.63085353, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 1, "text_snippet": " Abstract Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter count"}, {"rank": 2, "score": 0.62306654, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 53, "text_snippet": "rieved documents as the answer (Chen et al., 2017; Clark & Gardner, 2018; Wang et al., 2019; Karpukhin et al., 2020), a method inspired by reading comprehension (Richardson, 2013; Rajpurkar et al., 2016). Recently, generating the answer as "}, {"rank": 3, "score": 0.6198319, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 60, "text_snippet": "ut. Retrieval-Augmentation with Search Engines. Recently, diﬀerent works have proposed to train large language models to interact with a search engine, by generating text queries, and using the retrieved documents as additional context (Nak"}, {"rank": 4, "score": 0.6156531, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 2, "text_snippet": "ings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including"}, {"rank": 5, "score": 0.6137767, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 17, "text_snippet": "chmark in Figure 2. As many natural language processing tasks require knowledge , our goal is to enhance standard text-to-text models with retrieval, which, as we hypothesise in the introduction, may be crucial to endow models with few-shot"}]}
{"case_index": 22, "query": "According to the paper 'self rag learning to retrieve generate and critique through self reflection', what exact term completes this statement: \"Recent work on LLM refinement (Dhuliawala et al., 2023; Madaan et al., 2023; Paul et al., 2023) prompts a model to generate task output, natural language feedback and refined task output [BLANK], but at the cost of inference efficiency.\"?", "gold": "iteratively", "answer": "The missing term is **“iteratively”**:\n\n“…prompts a model to generate task output, natural language feedback and refined task output **iteratively**, but at the cost of inference efficiency.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'iteratively' and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 21.139, "llm_ms": 1685.488, "top_contexts": [{"rank": 1, "score": 0.77203673, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 27, "text_snippet": "n (reasoning path consistency) and without retrieval. Recent work on LLM refinement (Dhuliawala et al., 2023; Madaan et al., 2023; Paul et al., 2023) prompts a model to generate task output, natural language feedback and refined task output"}, {"rank": 2, "score": 0.7182317, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 135, "text_snippet": "ns con- sumed and generated by the model. Self-reflection is a prompt engineering technique where the LLM generates an answer, and is then prompted to evaluate its output for correctness, clarity, or completeness, then finally generate an i"}, {"rank": 3, "score": 0.70634586, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 24, "text_snippet": "ering tasks and to generate with tree search, guided by LM-generated value scores. While their value function simply indicates an overall score of each generation, SELF-RAGtrains to an arbitrary LM to learn to generate fine-grained self-ref"}, {"rank": 4, "score": 0.69547105, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 6, "text_snippet": "y trained to leverage and follow facts from provided passages. This work introduces Self-Reflective Retrieval-augmented Generation ( SELF-RAG)to improve an LLM’s generation quality, including its factual accuracy without hurting its versati"}, {"rank": 5, "score": 0.69314086, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 25, "text_snippet": " has proven effective in aligning LLMs with human preferences (Ouyang et al., 2022). Wu et al. (2023) introduce fine-grained RLHF with multiple reward models. Though our work also studies fine-grained critique on retrieval and generation, w"}]}
{"case_index": 23, "query": "According to the paper 'refrag rethinking rag based decoding', what exact term completes this statement: \"Results show that we achieve30 .75×TTFT [BLANK] without loss in perplexity which is3 .75×than previous method.\"?", "gold": "acceleration", "answer": "The missing term is **“acceleration”** — i.e., “**TTFT acceleration**.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context explicitly states 'time-to-first-token acceleration' and 'TTFT acceleration'; model answer matches expected term.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 16.824, "llm_ms": 1488.625, "top_contexts": [{"rank": 1, "score": 0.6799848, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 3, "text_snippet": " on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30 .85×the time-t"}, {"rank": 2, "score": 0.6550795, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 25, "text_snippet": "ation with cache and8 .59×without cache1, both surpassing CEPE (2.01×and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared to CEPE (table 1). We achieve up to6 .78×throughput acceleration compared "}, {"rank": 3, "score": 0.64478743, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 133, "text_snippet": "xt length s, we are able to achieve up to k2×acceleration in both TTFT and throughput. The details on the latency and throughput calculation are in section B.4. Empirical verification of latency/throughput improvement.Figure 2 shows the emp"}, {"rank": 4, "score": 0.6424887, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 96, "text_snippet": "ong document summarization, demonstrate thatREFRAG achieves up to30 .85×TTFT acceleration (3 .75×over previous state-of-the-art methods) without any loss in perplexity or downstream accuracy. Our results highlight the importance of speciali"}, {"rank": 5, "score": 0.6386867, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 134, "text_snippet": "ntext),REFRAG achieves16 .53×acceleration in TTFT with cache and8 .59×without cache. Both higher than CEPE (i.e., 2.01×and1 .04×acceleration respectively) while having better model performance (see table 1). With longer context, we are able"}]}
{"case_index": 24, "query": "According to the paper 'retrieval augmented generation for knowledge intensive nlp tasks', what exact term completes this statement: \"We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the [BLANK] memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever.\"?", "gold": "non-parametric", "answer": "The missing term is **“non-parametric”**, i.e., “the **non-parametric** memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'non-parametric' and is explicitly supported by the provided context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 11.607, "llm_ms": 1861.43, "top_contexts": [{"rank": 1, "score": 0.793992, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 9, "text_snippet": "y to the “workhorse of NLP,” i.e. sequence-to-sequence (seq2seq) models. We endow pre-trained, parametric-memory generation models with a non-parametric memory through a general-purpose ﬁne-tuning approach which we refer to as retrieval-aug"}, {"rank": 2, "score": 0.77475035, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 2, "text_snippet": "anism to explicit non-parametric memory have so far been only investigated for extractive downstream tasks. We explore a general-purpose ﬁne-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trained parametri"}, {"rank": 3, "score": 0.6637187, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 6, "text_snippet": "eval-augmented architecture. These models employ a non-parametric memory, e.g. a neural retriever over a large, external, potentially non-static knowledge source to enhance a parametric language model. In addition to their memorisation abil"}, {"rank": 4, "score": 0.6423405, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 1, "text_snippet": "e reliance on the paramet- ric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and inco"}, {"rank": 5, "score": 0.6354929, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 19, "text_snippet": " documents. It usually provides an extra knowledge source from a specific corpus, i.e., Wikipedia, which greatly improves the per- formance of LMs in a variety of tasks, especially in the knowledge-intensive ones. The proposed  methods gene"}]}
{"case_index": 25, "query": "According to the paper 'refrag rethinking rag based decoding', what exact term completes this statement: \"We argue that specialized techniques exploiting the unique structure and sparsity inherent in RAG contexts can [BLANK] reduce memory and computational overhead.\"?", "gold": "substantially", "answer": "The missing term is **“substantially”**:\n\n“…can **substantially** reduce memory and computational overhead.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'substantially' and is directly supported by Context 4 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.25, "hit_rank": 4, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.889, "llm_ms": 1937.07, "top_contexts": [{"rank": 1, "score": 0.71737134, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 95, "text_snippet": "oducedREFRAG, a novel and efficient decoding framework tailored for RAG applications. By leveraging the inherent sparsity and block-diagonal attention patterns present in RAG contexts,REFRAG compresses, senses, and expands context represent"}, {"rank": 2, "score": 0.70072055, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 3, "text_snippet": " on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30 .85×the time-t"}, {"rank": 3, "score": 0.6829953, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 12, "text_snippet": "is information is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-a"}, {"rank": 4, "score": 0.6677246, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 10, "text_snippet": "G-based applications, such as web-scale search, with the goal of improving inference latency, specifically, the TTFT. We argue that specialized techniques exploiting the unique structure and sparsity inherent in RAG contexts can substantial"}, {"rank": 5, "score": 0.65280306, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 94, "text_snippet": "Complementary approaches rank or prune context by estimated informativeness, e.g.,Selective Contextuses self-information to drop low-value tokens, and sentence-level methods learn context-aware encoders for question-specific compression and"}]}
{"case_index": 26, "query": "According to the paper 'recomp improving retrieval augmented lms with compression and selective augmentation', what exact term completes this statement: \"If the retrieved documents are irrelevant to the input or offer no additional information to LM, our compressor can return an empty string, [BLANK] selective augmentation.\"?", "gold": "implementing", "answer": "The missing term is **“implementing”**, as in: “…our compressor can return an empty string, **implementing** selective augmentation.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'implementing' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 13.787, "llm_ms": 1716.442, "top_contexts": [{"rank": 1, "score": 0.6815988, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 2, "text_snippet": "d an abstractive compressor which generates summaries by synthesizing information from multiple documents. Both compressors are trained to improve LMs’ performance on end tasks when the generated summaries are prepended to the LMs’ input, w"}, {"rank": 2, "score": 0.6519466, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 0, "text_snippet": "RECOMP: I MPROVING RETRIEVAL -AUGMENTED LM S WITH COMPRESSION AND SELECTIVE AUGMENTATION Fangyuan Xu1, Weijia Shi2, Eunsol Choi1 Department of Computer Science 1The University of Texas at Austin 2University of Washington {fangyuan,eunsol }@"}, {"rank": 3, "score": 0.64153093, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 77, "text_snippet": "by compressing retrieved documents into a concise summary or an empty sequence, facilitating selective retrieval augmentation. Prompt Compression Recent work (Wingate et al., 2022; Chevalier et al., 2023; Mu et al., 2023) proposes compressi"}, {"rank": 4, "score": 0.617815, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 6, "text_snippet": "further confuse LMs with irrelevant information, degrading model performances (Mallen et al., 2022; Shi et al., 2023a). To overcome such limitations, we propose RECOMP (Retrieve, Com press, Prepend), an inter- mediate step for RALMs which c"}, {"rank": 5, "score": 0.61434245, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 18, "text_snippet": "cy. If the retrieved documents do not contain relevant information or retrieval augmentation is not necessary, scan be an empty sequence. (2) Effecive : when sis prepended to input sequence xand provided to LM Mas a prompt, LM should genera"}]}
{"case_index": 27, "query": "According to the paper 'raptor recursive abstractive processing for tree organized retrieval', what exact term completes this statement: \"This suggests that selecting the most relevant information for [BLANK] tasks is still crucial.\"?", "gold": "knowledge-intensive", "answer": "The missing term is **“question-answering”**, i.e., the sentence reads:\n\n“This suggests that selecting the most relevant information for **question-answering** tasks is still crucial.”", "answer_score": 0.0, "answer_correctness": 0.0, "answer_groundedness": 0.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 0.0, "abstained": false, "should_abstain": false, "notes": "Expected term is 'knowledge-intensive', but the answer says 'question-answering'. The provided context does not contain the quoted sentence or support 'question-answering' as the blank.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 7.729, "llm_ms": 1903.752, "top_contexts": [{"rank": 1, "score": 0.614226, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 0, "text_snippet": "Published as a conference paper at ICLR 2024 RAPTOR: R ECURSIVE ABSTRACTIVE PROCESSING FOR TREE-ORGANIZED RETRIEVAL Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D. Manning Stanford University psarthi@cs."}, {"rank": 2, "score": 0.61020577, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 8, "text_snippet": " indexing and retrieval system that uses a tree structure to capture both high-level and low-level details about a text. As shown in Figure 1, our system, RAPTOR, clusters chunks of text, generates text summaries of those clusters, and then"}, {"rank": 3, "score": 0.6017036, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 74, "text_snippet": "employing recursive clustering and summarization techniques, RAPTOR creates a hierarchical tree structure that is capable of synthesizing information across various sections of the retrieval corpora. During the query phase, RAPTOR leverages"}, {"rank": 4, "score": 0.59778076, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 67, "text_snippet": "ring approaches, which allows it to capture a range of information, from general themes to specific details, contributing to its overall strong performance. 4.1 C ONTRIBUTION OF THE TREE STRUCTURE We examine the contribution of each layer o"}, {"rank": 5, "score": 0.597389, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 73, "text_snippet": "an effectively handle a wider range of questions, from higher-order thematic queries to detail-oriented questions. Detailed results for additional stories and an ablation study on layer contributions can be found in Appendix I. 5 C ONCLUSIO"}]}
{"case_index": 28, "query": "According to the paper 'retrieval augmented generation for knowledge intensive nlp tasks', what exact term completes this statement: \"This avoids the need to run [BLANK] forward passes once the candidate set Yhas been generated.\"?", "gold": "additional", "answer": "The missing term is **“B”**, i.e., the statement is:\n\n> “This avoids the need to run **B** forward passes once the candidate set *Y* has been generated.”", "answer_score": 0.0, "answer_correctness": 0.0, "answer_groundedness": 0.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 0.0, "abstained": false, "should_abstain": false, "notes": "Expected term is 'additional' but model answered 'B'. No support in provided context for 'B' completing that sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 10.422, "llm_ms": 1511.031, "top_contexts": [{"rank": 1, "score": 0.5604656, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 46, "text_snippet": "pervised data that precisely mimics the SELF- RAGinference-time process (Section 3.1). For each segment yt∈y, we run Cto assess whether additional passages could help to enhance generation. If retrieval is required, the retrieval special to"}, {"rank": 2, "score": 0.54455966, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 25, "text_snippet": "rator G. The retriever Raims to retrieve the top- Kdocuments D={dr1, ..., d rk}that are relevant to the input Xfrom the corpus C. Based on the input Xand the retrieved results D, the generator Gis responsible for generating the output Y. Th"}, {"rank": 3, "score": 0.5415492, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 20, "text_snippet": " prior work often retrieves only once at the beginning, Jiang et al. (2023) propose to adaptively retrieve passages for generation on top of a proprietary LLM or Schick et al. (2023) train an LM to generate API calls for named entities. Yet"}, {"rank": 4, "score": 0.54140824, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 53, "text_snippet": "old, we trigger retrieval (details in Appendix Section A.3). Tree-decoding with critique tokens. At each segment step t, when retrieval is required, based either on hard or soft conditions, Rretrieves Kpassages, and the generator Mprocesses"}, {"rank": 5, "score": 0.53967804, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 57, "text_snippet": "nted generators using a second “informed” retriever with access to the output, which the test-time retriever can be distilled from, and Hofstätter et al. (2022) recently proposed a training set ﬁltering/weighting approach to train stronger "}]}
{"case_index": 29, "query": "According to the paper 'raptor recursive abstractive processing for tree organized retrieval', what exact term completes this statement: \"vector embeddings presents a challenge for traditional GMMs, as dis- tance metrics may behave poorly when used to measure similarity in [BLANK] spaces (Ag- garwal et al., 2001).\"?", "gold": "high-dimensional", "answer": "The missing term is **“high-dimensional”**, completing the statement as:\n\n“…distance metrics may behave poorly when used to measure similarity in **high-dimensional** spaces (Aggarwal et al., 2001).”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'high-dimensional' and is directly supported by Context 1/3.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 20.177, "llm_ms": 1667.663, "top_contexts": [{"rank": 1, "score": 0.65884227, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 29, "text_snippet": "vector embeddings presents a challenge for traditional GMMs, as dis- tance metrics may behave poorly when used to measure similarity in high-dimensional spaces (Ag- garwal et al., 2001). To mitigate this, we employ Uniform Manifold Approxim"}, {"rank": 2, "score": 0.58943146, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 218, "text_snippet": "d to be measured. Evidence : Word embedding Word embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to v"}, {"rank": 3, "score": 0.57579553, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 28, "text_snippet": " Published as a conference paper at ICLR 2024 Given a set of Ntext segments, each represented as a d-dimensional dense vector embedding, the likelihood of a text vector, x, given its membership in the kthGaussian distribution, is denoted by"}, {"rank": 4, "score": 0.54957914, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 228, "text_snippet": "e Word embedding Word embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers. Con"}, {"rank": 5, "score": 0.52491343, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 227, "text_snippet": "rd embeddings are one of the most powerful tools available for Natural Language Processing (NLP). They are mathematical representations of words or phrases in a vector space, allowing similarities between words and the context in which they"}]}
{"case_index": 30, "query": "According to the paper 'replug retrieval augmented black box language models', what exact term completes this statement: \"model parameters), for both reducing LM perplexity and and im- proving in-context learning [BLANK].\"?", "gold": "performance", "answer": "The missing term is **“performance”** — the full phrase is: “improving in-context learning **performance**.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'performance' and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 9.235, "llm_ms": 1614.506, "top_contexts": [{"rank": 1, "score": 0.7618351, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 11, "text_snippet": " model parameters), for both reducing LM perplexity and and im- proving in-context learning performance. We summarize our contributions as follows: •We introduce REPLUG (§3), the first retrieval- augmented language modeling framework for en"}, {"rank": 2, "score": 0.7145648, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 9, "text_snippet": "22) that adapts language models to the retriever. We use a training objective which prefers retrieving documents that improve language model perplexity, while treating the LM as a frozen, black-box scoring function. Our experiments show tha"}, {"rank": 3, "score": 0.7084203, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 5, "text_snippet": "ting the LM’s parameters, REPLUG treats the language model as a black box and augments it with a frozen or tunable retriever. This black-box assumption makes REPLUG applicable to large LMs (i.e., >100B parameters), which are often served vi"}, {"rank": 4, "score": 0.70742804, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 6, "text_snippet": "PIs. Internal representations of such models are not exposed and fine-tuning is not supported. In this work, we introduce REPLUG (Retrieve and Plug ), a new retrieval-augmented LM framework where the lan- guage model is viewed as a black bo"}, {"rank": 5, "score": 0.7038494, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 8, "text_snippet": "Black-Box Language Models Figure 1, REPLUG is extremely flexible and can be used with any existing black-box LM and retrieval model. We also introduce REPLUG LSR (REPLUG with LM- Supervised Retrieval), a training scheme that can further imp"}]}
{"case_index": 31, "query": "According to the paper 'refrag rethinking rag based decoding', what exact term completes this statement: \"FRAG achieves better performance than LLaMA without incurring higher latency in the downstream [BLANK].\"?", "gold": "applications", "answer": "The missing term is **“tasks”** — i.e., “without incurring higher latency in the downstream **tasks**.”", "answer_score": 0.06, "answer_correctness": 0.0, "answer_groundedness": 0.2, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 0.2, "abstained": false, "should_abstain": false, "notes": "Expected blank is 'applications' (as in 'RAG applications'). Model answered 'tasks', which is not the exact term in the cited statement.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 10.441, "llm_ms": 1318.411, "top_contexts": [{"rank": 1, "score": 0.7285818, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 3, "text_snippet": " on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30 .85×the time-t"}, {"rank": 2, "score": 0.6958409, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 56, "text_snippet": "ance Performance vs. Latency Llama REFRAG 8× compressionFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a strong retriever scenario (right).REFRAGperform similarly to LLaMA model"}, {"rank": 3, "score": 0.67359734, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 12, "text_snippet": "is information is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-a"}, {"rank": 4, "score": 0.6670162, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 95, "text_snippet": "oducedREFRAG, a novel and efficient decoding framework tailored for RAG applications. By leveraging the inherent sparsity and block-diagonal attention patterns present in RAG contexts,REFRAG compresses, senses, and expands context represent"}, {"rank": 5, "score": 0.66232044, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 72, "text_snippet": "ual latency, for both strong and weak retriever settings.With a strong retriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5 .26× speedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),"}]}
{"case_index": 32, "query": "According to the paper 'raptor recursive abstractive processing for tree organized retrieval', what exact term completes this statement: \"This structure enables RAPTOR to load into an LLM’s context chunks [BLANK] the text at different levels so that it can effectively and efficiently answer questions at different levels.\"?", "gold": "representing", "answer": "The missing term is **“representing”**, as in:\n\n“This structure enables RAPTOR to load into an LLM’s context chunks **representing** the text at different levels so that it can effectively and efficiently answer questions at different levels.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'representing' and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 12.995, "llm_ms": 1373.271, "top_contexts": [{"rank": 1, "score": 0.773062, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 8, "text_snippet": " indexing and retrieval system that uses a tree structure to capture both high-level and low-level details about a text. As shown in Figure 1, our system, RAPTOR, clusters chunks of text, generates text summaries of those clusters, and then"}, {"rank": 2, "score": 0.69001997, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 22, "text_snippet": " RAPTOR addresses the issue of semantic depth and connection in reading by building a recursive tree structure that balances broader thematic comprehension with granular details and which allows nodes to be grouped based on semantic sim- il"}, {"rank": 3, "score": 0.6882765, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 9, "text_snippet": "uestions at different levels. 1arXiv:2401.18059v1 [cs.CL] 31 Jan 2024  Published as a conference paper at ICLR 2024 2 3 4 5  1 1  2  3  3  4 5  5 6  8  7 Index #8  Text: summary of  nodes 2 and 3  Child Nodes: 2, 3  Text Embedding  Text chu"}, {"rank": 4, "score": 0.6711687, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 24, "text_snippet": "r text chunks, we employ a clustering algorithm. Once clustered, a Language Model is used to summarize the grouped texts. These summarized texts are then re-embedded, and the cycle of embedding, clustering, and summarization continues until"}, {"rank": 5, "score": 0.66344494, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 1, "text_snippet": "rom a retrieval corpus, limiting holistic under- standing of the overall document context. We introduce the novel approach of recursively embedding, clustering, and summarizing chunks of text, constructing a tree with differing levels of su"}]}
{"case_index": 33, "query": "According to the paper 'atlas few shot learning with retrieval augmented language models', what exact term completes this statement: \"Atlasachieves strong downstream performance in both few-shot and [BLANK] settings.\"?", "gold": "resource-rich", "answer": "The missing term is **“full-dataset finetuning”**.", "answer_score": 0.0, "answer_correctness": 0.0, "answer_groundedness": 0.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 0.0, "abstained": false, "should_abstain": false, "notes": "Expected term is 'resource-rich', but model answered 'full-dataset finetuning', which does not complete the quoted statement and is not supported by the provided context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 9.018, "llm_ms": 1210.006, "top_contexts": [{"rank": 1, "score": 0.68855464, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 12, "text_snippet": "ntributions: •A thorough study on how to design and train retrieval-augmented language models, with a focus on downstream few-shot learning and sample eﬃciency. •The ﬁndings of this study lead to a retrieval-augmented language model, called"}, {"rank": 2, "score": 0.6658587, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 0, "text_snippet": "Atlas: Few-shot Learning with Retrieval Augmented Language Models Gautier Izacard∗ ∗,♦,♣,♥gizacard@fb.com Patrick Lewis∗,♦plewis@fb.com Maria Lomeli♦marialomeli@fb.com Lucas Hosseini♦hoss@fb.com Fabio Petroni♦fabiopetroni@fb.com Timo Schick"}, {"rank": 3, "score": 0.662028, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 1, "text_snippet": " Abstract Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter count"}, {"rank": 4, "score": 0.65735257, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 154, "text_snippet": "odel. By jointly pre-training the retriever module and the language model, we show that Atlashas strong few-shot learning capabilities on a wide range of knowledge intensive tasks, including NaturalQuestions, TriviaQA, FEVER, 8 KILT tasks a"}, {"rank": 5, "score": 0.6510422, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 13, "text_snippet": "ask at hand. •Thorough downstream experiments in few-shot settings, demonstrating state-of-the-art results on few-shot NaturalQuestions (+2.8%), TriviaQA (+3.3%), FEVER (+5.1%), and results on par or stronger than models with 15 ×more param"}]}
{"case_index": 34, "query": "According to the paper 'refrag rethinking rag based decoding', what exact term completes this statement: \"In real [BLANK] (e.g., RAG), the context is the dominating part of the input (i.e., s≫q) and hence the overall input to the decod\"?", "gold": "applications", "answer": "The missing term is **“applications”** — i.e., the sentence reads: *“In real applications (e.g., RAG), the context is the dominating part of the input (i.e., s≫q) …”*", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'applications' and is directly supported by Context 2 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 24.04, "llm_ms": 2030.331, "top_contexts": [{"rank": 1, "score": 0.6333525, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 12, "text_snippet": "is information is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-a"}, {"rank": 2, "score": 0.61413014, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 21, "text_snippet": "uery  Encoder Figure 1The main design ofREFRAG. The input context is chunked and processed by the light-weight encoder to produce chunk embeddings, which are precomputable for efficient reuse. A light-weight RL policy decide few chunks to e"}, {"rank": 3, "score": 0.6081029, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 95, "text_snippet": "oducedREFRAG, a novel and efficient decoding framework tailored for RAG applications. By leveraging the inherent sparsity and block-diagonal attention patterns present in RAG contexts,REFRAG compresses, senses, and expands context represent"}, {"rank": 4, "score": 0.6030437, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 3, "text_snippet": " on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30 .85×the time-t"}, {"rank": 5, "score": 0.5835534, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 15, "text_snippet": "igure 1) while preserving the autoregressive nature of the decoder, thereby supporting multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight reinforcement learning (RL) policy that sel"}]}
{"case_index": 35, "query": "According to the paper 'self rag learning to retrieve generate and critique through self reflection', what exact term completes this statement: \"Moreover, [BLANK] citations for each segment with its self-assessment of whether the output is supported by the passage, leading to easier fact verification.\"?", "gold": "self-ragprovides", "answer": "The exact term is **“SELF-RAG provides”**, i.e.:\n\n“Moreover, **SELF-RAG provides** citations for each segment with its self-assessment of whether the output is supported by the passage, leading to easier fact verification.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches context: the sentence reads 'Moreover, SELF-RAGprovides citations...' so blank is 'SELF-RAG provides' (expected tokenized as 'self-ragprovides'). Fully supported by provided context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 11.74, "llm_ms": 1374.736, "top_contexts": [{"rank": 1, "score": 0.7160168, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 13, "text_snippet": "erifiability. consistently retrieves a fixed number of documents for generation regardless of the retrieval necessity (e.g., the bottom figure example does not require factual knowledge) and never second visits the generation quality. Moreo"}, {"rank": 2, "score": 0.6432136, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 27, "text_snippet": "n (reasoning path consistency) and without retrieval. Recent work on LLM refinement (Dhuliawala et al., 2023; Madaan et al., 2023; Paul et al., 2023) prompts a model to generate task output, natural language feedback and refined task output"}, {"rank": 3, "score": 0.64289427, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 6, "text_snippet": "y trained to leverage and follow facts from provided passages. This work introduces Self-Reflective Retrieval-augmented Generation ( SELF-RAG)to improve an LLM’s generation quality, including its factual accuracy without hurting its versati"}, {"rank": 4, "score": 0.629017, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 95, "text_snippet": "higher S&P scores on short-form PopQA, which is consistent with Menick et al. (2022). Human annotators also find ISRELand ISSUPreflection token predictions are mostly aligned with their assessments. Appendix Table 6 shows several annotated "}, {"rank": 5, "score": 0.62315226, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 34, "text_snippet": "l predicts the next output segment, as it does in a standard LM. If retrieval is needed, the model generates: a critique token to evaluate the retrieved passage’s relevance, the next response segment, and a critique token to evaluate if the"}]}
{"case_index": 36, "query": "According to the paper 'leveraging passage retrieval with generative models for open domain question answering', what exact term completes this statement: \"(2020) in- troduced retrieval augmented [BLANK] models for open domain question answering.\"?", "gold": "generative", "answer": "The missing term is **“generative”** — i.e., “introduced retrieval augmented **generative** models for open domain question answering.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'generative' and is directly supported by Context 5 stating 'introduced retrieval augmented generative models'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.2, "hit_rank": 5, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.807, "llm_ms": 1532.519, "top_contexts": [{"rank": 1, "score": 0.7325114, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 16, "text_snippet": "las (Izacard et al., 2022), which fine-tunes an encoder- decoder model in conjunction with the retriever; REALM (Guu et al., 2020), a bidirectional, masked LM fine-tuned for open-domain question answering; and RAG (Retrieval-Augmented Gener"}, {"rank": 2, "score": 0.7148393, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 15, "text_snippet": " that answers do not correspond to spans in support documents, thus requiring ab- stractive models. Raffel et al. (2019) showed that generative models are competitive for reading com- prehension tasks such as SQuAD (Rajpurkar et al., 2016),"}, {"rank": 3, "score": 0.703514, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 0, "text_snippet": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering Gautier Izacard1,2,3Edouard Grave1 1Facebook AI Research, Paris 2ENS, PSL University, Paris 3Inria, Paris gizacard|egrave@fb.com Abstract Generative mode"}, {"rank": 4, "score": 0.69444877, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}, {"rank": 5, "score": 0.6914203, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 16, "text_snippet": "0) and Lewis et al. (2020) in- troduced retrieval augmented generative models for open domain question answering. Our approach differs from these works by how the generative model processes the retrieved passages. This al- lows to scale to "}]}
{"case_index": 37, "query": "According to the paper 'leveraging passage retrieval with generative models for open domain question answering', what exact term completes this statement: \"ve models, and multiple [BLANK] have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a).\"?", "gold": "techniques", "answer": "The missing term is **“techniques”**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'techniques' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 9.405, "llm_ms": 1118.095, "top_contexts": [{"rank": 1, "score": 0.74857926, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}, {"rank": 2, "score": 0.67695177, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 16, "text_snippet": "las (Izacard et al., 2022), which fine-tunes an encoder- decoder model in conjunction with the retriever; REALM (Guu et al., 2020), a bidirectional, masked LM fine-tuned for open-domain question answering; and RAG (Retrieval-Augmented Gener"}, {"rank": 3, "score": 0.6748758, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 15, "text_snippet": " that answers do not correspond to spans in support documents, thus requiring ab- stractive models. Raffel et al. (2019) showed that generative models are competitive for reading com- prehension tasks such as SQuAD (Rajpurkar et al., 2016),"}, {"rank": 4, "score": 0.6657441, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 0, "text_snippet": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering Gautier Izacard1,2,3Edouard Grave1 1Facebook AI Research, Paris 2ENS, PSL University, Paris 3Inria, Paris gizacard|egrave@fb.com Abstract Generative mode"}, {"rank": 5, "score": 0.6598372, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 84, "text_snippet": " and pre-training paradigm for generative LLMs, leveraging cross-attention and end-to-end pre-training with 9  retrieval from a trillion-token data store, achieving strong performance. Subsequent work by Shi et al. (2024) and Lin et al. (20"}]}
{"case_index": 38, "query": "According to the paper 'raptor recursive abstractive processing for tree organized retrieval', what exact term completes this statement: \"These texts are then embedded using SBERT, a BERT-based encoder ( [BLANK]1 ) (Reimers & Gurevych, 2019).\"?", "gold": "multi-qa-mpnet-base-cos-v", "answer": "The missing term is **“multi-qa-mpnet-base-cos-v1”**.", "answer_score": 0.3, "answer_correctness": 0.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context shows the term is 'multi-qa-mpnet-base-cos-v1', but expected answer is 'multi-qa-mpnet-base-cos-v' (no '1'). Answer is grounded but does not match expected exactly.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.25, "hit_rank": 4, "matched_by": "doc_id+chunk_id", "retrieval_ms": 13.944, "llm_ms": 1578.736, "top_contexts": [{"rank": 1, "score": 0.5879434, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 53, "text_snippet": "a speed-setting. We report accuracies for both the entire test set and the HARD subset. Controlled Baseline Comparisons We first present controlled comparisons using the UnifiedQA 3B as the reader, with SBERT (Reimers & Gurevych, 2019), BM2"}, {"rank": 2, "score": 0.5827619, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 10, "text_snippet": "eural retriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The retriever (Dense Passage Retriever [ 26], henceforth DPR) provides latent documents conditioned on the input, and the seq2seq model (BART"}, {"rank": 3, "score": 0.57799757, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 0, "text_snippet": "Published as a conference paper at ICLR 2024 RAPTOR: R ECURSIVE ABSTRACTIVE PROCESSING FOR TREE-ORGANIZED RETRIEVAL Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D. Manning Stanford University psarthi@cs."}, {"rank": 4, "score": 0.5701427, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 23, "text_snippet": "es. If a sentence exceeds the 100-token limit, we move the entire sentence to the next chunk, rather than cutting it mid-sentence. This preserves the contextual and semantic coherence of the text within each chunk. These texts are then embe"}, {"rank": 5, "score": 0.562789, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 111, "text_snippet": ", 2021. URL https://arxiv.org/abs/2112.11446 . Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton- Brown, and Yoav Shoham. In-context retrieval-augmented language models. arXiv preprint arXiv:2302.00083 , 2023. "}]}
{"case_index": 39, "query": "According to the paper 'replug retrieval augmented black box language models', what exact term completes this statement: \"Both methods require updating the model [BLANK] through gradient descent, which cannot be applied to black-box LMs.\"?", "gold": "parameters", "answer": "The missing term is **“parameters”** — i.e., “Both methods require updating the model **parameters** through gradient descent, which cannot be applied to black-box LMs.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'parameters' and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 15.413, "llm_ms": 1533.685, "top_contexts": [{"rank": 1, "score": 0.5875956, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 18, "text_snippet": "e to incorporate retrieved texts and pretrains the language model from scratch. Both methods require updating the model parameters through gradient descent, which cannot be applied to black-box LMs. Another line of retrieval-augmented LMs s"}, {"rank": 2, "score": 0.585045, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 5, "text_snippet": "ting the LM’s parameters, REPLUG treats the language model as a black box and augments it with a frozen or tunable retriever. This black-box assumption makes REPLUG applicable to large LMs (i.e., >100B parameters), which are often served vi"}, {"rank": 3, "score": 0.5694731, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 8, "text_snippet": "Black-Box Language Models Figure 1, REPLUG is extremely flexible and can be used with any existing black-box LM and retrieval model. We also introduce REPLUG LSR (REPLUG with LM- Supervised Retrieval), a training scheme that can further imp"}, {"rank": 4, "score": 0.5494009, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 9, "text_snippet": "22) that adapts language models to the retriever. We use a training objective which prefers retrieving documents that improve language model perplexity, while treating the LM as a frozen, black-box scoring function. Our experiments show tha"}, {"rank": 5, "score": 0.54074323, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 4, "text_snippet": "trieval-augmented language models require access to the internal LM repre- sentations (e.g., to train the model (Borgeaud et al., 2022; 1University of Washington2Stanford University3KAIST4Meta AI. *Work done while the first author was inter"}]}
{"case_index": 40, "query": "According to the paper 'from local to global a graph rag approach to query focused summarization', what exact term completes this statement: \"o substantial improvements over a conventional RAG baseline for both the [BLANK] and diversity of generated answers.\"?", "gold": "comprehensiveness", "answer": "The missing term is **“comprehensiveness”** (i.e., “the comprehensiveness and diversity of generated answers.”).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'comprehensiveness' and is directly supported by context stating 'comprehensiveness and diversity of generated answers'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.3333333333333333, "hit_rank": 3, "matched_by": "doc_id+chunk_id", "retrieval_ms": 20.505, "llm_ms": 1431.694, "top_contexts": [{"rank": 1, "score": 0.74135715, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 96, "text_snippet": "tigate these downstream risks for questions of a global nature, which might otherwise be answered by samples of retrieved facts falsely presented as global summaries. 7 Conclusion We have presented GraphRAG, a RAG approach that combines kno"}, {"rank": 2, "score": 0.69908106, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 29, "text_snippet": "lements (nodes, edges, covariates) that the LLM can summarize in parallel at both indexing time and query time. The “global answer” to a given query is produced using a final round of query-focused summarization over all community summaries"}, {"rank": 3, "score": 0.69879943, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 4, "text_snippet": "o substantial improvements over a conventional RAG baseline for both the comprehensiveness and diversity of generated answers. 1 Introduction Retrieval augmented generation (RAG) (Lewis et al., 2020) is an established approach to using LLMs"}, {"rank": 4, "score": 0.67329854, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 3, "text_snippet": "wo stages: first, to derive an entity knowledge graph from the source documents, then to pre- generate community summaries for all groups of closely related entities. Given a question, each community summary is used to generate a partial re"}, {"rank": 5, "score": 0.66923106, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 0, "text_snippet": "From Local to Global: A GraphRAG Approach to Query-Focused Summarization Darren Edge1†Ha Trinh1†Newman Cheng2Joshua Bradley2Alex Chao3 Apurva Mody3Steven Truitt2Dasha Metropolitansky1Robert Osazuwa Ness1 Jonathan Larson1 1Microsoft Research"}]}
{"case_index": 41, "query": "According to the paper 'raptor recursive abstractive processing for tree organized retrieval', what exact term completes this statement: \"This is particularly relevant for thematic questions that require integrating knowledge from multiple parts of a text, such as [BLANK] an entire book, as in the NarrativeQA dataset (Ko ˇcisk`y et al., 2018).\"?", "gold": "understanding", "answer": "The missing term is **“understanding”** — as in: “such as **understanding** an entire book, as in the NarrativeQA dataset (Kočiský et al., 2018).”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer correctly fills the blank with 'understanding', which is explicitly present in the provided context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 12.852, "llm_ms": 1526.464, "top_contexts": [{"rank": 1, "score": 0.703861, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 7, "text_snippet": "re. This is particularly relevant for thematic questions that require integrating knowledge from multiple parts of a text, such as understanding an entire book, as in the NarrativeQA dataset (Ko ˇcisk`y et al., 2018). Consider the fairy tal"}, {"rank": 2, "score": 0.6523248, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 147, "text_snippet": "ctly as a leaf node or indirectly as part of a summary from a higher layer. ”The first question we examine is “How does Cinderella find a happy ending?”, a multi-hop question best answered by synthesizing information from various text segme"}, {"rank": 3, "score": 0.6321842, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 72, "text_snippet": "e from the leaf nodes and each upper layer, as well as from different contiguous subsets of the layers. We show findings specific to one story in Table 8, revealing that a full-tree search, utilizing all layers, outperformed retrieval strat"}, {"rank": 4, "score": 0.6236924, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 73, "text_snippet": "an effectively handle a wider range of questions, from higher-order thematic queries to detail-oriented questions. Detailed results for additional stories and an ablation study on layer contributions can be found in Appendix I. 5 C ONCLUSIO"}, {"rank": 5, "score": 0.62162805, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 150, "text_snippet": "estion that requires holistic understanding of the entire text. The text retrieved by RAPTOR and DPR for this question is shown in Table 13. The text retrieved by RAPTOR contains short descriptions of all the major parts of the story, where"}]}
{"case_index": 42, "query": "According to the paper 'from local to global a graph rag approach to query focused summarization', what exact term completes this statement: \"However, in order to produce a fair evaluation, our method avoids generating the questions directly from the corpus itself (as an alternative [BLANK], one can use a subset of the corpus held out from subsequen\"?", "gold": "implementation", "answer": "The missing term is **“implementation”** — i.e., the sentence reads: “(as an alternative **implementation**, one can use a subset of the corpus held out from subsequent …)”.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'implementation' and is directly supported by the provided context quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 9.435, "llm_ms": 1885.259, "top_contexts": [{"rank": 1, "score": 0.6440627, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 23, "text_snippet": " this work, we propose an approach for generating a set of questions for evaluating global sensemaking over the entirety of the corpus. Our approach is related to LLM methods that use a corpus to generate questions whose answers would be su"}, {"rank": 2, "score": 0.60804856, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 24, "text_snippet": "tion, one can use a subset of the corpus held out from subsequent graph extraction and answer evaluation steps). Adaptive benchmarking refers to the process of dynamically generating evaluation benchmarks tai- lored to specific domains or u"}, {"rank": 3, "score": 0.5866277, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 1, "text_snippet": "ibuted equally to this work Abstract The use of retrieval-augmented generation (RAG) to retrieve relevant informa- tion from an external knowledge source enables large language models (LLMs) to answer questions over private and/or previousl"}, {"rank": 4, "score": 0.5843113, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 96, "text_snippet": "tigate these downstream risks for questions of a global nature, which might otherwise be answered by samples of retrieved facts falsely presented as global summaries. 7 Conclusion We have presented GraphRAG, a RAG approach that combines kno"}, {"rank": 5, "score": 0.5738793, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 22, "text_snippet": "gly global summaries by using the LLM to create summaries spanning this community hierarchy. 2.3 Adaptive benchmarking for RAG Evaluation Many benchmark datasets for open-domain question answering exist, including HotPotQA (Yang et al., 201"}]}
{"case_index": 43, "query": "According to the paper 'recomp improving retrieval augmented lms with compression and selective augmentation', what exact term completes this statement: \"1arXiv:2310.04408v1 [cs.CL] 6 Oct 2023 RECOMP during inference moved from Smyrna, [BLANK], to Nissan's facility in Canton, M\"?", "gold": "tennessee", "answer": "The missing term is **“Tennessee”** (i.e., “moved from Smyrna, Tennessee, to Nissan's facility in Canton, M…”).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'tennessee' and is directly supported by the provided context ('Smyrna, Tennessee').", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 15.091, "llm_ms": 1683.414, "top_contexts": [{"rank": 1, "score": 0.6960647, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 7, "text_snippet": "rieved evidence documents, and guide RALM to generate desired outputs when prepended to the input. To satisfy both efficiency and effectiveness constraints, our compressor strategically performs selective augmentation by generating an empty"}, {"rank": 2, "score": 0.509892, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 9, "text_snippet": "acility in Canton, Mississippi. Early US models include X, S and PRO-4X, with a choice of 6-speed manual or 5-speed automatic transmissions, a choice of [...] moved from Smyrna, Tennessee, to Nissan's facility in Canton, Mississippi. Early "}, {"rank": 3, "score": 0.5062218, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 0, "text_snippet": "RECOMP: I MPROVING RETRIEVAL -AUGMENTED LM S WITH COMPRESSION AND SELECTIVE AUGMENTATION Fangyuan Xu1, Weijia Shi2, Eunsol Choi1 Department of Computer Science 1The University of Texas at Austin 2University of Washington {fangyuan,eunsol }@"}, {"rank": 4, "score": 0.48887795, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 135, "text_snippet": "arXiv preprint arXiv:2305.00633 , 2023. URL https://arxiv.org/abs/2305.00633 . Fangyuan Xu, Weijia Shi, and Eunsol Choi. Recomp: Improving retrieval-augmented lms with compression and selective augmentation, 2023. URL https://arxiv.org/abs/"}, {"rank": 5, "score": 0.47595936, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 8, "text_snippet": " moved from Smyrna, Tennessee, to Nissan's facility in Canton, Mississippi. Early US models include X, S and PRO-4X, with a choice of 6-speed manual or 5-speed automatic transmissions, a choice of [...] moved from Smyrna, Tennessee, to Niss"}]}
{"case_index": 44, "query": "According to the paper 'retrieval augmented generation for knowledge intensive nlp tasks', what exact term completes this statement: \"Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform- ers Library [ 66] and can be found at https://github.com/huggingface/[BLANK]/blob/master/ examples/rag/ .\"?", "gold": "transformers", "answer": "The missing term is **`transformers`**, completing the URL as:\n\n`https://github.com/huggingface/transformers/blob/master/examples/rag/`", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'transformers' and is directly supported by the provided context URL.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.401, "llm_ms": 1402.715, "top_contexts": [{"rank": 1, "score": 0.69226015, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 16, "text_snippet": "Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform- ers Library [ 66] and can be found at https://github.com/huggingface/transformers/blob/master/ examples/rag/ . An interactive demo of RAG models ca"}, {"rank": 2, "score": 0.6664763, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 139, "text_snippet": "Wikipedia. After submission, We have ported our code to HuggingFace Transformers [ 66]3, which achieves equivalent performance to the previous version but is a cleaner and easier to use implementation. This version is also open-sourced. We "}, {"rank": 3, "score": 0.5654162, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 132, "text_snippet": "doi: 10.18653/v1/W18-5713. URL https://www.aclweb.org/anthology/W18-5713 . 15  [66] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Daviso"}, {"rank": 4, "score": 0.5385051, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 15, "text_snippet": " update the models’ knowledge as the world changes.1 2 Methods We explore RAG models, which use the input sequence xto retrieve text documents zand use them as additional context when generating the target sequence y. As shown in Figure 1, "}, {"rank": 5, "score": 0.531637, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 42, "text_snippet": " state of the art (only on the T5-comparable split for TQA). RAG combines the generation ﬂexibility of the “closed-book” (parametric only) approaches and the performance of \"open-book\" retrieval-based approaches. Unlike REALM and T5+SSM, RA"}]}
{"case_index": 45, "query": "According to the paper 'leveraging passage retrieval with generative models for open domain question answering', what exact term completes this statement: \"For the retrieval of support passages, we consider two methods: BM25 ([BLANK] et al., 1995) and DPR (Karpukhin et al., 2020).\"?", "gold": "robertson", "answer": "The blank is **“Robertson”** — i.e., **BM25 (Robertson et al., 1995)**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'Robertson' and is directly supported by Context 1/3 stating 'BM25 (Robertson et al., 1995)'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.905, "llm_ms": 1673.943, "top_contexts": [{"rank": 1, "score": 0.738309, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 19, "text_snippet": "den test set (right), competitions.codalab.org/competitions/17208#results ). Retrieval. For the retrieval of support passages, we consider two methods: BM25 (Robertson et al., 1995) and DPR (Karpukhin et al., 2020). In BM25, passages are re"}, {"rank": 2, "score": 0.61318994, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 90, "text_snippet": "age Processing (EMNLP) , pp. 6769–6781, Online, November 2020a. Association for Computational Lin- guistics. doi: 10.18653/v1/2020.emnlp-main.550. URL https://aclanthology.org/2020.emnlp-main.550 . Karpukhin, V ., Oguz, B., Min, S., Lewis, "}, {"rank": 3, "score": 0.6121893, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 14, "text_snippet": "omponents: the retriever, the reader, and end-to-end system training. Retrieval methods have transitioned from traditional term-based techniques like TF-IDF (Sp¨arck Jones, 1972) and BM25 (Robertson et al., 1995; Roberts et al., 2020) to de"}, {"rank": 4, "score": 0.60478103, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 20, "text_snippet": "es with SpaCy.2In DPR, passages and questions are represented as dense vector representations, computed using two BERT networks. The ranking function is the dot product between the query and passage represen- tations. Retrieval is performed"}, {"rank": 5, "score": 0.6044205, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 51, "text_snippet": "answering (Voorhees et al., 1999), documents are often retrieved from Wikipedia (Chen et al., 2017). Recently, dense retrievers based on neural networks have become popular. These usually follow a dual-encoder architecture (Yih et al., 2011"}]}
{"case_index": 46, "query": "According to the paper 'from local to global a graph rag approach to query focused summarization', what exact term completes this statement: \"3.1.1 Source [BLANK] →Text Chunks To start, the documents in the corpus are split into text chunks.\"?", "gold": "documents", "answer": "The missing term is **“Documents”** — the section title is:\n\n**“3.1.1 Source Documents → Text Chunks”**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context explicitly shows the heading '3.1.1 Source Documents →Text Chunks', so the blank is 'Documents'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 11.75, "llm_ms": 1400.401, "top_contexts": [{"rank": 1, "score": 0.62601024, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 31, "text_snippet": "oach. We also validate results using statistics derived from LLM-extracted statements of verifiable facts, or “claims.” 3 Methods 3.1 GraphRAG Workflow Figure 1 illustrates the high-level data flow of the GraphRAG approach and pipeline. In "}, {"rank": 2, "score": 0.6026504, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 27, "text_snippet": "ns (Wang et al., 2023a; Zheng et al., 2024). Some prior work proposes criteria for having LLMs quantify the quality of 3  Source Documents Text Chunkstext extraction and chunking Entities & Relationshipsdomain-tailored summarization Knowled"}, {"rank": 3, "score": 0.5808868, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 63, "text_snippet": "siness, sports, technology, health, and science (Tang and Yang, 2024). The corpus is divided into 3197 ×600-token text chunks, with 100-token overlaps between chunks ( ∼1.7 million tokens). 4.1.2 Conditions We compared six conditions includ"}, {"rank": 4, "score": 0.5784085, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 0, "text_snippet": "From Local to Global: A GraphRAG Approach to Query-Focused Summarization Darren Edge1†Ha Trinh1†Newman Cheng2Joshua Bradley2Alex Chao3 Apurva Mody3Steven Truitt2Dasha Metropolitansky1Robert Osazuwa Ness1 Jonathan Larson1 1Microsoft Research"}, {"rank": 5, "score": 0.5671882, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 32, "text_snippet": " The LLM extracts information from each chunk for downstream processing. Selecting the size of the chunk is a fundamental design decision; longer text chunks require fewer LLM calls for such extraction (which reduces cost) but suffer from d"}]}
{"case_index": 47, "query": "According to the paper 'from local to global a graph rag approach to query focused summarization', what exact term completes this statement: \"In ad- dition, versions of the GraphRAG approach are also available as extensions to multiple open- source libraries, including LangChain (LangChain, 2024), LlamaIndex (LlamaIndex, 2024), Nebu- laGraph ([BLANK], 2024), and Neo4J (Neo4J, 2024).\"?", "gold": "nebulagraph", "answer": "The missing term is **“NebulaGraph”**, completing the citation as:\n\n**“NebulaGraph (NebulaGraph, 2024)”**.", "answer_score": 0.3, "answer_correctness": 0.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Expected blank term is the lowercase citation key 'nebulagraph'. Model answered 'NebulaGraph' (the library name), which doesn't match the exact term requested. Context supports that the citation is 'NebulaGraph (NebulaGraph, 2024)' but question asks for the exact term completing [BLANK].", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.221, "llm_ms": 1229.808, "top_contexts": [{"rank": 1, "score": 0.77348924, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 13, "text_snippet": "ps://github .com/microsoft/graphrag. In ad- dition, versions of the GraphRAG approach are also available as extensions to multiple open- source libraries, including LangChain (LangChain, 2024), LlamaIndex (LlamaIndex, 2024), Nebu- laGraph ("}, {"rank": 2, "score": 0.67617, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 19, "text_snippet": "2016; Mooney and Bunescu, 2005; Yates et al., 2007). GraphRAG falls into a more recent body of research that use of LLMs for knowledge graph extraction (Ban et al., 2023; Melnyk et al., 2022; OpenAI, 2023; Tan et al., 2017; Trajanoska et al"}, {"rank": 3, "score": 0.67082095, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 17, "text_snippet": " are later used to answer queries as in Mao et al. 2020). These summaries are generated in parallel and iteratively aggregated into global summaries, similar to prior techniques (Feng et al., 2023; Gao et al., 2023; Khattab et al., 2022; Sh"}, {"rank": 4, "score": 0.6666598, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 16, "text_snippet": " may use alternative retrieval mechanisms, we collectively refer to the family of conventional approaches as vector RAG . GraphRAG contrasts with vector RAG in its ability to answer queries that require global sensemaking over the entire da"}, {"rank": 5, "score": 0.66371787, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 27, "text_snippet": "ns (Wang et al., 2023a; Zheng et al., 2024). Some prior work proposes criteria for having LLMs quantify the quality of 3  Source Documents Text Chunkstext extraction and chunking Entities & Relationshipsdomain-tailored summarization Knowled"}]}
{"case_index": 48, "query": "According to the paper 'recomp improving retrieval augmented lms with compression and selective augmentation', what exact term completes this statement: \"We summarize the key ideas for our two [BLANK], extractive compressors and abstractive compressor here, and discuss their training schemes formally in Section 3.\"?", "gold": "compressors", "answer": "The missing term is **“compressors”**.\n\nSo the sentence reads: “We summarize the key ideas for our two **compressors**, extractive compressors and abstractive compressor here, and discuss their training schemes formally in Section 3.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'compressors' and is directly supported by Context 2.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.808, "llm_ms": 1687.575, "top_contexts": [{"rank": 1, "score": 0.7455472, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 23, "text_snippet": "odels in the output should be faithful to the original input, yet the main goal is different. Instead of capturing salient information for humans readers, compressors aim to produce a concise text that are useful for a LM on an end task. In"}, {"rank": 2, "score": 0.73349345, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 19, "text_snippet": "ining compressors for conciseness and effectiveness. We summarize the key ideas for our two compressors, extractive compressors and abstractive compressor here, and discuss their training schemes formally in Section 3. 2Improving retriever "}, {"rank": 3, "score": 0.71919477, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 2, "text_snippet": "d an abstractive compressor which generates summaries by synthesizing information from multiple documents. Both compressors are trained to improve LMs’ performance on end tasks when the generated summaries are prepended to the LMs’ input, w"}, {"rank": 4, "score": 0.69857734, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 11, "text_snippet": " prepending it as input to a language model at inference time. The compressed summary guides the LM to generate the correct answer, while significantly reducing the computation costs required to encode the documents. We propose compressors:"}, {"rank": 5, "score": 0.69617414, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 24, "text_snippet": "TRACTIVE COMPRESSION As we formulate extractive compression as a ranking problem, training extractive compressor re- sembles training a reranker for the retrieved documents4with two differences. First, our compressor considers a different g"}]}
{"case_index": 49, "query": "According to the paper 'corrective retrieval augmented generation', what exact term completes this statement: \"[BLANK] Generation RAG (Lewis et al., 2020; Guu et al., 2020) is regarded as a useful method to address the issues above, which enhances the input questions of generative LMs with retrieved documents.\"?", "gold": "retrieval-augmented", "answer": "**Retrieval-Augmented**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "The missing term is 'Retrieval-Augmented' (i.e., retrieval-augmented), exactly as shown in the provided context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 9.502, "llm_ms": 949.684, "top_contexts": [{"rank": 1, "score": 0.77174544, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 18, "text_snippet": "pact the LLMs and exacerbate the problems. Thus, it is obvious that the lack of accurate and specific knowledge can lead to misleading or even inaccurate generation, which will severely hurt the experience of users in most practical applica"}, {"rank": 2, "score": 0.74529153, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 20, "text_snippet": "model that specializes in response generation. Despite this, the methods above usually ignore a question, what if the retrieval goes wrong? Since the purpose of introducing a retrieval is to secure that generative LMs can obtain relevant an"}, {"rank": 3, "score": 0.72209567, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 7, "text_snippet": "ieval-augmented generation (RAG) (Lewis et al., 2020). In this framework, the input to models is augmented by prepending relevant documents that are retrieved from an external knowledge corpus (Guu et al., 2020). While RAG serves as a pract"}, {"rank": 4, "score": 0.7215974, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 10, "text_snippet": " should not have been equally referred to and involved in RAG. On account of the above issues, this paper particularly studies the scenarios where the retriever returns inaccurate results. A method named Corrective Retrieval- Augmented Gene"}, {"rank": 5, "score": 0.70445603, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 4, "text_snippet": " for long-form generations relative to these models.1 1 I NTRODUCTION State-of-the-art LLMs continue to struggle with factual errors (Mallen et al., 2023; Min et al., 2023) despite their increased model and data scale (Ouyang et al., 2022)."}]}
{"case_index": 50, "query": "According to the paper 'atlas few shot learning with retrieval augmented language models', what exact term completes this statement: \"For this [BLANK] ability to emerge, the key ingredients are scaling both the parameter count of the model, and the size of the training data.\"?", "gold": "generalisation", "answer": "The missing term is **“generalisation”**, as in: “For this **generalisation** ability to emerge, the key ingredients are scaling both the parameter count of the model, and the size of the training data.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'generalisation' and is directly supported by Context 2 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.861, "llm_ms": 1284.288, "top_contexts": [{"rank": 1, "score": 0.6706397, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 4, "text_snippet": " model, and the size of the training data. Large language models owe this improvement to both a larger computational budget, enabling more complex reasoning, and the ability to memorize more information related to downstream tasks from the "}, {"rank": 2, "score": 0.623315, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 3, "text_snippet": "xamples, outperforming a 540B parameters model by 3% despite having 50x fewer parameters. 1 Introduction Large language models (LLMs) are impressive few-shot learners (Brown et al., 2020; Rae et al., 2021; Hoﬀmann et al., 2022; Chowdhery et"}, {"rank": 3, "score": 0.62151885, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 62, "text_snippet": "arge Language models. Providing language models with natural language descriptions of tasks, as proposed by Radford et al. (2019) has led to signiﬁcant developments in few-shot learning. GPT-3 (Brown et al., 2020) demonstrated the ability o"}, {"rank": 4, "score": 0.6173219, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 63, "text_snippet": "s learning ability, leading to the further development of large models (Lieber et al., 2021; Rae et al., 2021; Smith et al., 2022; Chowdhery et al., 2022; Smith et al., 2022). Hoﬀmann et al. (2022) revisited the scaling law from Kaplan et a"}, {"rank": 5, "score": 0.6122537, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 1, "text_snippet": " Abstract Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter count"}]}
{"case_index": 51, "query": "According to the paper 'raptor recursive abstractive processing for tree organized retrieval', what exact term completes this statement: \"The tree traversal method traverses the tree [BLANK], pruning and selecting the most relevant nodes at each level.\"?", "gold": "layer-by-layer", "answer": "The missing term is **“layer-by-layer”**.\n\nSo the full statement is: “The tree traversal method traverses the tree **layer-by-layer**, pruning and selecting the most relevant nodes at each level.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'layer-by-layer' and is explicitly supported by Context 3.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.3333333333333333, "hit_rank": 3, "matched_by": "doc_id+chunk_id", "retrieval_ms": 25.784, "llm_ms": 1977.471, "top_contexts": [{"rank": 1, "score": 0.7421078, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 36, "text_snippet": "by RAPTOR: tree traversal and collapsed tree. These methods offer unique ways of traversing the multi-layered RAPTOR tree to retrieve relevant information, each with its own advantages and trade-offs. We provide the pseudocode of both metho"}, {"rank": 2, "score": 0.6572237, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 40, "text_snippet": "between the query vector and the vector embeddings of these child nodes. 4. Select the top kchild nodes with the highest cosine similarity scores to the query, forming the set S2. 5. Continue this process recursively for dlayers, producing "}, {"rank": 3, "score": 0.6466464, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 25, "text_snippet": " linearly in terms of both build time and token expenditure, making it suitable for processing large and complex corpora. For a comprehensive discussion on RAPTOR’s scalability, please refer to the Appendix A. For querying within this tree,"}, {"rank": 4, "score": 0.638621, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 8, "text_snippet": " indexing and retrieval system that uses a tree structure to capture both high-level and low-level details about a text. As shown in Figure 1, our system, RAPTOR, clusters chunks of text, generates text summaries of those clusters, and then"}, {"rank": 5, "score": 0.6276422, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 26, "text_snippet": "d tree method evaluates nodes collectively across all layers to find the most relevant ones. Clustering Algorithm Clustering plays a key role in building the RAPTOR tree, organizing text segments into cohesive groups. This step groups relat"}]}
{"case_index": 52, "query": "According to the paper 'corrective retrieval augmented generation', what exact term completes this statement: \"Given an input query and the retrieved documents from any retriever, a [BLANK] retrieval evaluator is constructed to estimate the relevance score of retrieved d\"?", "gold": "lightweight", "answer": "The missing term is **“lightweight”** — i.e., “a **lightweight** retrieval evaluator is constructed to estimate the relevance score of retrieved documents.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'lightweight' and is directly supported by the provided context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 11.874, "llm_ms": 1517.74, "top_contexts": [{"rank": 1, "score": 0.6367283, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 10, "text_snippet": " should not have been equally referred to and involved in RAG. On account of the above issues, this paper particularly studies the scenarios where the retriever returns inaccurate results. A method named Corrective Retrieval- Augmented Gene"}, {"rank": 2, "score": 0.61473393, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 2, "text_snippet": "he robustness of generation. Specifically, a lightweight retrieval evaluator is designed to assess the overall quality of retrieved documents for a query, returning a confidence degree based on which different knowledge retrieval ac- tions "}, {"rank": 3, "score": 0.6112237, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 28, "text_snippet": "sition, filter, and recomposition (Section 4.4). If the action Incorrect is triggered, the retrieved documents will be discarded. Instead, web searches are resorted to and regarded as complementary knowledge sources for corrections (Section"}, {"rank": 4, "score": 0.60995287, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 31, "text_snippet": "neratorkex+xkex+ Figure 2: An overview of the proposed CRAG at inference. A retrieval evaluator is constructed to evaluate the relevance of the retrieved documents to the input, and estimate a confidence degree based on which different know"}, {"rank": 5, "score": 0.5881323, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 29, "text_snippet": "triever inREPLUG by using the LM itself to provide supervision about which documents should be retrieved. Inspired by Sachan et al. (2022), our approach can be seen as adjusting the probabilities of the retrieved documents to match the prob"}]}
{"case_index": 53, "query": "According to the paper 'leveraging passage retrieval with generative models for open domain question answering', what exact term completes this statement: \"that answers do not [BLANK] to spans in support documents, thus requiring ab- stractive models.\"?", "gold": "correspond", "answer": "The missing term is **“correspond”** (i.e., “that answers do not **correspond** to spans in support documents, thus requiring abstractive models.”).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'correspond' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.985, "llm_ms": 2855.314, "top_contexts": [{"rank": 1, "score": 0.64020634, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 15, "text_snippet": " that answers do not correspond to spans in support documents, thus requiring ab- stractive models. Raffel et al. (2019) showed that generative models are competitive for reading com- prehension tasks such as SQuAD (Rajpurkar et al., 2016),"}, {"rank": 2, "score": 0.5765556, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 6, "text_snippet": "tech- niques have been considered, either using sparse representations based on TF/IDF or using dense embeddings (Guu et al., 2020; Karpukhin et al., 2020). The models which extract the answers are often based on contextualized word represe"}, {"rank": 3, "score": 0.5685088, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 5, "text_snippet": "t, it retrieves support text passages from an external source of knowledge such as Wikipedia. Then, a generative encoder-decoder model produces the answer, conditioned on the question and the re- trieved passages. This approach scales well "}, {"rank": 4, "score": 0.5442591, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}, {"rank": 5, "score": 0.5400564, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 10, "text_snippet": " problem in natural lan- guage processing (V oorhees et al., 1999), this task has recently regained interest following the work by Chen et al. (2017). In that version of the prob- lem, strong supervision is available to the learning system,"}]}
{"case_index": 54, "query": "According to the paper 'self rag learning to retrieve generate and critique through self reflection', what exact term completes this statement: \"Empirical results on six tasks, including reasoning and long-form generation, demonstrate that SELF- RAGsignificantly outperforms pre-trained and [BLANK] LLMs that have more parameters and widely adopted RAG approaches with higher citation accuracy.\"?", "gold": "instruction-tuned", "answer": "The missing term is **“instruction-tuned”**, i.e., the statement reads: “…significantly outperforms pre-trained and **instruction-tuned** LLMs…”", "answer_score": 0.82, "answer_correctness": 1.0, "answer_groundedness": 0.4, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 0.4, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'instruction-tuned'. However, provided context snippets do not contain the exact phrase 'pre-trained and instruction-tuned', so support from context is weak.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 8.857, "llm_ms": 1655.865, "top_contexts": [{"rank": 1, "score": 0.8020218, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 96, "text_snippet": "ains an LM to learn to retrieve, generate, and critique text passages and its own generation by predicting the next tokens from its original vocabulary as well as newly added special tokens, called reflection tokens. SELF-RAGfurther enables"}, {"rank": 2, "score": 0.7739963, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 3, "text_snippet": "e phase, enabling it to tailor its behavior to diverse task requirements. Experiments show that SELF- RAG(7B and 13B parameters) significantly outperforms state-of-the-art LLMs and retrieval-augmented models on a diverse set of tasks. Speci"}, {"rank": 3, "score": 0.7329752, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 27, "text_snippet": "n (reasoning path consistency) and without retrieval. Recent work on LLM refinement (Dhuliawala et al., 2023; Madaan et al., 2023; Paul et al., 2023) prompts a model to generate task output, natural language feedback and refined task output"}, {"rank": 4, "score": 0.7277828, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 6, "text_snippet": "y trained to leverage and follow facts from provided passages. This work introduces Self-Reflective Retrieval-augmented Generation ( SELF-RAG)to improve an LLM’s generation quality, including its factual accuracy without hurting its versati"}, {"rank": 5, "score": 0.723318, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 2, "text_snippet": "ew framework called Self-Reflective Retrieval-Augmented Gen- eration ( SELF-RAG)that enhances an LM’s quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passage"}]}
{"case_index": 55, "query": "According to the paper 'refrag rethinking rag based decoding', what exact term completes this statement: \"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token [BLANK] ef\"?", "gold": "allocation", "answer": "The missing term is **“allocation”**, i.e., the statement is:\n\n“1) It shortens the decoder’s input length, improving token **allocation** efficiency.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context 2 explicitly contains the phrase 'improving token allocation efficiency,' matching the expected term.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.3333333333333333, "hit_rank": 3, "matched_by": "doc_id+chunk_id", "retrieval_ms": 15.35, "llm_ms": 1569.909, "top_contexts": [{"rank": 1, "score": 0.7083275, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 3, "text_snippet": " on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30 .85×the time-t"}, {"rank": 2, "score": 0.7030308, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 14, "text_snippet": "ortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now "}, {"rank": 3, "score": 0.6972369, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 13, "text_snippet": "nd memory usage during decoding, allwithout requiring modificationsto the LLM architecture or introducing new decoder parameters. REFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved passag"}, {"rank": 4, "score": 0.690068, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 12, "text_snippet": "is information is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-a"}, {"rank": 5, "score": 0.67752147, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 15, "text_snippet": "igure 1) while preserving the autoregressive nature of the decoder, thereby supporting multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight reinforcement learning (RL) policy that sel"}]}
{"case_index": 56, "query": "According to the paper 'replug retrieval augmented black box language models', what exact term completes this statement: \"For instance, REPLUG can improve Codex (175B) performance on MMLU by 4.5%, achieving compa- rable results to the 540B, [BLANK] Flan-PaLM.\"?", "gold": "instruction-finetuned", "answer": "The missing term is **“instruction-finetuned”**, as in: “the 540B, **instruction-finetuned** Flan-PaLM.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term and is explicitly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.278, "llm_ms": 1525.745, "top_contexts": [{"rank": 1, "score": 0.78716713, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 10, "text_snippet": "l., 2017). For instance, REPLUG can improve Codex (175B) performance on MMLU by 4.5%, achieving compa- rable results to the 540B, instruction-finetuned Flan-PaLM. Furthermore, tuning the retriever with our training scheme (i.e., REPLUG LSR "}, {"rank": 2, "score": 0.7711084, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 12, "text_snippet": "gnals, resulting in improved retrieval quality. •Evaluations on language modeling (§6), open-domain QA and MMLU demonstrate that REPLUG can im- prove the performance of various language models such as GPT, OPT and BLOOM, including very larg"}, {"rank": 3, "score": 0.73865765, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 71, "text_snippet": "a black box and augments it with a tuneable retrieval model. Our evaluation shows that REPLUG can be integrated with any existing language model to improve their performance  REPLUG: Retrieval-Augmented Black-Box Language Models on language"}, {"rank": 4, "score": 0.735682, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 8, "text_snippet": "Black-Box Language Models Figure 1, REPLUG is extremely flexible and can be used with any existing black-box LM and retrieval model. We also introduce REPLUG LSR (REPLUG with LM- Supervised Retrieval), a training scheme that can further imp"}, {"rank": 5, "score": 0.7345695, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 9, "text_snippet": "22) that adapts language models to the retriever. We use a training objective which prefers retrieving documents that improve language model perplexity, while treating the LM as a frozen, black-box scoring function. Our experiments show tha"}]}
{"case_index": 57, "query": "According to the paper 'from local to global a graph rag approach to query focused summarization', what exact term completes this statement: \"Some [BLANK] use subgraphs, elements of the graph, or properties of the graph structure dire\"?", "gold": "techniques", "answer": "The missing term is **“approaches”**:\n\n“Some **approaches** use subgraphs, elements of the graph, or properties of the graph structure …”", "answer_score": 0.0, "answer_correctness": 0.0, "answer_groundedness": 0.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 0.0, "abstained": false, "should_abstain": false, "notes": "Expected missing term is 'techniques', but the model answered 'approaches'. The provided context does not contain the quoted sentence fragment to support the model's choice.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 16.958, "llm_ms": 1560.848, "top_contexts": [{"rank": 1, "score": 0.64024025, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 29, "text_snippet": "lements (nodes, edges, covariates) that the LLM can summarize in parallel at both indexing time and query time. The “global answer” to a given query is produced using a final round of query-focused summarization over all community summaries"}, {"rank": 2, "score": 0.6370157, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 0, "text_snippet": "From Local to Global: A GraphRAG Approach to Query-Focused Summarization Darren Edge1†Ha Trinh1†Newman Cheng2Joshua Bradley2Alex Chao3 Apurva Mody3Steven Truitt2Dasha Metropolitansky1Robert Osazuwa Ness1 Jonathan Larson1 1Microsoft Research"}, {"rank": 3, "score": 0.63433015, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 17, "text_snippet": " are later used to answer queries as in Mao et al. 2020). These summaries are generated in parallel and iteratively aggregated into global summaries, similar to prior techniques (Feng et al., 2023; Gao et al., 2023; Khattab et al., 2022; Sh"}, {"rank": 4, "score": 0.63268346, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 44, "text_snippet": "ummaries at one level looking for general themes of interest, then read linked reports at a lower level that provide additional details for each subtopic. Here, however, we focus on their utility as part of a graph-based index used for answ"}, {"rank": 5, "score": 0.6325488, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 27, "text_snippet": "ns (Wang et al., 2023a; Zheng et al., 2024). Some prior work proposes criteria for having LLMs quantify the quality of 3  Source Documents Text Chunkstext extraction and chunking Entities & Relationshipsdomain-tailored summarization Knowled"}]}
{"case_index": 58, "query": "According to the paper 'replug retrieval augmented black box language models', what exact term completes this statement: \"triever inREPLUG by using the LM itself to provide [BLANK] about which documents should be retrieved.\"?", "gold": "supervision", "answer": "The missing term is **“supervision”** — i.e., “…using the LM itself to provide **supervision** about which documents should be retrieved.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'supervision' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 14.335, "llm_ms": 1240.454, "top_contexts": [{"rank": 1, "score": 0.76086444, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 29, "text_snippet": "triever inREPLUG by using the LM itself to provide supervision about which documents should be retrieved. Inspired by Sachan et al. (2022), our approach can be seen as adjusting the probabilities of the retrieved documents to match the prob"}, {"rank": 2, "score": 0.7435298, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 6, "text_snippet": "PIs. Internal representations of such models are not exposed and fine-tuning is not supported. In this work, we introduce REPLUG (Retrieve and Plug ), a new retrieval-augmented LM framework where the lan- guage model is viewed as a black bo"}, {"rank": 3, "score": 0.7289716, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 1, "text_snippet": "mechanisms to en- code the retrieved text, REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design can be eas- ily applied to any existing retrieval and language models. Furthermore, we show t"}, {"rank": 4, "score": 0.721771, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 8, "text_snippet": "Black-Box Language Models Figure 1, REPLUG is extremely flexible and can be used with any existing black-box LM and retrieval model. We also introduce REPLUG LSR (REPLUG with LM- Supervised Retrieval), a training scheme that can further imp"}, {"rank": 5, "score": 0.71836644, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 7, "text_snippet": " retrieved documents are prepended to the input context and fed into the black-box LM to make the final prediction. Because the LM context length limits the number of documents that can be prepended, we also introduce a new ensemble scheme "}]}
{"case_index": 59, "query": "According to the paper 'recomp improving retrieval augmented lms with compression and selective augmentation', what exact term completes this statement: \"For language modelling, both trained compressors achieve a compression ratio of 25% with minimal [BLANK] drop.\"?", "gold": "performance", "answer": "The missing term is **“performance”** (i.e., “with minimal **performance** drop.”).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'performance' and is directly supported by Context 1 stating 'minimal performance drop'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 11.835, "llm_ms": 1354.924, "top_contexts": [{"rank": 1, "score": 0.6485978, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 15, "text_snippet": "6% and significantly outperforms prepending full documents. Our trained compressors also show promising results. For language modelling, both trained compressors achieve a compression ratio of 25% with minimal performance drop. When applied"}, {"rank": 2, "score": 0.645764, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 3, "text_snippet": "n. We evaluate our approach on language modeling task and open domain question answering task. We achieve a compression rate of as low as 6% with minimal loss in performance for both tasks, significantly outperforming the off-the-shelf summ"}, {"rank": 3, "score": 0.6133772, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 58, "text_snippet": "ning task. We observe a performance regression as the compression rate increases; however, even at a compression rate of32, our model remains competitive (as shown in table 1). In contrast, a compression rate of64appears to be overly aggres"}, {"rank": 4, "score": 0.60562277, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 65, "text_snippet": "in Table 10 in the appendix. Overall, the performance is worse than the LM from which compressors are trained on, sometimes unable to outperform other compression baselines (e.g., no clear gain from using contriever vs. our trained contriev"}, {"rank": 5, "score": 0.6022603, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 52, "text_snippet": "modeling Table 1 reports the results on language modeling task. Allretrieval augmen- tation methods improve perplexity over no retrieval setting across three LMs. Heuristic token / phrase-level compression methods (BoW and NE) are worse tha"}]}
{"case_index": 60, "query": "According to the paper 'atlas few shot learning with retrieval augmented language models', what exact term completes this statement: \"As shown in the following section, an advantage of dense retrievers is that both query and document encoders can be trained without document annotation, using standard techniques such as gradient descent and [BLANK].\"?", "gold": "distillation", "answer": "The missing term is **“distillation”**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context 1 explicitly states the phrase ends with 'gradient descent and distillation,' matching the expected answer.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 11.426, "llm_ms": 1551.16, "top_contexts": [{"rank": 1, "score": 0.692333, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 20, "text_snippet": "eir corresponding embeddings. The Contriever model is pre-trained using the MoCo contrastive loss (He et al., 2020), and uses unsupervised data only. As shown in the following section, an advantage of dense retrievers is that both query and"}, {"rank": 2, "score": 0.6273898, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 40, "text_snippet": "ent retriever ﬁne-tuning Retrieval is facilitated by using a document index, which is a pre-computed collection of the document embeddings for all the documents in the retrieval corpus. When jointly training the retriever and language model"}, {"rank": 3, "score": 0.62334824, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 23, "text_snippet": "ent loss functions to train the retriever jointly with the language model. We consider loss functions that leverage the language model to provide supervisory signal to train the retriever. In other words, if the language model ﬁnds a docume"}, {"rank": 4, "score": 0.6158488, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 49, "text_snippet": "ead. As we will see in practice, the impact of ﬁxing the documents encoder varies greatly for diﬀerent tasks when a large training dataset is available. For most of the few-shot settings that we consider, query-side ﬁnetuning does not have "}, {"rank": 5, "score": 0.6139192, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 57, "text_snippet": "nted generators using a second “informed” retriever with access to the output, which the test-time retriever can be distilled from, and Hofstätter et al. (2022) recently proposed a training set ﬁltering/weighting approach to train stronger "}]}
{"case_index": 61, "query": "According to the paper 'corrective retrieval augmented generation', what exact term completes this statement: \"Eventually, when it cannot [BLANK] make a correct or incorrect judgment, a soft and balanced action Ambiguous which combines both of them is triggered.\"?", "gold": "confidently", "answer": "The missing term is **“confidently”**:\n\n“Eventually, when it cannot **confidently** make a correct or incorrect judgment, a soft and balanced action Ambiguous which combines both of them is triggered.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'confidently' and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 13.745, "llm_ms": 1281.501, "top_contexts": [{"rank": 1, "score": 0.6012717, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 28, "text_snippet": "sition, filter, and recomposition (Section 4.4). If the action Incorrect is triggered, the retrieved documents will be discarded. Instead, web searches are resorted to and regarded as complementary knowledge sources for corrections (Section"}, {"rank": 2, "score": 0.5751878, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 43, "text_snippet": "of the retrieval is hard to distinguish and the evaluator gives an intermediate score. Since the retrieval evaluator is not confident in its judgment, both types of processed knowledge in Correct andIncorrect are combined to comple- ment ea"}, {"rank": 3, "score": 0.5683228, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 42, "text_snippet": " fabricated facts. Therefore, we need to seek new sources of knowledge for correction. Here, web search is introduced to search from the Internet as elaborated in Section 4.5. This corrective action helps overcome the embarrassing challenge"}, {"rank": 4, "score": 0.5492054, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 1, "text_snippet": "evitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Al- though retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heav"}, {"rank": 5, "score": 0.5361689, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 44, "text_snippet": "ce. Discussion Preliminary experiments of employ- ing only the Correct andIncorrect actions show that the efficacy of CRAG was easily affected by the accuracy of the retrieval evaluator. The reason might be the distinct knowledge switch for"}]}
{"case_index": 62, "query": "According to the paper 'self rag learning to retrieve generate and critique through self reflection', what exact term completes this statement: \"Moreover, our [BLANK] mechanism also evaluates other aspects of the model output quality including factuality.\"?", "gold": "self-reflection", "answer": "The missing term is **“self-reflection”** — i.e., “Moreover, our **self-reflection** mechanism also evaluates other aspects of the model output quality including factuality.”", "answer_score": 0.9849999999999999, "answer_correctness": 1.0, "answer_groundedness": 0.95, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 0.95, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'self-reflection' and is supported by context mentioning 'self-reflection mechanism' and the paper title.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 26.79, "llm_ms": 1510.826, "top_contexts": [{"rank": 1, "score": 0.6844657, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 27, "text_snippet": "n (reasoning path consistency) and without retrieval. Recent work on LLM refinement (Dhuliawala et al., 2023; Madaan et al., 2023; Paul et al., 2023) prompts a model to generate task output, natural language feedback and refined task output"}, {"rank": 2, "score": 0.6622064, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 6, "text_snippet": "y trained to leverage and follow facts from provided passages. This work introduces Self-Reflective Retrieval-augmented Generation ( SELF-RAG)to improve an LLM’s generation quality, including its factual accuracy without hurting its versati"}, {"rank": 3, "score": 0.6589894, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 2, "text_snippet": "ew framework called Self-Reflective Retrieval-Augmented Gen- eration ( SELF-RAG)that enhances an LM’s quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passage"}, {"rank": 4, "score": 0.6541462, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 13, "text_snippet": "erifiability. consistently retrieves a fixed number of documents for generation regardless of the retrieval necessity (e.g., the bottom figure example does not require factual knowledge) and never second visits the generation quality. Moreo"}, {"rank": 5, "score": 0.65188813, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 0, "text_snippet": "Preprint. SELF-RAG: LEARNING TO RETRIEVE , GENERATE ,AND CRITIQUE THROUGH SELF-REFLECTION Akari Asai†, Zeqiu Wu†, Yizhong Wang†§, Avirup Sil‡, Hannaneh Hajishirzi†§ †University of Washington§Allen Institute for AI‡IBM Research AI {akari,zeq"}]}
{"case_index": 63, "query": "According to the paper 'corrective retrieval augmented generation', what exact term completes this statement: \"Considering that retrieval is sometimes [BLANK] for some queries, conversely, responses without retrieval are even more accurate in many situations.\"?", "gold": "unnecessary", "answer": "The missing term is **“unnecessary”**.", "answer_score": 0.8799999999999999, "answer_correctness": 1.0, "answer_groundedness": 0.6, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 0.6, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'unnecessary'. Context provided does not explicitly contain the quoted sentence/blank, but does discuss retrieval being unnecessary in some cases, so partial grounding.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 11.555, "llm_ms": 1090.54, "top_contexts": [{"rank": 1, "score": 0.6277093, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 10, "text_snippet": " should not have been equally referred to and involved in RAG. On account of the above issues, this paper particularly studies the scenarios where the retriever returns inaccurate results. A method named Corrective Retrieval- Augmented Gene"}, {"rank": 2, "score": 0.6099191, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 1, "text_snippet": "evitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Al- though retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heav"}, {"rank": 3, "score": 0.5914295, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 20, "text_snippet": "model that specializes in response generation. Despite this, the methods above usually ignore a question, what if the retrieval goes wrong? Since the purpose of introducing a retrieval is to secure that generative LMs can obtain relevant an"}, {"rank": 4, "score": 0.588661, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 6, "text_snippet": " The examples show that a low-quality retriever is prone to introducing a substantial amount of irrelevant information, impeding the generators from acquiring accurate knowledge and potentially misleading them. the parametric knowledge they"}, {"rank": 5, "score": 0.5836322, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 1, "text_snippet": "e reliance on the paramet- ric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and inco"}]}
{"case_index": 64, "query": "According to the paper 'atlas few shot learning with retrieval augmented language models', what exact term completes this statement: \"The Contriever uses a dual-encoder architecture, where the query and documents are embedded [BLANK] by a transformer encoder (Huang et al., 2013; Karpukhin et al., 2020).\"?", "gold": "independently", "answer": "The missing term is **“independently”** (i.e., “the query and documents are embedded **independently** by a transformer encoder”).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'independently' and is explicitly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 14.298, "llm_ms": 1493.001, "top_contexts": [{"rank": 1, "score": 0.7115346, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 19, "text_snippet": " based on continuous dense embeddings. The Contriever uses a dual-encoder architecture, where the query and documents are embedded independently by a transformer encoder (Huang et al., 2013; Karpukhin et al., 2020). Average pooling is appli"}, {"rank": 2, "score": 0.70167065, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 20, "text_snippet": "eir corresponding embeddings. The Contriever model is pre-trained using the MoCo contrastive loss (He et al., 2020), and uses unsupervised data only. As shown in the following section, an advantage of dense retrievers is that both query and"}, {"rank": 3, "score": 0.6808407, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 18, "text_snippet": "y retrieving the top-k relevant documents from a large corpus of text with the retriever. Then, these documents are fed to the language model, along with the query, which in turns generates the output. Both the retriever and the language mo"}, {"rank": 4, "score": 0.62478495, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 23, "text_snippet": "..dm}that are relevant to x. Following prior work (Qu et al., 2021; Izacard & Grave, 2021b; Ni et al., 2021), we use a dense retriever based on the dual encoder architecture, where an encoder is used to encode both the input context xand th"}, {"rank": 5, "score": 0.6021855, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 51, "text_snippet": "answering (Voorhees et al., 1999), documents are often retrieved from Wikipedia (Chen et al., 2017). Recently, dense retrievers based on neural networks have become popular. These usually follow a dual-encoder architecture (Yih et al., 2011"}]}
{"case_index": 65, "query": "According to the paper 'corrective retrieval augmented generation', what exact term completes this statement: \"Experiments on four datasets covering short- and long-form generation tasks show that CRAG can [BLANK] improve the performance of RAG-based approaches.1 1 Introduction Large language models (LLMs) have attracted increasing attention and exhibited impressive\"?", "gold": "significantly", "answer": "The missing term is **“significantly”** — i.e., “CRAG can **significantly** improve the performance of RAG-based approaches.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Model correctly fills blank with 'significantly', which is explicitly supported by the provided context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 18.161, "llm_ms": 1499.626, "top_contexts": [{"rank": 1, "score": 0.6915022, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 1, "text_snippet": "evitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Al- though retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heav"}, {"rank": 2, "score": 0.6629457, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 84, "text_snippet": "ed to improve the robustness of generation. Essentially, a lightweight retrieval evaluator is to estimate and trigger three knowledge retrieval actions discriminately. With the further leverage of web search and optimized knowledge utilizat"}, {"rank": 3, "score": 0.66106874, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 10, "text_snippet": " should not have been equally referred to and involved in RAG. On account of the above issues, this paper particularly studies the scenarios where the retriever returns inaccurate results. A method named Corrective Retrieval- Augmented Gene"}, {"rank": 4, "score": 0.6534438, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 4, "text_snippet": " for long-form generations relative to these models.1 1 I NTRODUCTION State-of-the-art LLMs continue to struggle with factual errors (Mallen et al., 2023; Min et al., 2023) despite their increased model and data scale (Ouyang et al., 2022)."}, {"rank": 5, "score": 0.65325385, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 85, "text_snippet": "s well as general- izability across short- and long-form generation tasks. While we primarily proposed to improve the RAG framework from a corrective perspective and CRAG can be seamlessly coupled with various RAG-based approaches, fine-tun"}]}
{"case_index": 66, "query": "According to the paper 'recomp improving retrieval augmented lms with compression and selective augmentation', what exact term completes this statement: \"further confuse LMs with irrelevant information, degrading model [BLANK] (Mallen et al., 2022; Shi et al., 2023a).\"?", "gold": "performances", "answer": "The missing term is **“performances”** — the paper states: “further confuse LMs with irrelevant information, degrading model **performances** (Mallen et al., 2022; Shi et al., 2023a).”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'performances' and is directly supported by Context 2 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 9.721, "llm_ms": 1859.51, "top_contexts": [{"rank": 1, "score": 0.68297964, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 135, "text_snippet": "arXiv preprint arXiv:2305.00633 , 2023. URL https://arxiv.org/abs/2305.00633 . Fangyuan Xu, Weijia Shi, and Eunsol Choi. Recomp: Improving retrieval-augmented lms with compression and selective augmentation, 2023. URL https://arxiv.org/abs/"}, {"rank": 2, "score": 0.6481149, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 6, "text_snippet": "further confuse LMs with irrelevant information, degrading model performances (Mallen et al., 2022; Shi et al., 2023a). To overcome such limitations, we propose RECOMP (Retrieve, Com press, Prepend), an inter- mediate step for RALMs which c"}, {"rank": 3, "score": 0.63631225, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 0, "text_snippet": "RECOMP: I MPROVING RETRIEVAL -AUGMENTED LM S WITH COMPRESSION AND SELECTIVE AUGMENTATION Fangyuan Xu1, Weijia Shi2, Eunsol Choi1 Department of Computer Science 1The University of Texas at Austin 2University of Washington {fangyuan,eunsol }@"}, {"rank": 4, "score": 0.6082971, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 4, "text_snippet": "trieval-augmented language models (RALMs) (Khandelwal et al., 2019; Izacard et al., 2022; Lewis et al., 2020; Borgeaud et al., 2022) have shown impressive performance on knowledge-intensive tasks (Kwiatkowski et al., 2019; Petroni et al., 2"}, {"rank": 5, "score": 0.60350275, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 6, "text_snippet": " The examples show that a low-quality retriever is prone to introducing a substantial amount of irrelevant information, impeding the generators from acquiring accurate knowledge and potentially misleading them. the parametric knowledge they"}]}
{"case_index": 67, "query": "According to the paper 'corrective retrieval augmented generation', what exact term completes this statement: \"In summary, our [BLANK] in this paper are three-fold: 1) This paper studies the scenarios where the retriever returns inaccurate results and, to the best of our knowledge, makes the first attempt to design corrective strategies for RAG to improve its robustness.\"?", "gold": "contributions", "answer": "The missing term is **“contributions”**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context 2 explicitly states: 'In summary, our contributions in this paper are three-fold'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 14.748, "llm_ms": 1200.711, "top_contexts": [{"rank": 1, "score": 0.735647, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 10, "text_snippet": " should not have been equally referred to and involved in RAG. On account of the above issues, this paper particularly studies the scenarios where the retriever returns inaccurate results. A method named Corrective Retrieval- Augmented Gene"}, {"rank": 2, "score": 0.70860195, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 15, "text_snippet": "hers to reproduce our results, we will publish all source code later. In summary, our contributions in this paper are three-fold: 1) This paper studies the scenarios where the retriever returns inaccurate results and, to the best of our kno"}, {"rank": 3, "score": 0.6818205, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 24, "text_snippet": " knowledge, this paper makes the first attempt to explore and design corrective strategies for RAG to improve its robustness of generation. 3 Task Formulation Following previous work (Lewis et al., 2020; Asai et al., 2024), given input Xand"}, {"rank": 4, "score": 0.6764389, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 23, "text_snippet": "t to retrieve in long-form generation. Compared with recent studies (Schick et al., 2023; Luo et al., 2023; Asai et al., 2024) that are the most relevant to our work, a main difference should be highlighted. These approaches target on explo"}, {"rank": 5, "score": 0.6633111, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 83, "text_snippet": " in Table 6. The findings indicate that the self-correction mecha- nism incurs only modest computational overheadwhile significantly enhancing performance, thereby validating its lightweight nature. 6 Conclusion & Limitation This paper stud"}]}
{"case_index": 68, "query": "According to the paper 'corrective retrieval augmented generation', what exact term completes this statement: \"Advanced RAG Many advanced [BLANK] have been developed from the original RAG in recent years (Zhang et al., 2024; Kim et al., 2024; Wa\"?", "gold": "approaches", "answer": "The missing term is **“approaches”** (i.e., “Many advanced **approaches** have been developed from the original RAG…”).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer correctly fills the blank with 'approaches', which is explicitly present in the provided context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.027, "llm_ms": 1554.32, "top_contexts": [{"rank": 1, "score": 0.77127355, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 20, "text_snippet": "model that specializes in response generation. Despite this, the methods above usually ignore a question, what if the retrieval goes wrong? Since the purpose of introducing a retrieval is to secure that generative LMs can obtain relevant an"}, {"rank": 2, "score": 0.7003275, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 10, "text_snippet": " should not have been equally referred to and involved in RAG. On account of the above issues, this paper particularly studies the scenarios where the retriever returns inaccurate results. A method named Corrective Retrieval- Augmented Gene"}, {"rank": 3, "score": 0.6935517, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 21, "text_snippet": "al RAG in recent years (Zhang et al., 2024; Kim et al., 2024; Wang et al., 2024; Liu et al., 2024). Considering that retrieval is sometimes unnecessary for some queries, conversely, responses without retrieval are even more accurate in many"}, {"rank": 4, "score": 0.67570794, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 21, "text_snippet": " al., 2023a; Gao et al., 2023). We introduce a method to train an arbitrary LM to learn to use retrieval on-demand for diverse instruction-following queries and introduce controlled generation guided by reflections tokens to further improve"}, {"rank": 5, "score": 0.67260575, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 24, "text_snippet": " knowledge, this paper makes the first attempt to explore and design corrective strategies for RAG to improve its robustness of generation. 3 Task Formulation Following previous work (Lewis et al., 2020; Asai et al., 2024), given input Xand"}]}
{"case_index": 69, "query": "According to the paper 'self rag learning to retrieve generate and critique through self reflection', what exact term completes this statement: \"This process differs from [BLANK] RAG (Figure 1 left), which 1Our code and trained models are available at https://selfrag.github.io/ .\"?", "gold": "conventional", "answer": "The missing term is **“conventional”** — i.e., “This process differs from **conventional** RAG (Figure 1 left) …”.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'conventional' and is directly supported by Context 2.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 9.387, "llm_ms": 1472.718, "top_contexts": [{"rank": 1, "score": 0.7393458, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 27, "text_snippet": "n (reasoning path consistency) and without retrieval. Recent work on LLM refinement (Dhuliawala et al., 2023; Madaan et al., 2023; Paul et al., 2023) prompts a model to generate task output, natural language feedback and refined task output"}, {"rank": 2, "score": 0.7261121, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 8, "text_snippet": "on demand (Step 1). Subsequently, SELF-RAGconcurrently processes multiple retrieved passages, evaluating their relevance and then generating corresponding task outputs (Step 2). It then generates critique tokens to criticize its own output "}, {"rank": 3, "score": 0.70566565, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 28, "text_snippet": "F-RAGis a framework that enhances the quality and factuality of an LLM through retrieval and self-reflection, without sacrificing LLM’s original creativity and versatility. Our end-to-end training lets an LM Mgenerate text informed by retri"}, {"rank": 4, "score": 0.679744, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 13, "text_snippet": "erifiability. consistently retrieves a fixed number of documents for generation regardless of the retrieval necessity (e.g., the bottom figure example does not require factual knowledge) and never second visits the generation quality. Moreo"}, {"rank": 5, "score": 0.6795806, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 96, "text_snippet": "ains an LM to learn to retrieve, generate, and critique text passages and its own generation by predicting the next tokens from its original vocabulary as well as newly added special tokens, called reflection tokens. SELF-RAGfurther enables"}]}
{"case_index": 70, "query": "According to the paper 'atlas few shot learning with retrieval augmented language models', what exact term completes this statement: \"The retrieved documents are processed, along with the current context, by a [BLANK] model using the Fusion-in-Decoder architecture (Izacard & Grave, 2020) that generates the corresponding output.\"?", "gold": "sequence-to-sequence", "answer": "The blank is **“sequence-to-sequence”** — i.e., “by a **sequence-to-sequence model** using the Fusion-in-Decoder architecture (Izacard & Grave, 2020) that generates the corresponding output.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'sequence-to-sequence' and is explicitly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 11.591, "llm_ms": 1574.997, "top_contexts": [{"rank": 1, "score": 0.70158505, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 9, "text_snippet": " strong few-shot learning, despite having lower parameter counts than other powerful recent few-shot learners. Atlasretrieves relevant documents based on the current context by using a general-purpose dense retriever using a dual-encoder ar"}, {"rank": 2, "score": 0.6472851, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 17, "text_snippet": "inal predic- tion. This style of retrieval can be added to both encoder- decoder (Yu, 2022; Izacard et al., 2022b) and decoder-only models (Khandelwal et al., 2020; Borgeaud et al., 2022; Shi et al., 2022; Rubin et al., 2022). For example, "}, {"rank": 3, "score": 0.6382009, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 20, "text_snippet": "eir corresponding embeddings. The Contriever model is pre-trained using the MoCo contrastive loss (He et al., 2020), and uses unsupervised data only. As shown in the following section, an advantage of dense retrievers is that both query and"}, {"rank": 4, "score": 0.6308049, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 21, "text_snippet": "ce architecture (Raﬀel et al., 2019). We rely on the Fusion-in-Decoder modiﬁcation of sequence-to-sequence models, and process each document independently in the encoder (Izacard & Grave, 2020). We then concatenate the outputs of the encode"}, {"rank": 5, "score": 0.63043576, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 18, "text_snippet": "y retrieving the top-k relevant documents from a large corpus of text with the retriever. Then, these documents are fed to the language model, along with the query, which in turns generates the output. Both the retriever and the language mo"}]}
{"case_index": 71, "query": "According to the paper 'leveraging passage retrieval with generative models for open domain question answering', what exact term completes this statement: \"(2019a) introduced a method based on hard expectation- [BLANK] to tackle noisy supervision from this setting.\"?", "gold": "maximization", "answer": "The missing term is **“maximization”**, completing the phrase **“hard expectation-maximization.”**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term and is explicitly supported by Context 1 ('hard expectation-maximization').", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 17.331, "llm_ms": 1366.582, "top_contexts": [{"rank": 1, "score": 0.64308476, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 11, "text_snippet": " Different methods were pro- posed to tackle the setting where no gold spans are given to the system, but only the correct answer. Clark and Gardner (2018) proposed to use a global normalization over all the span corresponding to the answer"}, {"rank": 2, "score": 0.61148465, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 14, "text_snippet": " be trained using weak supervision in the form of question-answer pairs (Karpukhin et al., 2020), or pretrained using a cloze task and ﬁnetuned end-to- end (Guu et al., 2020; Lee et al., 2019). Generative question answering was mostly consi"}, {"rank": 3, "score": 0.6092501, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}, {"rank": 4, "score": 0.5892986, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 84, "text_snippet": " and pre-training paradigm for generative LLMs, leveraging cross-attention and end-to-end pre-training with 9  retrieval from a trillion-token data store, achieving strong performance. Subsequent work by Shi et al. (2024) and Lin et al. (20"}, {"rank": 5, "score": 0.5851408, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 175, "text_snippet": "ext generation with relevance sampling. ArXiv, abs/2207.03030, 2022. 7, 15 Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry Heck. Learning deep structured semantic models for web search using clickthrough data. In Pro"}]}
{"case_index": 72, "query": "According to the paper 'atlas few shot learning with retrieval augmented language models', what exact term completes this statement: \"d as follows: the system gets a text query as input, and [BLANK] a text output .\"?", "gold": "generates", "answer": "The missing term is **“generates”** — the sentence reads: “the system gets a text query as input, and **generates** a text output.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'generates' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.399, "llm_ms": 1480.803, "top_contexts": [{"rank": 1, "score": 0.62934774, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 16, "text_snippet": "d as follows: the system gets a text query as input, and generates a text output . For example, in the case of question answering, the query corresponds to the question and the model needs to generate the answer. In the case of classiﬁcatio"}, {"rank": 2, "score": 0.5399485, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 17, "text_snippet": "chmark in Figure 2. As many natural language processing tasks require knowledge , our goal is to enhance standard text-to-text models with retrieval, which, as we hypothesise in the introduction, may be crucial to endow models with few-shot"}, {"rank": 3, "score": 0.535709, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 2, "text_snippet": "ings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including"}, {"rank": 4, "score": 0.53405064, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 9, "text_snippet": " strong few-shot learning, despite having lower parameter counts than other powerful recent few-shot learners. Atlasretrieves relevant documents based on the current context by using a general-purpose dense retriever using a dual-encoder ar"}, {"rank": 5, "score": 0.53156865, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 18, "text_snippet": "y retrieving the top-k relevant documents from a large corpus of text with the retriever. Then, these documents are fed to the language model, along with the query, which in turns generates the output. Both the retriever and the language mo"}]}
{"case_index": 73, "query": "According to the paper 'recomp improving retrieval augmented lms with compression and selective augmentation', what exact term completes this statement: \"er model encθwhich embeds sentence siand the input sequence xinto fixed- dimensional embeddings [BLANK].\"?", "gold": "respectively", "answer": "The missing term is:\n\n**“respectively”**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'respectively' and is directly supported by Context 1/2 showing the sentence ending with 'embeddings respectively.'", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 18.532, "llm_ms": 1343.487, "top_contexts": [{"rank": 1, "score": 0.62247294, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 20, "text_snippet": "er model encθwhich embeds sentence siand the input sequence xinto fixed- dimensional embeddings respectively. Their inner product represents how helpful it would be for the LMMto prepend sito the input xto generate y. The final summary sfro"}, {"rank": 2, "score": 0.6096412, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 26, "text_snippet": " 7: L ← L ∪ si 8: if|L|>0then 9: Ni←argTop5 sj∈L(⟨encθ(sj),encθ(xi)⟩) 10: T ← T ∪ { (xi,pi,Ni)} 11:encθ=Finetune (encθ,T) Figure 2: Learning an extractive compressor for lan- guage modeling task.Model We train a dual-encoder model encθwhich"}, {"rank": 3, "score": 0.5609545, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 229, "text_snippet": "nsionality reduction on the word co-occurrence matrix, probabilistic models, explainable knowledge base method, and explicit representation in terms of the context in which words appear. Word and phrase embeddings, when used as the underlyi"}, {"rank": 4, "score": 0.5547014, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 19, "text_snippet": "ining compressors for conciseness and effectiveness. We summarize the key ideas for our two compressors, extractive compressors and abstractive compressor here, and discuss their training schemes formally in Section 3. 2Improving retriever "}, {"rank": 5, "score": 0.5388202, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 228, "text_snippet": "e Word embedding Word embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers. Con"}]}
{"case_index": 74, "query": "According to the paper 'leveraging passage retrieval with generative models for open domain question answering', what exact term completes this statement: \"Clark and Gardner (2018) proposed to use a global [BLANK] over all the span corresponding to the answer, which was later applied to BERT based models (Wang et al., 2019).\"?", "gold": "normalization", "answer": "The missing term is **“normalization”** — i.e., “Clark and Gardner (2018) proposed to use a global **normalization** over all the span corresponding to the answer …”.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'normalization' and is explicitly supported by Context 2.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.364, "llm_ms": 1471.753, "top_contexts": [{"rank": 1, "score": 0.66916883, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 6, "text_snippet": "tech- niques have been considered, either using sparse representations based on TF/IDF or using dense embeddings (Guu et al., 2020; Karpukhin et al., 2020). The models which extract the answers are often based on contextualized word represe"}, {"rank": 2, "score": 0.64689416, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 11, "text_snippet": " Different methods were pro- posed to tackle the setting where no gold spans are given to the system, but only the correct answer. Clark and Gardner (2018) proposed to use a global normalization over all the span corresponding to the answer"}, {"rank": 3, "score": 0.6336403, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}, {"rank": 4, "score": 0.611222, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 15, "text_snippet": " that answers do not correspond to spans in support documents, thus requiring ab- stractive models. Raffel et al. (2019) showed that generative models are competitive for reading com- prehension tasks such as SQuAD (Rajpurkar et al., 2016),"}, {"rank": 5, "score": 0.6085144, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 21, "text_snippet": "Raffel et al., 2019; Lewis et al., 2019). The model takes as input the question, as well as the support passages, and generates the answer. More precisely, each retrieved passage and its title are concatenated with the question, and process"}]}
{"case_index": 75, "query": "According to the paper 'atlas few shot learning with retrieval augmented language models', what exact term completes this statement: \"Both the retriever and the language model are based on [BLANK] transformer networks, which we describe in more detail below.\"?", "gold": "pre-trained", "answer": "The missing term is **“pre-trained”**, i.e., the full sentence is:\n\n“Both the retriever and the language model are based on **pre-trained** transformer networks, which we describe in more detail below.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'pre-trained' and is directly supported by Context 1 sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 11.111, "llm_ms": 1603.477, "top_contexts": [{"rank": 1, "score": 0.65750164, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 18, "text_snippet": "y retrieving the top-k relevant documents from a large corpus of text with the retriever. Then, these documents are fed to the language model, along with the query, which in turns generates the output. Both the retriever and the language mo"}, {"rank": 2, "score": 0.63409114, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 20, "text_snippet": "eir corresponding embeddings. The Contriever model is pre-trained using the MoCo contrastive loss (He et al., 2020), and uses unsupervised data only. As shown in the following section, an advantage of dense retrievers is that both query and"}, {"rank": 3, "score": 0.6188772, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 12, "text_snippet": "ntributions: •A thorough study on how to design and train retrieval-augmented language models, with a focus on downstream few-shot learning and sample eﬃciency. •The ﬁndings of this study lead to a retrieval-augmented language model, called"}, {"rank": 4, "score": 0.61646545, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 9, "text_snippet": " strong few-shot learning, despite having lower parameter counts than other powerful recent few-shot learners. Atlasretrieves relevant documents based on the current context by using a general-purpose dense retriever using a dual-encoder ar"}, {"rank": 5, "score": 0.60640216, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 17, "text_snippet": "chmark in Figure 2. As many natural language processing tasks require knowledge , our goal is to enhance standard text-to-text models with retrieval, which, as we hypothesise in the introduction, may be crucial to endow models with few-shot"}]}
{"case_index": 76, "query": "According to the paper 'raptor recursive abstractive processing for tree organized retrieval', what exact term completes this statement: \"GMMs assume that data points are generated from a mixture of several Gaussian [BLANK].\"?", "gold": "distributions", "answer": "The missing term is **“distributions”** — the paper states: “GMMs assume that data points are generated from a mixture of several Gaussian **distributions**.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'distributions' and is directly supported by Context 3 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.3333333333333333, "hit_rank": 3, "matched_by": "doc_id+chunk_id", "retrieval_ms": 20.099, "llm_ms": 1230.63, "top_contexts": [{"rank": 1, "score": 0.5576605, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 28, "text_snippet": " Published as a conference paper at ICLR 2024 Given a set of Ntext segments, each represented as a d-dimensional dense vector embedding, the likelihood of a text vector, x, given its membership in the kthGaussian distribution, is denoted by"}, {"rank": 2, "score": 0.5157384, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 33, "text_snippet": "e Gaussian assumption in GMMs may not perfectly align with the nature of text data, which often exhibits a sparse and skewed distribution, our empirical observations suggest that it offers an effective model for our purpose. We run an ablat"}, {"rank": 3, "score": 0.5084058, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 27, "text_snippet": "uiring a fixed number of clusters. This flexibility is essen- tial because individual text segments often contain information relevant to various topics, thereby warranting their inclusion in multiple summaries. Our clustering algorithm is "}, {"rank": 4, "score": 0.50075233, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 32, "text_snippet": "oints), kis the number of model parameters, and ˆLis the maximized value of the likelihood function of the model. In the context of GMM, the number of parameters k is a function of the dimensionality of the input vectors and the number of c"}, {"rank": 5, "score": 0.4608205, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 29, "text_snippet": "vector embeddings presents a challenge for traditional GMMs, as dis- tance metrics may behave poorly when used to measure similarity in high-dimensional spaces (Ag- garwal et al., 2001). To mitigate this, we employ Uniform Manifold Approxim"}]}
{"case_index": 77, "query": "According to the paper 'leveraging passage retrieval with generative models for open domain question answering', what exact term completes this statement: \"The models which extract the answers are often based on contextualized word [BLANK] such as ELMo or BERT (Peters et al., 2018; De- vlin et al., 2019), and predict a span as answer.\"?", "gold": "representations", "answer": "The missing term is **“representations”**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term and is directly supported by Context 1 ('contextualized word representations').", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 13.622, "llm_ms": 1717.267, "top_contexts": [{"rank": 1, "score": 0.6764549, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 6, "text_snippet": "tech- niques have been considered, either using sparse representations based on TF/IDF or using dense embeddings (Guu et al., 2020; Karpukhin et al., 2020). The models which extract the answers are often based on contextualized word represe"}, {"rank": 2, "score": 0.5842004, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 20, "text_snippet": "es with SpaCy.2In DPR, passages and questions are represented as dense vector representations, computed using two BERT networks. The ranking function is the dot product between the query and passage represen- tations. Retrieval is performed"}, {"rank": 3, "score": 0.5755243, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 15, "text_snippet": " that answers do not correspond to spans in support documents, thus requiring ab- stractive models. Raffel et al. (2019) showed that generative models are competitive for reading com- prehension tasks such as SQuAD (Rajpurkar et al., 2016),"}, {"rank": 4, "score": 0.56174576, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 21, "text_snippet": "Raffel et al., 2019; Lewis et al., 2019). The model takes as input the question, as well as the support passages, and generates the answer. More precisely, each retrieved passage and its title are concatenated with the question, and process"}, {"rank": 5, "score": 0.5535935, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}]}
{"case_index": 78, "query": "According to the paper 'leveraging passage retrieval with generative models for open domain question answering', what exact term completes this statement: \"xact Match NaturalQuestions 5 10 25 50 100 Number of passages54565860626466 TriviaQA 5 10 25 50 100 Number of passages343638404244464850 SQuADFigure 3: Performance of [BLANK] (base) on valid sets as a function of the number of retrieved passages.\"?", "gold": "fusion-in-decoder", "answer": "The missing term is **“Fusion-in-Decoder”**, i.e., the full caption is:\n\n“Figure 3: Performance of **Fusion-in-Decoder** (base) on valid sets as a function of the number of retrieved passages.”", "answer_score": 0.3, "answer_correctness": 0.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context explicitly says 'Fusion-in-Decoder (base)'. However the expected exact term is 'fusion-in-decoder' (lowercase/hyphenated). Answer does not match expected exact term.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.554, "llm_ms": 1689.16, "top_contexts": [{"rank": 1, "score": 0.7742729, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 24, "text_snippet": " domain QA. Datasets. We consider the following datasets, and use the same setting as Lee et al. (2019): •NaturalQuestions (Kwiatkowski et al., 2019) contains questions corresponding to Google search queries. The open-domain version of this"}, {"rank": 2, "score": 0.74610364, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 25, "text_snippet": "xact Match NaturalQuestions 5 10 25 50 100 Number of passages54565860626466 TriviaQA 5 10 25 50 100 Number of passages343638404244464850 SQuADFigure 3: Performance of Fusion-in-Decoder (base) on valid sets as a function of the number of ret"}, {"rank": 3, "score": 0.7413429, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 33, "text_snippet": "es are competitive for knowl- edge retrieval tasks. Scaling with number of passages. In Figure 3, we report the performance with respect to the  NaturalQuestions TriviaQA Training Passages w/o ﬁnetuning w/ ﬁnetuning w/o ﬁnetuning w/ ﬁnetuni"}, {"rank": 4, "score": 0.71733314, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 34, "text_snippet": "s are reported on dev sets. number of retrieved passages. In particular, we observe that increasing the number of passages from 10 to 100 leads to 6% improvement on Trivi- aQA and 3.5% improvement on NaturalQuestions. On the other hand, the"}, {"rank": 5, "score": 0.6898508, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 17, "text_snippet": "pport passages before pro- cessing them with a sequence to sequence model.  Model NQ TriviaQA SQuAD Open EM EM EM EM F1 DrQA (Chen et al., 2017) - - - 29.8 - Multi-Passage BERT (Wang et al., 2019) - - - 53.0 60.9 Path Retriever (Asai et al."}]}
{"case_index": 79, "query": "According to the paper 'from local to global a graph rag approach to query focused summarization', what exact term completes this statement: \"GraphRAG contrasts with these approaches by generating a graph index from the source data, then applying graph-based community detection to create a thematic [BLANK] of the data.\"?", "gold": "partitioning", "answer": "The missing term is **“hierarchy”** — i.e., GraphRAG uses community detection “to create a thematic **hierarchy** of the data.”", "answer_score": 0.03, "answer_correctness": 0.0, "answer_groundedness": 0.1, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 0.1, "abstained": false, "should_abstain": false, "notes": "Expected term is 'partitioning' (context discusses ability to partition graphs into communities). Model answered 'hierarchy', which is not the exact term completing the quoted statement.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 12.323, "llm_ms": 2656.739, "top_contexts": [{"rank": 1, "score": 0.74626774, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 44, "text_snippet": "ummaries at one level looking for general themes of interest, then read linked reports at a lower level that provide additional details for each subtopic. Here, however, we focus on their utility as part of a graph-based index used for answ"}, {"rank": 2, "score": 0.7290051, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 10, "text_snippet": "cted communities, with summaries at higher levels of the hierarchy recursively incorporating lower-level summaries. Together, these community summaries provide global descriptions and insights over the corpus. Finally, GraphRAG answers quer"}, {"rank": 3, "score": 0.7070364, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 9, "text_snippet": "tirety of a large text corpus. GraphRAG first uses an LLM to construct a knowledge graph, where nodes correspond to key entities in the corpus and edges represent relationships between those entities. Next, it partitions the graph into a hi"}, {"rank": 4, "score": 0.6922736, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 21, "text_snippet": "edges encoding lexical and semantical similarity or structural relation- ships. GraphRAG contrasts with these approaches by focusing on a previously unexplored quality of graphs in this context: their inherent modularity (Newman, 2006) and "}, {"rank": 5, "score": 0.6919966, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 29, "text_snippet": "lements (nodes, edges, covariates) that the LLM can summarize in parallel at both indexing time and query time. The “global answer” to a given query is produced using a final round of query-focused summarization over all community summaries"}]}
{"case_index": 80, "query": "According to the paper 'corrective retrieval augmented generation', what exact term completes this statement: \"But a [BLANK] portion of the text within these retrieved documents is often non- essential for generation, which should not have been equally referred to and involved in RAG.\"?", "gold": "considerable", "answer": "The exact term is **“considerable”** (i.e., “But a **considerable** portion of the text within these retrieved documents is often non-essential for generation…”).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'considerable' and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.066, "llm_ms": 1252.168, "top_contexts": [{"rank": 1, "score": 0.6448036, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 9, "text_snippet": "nations (Zhang et al., 2023b). However, most conventional RAG ap- proaches indiscriminately incorporate the retrieved documents, regardless of whether these documents are relevant or not (Rony et al., 2022). Furthermore, current methods mos"}, {"rank": 2, "score": 0.637172, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 10, "text_snippet": " should not have been equally referred to and involved in RAG. On account of the above issues, this paper particularly studies the scenarios where the retriever returns inaccurate results. A method named Corrective Retrieval- Augmented Gene"}, {"rank": 3, "score": 0.6002366, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 6, "text_snippet": " The examples show that a low-quality retriever is prone to introducing a substantial amount of irrelevant information, impeding the generators from acquiring accurate knowledge and potentially misleading them. the parametric knowledge they"}, {"rank": 4, "score": 0.5947819, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 1, "text_snippet": "evitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Al- though retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heav"}, {"rank": 5, "score": 0.5859549, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 11, "text_snippet": "with many retrieved passages being uninformative and reused across multiple inferences. Allocating memory/computation for all the tokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The ret"}]}
{"case_index": 81, "query": "According to the paper 'leveraging passage retrieval with generative models for open domain question answering', what exact term completes this statement: \"rticular, we show that the [BLANK] of our method signiﬁcantly improves when the number of retrieved passages increases.\"?", "gold": "performance", "answer": "The missing term is **“performance”**:\n\n“…we show that the **performance** of our method significantly improves when the number of retrieved passages increases.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'performance' and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 12.944, "llm_ms": 1652.965, "top_contexts": [{"rank": 1, "score": 0.7409962, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 9, "text_snippet": "rticular, we show that the performance of our method signiﬁcantly improves when the number of retrieved passages increases. We believe that this is evidence that generative mod- els are good at combining evidence from multiple passages, com"}, {"rank": 2, "score": 0.71524924, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 31, "text_snippet": "tion answering. We observe that while conceptu- ally simple, this method outperforms existing work on the NaturalQuestion and TriviaQA benchmarks. In particular, generative models seem to perform well when evidence from multiple passages ne"}, {"rank": 3, "score": 0.69876456, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}, {"rank": 4, "score": 0.6901194, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 16, "text_snippet": "0) and Lewis et al. (2020) in- troduced retrieval augmented generative models for open domain question answering. Our approach differs from these works by how the generative model processes the retrieved passages. This al- lows to scale to "}, {"rank": 5, "score": 0.68910396, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 0, "text_snippet": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering Gautier Izacard1,2,3Edouard Grave1 1Facebook AI Research, Paris 2ENS, PSL University, Paris 3Inria, Paris gizacard|egrave@fb.com Abstract Generative mode"}]}
{"case_index": 82, "query": "According to the paper 'raptor recursive abstractive processing for tree organized retrieval', what exact term completes this statement: \"(2023) uses [BLANK] and snippets of passages, which improves correctness on most datasets but can sometimes be a lossy means of compression.\"?", "gold": "summarizations", "answer": "The missing term is **“summaries”** — i.e., “(2023) uses **summaries** and snippets of passages, which improves correctness on most datasets but can sometimes be a lossy means of compression.”", "answer_score": 0.48, "answer_correctness": 0.6, "answer_groundedness": 0.2, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.6, "groundedness": 0.2, "abstained": false, "should_abstain": false, "notes": "Expected term is 'summarizations'; model answered 'summaries' (close but not exact). Context provided does not include the quoted (2023) sentence, so support is weak.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 11.577, "llm_ms": 1621.366, "top_contexts": [{"rank": 1, "score": 0.6658407, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 0, "text_snippet": "Published as a conference paper at ICLR 2024 RAPTOR: R ECURSIVE ABSTRACTIVE PROCESSING FOR TREE-ORGANIZED RETRIEVAL Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D. Manning Stanford University psarthi@cs."}, {"rank": 2, "score": 0.66359586, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 8, "text_snippet": " indexing and retrieval system that uses a tree structure to capture both high-level and low-level details about a text. As shown in Figure 1, our system, RAPTOR, clusters chunks of text, generates text summaries of those clusters, and then"}, {"rank": 3, "score": 0.6600715, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 1, "text_snippet": "rom a retrieval corpus, limiting holistic under- standing of the overall document context. We introduce the novel approach of recursively embedding, clustering, and summarizing chunks of text, constructing a tree with differing levels of su"}, {"rank": 4, "score": 0.636631, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 22, "text_snippet": " RAPTOR addresses the issue of semantic depth and connection in reading by building a recursive tree structure that balances broader thematic comprehension with granular details and which allows nodes to be grouped based on semantic sim- il"}, {"rank": 5, "score": 0.6309221, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 61, "text_snippet": "extractive and abstractive, shows promising performances. On NQ and TQA, the abstractive approach is more effective. On NQ, it achieves a compression ratio of 5% tokens while losing 2 EM points compared to prepending full documents. On TQA,"}]}
{"case_index": 83, "query": "According to the paper 'retrieval augmented generation for knowledge intensive nlp tasks', what exact term completes this statement: \"For [BLANK] generation, we experiment with MS-MARCO [ 1] and Jeopardy question generation, and we ﬁnd that our models generate responses that are more factual, speciﬁc, and diverse than a BART baseline.\"?", "gold": "knowledge-intensive", "answer": "The missing term is **“knowledge-intensive”**, i.e.:\n\n“For **knowledge-intensive** generation, we experiment with MS-MARCO [1] and Jeopardy question generation, and we ﬁnd that our models generate responses that are more factual, speciﬁc, and diverse than a BART baseline.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer correctly fills the blank with 'knowledge-intensive' and is directly supported by Context 1/3.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 19.443, "llm_ms": 2032.778, "top_contexts": [{"rank": 1, "score": 0.7413151, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 14, "text_snippet": "active approaches. For knowledge-intensive generation, we experiment with MS-MARCO [ 1] and Jeopardy question generation, and we ﬁnd that our models generate responses that are more factual, speciﬁc, and diverse than a BART baseline. For FE"}, {"rank": 2, "score": 0.65639, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 15, "text_snippet": " that answers do not correspond to spans in support documents, thus requiring ab- stractive models. Raffel et al. (2019) showed that generative models are competitive for reading com- prehension tasks such as SQuAD (Rajpurkar et al., 2016),"}, {"rank": 3, "score": 0.65389824, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 13, "text_snippet": "sive tasks —tasks that humans could not reasonably be expected to perform without access to an external knowledge source. Our RAG models achieve state-of-the-art results on open Natural Questions [ 29], WebQuestions [ 3] and CuratedTrec [ 2"}, {"rank": 4, "score": 0.64781225, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 93, "text_snippet": "Transactions of the Association for Computa- tional Linguistics , 7:452–466, 2019. doi: 10.1162/tacl_a_ 00276. URL https://aclanthology.org/Q19-1026 . Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V ., Goyal, N., Küttler, H., Le"}, {"rank": 5, "score": 0.6438975, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 53, "text_snippet": "rieved documents as the answer (Chen et al., 2017; Clark & Gardner, 2018; Wang et al., 2019; Karpukhin et al., 2020), a method inspired by reading comprehension (Richardson, 2013; Rajpurkar et al., 2016). Recently, generating the answer as "}]}
{"case_index": 84, "query": "According to the paper 'replug retrieval augmented black box language models', what exact term completes this statement: \"n improve GPT- 3 performance on open-domain question answering, we approach the problem in a more general setting, including language modeling and [BLANK] tasks.\"?", "gold": "understanding", "answer": "The missing term is **“understanding”** (i.e., “including language modeling and **understanding** tasks.”).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'understanding' and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 30.361, "llm_ms": 1641.472, "top_contexts": [{"rank": 1, "score": 0.7315808, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 20, "text_snippet": "n improve GPT- 3 performance on open-domain question answering, we approach the problem in a more general setting, including language modeling and understanding tasks. We also pro- pose an ensemble method to incorporate more documents  REPL"}, {"rank": 2, "score": 0.66986775, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 12, "text_snippet": "gnals, resulting in improved retrieval quality. •Evaluations on language modeling (§6), open-domain QA and MMLU demonstrate that REPLUG can im- prove the performance of various language models such as GPT, OPT and BLOOM, including very larg"}, {"rank": 3, "score": 0.66841817, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 69, "text_snippet": "ance across a variety of NLP tasks when considered in isolation. Such tasks include open-domain question answering [ 5,29], fact checking [ 56], fact completion [ 48], long-form question answering [ 12], Wikipedia article generation [ 36], "}, {"rank": 4, "score": 0.6637083, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 9, "text_snippet": "22) that adapts language models to the retriever. We use a training objective which prefers retrieving documents that improve language model perplexity, while treating the LM as a frozen, black-box scoring function. Our experiments show tha"}, {"rank": 5, "score": 0.65683925, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 16, "text_snippet": "ous NLP tasks, including language mod- eling (Min et al., 2022; Borgeaud et al., 2022; Khandelwal et al., 2020) and open-domain question answering (Lewis et al., 2020; Izacard et al., 2022b; Hu et al., 2022). Specifi- cally, using the input"}]}
{"case_index": 85, "query": "According to the paper 'refrag rethinking rag based decoding', what exact term completes this statement: \"Encoder Encoder Encoder Context Text Decoder-only Foundation Model Sequence [BLANK] Light-weight Encoder Who is the President of USA?\"?", "gold": "precomputable", "answer": "The missing term is **“Precomputable”**, completing the statement as:\n\n“Encoder Encoder Encoder Context Text Decoder-only Foundation Model Sequence **Precomputable** Light-weight Encoder Who is the President of USA?”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'precomputable' and is directly supported by multiple context snippets showing 'Precomputable' in the figure text.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.988, "llm_ms": 1771.156, "top_contexts": [{"rank": 1, "score": 0.7168401, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 20, "text_snippet": " the 2  Donald Trump is  the President of  the United States . He assumed office  on January 20,  2025, making him the 47th  President of the  United States. Encoder Encoder Encoder  Context Text Decoder-only Foundation Model  Sequence  Pre"}, {"rank": 2, "score": 0.59624255, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 146, "text_snippet": "works (Bello et al., 2017) to constrain the action space. 5Unless specified, we use the pre-trained checkpoint. The reason of choosing this model is that existing baselines (Yen et al., 2024; Shi et al., 2024) adapts LLaMA-2-7B. If we use o"}, {"rank": 3, "score": 0.5803929, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 19, "text_snippet": "1 }. The encoder model then processes all the chunks to obtain a chunk embedding for each chunkc i=Menc(Ci). This chunk embedding is then projected with a projection layer ϕto match the size of the token embedding of the decoder model, ecnk"}, {"rank": 4, "score": 0.5577899, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 21, "text_snippet": "uery  Encoder Figure 1The main design ofREFRAG. The input context is chunked and processed by the light-weight encoder to produce chunk embeddings, which are precomputable for efficient reuse. A light-weight RL policy decide few chunks to e"}, {"rank": 5, "score": 0.545039, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 147, "text_snippet": "ecoder Tokenizer &  Embedding  Decoder Input Text Token Embedding  Chunk  Embedding  RL-trained chunk expansion policy  Reward = - Log(Perplexity)  Donald Trump  Answer Figure 5A demonstration of selective token compression. For all chunks,"}]}
{"case_index": 86, "query": "According to the paper 'from local to global a graph rag approach to query focused summarization', what exact term completes this statement: \"lements (nodes, edges, [BLANK]) that the LLM can summarize in parallel at both indexing time and query time.\"?", "gold": "covariates", "answer": "The missing term is **“covariates”**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Model answer matches expected term 'covariates' and is directly supported by context quoting '(nodes, edges, covariates)'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 11.45, "llm_ms": 1083.902, "top_contexts": [{"rank": 1, "score": 0.72493374, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 29, "text_snippet": "lements (nodes, edges, covariates) that the LLM can summarize in parallel at both indexing time and query time. The “global answer” to a given query is produced using a final round of query-focused summarization over all community summaries"}, {"rank": 2, "score": 0.68251383, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 27, "text_snippet": "ns (Wang et al., 2023a; Zheng et al., 2024). Some prior work proposes criteria for having LLMs quantify the quality of 3  Source Documents Text Chunkstext extraction and chunking Entities & Relationshipsdomain-tailored summarization Knowled"}, {"rank": 3, "score": 0.67386615, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 28, "text_snippet": "n Indexing Time Query Time Pipeline Stage Figure 1: Graph RAG pipeline using an LLM-derived graph index of source document text. This graph index spans nodes (e.g., entities), edges (e.g., relationships), and covariates (e.g., claims) that "}, {"rank": 4, "score": 0.6632016, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 9, "text_snippet": "tirety of a large text corpus. GraphRAG first uses an LLM to construct a knowledge graph, where nodes correspond to key entities in the corpus and edges represent relationships between those entities. Next, it partitions the graph into a hi"}, {"rank": 5, "score": 0.6503897, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 17, "text_snippet": " are later used to answer queries as in Mao et al. 2020). These summaries are generated in parallel and iteratively aggregated into global summaries, similar to prior techniques (Feng et al., 2023; Gao et al., 2023; Khattab et al., 2022; Sh"}]}
{"case_index": 87, "query": "According to the paper 'corrective retrieval augmented generation', what exact term completes this statement: \"In addition, in some [BLANK] generation tasks, external knowl- edge is needed more than once, and when to retrieve should be concerned.\"?", "gold": "long-text", "answer": "The exact term is **“long-text”**, i.e., the sentence reads: “In addition, in some **long-text** generation tasks, external knowledge is needed more than once, and when to retrieve should be concerned.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'long-text' and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.647, "llm_ms": 1491.346, "top_contexts": [{"rank": 1, "score": 0.6271897, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 22, "text_snippet": "rrelevant context and improve robustness. SAIL (Luo et al., 2023) is tuned on instructions to insert retrieved documents before instructions. While Toolformer (Schick et al., 2023) is pre-trained for calling APIs such as Wikipedia. In addit"}, {"rank": 2, "score": 0.6123936, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 6, "text_snippet": " The examples show that a low-quality retriever is prone to introducing a substantial amount of irrelevant information, impeding the generators from acquiring accurate knowledge and potentially misleading them. the parametric knowledge they"}, {"rank": 3, "score": 0.6087612, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 1, "text_snippet": "e reliance on the paramet- ric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and inco"}, {"rank": 4, "score": 0.60607684, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 18, "text_snippet": "pact the LLMs and exacerbate the problems. Thus, it is obvious that the lack of accurate and specific knowledge can lead to misleading or even inaccurate generation, which will severely hurt the experience of users in most practical applica"}, {"rank": 5, "score": 0.5941434, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 7, "text_snippet": "ieval-augmented generation (RAG) (Lewis et al., 2020). In this framework, the input to models is augmented by prepending relevant documents that are retrieved from an external knowledge corpus (Guu et al., 2020). While RAG serves as a pract"}]}
{"case_index": 88, "query": "According to the paper 'replug retrieval augmented black box language models', what exact term completes this statement: \"In other words, we would like the [BLANK] to find documents that result in lower perplex- ity scores.\"?", "gold": "retriever", "answer": "The missing term is **“retriever”** — i.e., “we would like the **retriever** to find documents that result in lower perplexity scores.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'retriever' and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 12.389, "llm_ms": 1590.566, "top_contexts": [{"rank": 1, "score": 0.7129861, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 29, "text_snippet": "triever inREPLUG by using the LM itself to provide supervision about which documents should be retrieved. Inspired by Sachan et al. (2022), our approach can be seen as adjusting the probabilities of the retrieved documents to match the prob"}, {"rank": 2, "score": 0.6978718, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 9, "text_snippet": "22) that adapts language models to the retriever. We use a training objective which prefers retrieving documents that improve language model perplexity, while treating the LM as a frozen, black-box scoring function. Our experiments show tha"}, {"rank": 3, "score": 0.6737799, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 63, "text_snippet": " the performance of REPLUG andREPLUG LSR improved monotonically. How- ever, a small number of documents (e.g., 10) was sufficient to achieve large performance gains.  REPLUG: Retrieval-Augmented Black-Box Language Models Perplexity 14.0016."}, {"rank": 4, "score": 0.6615778, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 7, "text_snippet": " retrieved documents are prepended to the input context and fed into the black-box LM to make the final prediction. Because the LM context length limits the number of documents that can be prepended, we also introduce a new ensemble scheme "}, {"rank": 5, "score": 0.6614419, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 1, "text_snippet": "mechanisms to en- code the retrieved text, REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design can be eas- ily applied to any existing retrieval and language models. Furthermore, we show t"}]}
{"case_index": 89, "query": "According to the paper 'raptor recursive abstractive processing for tree organized retrieval', what exact term completes this statement: \"Retrieval Methods [BLANK] language models (RALMs) have seen improvements in various components: the retriever, the reader, and end-to-end system trai\"?", "gold": "retrieval-augmented", "answer": "The missing term is **“Retrieval-augmented”**, completing the statement as:\n\n“Retrieval Methods **Retrieval-augmented** language models (RALMs) have seen improvements in various components: the retriever, the reader, and end-to-end system trai”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'retrieval-augmented' and is directly supported by the provided context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 15.921, "llm_ms": 2024.569, "top_contexts": [{"rank": 1, "score": 0.7076664, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 13, "text_snippet": "nd to underutilize long-range context and see diminishing performance as con- text length increases, especially when pertinent information is embedded within a lengthy context. Moreover, practically, use of long contexts is expensive and sl"}, {"rank": 2, "score": 0.70235443, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 4, "text_snippet": "trieval-augmented language models (RALMs) (Khandelwal et al., 2019; Izacard et al., 2022; Lewis et al., 2020; Borgeaud et al., 2022) have shown impressive performance on knowledge-intensive tasks (Kwiatkowski et al., 2019; Petroni et al., 2"}, {"rank": 3, "score": 0.6768182, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 14, "text_snippet": "omponents: the retriever, the reader, and end-to-end system training. Retrieval methods have transitioned from traditional term-based techniques like TF-IDF (Sp¨arck Jones, 1972) and BM25 (Robertson et al., 1995; Roberts et al., 2020) to de"}, {"rank": 4, "score": 0.67405736, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 0, "text_snippet": "Published as a conference paper at ICLR 2024 RAPTOR: R ECURSIVE ABSTRACTIVE PROCESSING FOR TREE-ORGANIZED RETRIEVAL Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D. Manning Stanford University psarthi@cs."}, {"rank": 5, "score": 0.6715652, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 2, "text_snippet": "hat retrieval with recursive summaries offers significant improvements over tra- ditional retrieval-augmented LMs on several tasks. On question-answering tasks that involve complex, multi-step reasoning, we show state-of-the-art results; fo"}]}
{"case_index": 90, "query": "According to the paper 'raptor recursive abstractive processing for tree organized retrieval', what exact term completes this statement: \"Our main contribution is the idea of using text [BLANK] to allow retrieval augmentation of context at different scales, and to show its effectiveness in experiments on collections of long doc- uments.\"?", "gold": "summarization", "answer": "The missing term is **“summaries”** — i.e., “using text **summaries** to allow retrieval augmentation of context at different scales …”.", "answer_score": 0.63, "answer_correctness": 0.6, "answer_groundedness": 0.7, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.6, "groundedness": 0.7, "abstained": false, "should_abstain": false, "notes": "Expected term is 'summarization' (process), but model answered 'summaries' (noun). Context supports the idea of summarizing/recursive summaries, but not the exact quoted wording.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 11.85, "llm_ms": 1693.727, "top_contexts": [{"rank": 1, "score": 0.71021235, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 1, "text_snippet": "rom a retrieval corpus, limiting holistic under- standing of the overall document context. We introduce the novel approach of recursively embedding, clustering, and summarizing chunks of text, constructing a tree with differing levels of su"}, {"rank": 2, "score": 0.69716156, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 0, "text_snippet": "Published as a conference paper at ICLR 2024 RAPTOR: R ECURSIVE ABSTRACTIVE PROCESSING FOR TREE-ORGANIZED RETRIEVAL Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D. Manning Stanford University psarthi@cs."}, {"rank": 3, "score": 0.69008684, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 8, "text_snippet": " indexing and retrieval system that uses a tree structure to capture both high-level and low-level details about a text. As shown in Figure 1, our system, RAPTOR, clusters chunks of text, generates text summaries of those clusters, and then"}, {"rank": 4, "score": 0.68263435, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 22, "text_snippet": " RAPTOR addresses the issue of semantic depth and connection in reading by building a recursive tree structure that balances broader thematic comprehension with granular details and which allows nodes to be grouped based on semantic sim- il"}, {"rank": 5, "score": 0.67668295, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 73, "text_snippet": "an effectively handle a wider range of questions, from higher-order thematic queries to detail-oriented questions. Detailed results for additional stories and an ablation study on layer contributions can be found in Appendix I. 5 C ONCLUSIO"}]}
{"case_index": 91, "query": "According to the paper 'leveraging passage retrieval with generative models for open domain question answering', what exact term completes this statement: \"By processing passages [BLANK] in the en- coder, but jointly in the decoder, this method dif- fers from Min et al.\"?", "gold": "independently", "answer": "The missing term is **“independently”**:\n\n“By processing passages **independently** in the encoder, but jointly in the decoder, this method differs from Min et al.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'independently' and is explicitly supported by Context 5.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.2, "hit_rank": 5, "matched_by": "doc_id+chunk_id", "retrieval_ms": 18.621, "llm_ms": 1602.702, "top_contexts": [{"rank": 1, "score": 0.70228875, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}, {"rank": 2, "score": 0.6853261, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 8, "text_snippet": "3 Feb 2021  Question + Passage 1encoderQuestion + Passage 2encoderQuestion + Passage NencoderdecoderAnswerconcat… …… …Figure 2: Architecture of the Fusion-in-Decoder method. representations. Then, a sequence-to-sequence model generates the "}, {"rank": 3, "score": 0.67293245, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 21, "text_snippet": "Raffel et al., 2019; Lewis et al., 2019). The model takes as input the question, as well as the support passages, and generates the answer. More precisely, each retrieved passage and its title are concatenated with the question, and process"}, {"rank": 4, "score": 0.66777253, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 16, "text_snippet": "las (Izacard et al., 2022), which fine-tunes an encoder- decoder model in conjunction with the retriever; REALM (Guu et al., 2020), a bidirectional, masked LM fine-tuned for open-domain question answering; and RAG (Retrieval-Augmented Gener"}, {"rank": 5, "score": 0.6541325, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 22, "text_snippet": "ache.org 2spacy.io 3github.com/facebookresearch/faisstion over the concatenation of the resulting repre- sentations of all the retrieved passages. The model thus performs evidence fusion in the decoder only, and we refer to it as Fusion-in-"}]}
{"case_index": 92, "query": "According to the paper 'raptor recursive abstractive processing for tree organized retrieval', what exact term completes this statement: \"This flexibility is essen- tial because individual text segments often contain [BLANK] relevant to various topics, thereby warranting their inclusion in multiple summaries.\"?", "gold": "information", "answer": "The missing term is **“information”**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context 5 explicitly contains the sentence with the blank filled by 'information'; model answer matches expected.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.2, "hit_rank": 5, "matched_by": "doc_id+chunk_id", "retrieval_ms": 11.972, "llm_ms": 1043.438, "top_contexts": [{"rank": 1, "score": 0.5946044, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 1, "text_snippet": "rom a retrieval corpus, limiting holistic under- standing of the overall document context. We introduce the novel approach of recursively embedding, clustering, and summarizing chunks of text, constructing a tree with differing levels of su"}, {"rank": 2, "score": 0.5819377, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 72, "text_snippet": "e from the leaf nodes and each upper layer, as well as from different contiguous subsets of the layers. We show findings specific to one story in Table 8, revealing that a full-tree search, utilizing all layers, outperformed retrieval strat"}, {"rank": 3, "score": 0.5812582, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 8, "text_snippet": " indexing and retrieval system that uses a tree structure to capture both high-level and low-level details about a text. As shown in Figure 1, our system, RAPTOR, clusters chunks of text, generates text summaries of those clusters, and then"}, {"rank": 4, "score": 0.57315964, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 22, "text_snippet": " RAPTOR addresses the issue of semantic depth and connection in reading by building a recursive tree structure that balances broader thematic comprehension with granular details and which allows nodes to be grouped based on semantic sim- il"}, {"rank": 5, "score": 0.57251215, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 27, "text_snippet": "uiring a fixed number of clusters. This flexibility is essen- tial because individual text segments often contain information relevant to various topics, thereby warranting their inclusion in multiple summaries. Our clustering algorithm is "}]}
{"case_index": 93, "query": "According to the paper 'retrieval augmented generation for knowledge intensive nlp tasks', what exact term completes this statement: \"t prior probability pη(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be [BLANK] solved in sub-linear time [ 23].\"?", "gold": "approximately", "answer": "The missing term is **“approximately”**, as in:\n\n“…is a Maximum Inner Product Search (MIPS) problem, which can be **approximately** solved in sub-linear time [23].”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'approximately' and is directly supported by Context 2.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.345, "llm_ms": 1485.115, "top_contexts": [{"rank": 1, "score": 0.5275816, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 8, "text_snippet": " retriever ( Query Encoder +Document Index ) with a pre-trained seq2seq model ( Generator ) and ﬁne-tune end-to-end. For query x, we use Maximum Inner Product Search (MIPS) to ﬁnd the top-K documents zi. For ﬁnal prediction y, we treatzas a"}, {"rank": 2, "score": 0.5216762, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 22, "text_snippet": "t prior probability pη(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [ 23]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index."}, {"rank": 3, "score": 0.47725728, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 31, "text_snippet": "′⊂ D with the highest simi- larity scores from a corpus Dgiven an input context x, as described in §3.1. We then compute the retrieval likelihood of each retrieved document d: PR(d|x) =es(d,x)/γ P d∈D′es(d,x)/γ where γis a hyperparameter th"}, {"rank": 4, "score": 0.4712557, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 21, "text_snippet": "-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR The retrieval component pη(z|x)is based on DPR [26]. DPR follows a bi-encoder architecture: pη(z|x)∝exp( d(z)⊤q(x)) d(z) =BERTd(z),q(x) =BERTq(x) where d(z)is a dense representation"}, {"rank": 5, "score": 0.46958518, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 59, "text_snippet": " memory by leveraging approximate nearest neighbors search (Grave et al., 2017a). The related kNN-LM model (Khandelwal et al., 2020) replaced LSTMs by transformer networks, and scaled the memory to billions of tokens, leading to strong perf"}]}
{"case_index": 94, "query": "According to the paper 'refrag rethinking rag based decoding', what exact term completes this statement: \"2 Model [BLANK] We denote the decoder model as Mdecand the encoder model as Menc.\"?", "gold": "architecture", "answer": "The missing term is **“Architecture”** — i.e., the section title is:\n\n**“2 Model Architecture”**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context 2 explicitly shows the section title '2 Model Architecture' followed by the quoted sentence, so the blank is 'architecture'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.017, "llm_ms": 1803.941, "top_contexts": [{"rank": 1, "score": 0.6124336, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 59, "text_snippet": "decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B as decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance varies with different encoder and decoder sizes. Figure 11 presents results for"}, {"rank": 2, "score": 0.60458195, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 17, "text_snippet": "FRAG achieves better performance than LLaMA without incurring higher latency in the downstream applications. 2 Model Architecture We denote the decoder model as Mdecand the encoder model as Menc. Given an input with Ttokens x1, x2, . . . , "}, {"rank": 3, "score": 0.58786964, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 58, "text_snippet": "ning task. We observe a performance regression as the compression rate increases; however, even at a compression rate of32, our model remains competitive (as shown in table 1). In contrast, a compression rate of64appears to be overly aggres"}, {"rank": 4, "score": 0.58644474, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 18, "text_snippet": "hen describe the pηandpθcomponents, as well as the training and decoding procedure. 2.1 Models RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate the complete sequence . Technically, it treats the retriev"}, {"rank": 5, "score": 0.5624527, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 15, "text_snippet": "igure 1) while preserving the autoregressive nature of the decoder, thereby supporting multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight reinforcement learning (RL) policy that sel"}]}
{"case_index": 95, "query": "According to the paper 'retrieval augmented generation for knowledge intensive nlp tasks', what exact term completes this statement: \"is exciting, such models do have down- sides: They cannot easily expand or revise their memory, can’t [BLANK] provide insight into their predictions, and may produce “hallucinations” [ 38].\"?", "gold": "straightforwardly", "answer": "The missing term is **“straightforwardly”** (i.e., “can’t straightforwardly provide insight into their predictions”).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'straightforwardly' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 25.575, "llm_ms": 1250.933, "top_contexts": [{"rank": 1, "score": 0.69686675, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 5, "text_snippet": " is exciting, such models do have down- sides: They cannot easily expand or revise their memory, can’t straightforwardly provide insight into their predictions, and may produce “hallucinations” [ 38]. Hybrid models that combine parametric m"}, {"rank": 2, "score": 0.62416625, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 3, "text_snippet": "itly in their parameters. However, they are also prone to hallucination and cannot represent the full long tail of knowledge from the training corpus. Retrieval-augmented language models (Khandelwal et al., 2020; Borgeaud et al., 2022; Izac"}, {"rank": 3, "score": 0.6105882, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 1, "text_snippet": "evitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Al- though retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heav"}, {"rank": 4, "score": 0.6024073, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 1, "text_snippet": "ameters, and achieve state-of-the-art results when ﬁne-tuned on down- stream NLP tasks. However, their ability to access and precisely manipulate knowl- edge is still limited, and hence on knowledge-intensive tasks, their performance lags b"}, {"rank": 5, "score": 0.5861809, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 8, "text_snippet": "wledge raises significant concerns about the model’s behavior and performance in scenarios where retrieval may fail or return inaccu- rate results (Shi et al., 2023). As Figure 1 shows that a low-quality retriever is prone to introducingarX"}]}
{"case_index": 96, "query": "According to the paper 'retrieval augmented generation for knowledge intensive nlp tasks', what exact term completes this statement: \"We marginalize the latent documents with a top-K [BLANK], either on a per-output basis (assuming the same document is responsible for all tokens) or a per-token basis (\"?", "gold": "approximation", "answer": "The missing term is **“approximation”** — i.e., “We marginalize the latent documents with a top-K **approximation**, either on a per-output basis … or a per-token basis …”.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'approximation' and is directly supported by Context 2/5 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.426, "llm_ms": 1310.243, "top_contexts": [{"rank": 1, "score": 0.65265113, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 19, "text_snippet": "r produces the output sequence probability for each document, which are then marginalized, pRAG-Sequence (y|x)≈∑ z∈top-k(p(·|x))pη(z|x)pθ(y|x,z) =∑ z∈top-k(p(·|x))pη(z|x)N∏ ipθ(yi|x,z,y 1:i−1) RAG-Token Model In the RAG-Token model we can d"}, {"rank": 2, "score": 0.6151317, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 10, "text_snippet": "eural retriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The retriever (Dense Passage Retriever [ 26], henceforth DPR) provides latent documents conditioned on the input, and the seq2seq model (BART"}, {"rank": 3, "score": 0.59280175, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 25, "text_snippet": "beddings. 3.2. Input Reformulation The retrieved top- kdocuments provide rich information about the original input context xand can potentially help the LM to make a better prediction. One simple way to incorporate the retrieved documents a"}, {"rank": 4, "score": 0.58083, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 33, "text_snippet": "istribution according to the language model, using a uniform prior: pk∝pLM(a|dk,q). Using the Softmax operator, we have that pk=exp(logpLM(a|dk,q))∑K i=1exp(logpLM(a|di,q)). Leave-one-out Perplexity Distillation (LOOP). Finally, we propose "}, {"rank": 5, "score": 0.5771942, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 11, "text_snippet": "e document is responsible for all tokens) or a per-token basis (where different documents are responsible for different tokens). Like T5 [ 51] or BART, RAG can be ﬁne-tuned on any seq2seq task, whereby both the generator and retriever are j"}]}
{"case_index": 97, "query": "According to the paper 'self rag learning to retrieve generate and critique through self reflection', what exact term completes this statement: \"Type Input Output Definitions Retrieve x/x, y {yes, no, continue } Decides when to retrieve with R ISREL x, d {relevant , irrelevant } dprovides useful [BLANK] to solve x.\"?", "gold": "information", "answer": "The missing term is **“information”** — the full definition is:\n\n“**d provides useful information to solve x.**”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'information' and is directly supported by the provided context (Table 1 excerpt).", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 9.276, "llm_ms": 1387.643, "top_contexts": [{"rank": 1, "score": 0.59557104, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 30, "text_snippet": "ection tokens (Table 1). 2All work is arXived within a week of this preprint. 3In this paper, we treat one sentence as a segment in our experiments, but our framework is applicable to any segment unit (i.e., sub-sentence). 3  Preprint. Type"}, {"rank": 2, "score": 0.59439504, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 32, "text_snippet": ", respectively. Algorithm 1 SELF-RAGInference Require: Generator LM M, Retriever R, Large-scale passage collections {d1, . . . , d N} 1:Input: input prompt xand preceding generation y<t,Output: next output segment yt 2:Mpredicts Retrieve gi"}, {"rank": 3, "score": 0.59357, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 27, "text_snippet": "n (reasoning path consistency) and without retrieval. Recent work on LLM refinement (Dhuliawala et al., 2023; Madaan et al., 2023; Paul et al., 2023) prompts a model to generate task output, natural language feedback and refined task output"}, {"rank": 4, "score": 0.58436817, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 31, "text_snippet": "tially supported, no support }All of the verification-worthy statement in y is supported by d. ISUSE x, y {5, 4, 3, 2, 1 } yis a useful response to x. Table 1: Four types of reflection tokens used in SELF-RAG. Each type uses several tokens "}, {"rank": 5, "score": 0.5841521, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 46, "text_snippet": "pervised data that precisely mimics the SELF- RAGinference-time process (Section 3.1). For each segment yt∈y, we run Cto assess whether additional passages could help to enhance generation. If retrieval is required, the retrieval special to"}]}
{"case_index": 98, "query": "According to the paper 'retrieval augmented generation for knowledge intensive nlp tasks', what exact term completes this statement: \"Our RAG models achieve [BLANK] results on open Natural Questions [ 29], WebQuestions [ 3] and CuratedTrec [ 2] and strongly outperform recent approaches that use specialised pre-training objectives on TriviaQA [ 24].\"?", "gold": "state-of-the-art", "answer": "The missing term is **“state-of-the-art”** (“Our RAG models achieve **state-of-the-art** results on open Natural Questions…”).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'state-of-the-art' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 15.836, "llm_ms": 1461.333, "top_contexts": [{"rank": 1, "score": 0.74622214, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 13, "text_snippet": "sive tasks —tasks that humans could not reasonably be expected to perform without access to an external knowledge source. Our RAG models achieve state-of-the-art results on open Natural Questions [ 29], WebQuestions [ 3] and CuratedTrec [ 2"}, {"rank": 2, "score": 0.6923426, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 3, "text_snippet": "cessed with a pre-trained neural retriever. We com- pare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. We ﬁne-tune and "}, {"rank": 3, "score": 0.66166395, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 46, "text_snippet": "2 40.1 41.572.5 89.5RAG-Seq. 14.7 21.4 40.8 44.2 to more effective marginalization over documents. Furthermore, RAG can generate correct answers even when the correct answer is not in any retrieved document, achieving 11.8% accuracy in such"}, {"rank": 4, "score": 0.6614696, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 41, "text_snippet": "o not require such supervision will be applicable to a wider range of tasks. We explore two variants: the standard 3-way classiﬁcation task (supports/refutes/not enough info) and the 2-way (supports/refutes) task studied in Thorne and Vlach"}, {"rank": 5, "score": 0.65999126, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 32, "text_snippet": "ametric knowledge. We consider four popular open-domain QA datasets: Natural Questions (NQ) [ 29], TriviaQA (TQA) [ 24]. WebQuestions (WQ) [ 3] and CuratedTrec (CT) [ 2]. As CT and WQ are small, we follow DPR [ 26] by initializing CT and WQ"}]}
{"case_index": 99, "query": "According to the paper 'from local to global a graph rag approach to query focused summarization', what exact term completes this statement: \"GraphRAG leverages summaries over large sections of the data source as a form of ”[BLANK]” (described in Cheng et al.\"?", "gold": "self-memory", "answer": "The missing term is **“self-memory”**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'self-memory' and is explicitly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 9.536, "llm_ms": 1114.581, "top_contexts": [{"rank": 1, "score": 0.7633954, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 16, "text_snippet": " may use alternative retrieval mechanisms, we collectively refer to the family of conventional approaches as vector RAG . GraphRAG contrasts with vector RAG in its ability to answer queries that require global sensemaking over the entire da"}, {"rank": 2, "score": 0.7449576, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 17, "text_snippet": " are later used to answer queries as in Mao et al. 2020). These summaries are generated in parallel and iteratively aggregated into global summaries, similar to prior techniques (Feng et al., 2023; Gao et al., 2023; Khattab et al., 2022; Sh"}, {"rank": 3, "score": 0.73233306, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 0, "text_snippet": "From Local to Global: A GraphRAG Approach to Query-Focused Summarization Darren Edge1†Ha Trinh1†Newman Cheng2Joshua Bradley2Alex Chao3 Apurva Mody3Steven Truitt2Dasha Metropolitansky1Robert Osazuwa Ness1 Jonathan Larson1 1Microsoft Research"}, {"rank": 4, "score": 0.7198969, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 10, "text_snippet": "cted communities, with summaries at higher levels of the hierarchy recursively incorporating lower-level summaries. Together, these community summaries provide global descriptions and insights over the corpus. Finally, GraphRAG answers quer"}, {"rank": 5, "score": 0.7195611, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 96, "text_snippet": "tigate these downstream risks for questions of a global nature, which might otherwise be answered by samples of retrieved facts falsely presented as global summaries. 7 Conclusion We have presented GraphRAG, a RAG approach that combines kno"}]}
{"case_index": 100, "query": "According to the paper 'atlas few shot learning with retrieval augmented language models', what exact term completes this statement: \"They are able to learn new tasks with very few examples or even from [BLANK] alone.\"?", "gold": "instructions", "answer": "The missing term is **“descriptions”** — i.e., “They are able to learn new tasks with very few examples or even from **descriptions** alone.”", "answer_score": 0.0, "answer_correctness": 0.0, "answer_groundedness": 0.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 0.0, "abstained": false, "should_abstain": false, "notes": "Expected term is \"instructions\" but the answer says \"descriptions\". The provided context snippet mentions \"descriptions of tasks\" but does not support the exact quoted statement with \"descriptions alone\".", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 9.054, "llm_ms": 1092.815, "top_contexts": [{"rank": 1, "score": 0.65258324, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 2, "text_snippet": "ings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including"}, {"rank": 2, "score": 0.63538253, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 0, "text_snippet": "Atlas: Few-shot Learning with Retrieval Augmented Language Models Gautier Izacard∗ ∗,♦,♣,♥gizacard@fb.com Patrick Lewis∗,♦plewis@fb.com Maria Lomeli♦marialomeli@fb.com Lucas Hosseini♦hoss@fb.com Fabio Petroni♦fabiopetroni@fb.com Timo Schick"}, {"rank": 3, "score": 0.63484734, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 61, "text_snippet": " added to the prompt of a large language model performing in-context learning. 3.2 Few-shot learning Few-shot learning, the task of learning from very few examples, has been studied for decades (Thrun & Pratt, 1998; Fink, 2005; Vinyals et a"}, {"rank": 4, "score": 0.631372, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 1, "text_snippet": " Abstract Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter count"}, {"rank": 5, "score": 0.62903845, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 62, "text_snippet": "arge Language models. Providing language models with natural language descriptions of tasks, as proposed by Radford et al. (2019) has led to signiﬁcant developments in few-shot learning. GPT-3 (Brown et al., 2020) demonstrated the ability o"}]}
{"case_index": 101, "query": "According to the paper 'leveraging passage retrieval with generative models for open domain question answering', what exact term completes this statement: \"1 [BLANK] Recently, several works have shown that factual information can be extracted from large scale language models trained on vast quantities of data (Radford et al., 2019; Petroni et al., 2019; Jiang et al., 2019; Talmor et al., 2019).\"?", "gold": "introduction", "answer": "The missing term is **“Introduction”**, completing the statement as:\n\n**“1 Introduction Recently, several works have shown that factual information can be extracted from large scale language models trained on vast quantities of data (Radford et al., 2019; Petroni et al., 2019; Jiang et al., 2019; Talmor et al., 2019).”**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer correctly identifies the blank as 'Introduction' and is directly supported by Context 1 showing '1 Introduction' followed by the quoted sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 9.163, "llm_ms": 2278.275, "top_contexts": [{"rank": 1, "score": 0.74036086, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 2, "text_snippet": "s a ﬂexible framework to efﬁciently aggregate and com- bine evidence from multiple passages. 1 Introduction Recently, several works have shown that factual information can be extracted from large scale language models trained on vast quanti"}, {"rank": 2, "score": 0.69842637, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 0, "text_snippet": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering Gautier Izacard1,2,3Edouard Grave1 1Facebook AI Research, Paris 2ENS, PSL University, Paris 3Inria, Paris gizacard|egrave@fb.com Abstract Generative mode"}, {"rank": 3, "score": 0.6903132, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 15, "text_snippet": " that answers do not correspond to spans in support documents, thus requiring ab- stractive models. Raffel et al. (2019) showed that generative models are competitive for reading com- prehension tasks such as SQuAD (Rajpurkar et al., 2016),"}, {"rank": 4, "score": 0.67974615, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 4, "text_snippet": " tasks (Roberts et al., 2020). Nevertheless, even a large model does not contain sufficient domain- specific knowledge for particular tasks and the world continues to change, invalidating facts in the LLM. Updating the knowledge of these mo"}, {"rank": 5, "score": 0.67825294, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}]}
{"case_index": 102, "query": "According to the paper 'retrieval augmented generation for knowledge intensive nlp tasks', what exact term completes this statement: \"(y) Question Answering: Answer [BLANK] pη (Non-Parametric) z 4 z3 z2 z 1d(z) Jeopardy Question Generation: Answer QueryFigure 1: Overview of our approach.\"?", "gold": "generationretriever", "answer": "The missing term is **“y”** — i.e., the statement is **“Answer y”**.", "answer_score": 0.06, "answer_correctness": 0.0, "answer_groundedness": 0.2, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 0.2, "abstained": false, "should_abstain": false, "notes": "Expected term is 'generationretriever' (likely 'generation retriever'); model answered 'y', which does not match. Context mentions retriever/generator but does not support 'y' as the blank completion.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 8.03, "llm_ms": 1285.169, "top_contexts": [{"rank": 1, "score": 0.62822115, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 8, "text_snippet": " retriever ( Query Encoder +Document Index ) with a pre-trained seq2seq model ( Generator ) and ﬁne-tune end-to-end. For query x, we use Maximum Inner Product Search (MIPS) to ﬁnd the top-K documents zi. For ﬁnal prediction y, we treatzas a"}, {"rank": 2, "score": 0.62435365, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 22, "text_snippet": "t prior probability pη(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [ 23]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index."}, {"rank": 3, "score": 0.59479606, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 53, "text_snippet": "rieved documents as the answer (Chen et al., 2017; Clark & Gardner, 2018; Wang et al., 2019; Karpukhin et al., 2020), a method inspired by reading comprehension (Richardson, 2013; Rajpurkar et al., 2016). Recently, generating the answer as "}, {"rank": 4, "score": 0.5902866, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 14, "text_snippet": "active approaches. For knowledge-intensive generation, we experiment with MS-MARCO [ 1] and Jeopardy question generation, and we ﬁnd that our models generate responses that are more factual, speciﬁc, and diverse than a BART baseline. For FE"}, {"rank": 5, "score": 0.58420074, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 14, "text_snippet": " be trained using weak supervision in the form of question-answer pairs (Karpukhin et al., 2020), or pretrained using a cloze task and ﬁnetuned end-to- end (Guu et al., 2020; Lee et al., 2019). Generative question answering was mostly consi"}]}
{"case_index": 103, "query": "According to the paper 'atlas few shot learning with retrieval augmented language models', what exact term completes this statement: \"xamples, [BLANK] a 540B parameters model by 3% despite having 50x fewer parameters.\"?", "gold": "outperforming", "answer": "The missing term is **“Atlas”** (as in: “For example, **Atlas** … outperforming a 540B parameters model by 3% despite having 50x fewer parameters.”).", "answer_score": 0.0, "answer_correctness": 0.0, "answer_groundedness": 0.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 0.0, "abstained": false, "should_abstain": false, "notes": "Blank should be 'outperforming' per context; model answered 'Atlas', which is incorrect.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 9.322, "llm_ms": 1580.479, "top_contexts": [{"rank": 1, "score": 0.7181094, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 3, "text_snippet": "xamples, outperforming a 540B parameters model by 3% despite having 50x fewer parameters. 1 Introduction Large language models (LLMs) are impressive few-shot learners (Brown et al., 2020; Rae et al., 2021; Hoﬀmann et al., 2022; Chowdhery et"}, {"rank": 2, "score": 0.6570934, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 103, "text_snippet": "0M parameters, whereas the equivalent Atlasachieves around 40%, signiﬁcantly better than random, despite its small size. All models improve with more data, but interestingly, the 770M models do not beneﬁt as much from few-shot multitask lea"}, {"rank": 3, "score": 0.64687467, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 1, "text_snippet": " Abstract Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter count"}, {"rank": 4, "score": 0.63734406, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 2, "text_snippet": "ings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including"}, {"rank": 5, "score": 0.6320958, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 154, "text_snippet": "odel. By jointly pre-training the retriever module and the language model, we show that Atlashas strong few-shot learning capabilities on a wide range of knowledge intensive tasks, including NaturalQuestions, TriviaQA, FEVER, 8 KILT tasks a"}]}
{"case_index": 104, "query": "According to the paper 'self rag learning to retrieve generate and critique through self reflection', what exact term completes this statement: \"A more recent work (Luo et al., 2023) [BLANK] an LM with a fixed number 2 Preprint.\"?", "gold": "instruction-tunes", "answer": "The missing term is **“augments”** — i.e., “A more recent work (Luo et al., 2023) **augments** an LM with a fixed number …”", "answer_score": 0.0, "answer_correctness": 0.0, "answer_groundedness": 0.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 0.0, "abstained": false, "should_abstain": false, "notes": "Expected term is \"instruction-tunes\" but the answer says \"augments\". The provided context snippet does not contain the quoted sentence, so the answer is not supported by context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 16.435, "llm_ms": 1583.035, "top_contexts": [{"rank": 1, "score": 0.6591948, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 0, "text_snippet": "Preprint. SELF-RAG: LEARNING TO RETRIEVE , GENERATE ,AND CRITIQUE THROUGH SELF-REFLECTION Akari Asai†, Zeqiu Wu†, Yizhong Wang†§, Avirup Sil‡, Hannaneh Hajishirzi†§ †University of Washington§Allen Institute for AI‡IBM Research AI {akari,zeq"}, {"rank": 2, "score": 0.65544677, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 21, "text_snippet": " al., 2023a; Gao et al., 2023). We introduce a method to train an arbitrary LM to learn to use retrieval on-demand for diverse instruction-following queries and introduce controlled generation guided by reflections tokens to further improve"}, {"rank": 3, "score": 0.6521857, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 2, "text_snippet": "ew framework called Self-Reflective Retrieval-Augmented Gen- eration ( SELF-RAG)that enhances an LM’s quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passage"}, {"rank": 4, "score": 0.645627, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 22, "text_snippet": " the retriever and LM on instruction-tuning datasets in two steps. While we also train our model on diverse instruction-following datasets, SELF-RAGenables retrieval on demand and selection of the best possible model output via fine-grained"}, {"rank": 5, "score": 0.64235234, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 96, "text_snippet": "ains an LM to learn to retrieve, generate, and critique text passages and its own generation by predicting the next tokens from its original vocabulary as well as newly added special tokens, called reflection tokens. SELF-RAGfurther enables"}]}
{"case_index": 105, "query": "According to the paper 'corrective retrieval augmented generation', what exact term completes this statement: \"In this framework, the input to models is augmented by [BLANK] relevant documents that are retrieved from an external knowledge corpus (Guu et al., 2020).\"?", "gold": "prepending", "answer": "The missing term is **“prepending”** — i.e., “the input to models is augmented by **prepending** relevant documents that are retrieved from an external knowledge corpus (Guu et al., 2020).”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'prepending' and is directly supported by Context 1 (and Context 5).", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 9.091, "llm_ms": 1512.356, "top_contexts": [{"rank": 1, "score": 0.63902307, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 7, "text_snippet": "ieval-augmented generation (RAG) (Lewis et al., 2020). In this framework, the input to models is augmented by prepending relevant documents that are retrieved from an external knowledge corpus (Guu et al., 2020). While RAG serves as a pract"}, {"rank": 2, "score": 0.6281854, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 19, "text_snippet": " documents. It usually provides an extra knowledge source from a specific corpus, i.e., Wikipedia, which greatly improves the per- formance of LMs in a variety of tasks, especially in the knowledge-intensive ones. The proposed  methods gene"}, {"rank": 3, "score": 0.6179273, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 18, "text_snippet": "pact the LLMs and exacerbate the problems. Thus, it is obvious that the lack of accurate and specific knowledge can lead to misleading or even inaccurate generation, which will severely hurt the experience of users in most practical applica"}, {"rank": 4, "score": 0.6130544, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 6, "text_snippet": " The examples show that a low-quality retriever is prone to introducing a substantial amount of irrelevant information, impeding the generators from acquiring accurate knowledge and potentially misleading them. the parametric knowledge they"}, {"rank": 5, "score": 0.60732734, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 4, "text_snippet": "trieval-augmented language models (RALMs) (Khandelwal et al., 2019; Izacard et al., 2022; Lewis et al., 2020; Borgeaud et al., 2022) have shown impressive performance on knowledge-intensive tasks (Kwiatkowski et al., 2019; Petroni et al., 2"}]}
{"case_index": 106, "query": "According to the paper 'replug retrieval augmented black box language models', what exact term completes this statement: \"These models are typically trained on very large datasets and store a [BLANK] amount of world or domain knowledge implicitly in their parameters.\"?", "gold": "substantial", "answer": "The missing term is **“significant”** — i.e., “store a **significant** amount of world or domain knowledge implicitly in their parameters.”", "answer_score": 0.0, "answer_correctness": 0.0, "answer_groundedness": 0.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 0.0, "abstained": false, "should_abstain": false, "notes": "Expected term is 'substantial' per the paper; model answered 'significant'. The provided context snippet does not include the target sentence/word, so the answer is not supported by context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 27.781, "llm_ms": 1289.089, "top_contexts": [{"rank": 1, "score": 0.6190034, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 3, "text_snippet": "itly in their parameters. However, they are also prone to hallucination and cannot represent the full long tail of knowledge from the training corpus. Retrieval-augmented language models (Khandelwal et al., 2020; Borgeaud et al., 2022; Izac"}, {"rank": 2, "score": 0.60714483, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 5, "text_snippet": "ting the LM’s parameters, REPLUG treats the language model as a black box and augments it with a frozen or tunable retriever. This black-box assumption makes REPLUG applicable to large LMs (i.e., >100B parameters), which are often served vi"}, {"rank": 3, "score": 0.60577536, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 12, "text_snippet": "gnals, resulting in improved retrieval quality. •Evaluations on language modeling (§6), open-domain QA and MMLU demonstrate that REPLUG can im- prove the performance of various language models such as GPT, OPT and BLOOM, including very larg"}, {"rank": 4, "score": 0.5946985, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 15, "text_snippet": "anguage models makes this approach infeasible. To ad- dress the challenges posed by large language models, we investigate retrieval-augmentation in the black-box setting , where users only have access to the model predictions and cannot acc"}, {"rank": 5, "score": 0.5942503, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 1, "text_snippet": "ameters, and achieve state-of-the-art results when ﬁne-tuned on down- stream NLP tasks. However, their ability to access and precisely manipulate knowl- edge is still limited, and hence on knowledge-intensive tasks, their performance lags b"}]}
{"case_index": 107, "query": "According to the paper 'refrag rethinking rag based decoding', what exact term completes this statement: \"Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG with k= 16achieves16 .53×TTFT [BLANK] with cache and8 .59×without cache1, both surpassing CEPE (\"?", "gold": "acceleration", "answer": "The missing term is **“acceleration”** — i.e., “**16.53× TTFT acceleration** with cache …”.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Blank is 'acceleration', explicitly stated in provided context (Context 1/4). Model answer matches exactly.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 13.191, "llm_ms": 1890.421, "top_contexts": [{"rank": 1, "score": 0.74410266, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 24, "text_snippet": " the time to generate each subsequent token; and Throughput, the number of tokens generated per unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to k×acceleration in TTFT and throughpu"}, {"rank": 2, "score": 0.7430471, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 25, "text_snippet": "ation with cache and8 .59×without cache1, both surpassing CEPE (2.01×and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared to CEPE (table 1). We achieve up to6 .78×throughput acceleration compared "}, {"rank": 3, "score": 0.7425977, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 134, "text_snippet": "ntext),REFRAG achieves16 .53×acceleration in TTFT with cache and8 .59×without cache. Both higher than CEPE (i.e., 2.01×and1 .04×acceleration respectively) while having better model performance (see table 1). With longer context, we are able"}, {"rank": 4, "score": 0.7398436, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 133, "text_snippet": "xt length s, we are able to achieve up to k2×acceleration in both TTFT and throughput. The details on the latency and throughput calculation are in section B.4. Empirical verification of latency/throughput improvement.Figure 2 shows the emp"}, {"rank": 5, "score": 0.66643924, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 42, "text_snippet": "xt lengths s∈ { 4096,8192,16384}. Although our model is trained on s+o= 6144, both REFRAG 8andREFRAG 16maintain superior performance at longer contexts. The original Llama-2-7B supports only a4k context window, whereas our approach enables "}]}
{"case_index": 108, "query": "According to the paper 'retrieval augmented generation for knowledge intensive nlp tasks', what exact term completes this statement: \"We refer to this decoding [BLANK] as “Thorough Decoding.” For longer output seq\"?", "gold": "procedure", "answer": "The missing term is:\n\n**procedure**\n\nAs in: “We refer to this decoding **procedure** as ‘Thorough Decoding.’ For longer output seq…”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context 2 explicitly states: 'We refer to this decoding procedure as “Thorough Decoding.”' Model answer matches expected term.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 12.482, "llm_ms": 1488.743, "top_contexts": [{"rank": 1, "score": 0.57076246, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 53, "text_snippet": "old, we trigger retrieval (details in Appendix Section A.3). Tree-decoding with critique tokens. At each segment step t, when retrieval is required, based either on hard or soft conditions, Rretrieves Kpassages, and the generator Mprocesses"}, {"rank": 2, "score": 0.56806076, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 27, "text_snippet": "n beam search for each document z, scoring each hypothesis using pθ(yi|x,z,y 1:i−1). This yields a set of hypotheses Y, some of which may not have appeared in the beams of all documents. To estimate the probability of an hypothesis ywe run "}, {"rank": 3, "score": 0.550775, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 28, "text_snippet": "decoding procedure as “Thorough Decoding.” For longer output sequences,|Y|can become large, requiring many forward passes. For more efﬁcient decoding, we can make a further approximation that pθ(y|x,zi)≈0whereywas not generated during beam "}, {"rank": 4, "score": 0.5345217, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 4, "text_snippet": " for long-form generations relative to these models.1 1 I NTRODUCTION State-of-the-art LLMs continue to struggle with factual errors (Mallen et al., 2023; Min et al., 2023) despite their increased model and data scale (Ouyang et al., 2022)."}, {"rank": 5, "score": 0.52985597, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 10, "text_snippet": "eural retriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The retriever (Dense Passage Retriever [ 26], henceforth DPR) provides latent documents conditioned on the input, and the seq2seq model (BART"}]}
{"case_index": 109, "query": "According to the paper 'leveraging passage retrieval with generative models for open domain question answering', what exact term completes this statement: \"This al- lows to scale to large numbers of [BLANK], and to beneﬁt from this large amount of evidence.\"?", "gold": "documents", "answer": "The missing term is **“passages”** (i.e., “This allows to scale to large numbers of **passages**, and to benefit from this large amount of evidence.”).", "answer_score": 0.27, "answer_correctness": 0.0, "answer_groundedness": 0.9, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 0.9, "abstained": false, "should_abstain": false, "notes": "Expected term is 'documents' but model answered 'passages'. Context discusses scaling to large numbers of passages, which supports the provided (incorrect vs expected) answer.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 12.584, "llm_ms": 1434.604, "top_contexts": [{"rank": 1, "score": 0.6913715, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 1, "text_snippet": "x- pensive to train and query. In this paper, we investigate how much these models can ben- eﬁt from retrieving text passages, potentially containing evidence. We obtain state-of-the- art results on the Natural Questions and Triv- iaQA open"}, {"rank": 2, "score": 0.6743189, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 31, "text_snippet": "tion answering. We observe that while conceptu- ally simple, this method outperforms existing work on the NaturalQuestion and TriviaQA benchmarks. In particular, generative models seem to perform well when evidence from multiple passages ne"}, {"rank": 3, "score": 0.6725228, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 9, "text_snippet": "rticular, we show that the performance of our method signiﬁcantly improves when the number of retrieved passages increases. We believe that this is evidence that generative mod- els are good at combining evidence from multiple passages, com"}, {"rank": 4, "score": 0.66910154, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 0, "text_snippet": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering Gautier Izacard1,2,3Edouard Grave1 1Facebook AI Research, Paris 2ENS, PSL University, Paris 3Inria, Paris gizacard|egrave@fb.com Abstract Generative mode"}, {"rank": 5, "score": 0.6620178, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 2, "text_snippet": "s a ﬂexible framework to efﬁciently aggregate and com- bine evidence from multiple passages. 1 Introduction Recently, several works have shown that factual information can be extracted from large scale language models trained on vast quanti"}]}
{"case_index": 110, "query": "According to the paper 'replug retrieval augmented black box language models', what exact term completes this statement: \"How- ever, the increasing scale and black-box nature of large language models makes this approach [BLANK].\"?", "gold": "infeasible", "answer": "The missing term is **“infeasible”**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'infeasible' and is directly supported by Context 4.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 8.137, "llm_ms": 1268.554, "top_contexts": [{"rank": 1, "score": 0.7313144, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 5, "text_snippet": "ting the LM’s parameters, REPLUG treats the language model as a black box and augments it with a frozen or tunable retriever. This black-box assumption makes REPLUG applicable to large LMs (i.e., >100B parameters), which are often served vi"}, {"rank": 2, "score": 0.72089326, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 71, "text_snippet": "a black box and augments it with a tuneable retrieval model. Our evaluation shows that REPLUG can be integrated with any existing language model to improve their performance  REPLUG: Retrieval-Augmented Black-Box Language Models on language"}, {"rank": 3, "score": 0.7076823, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 8, "text_snippet": "Black-Box Language Models Figure 1, REPLUG is extremely flexible and can be used with any existing black-box LM and retrieval model. We also introduce REPLUG LSR (REPLUG with LM- Supervised Retrieval), a training scheme that can further imp"}, {"rank": 4, "score": 0.703528, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 15, "text_snippet": "anguage models makes this approach infeasible. To ad- dress the challenges posed by large language models, we investigate retrieval-augmentation in the black-box setting , where users only have access to the model predictions and cannot acc"}, {"rank": 5, "score": 0.69469386, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 12, "text_snippet": "gnals, resulting in improved retrieval quality. •Evaluations on language modeling (§6), open-domain QA and MMLU demonstrate that REPLUG can im- prove the performance of various language models such as GPT, OPT and BLOOM, including very larg"}]}
{"case_index": 111, "query": "According to the paper 'from local to global a graph rag approach to query focused summarization', what exact term completes this statement: \"generated texts such as “fluency” (Wang et al., 2023a) Some of these criteria are generic to vector RAG systems and not relevant to global sensemaking, such as “context relevance”, “[BLANK]”, and “answer relevance” (RAGAS, Es et al\"?", "gold": "faithfulness", "answer": "The missing term is **“faithfulness”**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Model correctly fills the blank with 'faithfulness', which is explicitly present in the provided context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.935, "llm_ms": 1062.454, "top_contexts": [{"rank": 1, "score": 0.82511544, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 29, "text_snippet": "lements (nodes, edges, covariates) that the LLM can summarize in parallel at both indexing time and query time. The “global answer” to a given query is produced using a final round of query-focused summarization over all community summaries"}, {"rank": 2, "score": 0.6946346, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 30, "text_snippet": "evance”, “faithfulness”, and “answer relevance” (RAGAS, Es et al. 2023). Lacking a gold standard for evaluation, one can quantify relative performance for a given criterion by prompting the LLM to compare generations from two different comp"}, {"rank": 3, "score": 0.6567207, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 96, "text_snippet": "tigate these downstream risks for questions of a global nature, which might otherwise be answered by samples of retrieved facts falsely presented as global summaries. 7 Conclusion We have presented GraphRAG, a RAG approach that combines kno"}, {"rank": 4, "score": 0.6356598, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 27, "text_snippet": "ns (Wang et al., 2023a; Zheng et al., 2024). Some prior work proposes criteria for having LLMs quantify the quality of 3  Source Documents Text Chunkstext extraction and chunking Entities & Relationshipsdomain-tailored summarization Knowled"}, {"rank": 5, "score": 0.6340087, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 8, "text_snippet": "nce analysis (Ranade and Joshi, 2023). Given a sensemaking query and a text with an implicit and interconnected set of concepts, an LLM can generate a summary that answers the query. The challenge, however, arises when the volume of data re"}]}
{"case_index": 112, "query": "According to the paper 'from local to global a graph rag approach to query focused summarization', what exact term completes this statement: \"In this work, we de- sign criteria for evaluating [BLANK] answers to global sensemaking questions and evaluate our results using the comparative approach.\"?", "gold": "rag-generated", "answer": "The missing term is **“global”** — i.e., “criteria for evaluating **global** answers to global sensemaking questions.”", "answer_score": 0.0, "answer_correctness": 0.0, "answer_groundedness": 0.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 0.0, "abstained": false, "should_abstain": false, "notes": "Expected term is 'rag-generated' but model answered 'global'. Provided context snippet does not support 'global' as the exact missing term.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 16.173, "llm_ms": 1611.704, "top_contexts": [{"rank": 1, "score": 0.72066605, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 29, "text_snippet": "lements (nodes, edges, covariates) that the LLM can summarize in parallel at both indexing time and query time. The “global answer” to a given query is produced using a final round of query-focused summarization over all community summaries"}, {"rank": 2, "score": 0.70709705, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 96, "text_snippet": "tigate these downstream risks for questions of a global nature, which might otherwise be answered by samples of retrieved facts falsely presented as global summaries. 7 Conclusion We have presented GraphRAG, a RAG approach that combines kno"}, {"rank": 3, "score": 0.68776834, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 11, "text_snippet": "swers are combined and used to generate a final global answer. The GraphRAG method and its ability to perform global sensemaking over an entire corpus form the main contribution of this work. To demonstrate this ability, we developed a nove"}, {"rank": 4, "score": 0.66230416, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 0, "text_snippet": "From Local to Global: A GraphRAG Approach to Query-Focused Summarization Darren Edge1†Ha Trinh1†Newman Cheng2Joshua Bradley2Alex Chao3 Apurva Mody3Steven Truitt2Dasha Metropolitansky1Robert Osazuwa Ness1 Jonathan Larson1 1Microsoft Research"}, {"rank": 5, "score": 0.65097356, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 3, "text_snippet": "wo stages: first, to derive an entity knowledge graph from the source documents, then to pre- generate community summaries for all groups of closely related entities. Given a question, each community summary is used to generate a partial re"}]}
{"case_index": 113, "query": "According to the paper 'replug retrieval augmented black box language models', what exact term completes this statement: \"We use a training objective which prefers [BLANK] documents that improve language model perplexity, while treating the LM as a frozen, black-box scoring function.\"?", "gold": "retrieving", "answer": "The missing term is **“retrieving”**, i.e.:\n\n“We use a training objective which prefers **retrieving** documents that improve language model perplexity, while treating the LM as a frozen, black-box scoring function.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'retrieving' and is directly supported by Context 1 verbatim.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 11.823, "llm_ms": 1450.65, "top_contexts": [{"rank": 1, "score": 0.72548664, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 9, "text_snippet": "22) that adapts language models to the retriever. We use a training objective which prefers retrieving documents that improve language model perplexity, while treating the LM as a frozen, black-box scoring function. Our experiments show tha"}, {"rank": 2, "score": 0.65566236, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 1, "text_snippet": "mechanisms to en- code the retrieved text, REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design can be eas- ily applied to any existing retrieval and language models. Furthermore, we show t"}, {"rank": 3, "score": 0.6519619, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 7, "text_snippet": " retrieved documents are prepended to the input context and fed into the black-box LM to make the final prediction. Because the LM context length limits the number of documents that can be prepended, we also introduce a new ensemble scheme "}, {"rank": 4, "score": 0.649555, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 29, "text_snippet": "triever inREPLUG by using the LM itself to provide supervision about which documents should be retrieved. Inspired by Sachan et al. (2022), our approach can be seen as adjusting the probabilities of the retrieved documents to match the prob"}, {"rank": 5, "score": 0.6453359, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 11, "text_snippet": " model parameters), for both reducing LM perplexity and and im- proving in-context learning performance. We summarize our contributions as follows: •We introduce REPLUG (§3), the first retrieval- augmented language modeling framework for en"}]}
{"case_index": 114, "query": "According to the paper 'corrective retrieval augmented generation', what exact term completes this statement: \"This [BLANK] is implemented to broaden the spectrum of retrieved information, harnessing the expansive and dynamic nature of the web to complement and enrich the initially obtained documents.\"?", "gold": "augmentation", "answer": "The missing term is **“web searches”**.\n\nSo the statement reads: “This **web searches** is implemented to broaden the spectrum of retrieved information, harnessing the expansive and dynamic nature of the web to complement and enrich the initially obtained documents.”", "answer_score": 0.0, "answer_correctness": 0.0, "answer_groundedness": 0.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 0.0, "abstained": false, "should_abstain": false, "notes": "The blank is 'augmentation' per Context 1 ('This augmentation is implemented...'). The model answered 'web searches', which is incorrect and not supported by the quoted sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.583, "llm_ms": 1535.799, "top_contexts": [{"rank": 1, "score": 0.6721319, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 12, "text_snippet": " are integrated as a strategic extension, since retrieval from static and limited corpora can only return sub-optimal documents in terms of scope and diversity. This augmentation is implemented to broaden the spectrum of retrieved informati"}, {"rank": 2, "score": 0.6410848, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 10, "text_snippet": " should not have been equally referred to and involved in RAG. On account of the above issues, this paper particularly studies the scenarios where the retriever returns inaccurate results. A method named Corrective Retrieval- Augmented Gene"}, {"rank": 3, "score": 0.6236527, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 79, "text_snippet": "ges are provided as input (correct, incorrect and ambiguous content). All the data in the table only represents a rough estimate of the generation phase, the retrieval and data-processing stages are not included. anism, rather than solely f"}, {"rank": 4, "score": 0.5982205, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 84, "text_snippet": "ed to improve the robustness of generation. Essentially, a lightweight retrieval evaluator is to estimate and trigger three knowledge retrieval actions discriminately. With the further leverage of web search and optimized knowledge utilizat"}, {"rank": 5, "score": 0.59081316, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 77, "text_snippet": "e dropped, the generation performance of Self-CRAG dropped more slightly than that of Self-RAG. These results imply the superiority of Self-CRAG over Self-RAG on en- hancing the robustness to retrieval performance. 5.7 Consistent Supplement"}]}
{"case_index": 115, "query": "According to the paper 'from local to global a graph rag approach to query focused summarization', what exact term completes this statement: \"Together, these community summaries provide global [BLANK] and insights over the corpus.\"?", "gold": "descriptions", "answer": "The missing term is **“descriptions”**:\n\n“Together, these community summaries provide global **descriptions** and insights over the corpus.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches the exact wording in Context 3: 'provide global descriptions and insights over the corpus.'", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.3333333333333333, "hit_rank": 3, "matched_by": "doc_id+chunk_id", "retrieval_ms": 11.34, "llm_ms": 1292.982, "top_contexts": [{"rank": 1, "score": 0.6930157, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 44, "text_snippet": "ummaries at one level looking for general themes of interest, then read linked reports at a lower level that provide additional details for each subtopic. Here, however, we focus on their utility as part of a graph-based index used for answ"}, {"rank": 2, "score": 0.68778276, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 29, "text_snippet": "lements (nodes, edges, covariates) that the LLM can summarize in parallel at both indexing time and query time. The “global answer” to a given query is produced using a final round of query-focused summarization over all community summaries"}, {"rank": 3, "score": 0.68528616, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 10, "text_snippet": "cted communities, with summaries at higher levels of the hierarchy recursively incorporating lower-level summaries. Together, these community summaries provide global descriptions and insights over the corpus. Finally, GraphRAG answers quer"}, {"rank": 4, "score": 0.6575172, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 0, "text_snippet": "From Local to Global: A GraphRAG Approach to Query-Focused Summarization Darren Edge1†Ha Trinh1†Newman Cheng2Joshua Bradley2Alex Chao3 Apurva Mody3Steven Truitt2Dasha Metropolitansky1Robert Osazuwa Ness1 Jonathan Larson1 1Microsoft Research"}, {"rank": 5, "score": 0.6473743, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 97, "text_snippet": "omprehensive- ness and diversity of answers, as well as favorable comparisons to a global but graph-free approach using map-reduce source text summarization. For situations requiring many global queries over the same dataset, summaries of r"}]}
{"case_index": 116, "query": "According to the paper 'replug retrieval augmented black box language models', what exact term completes this statement: \"As shown in Figure 2, given an input context, REPLUG first [BLANK] a small set of relevant documents from an external corpus using a retriever (§3.1).\"?", "gold": "retrieves", "answer": "The missing term is **“retrieves”**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'retrieves' and is directly supported by the provided context (Figure 2 sentence).", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.826, "llm_ms": 1155.612, "top_contexts": [{"rank": 1, "score": 0.75776315, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 22, "text_snippet": " module. As shown in Figure 2, given an input context, REPLUG first retrieves a small set of relevant documents from an external corpus using a retriever (§3.1). Then we pass the concate- nation of each retrieved document with the input con"}, {"rank": 2, "score": 0.7391566, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 21, "text_snippet": " (§3.1 Document Retrieval ). Then it prepends each document separately to the input context and ensembles output probabilities from different passes (§3.2 Input Reformulation ). and a training scheme to further adapt the retriever to large "}, {"rank": 3, "score": 0.73337364, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 7, "text_snippet": " retrieved documents are prepended to the input context and fed into the black-box LM to make the final prediction. Because the LM context length limits the number of documents that can be prepended, we also introduce a new ensemble scheme "}, {"rank": 4, "score": 0.72484726, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 6, "text_snippet": "PIs. Internal representations of such models are not exposed and fine-tuning is not supported. In this work, we introduce REPLUG (Retrieve and Plug ), a new retrieval-augmented LM framework where the lan- guage model is viewed as a black bo"}, {"rank": 5, "score": 0.71609604, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 20, "text_snippet": "n improve GPT- 3 performance on open-domain question answering, we approach the problem in a more general setting, including language modeling and understanding tasks. We also pro- pose an ensemble method to incorporate more documents  REPL"}]}
{"case_index": 117, "query": "According to the paper 'replug retrieval augmented black box language models', what exact term completes this statement: \"[BLANK] Large language models (LLMs) such as GPT-3 (Brown et al., 2020a) and Codex (Chen et al., 2021a), have demonstrated impressive performance on a wide range of language tasks.\"?", "gold": "introduction", "answer": "The exact term is **“Large”** — the sentence begins:\n\n**“Large language models (LLMs) such as GPT-3 (Brown et al., 2020a) and Codex (Chen et al., 2021a), have demonstrated impressive performance on a wide range of language tasks.”**", "answer_score": 0.06, "answer_correctness": 0.0, "answer_groundedness": 0.2, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 0.2, "abstained": false, "should_abstain": false, "notes": "The blank refers to the section heading '1. Introduction' preceding the sentence in the paper. The model answered 'Large', which is part of the sentence but not the missing term. Context 3 clearly shows the correct completion is 'Introduction'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.3333333333333333, "hit_rank": 3, "matched_by": "doc_id+chunk_id", "retrieval_ms": 26.847, "llm_ms": 2118.226, "top_contexts": [{"rank": 1, "score": 0.7179713, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 12, "text_snippet": "gnals, resulting in improved retrieval quality. •Evaluations on language modeling (§6), open-domain QA and MMLU demonstrate that REPLUG can im- prove the performance of various language models such as GPT, OPT and BLOOM, including very larg"}, {"rank": 2, "score": 0.69298524, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 5, "text_snippet": "ting the LM’s parameters, REPLUG treats the language model as a black box and augments it with a frozen or tunable retriever. This black-box assumption makes REPLUG applicable to large LMs (i.e., >100B parameters), which are often served vi"}, {"rank": 3, "score": 0.6821409, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 2, "text_snippet": "mproves the performance of GPT-3 (175B) on language modeling by 6.3%, as well as the perfor- mance of Codex on five-shot MMLU by 5.1%. 1. Introduction Large language models (LLMs) such as GPT-3 (Brown et al., 2020a) and Codex (Chen et al., "}, {"rank": 4, "score": 0.67642856, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 8, "text_snippet": "Black-Box Language Models Figure 1, REPLUG is extremely flexible and can be used with any existing black-box LM and retrieval model. We also introduce REPLUG LSR (REPLUG with LM- Supervised Retrieval), a training scheme that can further imp"}, {"rank": 5, "score": 0.6705456, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 71, "text_snippet": "a black box and augments it with a tuneable retrieval model. Our evaluation shows that REPLUG can be integrated with any existing language model to improve their performance  REPLUG: Retrieval-Augmented Black-Box Language Models on language"}]}
{"case_index": 118, "query": "According to the paper 'raptor recursive abstractive processing for tree organized retrieval', what exact term completes this statement: \"As shown in Figure 1, our system, RAPTOR, clusters chunks of text, generates text summaries of those clusters, and then repeats, [BLANK] a tree from the bottom up.\"?", "gold": "generating", "answer": "The missing term is **“generating”** — as in: “…and then repeats, **generating** a tree from the bottom up.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'generating' and is directly supported by Context 1/5 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 9.252, "llm_ms": 1873.344, "top_contexts": [{"rank": 1, "score": 0.78057796, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 8, "text_snippet": " indexing and retrieval system that uses a tree structure to capture both high-level and low-level details about a text. As shown in Figure 1, our system, RAPTOR, clusters chunks of text, generates text summaries of those clusters, and then"}, {"rank": 2, "score": 0.76654243, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 1, "text_snippet": "rom a retrieval corpus, limiting holistic under- standing of the overall document context. We introduce the novel approach of recursively embedding, clustering, and summarizing chunks of text, constructing a tree with differing levels of su"}, {"rank": 3, "score": 0.7594538, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 24, "text_snippet": "r text chunks, we employ a clustering algorithm. Once clustered, a Language Model is used to summarize the grouped texts. These summarized texts are then re-embedded, and the cycle of embedding, clustering, and summarization continues until"}, {"rank": 4, "score": 0.7423116, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 9, "text_snippet": "uestions at different levels. 1arXiv:2401.18059v1 [cs.CL] 31 Jan 2024  Published as a conference paper at ICLR 2024 2 3 4 5  1 1  2  3  3  4 5  5 6  8  7 Index #8  Text: summary of  nodes 2 and 3  Child Nodes: 2, 3  Text Embedding  Text chu"}, {"rank": 5, "score": 0.7225225, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 10, "text_snippet": "rsively clusters chunks of text based on their vector embeddings and generates text summaries of those clusters, constructing a tree from the bottom up. Nodes clustered together are siblings; a parent node contains the text summary of that "}]}
{"case_index": 119, "query": "According to the paper 'leveraging passage retrieval with generative models for open domain question answering', what exact term completes this statement: \"[BLANK] methods were pro- posed to tackle the setting where no gold spans are given to the system, but only the correct answer.\"?", "gold": "different", "answer": "**“Different”** methods were proposed to tackle the setting where no gold spans are given to the system, but only the correct answer.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "The blank is correctly filled with 'Different', exactly matching the provided context and expected answer.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 11.835, "llm_ms": 1336.458, "top_contexts": [{"rank": 1, "score": 0.6791339, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 11, "text_snippet": " Different methods were pro- posed to tackle the setting where no gold spans are given to the system, but only the correct answer. Clark and Gardner (2018) proposed to use a global normalization over all the span corresponding to the answer"}, {"rank": 2, "score": 0.65920377, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 6, "text_snippet": "tech- niques have been considered, either using sparse representations based on TF/IDF or using dense embeddings (Guu et al., 2020; Karpukhin et al., 2020). The models which extract the answers are often based on contextualized word represe"}, {"rank": 3, "score": 0.648139, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 15, "text_snippet": " that answers do not correspond to spans in support documents, thus requiring ab- stractive models. Raffel et al. (2019) showed that generative models are competitive for reading com- prehension tasks such as SQuAD (Rajpurkar et al., 2016),"}, {"rank": 4, "score": 0.6416157, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}, {"rank": 5, "score": 0.6265983, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 14, "text_snippet": " be trained using weak supervision in the form of question-answer pairs (Karpukhin et al., 2020), or pretrained using a cloze task and ﬁnetuned end-to- end (Guu et al., 2020; Lee et al., 2019). Generative question answering was mostly consi"}]}
{"case_index": 120, "query": "According to the paper 'refrag rethinking rag based decoding', what exact term completes this statement: \"For clarity, we focus on a single turn of [BLANK] and retrieval in this section.\"?", "gold": "question", "answer": "The missing term is **“question”**, i.e., the full statement is:\n\n“For clarity, we focus on a single turn of **question** and retrieval in this section.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context 4 explicitly contains the sentence with 'question' completing the blank; model answer matches exactly.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.25, "hit_rank": 4, "matched_by": "doc_id+chunk_id", "retrieval_ms": 11.212, "llm_ms": 1605.475, "top_contexts": [{"rank": 1, "score": 0.63503855, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 12, "text_snippet": "is information is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-a"}, {"rank": 2, "score": 0.6219951, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 15, "text_snippet": "igure 1) while preserving the autoregressive nature of the decoder, thereby supporting multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight reinforcement learning (RL) policy that sel"}, {"rank": 3, "score": 0.618161, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 95, "text_snippet": "oducedREFRAG, a novel and efficient decoding framework tailored for RAG applications. By leveraging the inherent sparsity and block-diagonal attention patterns present in RAG contexts,REFRAG compresses, senses, and expands context represent"}, {"rank": 4, "score": 0.616277, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 17, "text_snippet": "FRAG achieves better performance than LLaMA without incurring higher latency in the downstream applications. 2 Model Architecture We denote the decoder model as Mdecand the encoder model as Menc. Given an input with Ttokens x1, x2, . . . , "}, {"rank": 5, "score": 0.60519, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 3, "text_snippet": " on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30 .85×the time-t"}]}
{"case_index": 121, "query": "According to the paper 'from local to global a graph rag approach to query focused summarization', what exact term completes this statement: \"The challenge, however, arises when the volume of data requires a RAG approach, since vector RAG approaches are unable to support [BLANK] over an entire corpus.\"?", "gold": "sensemaking", "answer": "The missing term is **“sensemaking”** — i.e., “vector RAG approaches are unable to support **sensemaking** over an entire corpus.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer correctly fills the blank with 'sensemaking', explicitly supported by Context 2 and echoed in Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 12.307, "llm_ms": 1555.849, "top_contexts": [{"rank": 1, "score": 0.74203354, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 16, "text_snippet": " may use alternative retrieval mechanisms, we collectively refer to the family of conventional approaches as vector RAG . GraphRAG contrasts with vector RAG in its ability to answer queries that require global sensemaking over the entire da"}, {"rank": 2, "score": 0.72441125, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 8, "text_snippet": "nce analysis (Ranade and Joshi, 2023). Given a sensemaking query and a text with an implicit and interconnected set of concepts, an LLM can generate a summary that answers the query. The challenge, however, arises when the volume of data re"}, {"rank": 3, "score": 0.7061901, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 29, "text_snippet": "lements (nodes, edges, covariates) that the LLM can summarize in parallel at both indexing time and query time. The “global answer” to a given query is produced using a final round of query-focused summarization over all community summaries"}, {"rank": 4, "score": 0.70099926, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 96, "text_snippet": "tigate these downstream risks for questions of a global nature, which might otherwise be answered by samples of retrieved facts falsely presented as global summaries. 7 Conclusion We have presented GraphRAG, a RAG approach that combines kno"}, {"rank": 5, "score": 0.70020527, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 10, "text_snippet": "cted communities, with summaries at higher levels of the hierarchy recursively incorporating lower-level summaries. Together, these community summaries provide global descriptions and insights over the corpus. Finally, GraphRAG answers quer"}]}
{"case_index": 122, "query": "According to the paper 'self rag learning to retrieve generate and critique through self reflection', what exact term completes this statement: \"Step 3: Critique outputs and select best [BLANK] in a 16th-century novel Las Sergas de Esplandián.\"?", "gold": "segmentorigins", "answer": "The missing term is **“one”** — i.e., “choose best **one** (Step 3) …”.", "answer_score": 0.27, "answer_correctness": 0.0, "answer_groundedness": 0.9, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 0.9, "abstained": false, "should_abstain": false, "notes": "Expected term is 'segmentorigins', but the answer says 'one'. Context supports 'choose best one (Step 3)' wording, but that does not match the question’s required exact term.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 6.643, "llm_ms": 1893.022, "top_contexts": [{"rank": 1, "score": 0.5601586, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 8, "text_snippet": "on demand (Step 1). Subsequently, SELF-RAGconcurrently processes multiple retrieved passages, evaluating their relevance and then generating corresponding task outputs (Step 2). It then generates critique tokens to criticize its own output "}, {"rank": 2, "score": 0.5576184, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 34, "text_snippet": "l predicts the next output segment, as it does in a standard LM. If retrieval is needed, the model generates: a critique token to evaluate the retrieved passage’s relevance, the next response segment, and a critique token to evaluate if the"}, {"rank": 3, "score": 0.5495433, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 27, "text_snippet": "n (reasoning path consistency) and without retrieval. Recent work on LLM refinement (Dhuliawala et al., 2023; Madaan et al., 2023; Paul et al., 2023) prompts a model to generate task output, natural language feedback and refined task output"}, {"rank": 4, "score": 0.5431095, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 15, "text_snippet": "st a critic model during training, reducing overhead. The critic model, in part, is supervised on a dataset of input, output, and corresponding reflection tokens collected by prompting a propriety LM (i.e., GPT-4; OpenAI 2023). While we dra"}, {"rank": 5, "score": 0.53377086, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 7, "text_snippet": "rmittent special tokens (i.e., reflection tokens ). Reflection tokens are categorized into retrieval andcritique tokens to indicate the need for retrieval and its generation quality respectively (Figure 1 right). In particular, given an inp"}]}
{"case_index": 123, "query": "According to the paper 'from local to global a graph rag approach to query focused summarization', what exact term completes this statement: \"Given a question, each community summary is used to generate a partial response, before all partial responses are again [BLANK] in a final response to the user.\"?", "gold": "summarized", "answer": "The missing term is **“summarized”**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context 4 explicitly states: 'all partial responses are again summarized in a final response'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.25, "hit_rank": 4, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.278, "llm_ms": 1406.967, "top_contexts": [{"rank": 1, "score": 0.684439, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 47, "text_snippet": "t within the context window. 3.1.6 Community Summaries →Community Answers →Global Answer Given a user query, the community summaries generated in the previous step can be used to generate a final answer in a multi-stage process. The hierarc"}, {"rank": 2, "score": 0.6792383, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 10, "text_snippet": "cted communities, with summaries at higher levels of the hierarchy recursively incorporating lower-level summaries. Together, these community summaries provide global descriptions and insights over the corpus. Finally, GraphRAG answers quer"}, {"rank": 3, "score": 0.65710735, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 29, "text_snippet": "lements (nodes, edges, covariates) that the LLM can summarize in parallel at both indexing time and query time. The “global answer” to a given query is produced using a final round of query-focused summarization over all community summaries"}, {"rank": 4, "score": 0.64831316, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 3, "text_snippet": "wo stages: first, to derive an entity knowledge graph from the source documents, then to pre- generate community summaries for all groups of closely related entities. Given a question, each community summary is used to generate a partial re"}, {"rank": 5, "score": 0.642279, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 48, "text_snippet": "mmunity structure offers the best balance of summary detail and scope for general sensemaking questions (evaluated in section 4). For a given community level, the global answer to any user query is generated as follows: •Prepare community s"}]}
{"case_index": 124, "query": "According to the paper 'from local to global a graph rag approach to query focused summarization', what exact term completes this statement: \"In the canonical RAG setup, the system has access to a large external corpus of text records and retrieves a subset of records that are [BLANK] relevant to the query and collectively small enough to fit into the context window of the LLM.\"?", "gold": "individually", "answer": "The missing term is **“individually”** (i.e., “records that are **individually** relevant to the query…”).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'individually' and is directly supported by Context 2 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 12.884, "llm_ms": 1507.075, "top_contexts": [{"rank": 1, "score": 0.66390425, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 29, "text_snippet": "lements (nodes, edges, covariates) that the LLM can summarize in parallel at both indexing time and query time. The “global answer” to a given query is produced using a final round of query-focused summarization over all community summaries"}, {"rank": 2, "score": 0.63307166, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 5, "text_snippet": "l., 2024; Liu et al., 2023). In the canonical RAG setup, the system has access to a large external corpus of text records and retrieves a subset of records that are individually relevant to the query and collectively small enough to fit int"}, {"rank": 3, "score": 0.6305927, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 15, "text_snippet": "e LLM’s context window. In canonical RAG approaches, the retrieval process returns a set number of records that are seman- tically similar to the query and the generated answer uses only the information in those retrieved records. A common "}, {"rank": 4, "score": 0.6077379, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 27, "text_snippet": "ns (Wang et al., 2023a; Zheng et al., 2024). Some prior work proposes criteria for having LLMs quantify the quality of 3  Source Documents Text Chunkstext extraction and chunking Entities & Relationshipsdomain-tailored summarization Knowled"}, {"rank": 5, "score": 0.6056894, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 0, "text_snippet": "From Local to Global: A GraphRAG Approach to Query-Focused Summarization Darren Edge1†Ha Trinh1†Newman Cheng2Joshua Bradley2Alex Chao3 Apurva Mody3Steven Truitt2Dasha Metropolitansky1Robert Osazuwa Ness1 Jonathan Larson1 1Microsoft Research"}]}
{"case_index": 125, "query": "According to the paper 'atlas few shot learning with retrieval augmented language models', what exact term completes this statement: \"In this work we address this gap, and present Atlas, a [BLANK] language model capable of strong few-shot learning, despite having lower parameter counts\"?", "gold": "retrieval-augmented", "answer": "The missing term is **“retrieval augmented”** — i.e., “Atlas, a **retrieval augmented** language model capable of strong few-shot learning, despite having lower parameter counts.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'retrieval-augmented' (paper uses 'retrieval augmented'); directly supported by context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 9.766, "llm_ms": 1582.66, "top_contexts": [{"rank": 1, "score": 0.75329065, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 1, "text_snippet": " Abstract Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter count"}, {"rank": 2, "score": 0.7531444, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 12, "text_snippet": "ntributions: •A thorough study on how to design and train retrieval-augmented language models, with a focus on downstream few-shot learning and sample eﬃciency. •The ﬁndings of this study lead to a retrieval-augmented language model, called"}, {"rank": 3, "score": 0.7260166, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 0, "text_snippet": "Atlas: Few-shot Learning with Retrieval Augmented Language Models Gautier Izacard∗ ∗,♦,♣,♥gizacard@fb.com Patrick Lewis∗,♦plewis@fb.com Maria Lomeli♦marialomeli@fb.com Lucas Hosseini♦hoss@fb.com Fabio Petroni♦fabiopetroni@fb.com Timo Schick"}, {"rank": 4, "score": 0.69710356, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 2, "text_snippet": "ings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including"}, {"rank": 5, "score": 0.69138813, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 9, "text_snippet": " strong few-shot learning, despite having lower parameter counts than other powerful recent few-shot learners. Atlasretrieves relevant documents based on the current context by using a general-purpose dense retriever using a dual-encoder ar"}]}
{"case_index": 126, "query": "According to the paper 'recomp improving retrieval augmented lms with compression and selective augmentation', what exact term completes this statement: \"We evaluate our approach on language modeling task and open domain question [BLANK] task.\"?", "gold": "answering", "answer": "The missing term is **“answering”** — the full statement is: “We evaluate our approach on language modeling task and open domain question **answering** task.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'answering' and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 33.861, "llm_ms": 1485.219, "top_contexts": [{"rank": 1, "score": 0.6592494, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 3, "text_snippet": "n. We evaluate our approach on language modeling task and open domain question answering task. We achieve a compression rate of as low as 6% with minimal loss in performance for both tasks, significantly outperforming the off-the-shelf summ"}, {"rank": 2, "score": 0.649277, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 69, "text_snippet": "ance across a variety of NLP tasks when considered in isolation. Such tasks include open-domain question answering [ 5,29], fact checking [ 56], fact completion [ 48], long-form question answering [ 12], Wikipedia article generation [ 36], "}, {"rank": 3, "score": 0.64205444, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 16, "text_snippet": "ous NLP tasks, including language mod- eling (Min et al., 2022; Borgeaud et al., 2022; Khandelwal et al., 2020) and open-domain question answering (Lewis et al., 2020; Izacard et al., 2022b; Hu et al., 2022). Specifi- cally, using the input"}, {"rank": 4, "score": 0.627481, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 58, "text_snippet": "pen-domain QA We report the results on QA tasks in Table 2. Similar to the language modeling task, all retrieval augmentation methods improve performance over no retrieval setting, across three datasets, consistent with previous study on ot"}, {"rank": 5, "score": 0.6212417, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 60, "text_snippet": "ut. Retrieval-Augmentation with Search Engines. Recently, diﬀerent works have proposed to train large language models to interact with a search engine, by generating text queries, and using the retrieved documents as additional context (Nak"}]}
{"case_index": 127, "query": "According to the paper 'corrective retrieval augmented generation', what exact term completes this statement: \"Earlier studies adopt either sparse or dense retrievers at the front end of a pre- trained language model that [BLANK] in response generation.\"?", "gold": "specializes", "answer": "The missing term is **“specializes”**, as in:\n\n“…a pre-trained language model that **specializes** in response generation.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'specializes' and is directly supported by Context 2/3 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.614, "llm_ms": 1507.673, "top_contexts": [{"rank": 1, "score": 0.6437059, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 57, "text_snippet": "nted generators using a second “informed” retriever with access to the output, which the test-time retriever can be distilled from, and Hofstätter et al. (2022) recently proposed a training set ﬁltering/weighting approach to train stronger "}, {"rank": 2, "score": 0.63150764, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 19, "text_snippet": " documents. It usually provides an extra knowledge source from a specific corpus, i.e., Wikipedia, which greatly improves the per- formance of LMs in a variety of tasks, especially in the knowledge-intensive ones. The proposed  methods gene"}, {"rank": 3, "score": 0.6158462, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 20, "text_snippet": "model that specializes in response generation. Despite this, the methods above usually ignore a question, what if the retrieval goes wrong? Since the purpose of introducing a retrieval is to secure that generative LMs can obtain relevant an"}, {"rank": 4, "score": 0.61355394, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 17, "text_snippet": "inal predic- tion. This style of retrieval can be added to both encoder- decoder (Yu, 2022; Izacard et al., 2022b) and decoder-only models (Khandelwal et al., 2020; Borgeaud et al., 2022; Shi et al., 2022; Rubin et al., 2022). For example, "}, {"rank": 5, "score": 0.6133315, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 6, "text_snippet": " The examples show that a low-quality retriever is prone to introducing a substantial amount of irrelevant information, impeding the generators from acquiring accurate knowledge and potentially misleading them. the parametric knowledge they"}]}
{"case_index": 128, "query": "According to the paper 'recomp improving retrieval augmented lms with compression and selective augmentation', what exact term completes this statement: \"As this approach is extractive, we assume the faithfulness criteria is mostly satisfied.3 Abstractive Compressor We train an [BLANK] model encdec θto serve as\"?", "gold": "encoder-decoder", "answer": "The missing term is **“encoder-decoder”**.\n\nSo the sentence is: “Abstractive Compressor We train an **encoder-decoder** model encdec θ to serve as …”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'encoder-decoder' and is directly supported by the provided context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 21.889, "llm_ms": 1743.036, "top_contexts": [{"rank": 1, "score": 0.70610166, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 22, "text_snippet": "y. Yet, using an extreme-scale model as the compressor is not desirable as we want the compressor to be substantially smaller than the LMs. Thus, we perform distillation (Hinton et al., 2015) of extreme-scale LMs to build a lightweight abst"}, {"rank": 2, "score": 0.7042562, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 20, "text_snippet": "er model encθwhich embeds sentence siand the input sequence xinto fixed- dimensional embeddings respectively. Their inner product represents how helpful it would be for the LMMto prepend sito the input xto generate y. The final summary sfro"}, {"rank": 3, "score": 0.68768525, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 21, "text_snippet": "Compressor We train an encoder-decoder model encdec θto serve as an abstractive compressor, which takes the input sequence xand a concatenation of retrieved document set D [d1;d2;...dN]) and output a summary s. Although we do not have human"}, {"rank": 4, "score": 0.68160176, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 23, "text_snippet": "odels in the output should be faithful to the original input, yet the main goal is different. Instead of capturing salient information for humans readers, compressors aim to produce a concise text that are useful for a LM on an end task. In"}, {"rank": 5, "score": 0.67361987, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 2, "text_snippet": "d an abstractive compressor which generates summaries by synthesizing information from multiple documents. Both compressors are trained to improve LMs’ performance on end tasks when the generated summaries are prepended to the LMs’ input, w"}]}
{"case_index": 129, "query": "According to the paper 'replug retrieval augmented black box language models', what exact term completes this statement: \"[BLANK] Models Augmenting language models with relevant information retrieved from various knowledge stores has shown to be effective in improving performance on various NLP tasks, including language mod- eling (Min et al., 2022;\"?", "gold": "retrieval-augmented", "answer": "The missing term is **“Retrieval-augmented”**, i.e., the statement begins:\n\n**“Retrieval-augmented Models Augmenting language models with relevant information retrieved from various knowledge stores…”**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'retrieval-augmented' and is directly supported by Context 1 showing the heading 'Retrieval-augmented Models'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.807, "llm_ms": 1572.244, "top_contexts": [{"rank": 1, "score": 0.7611137, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 15, "text_snippet": "anguage models makes this approach infeasible. To ad- dress the challenges posed by large language models, we investigate retrieval-augmentation in the black-box setting , where users only have access to the model predictions and cannot acc"}, {"rank": 2, "score": 0.7379103, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 71, "text_snippet": "a black box and augments it with a tuneable retrieval model. Our evaluation shows that REPLUG can be integrated with any existing language model to improve their performance  REPLUG: Retrieval-Augmented Black-Box Language Models on language"}, {"rank": 3, "score": 0.7089745, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 6, "text_snippet": "PIs. Internal representations of such models are not exposed and fine-tuning is not supported. In this work, we introduce REPLUG (Retrieve and Plug ), a new retrieval-augmented LM framework where the lan- guage model is viewed as a black bo"}, {"rank": 4, "score": 0.70654964, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 8, "text_snippet": "Black-Box Language Models Figure 1, REPLUG is extremely flexible and can be used with any existing black-box LM and retrieval model. We also introduce REPLUG LSR (REPLUG with LM- Supervised Retrieval), a training scheme that can further imp"}, {"rank": 5, "score": 0.7012987, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 6, "text_snippet": "eval-augmented architecture. These models employ a non-parametric memory, e.g. a neural retriever over a large, external, potentially non-static knowledge source to enhance a parametric language model. In addition to their memorisation abil"}]}
{"case_index": 130, "query": "According to the paper 'retrieval augmented generation for knowledge intensive nlp tasks', what exact term completes this statement: \"In the following, we formally introduce both models and then describe the pηandpθ[BLANK], as well as the training and\"?", "gold": "components", "answer": "The missing term is **“components”**, i.e.:\n\n“…describe the **pη and pθ components**, as well as the training and …”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'components' and is directly supported by the provided context quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 15.035, "llm_ms": 1378.735, "top_contexts": [{"rank": 1, "score": 0.6004991, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 17, "text_snippet": "rator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a distribution over generated text. In one approach, RAG-Sequence , the mo"}, {"rank": 2, "score": 0.59777033, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 18, "text_snippet": "hen describe the pηandpθcomponents, as well as the training and decoding procedure. 2.1 Models RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate the complete sequence . Technically, it treats the retriev"}, {"rank": 3, "score": 0.5914796, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 15, "text_snippet": " update the models’ knowledge as the world changes.1 2 Methods We explore RAG models, which use the input sequence xto retrieve text documents zand use them as additional context when generating the target sequence y. As shown in Figure 1, "}, {"rank": 4, "score": 0.5826851, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 17, "text_snippet": "chmark in Figure 2. As many natural language processing tasks require knowledge , our goal is to enhance standard text-to-text models with retrieval, which, as we hypothesise in the introduction, may be crucial to endow models with few-shot"}, {"rank": 5, "score": 0.5762529, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 9, "text_snippet": "y to the “workhorse of NLP,” i.e. sequence-to-sequence (seq2seq) models. We endow pre-trained, parametric-memory generation models with a non-parametric memory through a general-purpose ﬁne-tuning approach which we refer to as retrieval-aug"}]}
{"case_index": 131, "query": "According to the paper 'refrag rethinking rag based decoding', what exact term completes this statement: \"These projected chunk [BLANK] are then fed to the decoder model along with the token embeddings for the question to generate the answer y∼ M dec({e1, .\"?", "gold": "embeddings", "answer": "The missing term is **“embeddings”** — i.e., “These projected chunk **embeddings** are then fed to the decoder model along with the token embeddings for the question to generate the answer …”.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'embeddings' and is directly supported by context stating chunk embeddings are fed to the decoder along with question token embeddings.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 10.908, "llm_ms": 1642.178, "top_contexts": [{"rank": 1, "score": 0.7345496, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 21, "text_snippet": "uery  Encoder Figure 1The main design ofREFRAG. The input context is chunked and processed by the light-weight encoder to produce chunk embeddings, which are precomputable for efficient reuse. A light-weight RL policy decide few chunks to e"}, {"rank": 2, "score": 0.69241834, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 14, "text_snippet": "ortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now "}, {"rank": 3, "score": 0.6848024, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 15, "text_snippet": "igure 1) while preserving the autoregressive nature of the decoder, thereby supporting multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight reinforcement learning (RL) policy that sel"}, {"rank": 4, "score": 0.6755229, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 13, "text_snippet": "nd memory usage during decoding, allwithout requiring modificationsto the LLM architecture or introducing new decoder parameters. REFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved passag"}, {"rank": 5, "score": 0.6672034, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 18, "text_snippet": "f question and retrieval in this section. Model overview.Figure 1 shows the main architecture ofREFRAG. This model consists of a decoder-only foundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Rober"}]}
{"case_index": 132, "query": "According to the paper 'corrective retrieval augmented generation', what exact term completes this statement: \"Despite this, the methods above usually ignore a [BLANK], what if the retrieval goes wrong?\"?", "gold": "question", "answer": "The missing term is **“question”** — the paper states: “Despite this, the methods above usually ignore a **question**, what if the retrieval goes wrong?”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'question' and is directly supported by Context 3 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.3333333333333333, "hit_rank": 3, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.172, "llm_ms": 1400.527, "top_contexts": [{"rank": 1, "score": 0.64403784, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 1, "text_snippet": "evitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Al- though retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heav"}, {"rank": 2, "score": 0.64308715, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 10, "text_snippet": " should not have been equally referred to and involved in RAG. On account of the above issues, this paper particularly studies the scenarios where the retriever returns inaccurate results. A method named Corrective Retrieval- Augmented Gene"}, {"rank": 3, "score": 0.6199116, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 20, "text_snippet": "model that specializes in response generation. Despite this, the methods above usually ignore a question, what if the retrieval goes wrong? Since the purpose of introducing a retrieval is to secure that generative LMs can obtain relevant an"}, {"rank": 4, "score": 0.6129068, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 83, "text_snippet": " in Table 6. The findings indicate that the self-correction mecha- nism incurs only modest computational overheadwhile significantly enhancing performance, thereby validating its lightweight nature. 6 Conclusion & Limitation This paper stud"}, {"rank": 5, "score": 0.5977485, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 6, "text_snippet": " The examples show that a low-quality retriever is prone to introducing a substantial amount of irrelevant information, impeding the generators from acquiring accurate knowledge and potentially misleading them. the parametric knowledge they"}]}
{"case_index": 133, "query": "According to the paper 'self rag learning to retrieve generate and critique through self reflection', what exact term completes this statement: \"Experiments show that SELF- RAG(7B and 13B parameters) significantly outperforms state-of-the-art LLMs and [BLANK] models on a diverse set of tasks.\"?", "gold": "retrieval-augmented", "answer": "The missing term is **“retrieval-augmented”**, i.e., the full statement is:\n\n“Experiments show that SELF-RAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs and **retrieval-augmented** models on a diverse set of tasks.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'retrieval-augmented' and is directly supported by Context 2.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.575, "llm_ms": 1771.058, "top_contexts": [{"rank": 1, "score": 0.7755081, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 96, "text_snippet": "ains an LM to learn to retrieve, generate, and critique text passages and its own generation by predicting the next tokens from its original vocabulary as well as newly added special tokens, called reflection tokens. SELF-RAGfurther enables"}, {"rank": 2, "score": 0.7737678, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 3, "text_snippet": "e phase, enabling it to tailor its behavior to diverse task requirements. Experiments show that SELF- RAG(7B and 13B parameters) significantly outperforms state-of-the-art LLMs and retrieval-augmented models on a diverse set of tasks. Speci"}, {"rank": 3, "score": 0.72509456, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 27, "text_snippet": "n (reasoning path consistency) and without retrieval. Recent work on LLM refinement (Dhuliawala et al., 2023; Madaan et al., 2023; Paul et al., 2023) prompts a model to generate task output, natural language feedback and refined task output"}, {"rank": 4, "score": 0.71302956, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 2, "text_snippet": "ew framework called Self-Reflective Retrieval-Augmented Gen- eration ( SELF-RAG)that enhances an LM’s quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passage"}, {"rank": 5, "score": 0.6951685, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 6, "text_snippet": "y trained to leverage and follow facts from provided passages. This work introduces Self-Reflective Retrieval-augmented Generation ( SELF-RAG)to improve an LLM’s generation quality, including its factual accuracy without hurting its versati"}]}
{"case_index": 134, "query": "According to the paper 'leveraging passage retrieval with generative models for open domain question answering', what exact term completes this statement: \"Without relying on external knowledge, this method obtained compet- itive results on several [BLANK].\"?", "gold": "benchmarks", "answer": "The missing term is **“benchmarks”**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context 1 explicitly states 'competitive results on several benchmarks,' matching the expected term.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 34.734, "llm_ms": 1519.242, "top_contexts": [{"rank": 1, "score": 0.7024018, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 3, "text_snippet": "s et al. (2020) introduced a generative model for open domain question answering. Without relying on external knowledge, this method obtained compet- itive results on several benchmarks. However, it requires models containing billions of pa"}, {"rank": 2, "score": 0.687862, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 0, "text_snippet": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering Gautier Izacard1,2,3Edouard Grave1 1Facebook AI Research, Paris 2ENS, PSL University, Paris 3Inria, Paris gizacard|egrave@fb.com Abstract Generative mode"}, {"rank": 3, "score": 0.68420696, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 13, "text_snippet": "sive tasks —tasks that humans could not reasonably be expected to perform without access to an external knowledge source. Our RAG models achieve state-of-the-art results on open Natural Questions [ 29], WebQuestions [ 3] and CuratedTrec [ 2"}, {"rank": 4, "score": 0.6674733, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 31, "text_snippet": "tion answering. We observe that while conceptu- ally simple, this method outperforms existing work on the NaturalQuestion and TriviaQA benchmarks. In particular, generative models seem to perform well when evidence from multiple passages ne"}, {"rank": 5, "score": 0.66516304, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 9, "text_snippet": "rticular, we show that the performance of our method signiﬁcantly improves when the number of retrieved passages increases. We believe that this is evidence that generative mod- els are good at combining evidence from multiple passages, com"}]}
{"case_index": 135, "query": "According to the paper 'leveraging passage retrieval with generative models for open domain question answering', what exact term completes this statement: \"This approach scales well with the number of retrieved passages, as the [BLANK] keeps improving when retrieving up to one hundred passages.\"?", "gold": "performance", "answer": "The missing term is **“performance”** — i.e., “the **performance** keeps improving when retrieving up to one hundred passages.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'performance' and is directly supported by Context 4.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 9.406, "llm_ms": 1554.666, "top_contexts": [{"rank": 1, "score": 0.71070033, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}, {"rank": 2, "score": 0.70742124, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 0, "text_snippet": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering Gautier Izacard1,2,3Edouard Grave1 1Facebook AI Research, Paris 2ENS, PSL University, Paris 3Inria, Paris gizacard|egrave@fb.com Abstract Generative mode"}, {"rank": 3, "score": 0.70499176, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 31, "text_snippet": "tion answering. We observe that while conceptu- ally simple, this method outperforms existing work on the NaturalQuestion and TriviaQA benchmarks. In particular, generative models seem to perform well when evidence from multiple passages ne"}, {"rank": 4, "score": 0.6957909, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 9, "text_snippet": "rticular, we show that the performance of our method signiﬁcantly improves when the number of retrieved passages increases. We believe that this is evidence that generative mod- els are good at combining evidence from multiple passages, com"}, {"rank": 5, "score": 0.69560957, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 16, "text_snippet": "0) and Lewis et al. (2020) in- troduced retrieval augmented generative models for open domain question answering. Our approach differs from these works by how the generative model processes the retrieved passages. This al- lows to scale to "}]}
{"case_index": 136, "query": "According to the paper 'corrective retrieval augmented generation', what exact term completes this statement: \"A [BLANK] retrieval evaluator is designed to assess the overall quality of retrieved documents for a query.\"?", "gold": "lightweight", "answer": "The missing term is **“lightweight”** — i.e., “A **lightweight** retrieval evaluator is designed to assess the overall quality of retrieved documents for a query.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer correctly fills the blank with 'lightweight', which is explicitly stated in the provided context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 21.635, "llm_ms": 1358.001, "top_contexts": [{"rank": 1, "score": 0.6579839, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 2, "text_snippet": "he robustness of generation. Specifically, a lightweight retrieval evaluator is designed to assess the overall quality of retrieved documents for a query, returning a confidence degree based on which different knowledge retrieval ac- tions "}, {"rank": 2, "score": 0.6537626, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 10, "text_snippet": " should not have been equally referred to and involved in RAG. On account of the above issues, this paper particularly studies the scenarios where the retriever returns inaccurate results. A method named Corrective Retrieval- Augmented Gene"}, {"rank": 3, "score": 0.6140404, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 28, "text_snippet": "sition, filter, and recomposition (Section 4.4). If the action Incorrect is triggered, the retrieved documents will be discarded. Instead, web searches are resorted to and regarded as complementary knowledge sources for corrections (Section"}, {"rank": 4, "score": 0.57334185, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 31, "text_snippet": "neratorkex+xkex+ Figure 2: An overview of the proposed CRAG at inference. A retrieval evaluator is constructed to evaluate the relevance of the retrieved documents to the input, and estimate a confidence degree based on which different know"}, {"rank": 5, "score": 0.56511116, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 84, "text_snippet": "ed to improve the robustness of generation. Essentially, a lightweight retrieval evaluator is to estimate and trigger three knowledge retrieval actions discriminately. With the further leverage of web search and optimized knowledge utilizat"}]}
{"case_index": 137, "query": "According to the paper 'replug retrieval augmented black box language models', what exact term completes this statement: \"Background and Related Work Black-box Language Models Large language models (i.e., >100B), such as GPT-3 (Brown et al., 2020a), Codex (Chen et al., 2021a), and Yuan 1.0 (Wu et al., 2021), are not [BLANK] due to commercial consi\"?", "gold": "open-sourced", "answer": "The missing term is **“open-sourced”**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'open-sourced' and is directly supported by the provided context snippet.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.407, "llm_ms": 1608.062, "top_contexts": [{"rank": 1, "score": 0.6998209, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 12, "text_snippet": "gnals, resulting in improved retrieval quality. •Evaluations on language modeling (§6), open-domain QA and MMLU demonstrate that REPLUG can im- prove the performance of various language models such as GPT, OPT and BLOOM, including very larg"}, {"rank": 2, "score": 0.6944915, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 13, "text_snippet": " (Wu et al., 2021), are not open-sourced due to commercial considerations and are only available as black-box APIs, through which users can send queries and receive responses. On the other hand, even open sourced language models such as OPT"}, {"rank": 3, "score": 0.6796679, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 5, "text_snippet": "ting the LM’s parameters, REPLUG treats the language model as a black box and augments it with a frozen or tunable retriever. This black-box assumption makes REPLUG applicable to large LMs (i.e., >100B parameters), which are often served vi"}, {"rank": 4, "score": 0.6742963, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 14, "text_snippet": "5k each (Younes Belkda, 2022)), making them inac- cessible to researchers and developers with limited re- sources. Traditionally, retrieval-augmented model frame- works (Khandelwal et al., 2020; Borgeaud et al., 2022; Yu, 2022; Izacard et a"}, {"rank": 5, "score": 0.61102045, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 8, "text_snippet": "Black-Box Language Models Figure 1, REPLUG is extremely flexible and can be used with any existing black-box LM and retrieval model. We also introduce REPLUG LSR (REPLUG with LM- Supervised Retrieval), a training scheme that can further imp"}]}
{"case_index": 138, "query": "According to the paper 'replug retrieval augmented black box language models', what exact term completes this statement: \"Therefore, com- pared with the method of prepending all the retrieved docu- REPLUG: [BLANK]\"?", "gold": "retrieval-a", "answer": "The missing term is:\n\n**LSR (REPLUG with LM-Supervised Retrieval)**", "answer_score": 0.06, "answer_correctness": 0.0, "answer_groundedness": 0.2, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 0.2, "abstained": false, "should_abstain": false, "notes": "Expected blank is 'retrieval-a' (likely start of 'retrieval-augmented'). Model answered 'LSR', which is unrelated to the quoted statement. Context mentions LSR but does not support it as the completion of that specific fragment.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 12.827, "llm_ms": 1520.191, "top_contexts": [{"rank": 1, "score": 0.76036245, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 7, "text_snippet": " retrieved documents are prepended to the input context and fed into the black-box LM to make the final prediction. Because the LM context length limits the number of documents that can be prepended, we also introduce a new ensemble scheme "}, {"rank": 2, "score": 0.7508497, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 6, "text_snippet": "PIs. Internal representations of such models are not exposed and fine-tuning is not supported. In this work, we introduce REPLUG (Retrieve and Plug ), a new retrieval-augmented LM framework where the lan- guage model is viewed as a black bo"}, {"rank": 3, "score": 0.7486046, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 1, "text_snippet": "mechanisms to en- code the retrieved text, REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design can be eas- ily applied to any existing retrieval and language models. Furthermore, we show t"}, {"rank": 4, "score": 0.74448293, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 71, "text_snippet": "a black box and augments it with a tuneable retrieval model. Our evaluation shows that REPLUG can be integrated with any existing language model to improve their performance  REPLUG: Retrieval-Augmented Black-Box Language Models on language"}, {"rank": 5, "score": 0.74382144, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 8, "text_snippet": "Black-Box Language Models Figure 1, REPLUG is extremely flexible and can be used with any existing black-box LM and retrieval model. We also introduce REPLUG LSR (REPLUG with LM- Supervised Retrieval), a training scheme that can further imp"}]}
{"case_index": 139, "query": "According to the paper 'refrag rethinking rag based decoding', what exact term completes this statement: \"As a result, LLM inference throughput degrades with larger contexts, limiting their [BLANK] in scenarios demanding high throughput and low latency, such as web-scale discovery.\"?", "gold": "applicability", "answer": "The missing term is **“applicability”**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'applicability' and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 9.391, "llm_ms": 1495.219, "top_contexts": [{"rank": 1, "score": 0.74170166, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 8, "text_snippet": "As a result, LLM inference throughput degrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low latency, such as web-scale discovery. Therefore, developing novel model architectures that opti"}, {"rank": 2, "score": 0.7283695, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 3, "text_snippet": " on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30 .85×the time-t"}, {"rank": 3, "score": 0.697896, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 10, "text_snippet": "G-based applications, such as web-scale search, with the goal of improving inference latency, specifically, the TTFT. We argue that specialized techniques exploiting the unique structure and sparsity inherent in RAG contexts can substantial"}, {"rank": 4, "score": 0.69354624, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 12, "text_snippet": "is information is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-a"}, {"rank": 5, "score": 0.68933535, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 1, "text_snippet": "AG). However, processing long-context inputs introduces significant system latency and demands substantial memory for the key-value cache, resulting in reduced throughput and a fundamental trade-off between knowledge enrichment and system e"}]}
{"case_index": 140, "query": "According to the paper 'recomp improving retrieval augmented lms with compression and selective augmentation', what exact term completes this statement: \"odels in the output should be faithful to the original input, yet the main goal is [BLANK].\"?", "gold": "different", "answer": "The blank is completed by:\n\n**“different.”**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context 2 explicitly states: '...yet the main goal is different.' The model answer matches the expected term and is fully supported by the provided context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.85, "llm_ms": 1569.101, "top_contexts": [{"rank": 1, "score": 0.588121, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 22, "text_snippet": "y. Yet, using an extreme-scale model as the compressor is not desirable as we want the compressor to be substantially smaller than the LMs. Thus, we perform distillation (Hinton et al., 2015) of extreme-scale LMs to build a lightweight abst"}, {"rank": 2, "score": 0.5672456, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 23, "text_snippet": "odels in the output should be faithful to the original input, yet the main goal is different. Instead of capturing salient information for humans readers, compressors aim to produce a concise text that are useful for a LM on an end task. In"}, {"rank": 3, "score": 0.5329191, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 75, "text_snippet": "abstractive compressors were less faithful compared to the original GPT-3.5, while improving comprehensiveness. The effectiveness of summarization also depends on the datasets – summaries from both mod- els were the most faithful for TQA an"}, {"rank": 4, "score": 0.529796, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 45, "text_snippet": " with minimal information loss. Ideally, this approach should outperform random selection, which is indeed observed in figure 3. The RL-based selective compression policy consistently achieves superior performance across varying compression"}, {"rank": 5, "score": 0.5283917, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 6, "text_snippet": "further confuse LMs with irrelevant information, degrading model performances (Mallen et al., 2022; Shi et al., 2023a). To overcome such limitations, we propose RECOMP (Retrieve, Com press, Prepend), an inter- mediate step for RALMs which c"}]}
{"case_index": 141, "query": "According to the paper 'atlas few shot learning with retrieval augmented language models', what exact term completes this statement: \"The ﬁrst loss that we consider is based on the attention scores of the language model, and is heavily [BLANK] by Izacard & Grave (2021).\"?", "gold": "inspired", "answer": "The missing term is **“inspired”** (as in: “heavily inspired by Izacard & Grave (2021)”).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'inspired' and is directly supported by Context 1/4 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 18.449, "llm_ms": 2643.081, "top_contexts": [{"rank": 1, "score": 0.6664118, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 25, "text_snippet": "n Distillation (ADist). The ﬁrst loss that we consider is based on the attention scores of the language model, and is heavily inspired by Izacard & Grave (2021). The main idea is that the cross-attention scores between the input documents a"}, {"rank": 2, "score": 0.62912333, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 23, "text_snippet": "ent loss functions to train the retriever jointly with the language model. We consider loss functions that leverage the language model to provide supervisory signal to train the retriever. In other words, if the language model ﬁnds a docume"}, {"rank": 3, "score": 0.58826613, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 0, "text_snippet": "Atlas: Few-shot Learning with Retrieval Augmented Language Models Gautier Izacard∗ ∗,♦,♣,♥gizacard@fb.com Patrick Lewis∗,♦plewis@fb.com Maria Lomeli♦marialomeli@fb.com Lucas Hosseini♦hoss@fb.com Fabio Petroni♦fabiopetroni@fb.com Timo Schick"}, {"rank": 4, "score": 0.58553743, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 24, "text_snippet": "erest, without relying on document annotations. For example, in the case of fact checking, a model only requires pairs of claims and corresponding verdicts but no documents containing the evidence to back up the verdict. In practice, we can"}, {"rank": 5, "score": 0.5736065, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 12, "text_snippet": "ntributions: •A thorough study on how to design and train retrieval-augmented language models, with a focus on downstream few-shot learning and sample eﬃciency. •The ﬁndings of this study lead to a retrieval-augmented language model, called"}]}
{"case_index": 142, "query": "According to the paper 'atlas few shot learning with retrieval augmented language models', what exact term completes this statement: \"1 [BLANK] Large language models (LLMs) are impressive few-shot learners (Brown et al., 2020; Rae et al., 2021; Hoﬀmann et al., 2022; Chowdhery et al., 2022).\"?", "gold": "introduction", "answer": "The missing term is **“Introduction”** (i.e., “**1 Introduction** Large language models (LLMs) are impressive few-shot learners …”).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term ('introduction') and is directly supported by Context 1 showing '1 Introduction'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 17.154, "llm_ms": 1158.348, "top_contexts": [{"rank": 1, "score": 0.7540444, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 3, "text_snippet": "xamples, outperforming a 540B parameters model by 3% despite having 50x fewer parameters. 1 Introduction Large language models (LLMs) are impressive few-shot learners (Brown et al., 2020; Rae et al., 2021; Hoﬀmann et al., 2022; Chowdhery et"}, {"rank": 2, "score": 0.7303988, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 62, "text_snippet": "arge Language models. Providing language models with natural language descriptions of tasks, as proposed by Radford et al. (2019) has led to signiﬁcant developments in few-shot learning. GPT-3 (Brown et al., 2020) demonstrated the ability o"}, {"rank": 3, "score": 0.7096732, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 0, "text_snippet": "Atlas: Few-shot Learning with Retrieval Augmented Language Models Gautier Izacard∗ ∗,♦,♣,♥gizacard@fb.com Patrick Lewis∗,♦plewis@fb.com Maria Lomeli♦marialomeli@fb.com Lucas Hosseini♦hoss@fb.com Fabio Petroni♦fabiopetroni@fb.com Timo Schick"}, {"rank": 4, "score": 0.70726097, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 1, "text_snippet": " Abstract Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter count"}, {"rank": 5, "score": 0.69879293, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 12, "text_snippet": "ntributions: •A thorough study on how to design and train retrieval-augmented language models, with a focus on downstream few-shot learning and sample eﬃciency. •The ﬁndings of this study lead to a retrieval-augmented language model, called"}]}
{"case_index": 143, "query": "According to the paper 'raptor recursive abstractive processing for tree organized retrieval', what exact term completes this statement: \"Controlled experiments show that retrieval with recursive summaries offers [BLANK] improv\"?", "gold": "significant", "answer": "The missing term is **“significant”**, as in: “Controlled experiments show that retrieval with recursive summaries offers **significant** improv…”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'significant' and is directly supported by the provided context snippet.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 9.702, "llm_ms": 1563.789, "top_contexts": [{"rank": 1, "score": 0.6820751, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 2, "text_snippet": "hat retrieval with recursive summaries offers significant improvements over tra- ditional retrieval-augmented LMs on several tasks. On question-answering tasks that involve complex, multi-step reasoning, we show state-of-the-art results; fo"}, {"rank": 2, "score": 0.6598342, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 1, "text_snippet": "rom a retrieval corpus, limiting holistic under- standing of the overall document context. We introduce the novel approach of recursively embedding, clustering, and summarizing chunks of text, constructing a tree with differing levels of su"}, {"rank": 3, "score": 0.65104795, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 129, "text_snippet": "esented in table 9. The results from this ablation study clearly indicate an improvement in accuracy when employing RAPTOR’s clustering mechanism over the recency-based tree approach. This finding substantiates our hypothesis that the clust"}, {"rank": 4, "score": 0.6482594, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 74, "text_snippet": "employing recursive clustering and summarization techniques, RAPTOR creates a hierarchical tree structure that is capable of synthesizing information across various sections of the retrieval corpora. During the query phase, RAPTOR leverages"}, {"rank": 5, "score": 0.6463499, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 73, "text_snippet": "an effectively handle a wider range of questions, from higher-order thematic queries to detail-oriented questions. Detailed results for additional stories and an ablation study on layer contributions can be found in Appendix I. 5 C ONCLUSIO"}]}
{"case_index": 144, "query": "According to the paper 'self rag learning to retrieve generate and critique through self reflection', what exact term completes this statement: \"(2023) fine-tune both the retriever and LM on [BLANK] datasets in two step\"?", "gold": "instruction-tuning", "answer": "The missing term is **“instruction-tuning”**, i.e., “fine-tune both the retriever and LM on **instruction-tuning datasets** in two steps.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'instruction-tuning' and is directly supported by the provided context stating 'fine-tune both the retriever and LM on instruction-tuning datasets in two steps.'", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.25, "hit_rank": 4, "matched_by": "doc_id+chunk_id", "retrieval_ms": 11.128, "llm_ms": 2923.561, "top_contexts": [{"rank": 1, "score": 0.6707126, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 22, "text_snippet": " the retriever and LM on instruction-tuning datasets in two steps. While we also train our model on diverse instruction-following datasets, SELF-RAGenables retrieval on demand and selection of the best possible model output via fine-grained"}, {"rank": 2, "score": 0.65076256, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 41, "text_snippet": "lding are unforgettable experience.No RetrievalNo Retrieval Retriever Figure 2: SELF-RAGtraining examples. The left example does not require retrieval while the right one requires retrieval; thus, passages are inserted. More examples are in"}, {"rank": 3, "score": 0.6326674, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 6, "text_snippet": "y trained to leverage and follow facts from provided passages. This work introduces Self-Reflective Retrieval-augmented Generation ( SELF-RAG)to improve an LLM’s generation quality, including its factual accuracy without hurting its versati"}, {"rank": 4, "score": 0.6324665, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 21, "text_snippet": " al., 2023a; Gao et al., 2023). We introduce a method to train an arbitrary LM to learn to use retrieval on-demand for diverse instruction-following queries and introduce controlled generation guided by reflections tokens to further improve"}, {"rank": 5, "score": 0.6236923, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 27, "text_snippet": "n (reasoning path consistency) and without retrieval. Recent work on LLM refinement (Dhuliawala et al., 2023; Madaan et al., 2023; Paul et al., 2023) prompts a model to generate task output, natural language feedback and refined task output"}]}
{"case_index": 145, "query": "According to the paper 'corrective retrieval augmented generation', what exact term completes this statement: \"This is exactly the focus of this paper to improve the [BLANK] of generation.\"?", "gold": "robustness", "answer": "The missing term is **“robustness”** — the paper states it aims “to improve the **robustness** of generation.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'robustness' and is directly supported by the provided context stating CRAG aims to improve the robustness of generation.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 10.334, "llm_ms": 1395.527, "top_contexts": [{"rank": 1, "score": 0.63482726, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 1, "text_snippet": "evitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Al- though retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heav"}, {"rank": 2, "score": 0.6201409, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 0, "text_snippet": "Corrective Retrieval Augmented Generation Shi-Qi Yan1*, Jia-Chen Gu2*, Yun Zhu3, Zhen-Hua Ling1 1National Engineering Research Center of Speech and Language Information Processing, University of Science and Technology of China, Hefei, China"}, {"rank": 3, "score": 0.61915725, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 10, "text_snippet": " should not have been equally referred to and involved in RAG. On account of the above issues, this paper particularly studies the scenarios where the retriever returns inaccurate results. A method named Corrective Retrieval- Augmented Gene"}, {"rank": 4, "score": 0.60611063, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 83, "text_snippet": " in Table 6. The findings indicate that the self-correction mecha- nism incurs only modest computational overheadwhile significantly enhancing performance, thereby validating its lightweight nature. 6 Conclusion & Limitation This paper stud"}, {"rank": 5, "score": 0.5807377, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 4, "text_snippet": " for long-form generations relative to these models.1 1 I NTRODUCTION State-of-the-art LLMs continue to struggle with factual errors (Mallen et al., 2023; Min et al., 2023) despite their increased model and data scale (Ouyang et al., 2022)."}]}
{"case_index": 146, "query": "According to the paper 'recomp improving retrieval augmented lms with compression and selective augmentation', what exact term completes this statement: \"Training a [BLANK] for QA task works similarly, but scoring will evaluate whether the LM will generate t\"?", "gold": "compressor", "answer": "The missing term is **“compressor”**.\n\nSo the statement is: “Training a **compressor** for QA task works similarly, but scoring will evaluate whether the LM will generate t…”", "answer_score": 0.9099999999999999, "answer_correctness": 1.0, "answer_groundedness": 0.7, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 0.7, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'compressor'. Context snippet discusses QA training and implies training a compressor, though the exact blanked sentence is only partially shown, so support is moderate.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 11.14, "llm_ms": 1879.599, "top_contexts": [{"rank": 1, "score": 0.6302627, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 30, "text_snippet": "ilarly, but scoring will evaluate whether the LM will generate the correct answer with summary prepended (change in line 6). Pseudo code for the QA tasks is in Figure 6 the Appendix. We train our encoder with a contrastive loss (Karpukhin e"}, {"rank": 2, "score": 0.62127954, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 5, "text_snippet": "uestion answering systems (Chen et al., 2017; Yu et al., 2018), is to index large quantities of text, after splitting it into chunks (paragraphs), in a separate information retrieval system. Retrieved information is then presented to the LL"}, {"rank": 3, "score": 0.61845607, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 3, "text_snippet": "n. We evaluate our approach on language modeling task and open domain question answering task. We achieve a compression rate of as low as 6% with minimal loss in performance for both tasks, significantly outperforming the off-the-shelf summ"}, {"rank": 4, "score": 0.61805856, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 60, "text_snippet": "ut. Retrieval-Augmentation with Search Engines. Recently, diﬀerent works have proposed to train large language models to interact with a search engine, by generating text queries, and using the retrieved documents as additional context (Nak"}, {"rank": 5, "score": 0.60861707, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 13, "text_snippet": " outputs, and our abstractive compressor is distilled (West et al., 2022) from an extreme-scale LM (e.g. GPT-3), which achieves impressive summarization performance. Our experiments show that RECOMP can improve performance of frozen LMs on "}]}
{"case_index": 147, "query": "According to the paper 'from local to global a graph rag approach to query focused summarization', what exact term completes this statement: \"In particular, GraphRAG is similar to other approaches that use [BLANK] indexing to create summaries (similar to Kim et al.\"?", "gold": "hierarchical", "answer": "The missing term is **“hierarchical”**, as in **“hierarchical indexing.”**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer correctly fills the blank with 'hierarchical' and is directly supported by Context 1 ('hierarchical indexing').", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 15.961, "llm_ms": 1266.302, "top_contexts": [{"rank": 1, "score": 0.76178646, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 17, "text_snippet": " are later used to answer queries as in Mao et al. 2020). These summaries are generated in parallel and iteratively aggregated into global summaries, similar to prior techniques (Feng et al., 2023; Gao et al., 2023; Khattab et al., 2022; Sh"}, {"rank": 2, "score": 0.7240592, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 16, "text_snippet": " may use alternative retrieval mechanisms, we collectively refer to the family of conventional approaches as vector RAG . GraphRAG contrasts with vector RAG in its ability to answer queries that require global sensemaking over the entire da"}, {"rank": 3, "score": 0.7031987, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 0, "text_snippet": "From Local to Global: A GraphRAG Approach to Query-Focused Summarization Darren Edge1†Ha Trinh1†Newman Cheng2Joshua Bradley2Alex Chao3 Apurva Mody3Steven Truitt2Dasha Metropolitansky1Robert Osazuwa Ness1 Jonathan Larson1 1Microsoft Research"}, {"rank": 4, "score": 0.69110036, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 44, "text_snippet": "ummaries at one level looking for general themes of interest, then read linked reports at a lower level that provide additional details for each subtopic. Here, however, we focus on their utility as part of a graph-based index used for answ"}, {"rank": 5, "score": 0.68832177, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 10, "text_snippet": "cted communities, with summaries at higher levels of the hierarchy recursively incorporating lower-level summaries. Together, these community summaries provide global descriptions and insights over the corpus. Finally, GraphRAG answers quer"}]}
{"case_index": 148, "query": "According to the paper 'recomp improving retrieval augmented lms with compression and selective augmentation', what exact term completes this statement: \"3 L EARNING THE COMPRESSORS Our compressor resembles text [BLANK] models in the output should be faithful to the original input, ye\"?", "gold": "summarization", "answer": "The missing term is **“summarization”** — i.e., “Our compressor resembles text **summarization** models …”.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "The context explicitly contains the phrase 'resembles text summarization models', matching the expected term.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.313, "llm_ms": 1411.47, "top_contexts": [{"rank": 1, "score": 0.6693742, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 23, "text_snippet": "odels in the output should be faithful to the original input, yet the main goal is different. Instead of capturing salient information for humans readers, compressors aim to produce a concise text that are useful for a LM on an end task. In"}, {"rank": 2, "score": 0.66682136, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 22, "text_snippet": "y. Yet, using an extreme-scale model as the compressor is not desirable as we want the compressor to be substantially smaller than the LMs. Thus, we perform distillation (Hinton et al., 2015) of extreme-scale LMs to build a lightweight abst"}, {"rank": 3, "score": 0.6609837, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 2, "text_snippet": "d an abstractive compressor which generates summaries by synthesizing information from multiple documents. Both compressors are trained to improve LMs’ performance on end tasks when the generated summaries are prepended to the LMs’ input, w"}, {"rank": 4, "score": 0.64307225, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 24, "text_snippet": "TRACTIVE COMPRESSION As we formulate extractive compression as a ranking problem, training extractive compressor re- sembles training a reranker for the retrieved documents4with two differences. First, our compressor considers a different g"}, {"rank": 5, "score": 0.63059735, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 3, "text_snippet": "n. We evaluate our approach on language modeling task and open domain question answering task. We achieve a compression rate of as low as 6% with minimal loss in performance for both tasks, significantly outperforming the off-the-shelf summ"}]}
{"case_index": 149, "query": "According to the paper 'atlas few shot learning with retrieval augmented language models', what exact term completes this statement: \"Here, this loss is only used to optimize the [BLANK] of the retriever, and not the language model.\"?", "gold": "parameters", "answer": "The missing term is **“parameters”** — i.e., “this loss is only used to optimize the **parameters** of the retriever, and not the language model.”", "answer_score": 0.865, "answer_correctness": 1.0, "answer_groundedness": 0.55, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 0.55, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'parameters'. Context provided does not explicitly show the quoted sentence with the blank, so support is partial/inferred rather than directly grounded.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 10.92, "llm_ms": 1602.51, "top_contexts": [{"rank": 1, "score": 0.6618624, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 23, "text_snippet": "ent loss functions to train the retriever jointly with the language model. We consider loss functions that leverage the language model to provide supervisory signal to train the retriever. In other words, if the language model ﬁnds a docume"}, {"rank": 2, "score": 0.64292717, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 22, "text_snippet": "ther way to process the retrieved documents in the language model would be to concatenate the query and all the documents, and to use this long sequence as input of the model. Unfortunately, this approach does not scale with the number of d"}, {"rank": 3, "score": 0.64021194, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 29, "text_snippet": "triever inREPLUG by using the LM itself to provide supervision about which documents should be retrieved. Inspired by Sachan et al. (2022), our approach can be seen as adjusting the probabilities of the retrieved documents to match the prob"}, {"rank": 4, "score": 0.6188535, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 18, "text_snippet": "y retrieving the top-k relevant documents from a large corpus of text with the retriever. Then, these documents are fed to the language model, along with the query, which in turns generates the output. Both the retriever and the language mo"}, {"rank": 5, "score": 0.6060154, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 40, "text_snippet": "ent retriever ﬁne-tuning Retrieval is facilitated by using a document index, which is a pre-computed collection of the document embeddings for all the documents in the retrieval corpus. When jointly training the retriever and language model"}]}
{"case_index": 150, "query": "According to the paper 'retrieval augmented generation for knowledge intensive nlp tasks', what exact term completes this statement: \"To train the retriever and generator [BLANK], we treat the retrieved document as a latent va\"?", "gold": "end-to-end", "answer": "The missing term is **“jointly”** — i.e., the paper says: “To train the retriever and generator **jointly**, we treat the retrieved document as a latent va…”", "answer_score": 0.0, "answer_correctness": 0.0, "answer_groundedness": 0.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 0.0, "abstained": false, "should_abstain": false, "notes": "Expected term is \"end-to-end\"; model answered \"jointly\". Context mentions \"trained end-to-end\" but does not support \"jointly\".", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 10.532, "llm_ms": 1545.744, "top_contexts": [{"rank": 1, "score": 0.63204634, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 57, "text_snippet": "nted generators using a second “informed” retriever with access to the output, which the test-time retriever can be distilled from, and Hofstätter et al. (2022) recently proposed a training set ﬁltering/weighting approach to train stronger "}, {"rank": 2, "score": 0.6170063, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 29, "text_snippet": "triever inREPLUG by using the LM itself to provide supervision about which documents should be retrieved. Inspired by Sachan et al. (2022), our approach can be seen as adjusting the probabilities of the retrieved documents to match the prob"}, {"rank": 3, "score": 0.61630845, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 10, "text_snippet": "eural retriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The retriever (Dense Passage Retriever [ 26], henceforth DPR) provides latent documents conditioned on the input, and the seq2seq model (BART"}, {"rank": 4, "score": 0.6146687, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 19, "text_snippet": " documents. It usually provides an extra knowledge source from a specific corpus, i.e., Wikipedia, which greatly improves the per- formance of LMs in a variety of tasks, especially in the knowledge-intensive ones. The proposed  methods gene"}, {"rank": 5, "score": 0.6091327, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 72, "text_snippet": "augment pre-trained, generative language models. Learned Retrieval There is signiﬁcant work on learning to retrieve documents in information retrieval, more recently with pre-trained, neural language models [ 44,26] similar to ours. Some wo"}]}
{"case_index": 151, "query": "According to the paper 'leveraging passage retrieval with generative models for open domain question answering', what exact term completes this statement: \"These datasets were gen- erated in a way that answers do not [BLANK] to spans in support documents, t\"?", "gold": "correspond", "answer": "The missing term is **“correspond”** (as in: “answers do not **correspond** to spans in support documents”).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'correspond' and is directly supported by the provided context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.34, "llm_ms": 1169.049, "top_contexts": [{"rank": 1, "score": 0.63372236, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 15, "text_snippet": " that answers do not correspond to spans in support documents, thus requiring ab- stractive models. Raffel et al. (2019) showed that generative models are competitive for reading com- prehension tasks such as SQuAD (Rajpurkar et al., 2016),"}, {"rank": 2, "score": 0.5931635, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 14, "text_snippet": " be trained using weak supervision in the form of question-answer pairs (Karpukhin et al., 2020), or pretrained using a cloze task and ﬁnetuned end-to- end (Guu et al., 2020; Lee et al., 2019). Generative question answering was mostly consi"}, {"rank": 3, "score": 0.59152704, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 5, "text_snippet": "t, it retrieves support text passages from an external source of knowledge such as Wikipedia. Then, a generative encoder-decoder model produces the answer, conditioned on the question and the re- trieved passages. This approach scales well "}, {"rank": 4, "score": 0.56979173, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 10, "text_snippet": " problem in natural lan- guage processing (V oorhees et al., 1999), this task has recently regained interest following the work by Chen et al. (2017). In that version of the prob- lem, strong supervision is available to the learning system,"}, {"rank": 5, "score": 0.5675045, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 6, "text_snippet": "tech- niques have been considered, either using sparse representations based on TF/IDF or using dense embeddings (Guu et al., 2020; Karpukhin et al., 2020). The models which extract the answers are often based on contextualized word represe"}]}
{"case_index": 152, "query": "According to the paper 'refrag rethinking rag based decoding', what exact term completes this statement: \"provide rigorous validation [BLANK] diverse long-context tasks, including RAG, multi-turn conversations, and long document summarization, spanning a wide range of datasets.\"?", "gold": "ofrefragacross", "answer": "The missing term is **“of REFRAG”** — i.e., the full phrase is: **“provide rigorous validation of REFRAG across diverse long-context tasks, including RAG, multi-turn conversations, and long document summarization, spanning a wide range of datasets.”**", "answer_score": 0.72, "answer_correctness": 0.6, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.6, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context shows the phrase 'provide rigorous validation ofREFRAGacross...'. Model answered 'of REFRAG', which matches the words but not the exact concatenated term expected ('ofrefragacross').", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 12.559, "llm_ms": 1664.511, "top_contexts": [{"rank": 1, "score": 0.7137537, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 4, "text_snippet": " provide rigorous validation ofREFRAGacross diverse long-context tasks, including RAG, multi-turn conversations, and long document summarization, spanning a wide range of datasets. Experimental results confirm thatREFRAGdelivers substantial"}, {"rank": 2, "score": 0.6847056, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 16, "text_snippet": "nsing most chunks for the query in RAG settings. We provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training and many real word long-context applications including RAG, multi-turn conversation with RA"}, {"rank": 3, "score": 0.6461941, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 13, "text_snippet": "sive tasks —tasks that humans could not reasonably be expected to perform without access to an external knowledge source. Our RAG models achieve state-of-the-art results on open Natural Questions [ 29], WebQuestions [ 3] and CuratedTrec [ 2"}, {"rank": 4, "score": 0.6448775, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 3, "text_snippet": " on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30 .85×the time-t"}, {"rank": 5, "score": 0.6443853, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 0, "text_snippet": "REFRAG: Rethinking RAG based Decoding Xiaoqiang Lin1,2,∗,Aritra Ghosh1,Bryan Kian Hsiang Low2,Anshumali Shrivastava1,3,Vijai Mohan1 1Meta Superintelligence Labs,2National University of Singapore,3Rice University ∗Work done at Meta Large Lan"}]}
{"case_index": 153, "query": "According to the paper 'raptor recursive abstractive processing for tree organized retrieval', what exact term completes this statement: \"1 I NTRODUCTION Large Language Models (LLMs) have emerged as [BLANK] tools showing impressive perfor- mance on m\"?", "gold": "transformative", "answer": "The missing term is **“transformative”**, i.e.,\n\n“Large Language Models (LLMs) have emerged as **transformative** tools showing impressive perfor- mance on m…”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'transformative' and is directly supported by the provided context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.841, "llm_ms": 1844.113, "top_contexts": [{"rank": 1, "score": 0.6873945, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 2, "text_snippet": "hat retrieval with recursive summaries offers significant improvements over tra- ditional retrieval-augmented LMs on several tasks. On question-answering tasks that involve complex, multi-step reasoning, we show state-of-the-art results; fo"}, {"rank": 2, "score": 0.63827085, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 0, "text_snippet": "Published as a conference paper at ICLR 2024 RAPTOR: R ECURSIVE ABSTRACTIVE PROCESSING FOR TREE-ORGANIZED RETRIEVAL Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D. Manning Stanford University psarthi@cs."}, {"rank": 3, "score": 0.6329668, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 3, "text_snippet": "ed as transformative tools showing impressive perfor- mance on many tasks. With the growing size of LLMs, they can serve standalone as very effective knowledge stores, with facts encoded within their parameters (Petroni et al., 2019; Jiang "}, {"rank": 4, "score": 0.632028, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 14, "text_snippet": "omponents: the retriever, the reader, and end-to-end system training. Retrieval methods have transitioned from traditional term-based techniques like TF-IDF (Sp¨arck Jones, 1972) and BM25 (Robertson et al., 1995; Roberts et al., 2020) to de"}, {"rank": 5, "score": 0.61527455, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 70, "text_snippet": "re is capable of achieving strong performance across several tasks. 8  General-Purpose Architectures for NLP Prior work on general-purpose architectures for NLP tasks has shown great success without the use of retrieval. A single, pre-train"}]}
{"case_index": 154, "query": "According to the paper 'leveraging passage retrieval with generative models for open domain question answering', what exact term completes this statement: \"In BM25, passages are [BLANK] as bag of words, and the ranking function is based on term and inverse doc- ument frequencies.\"?", "gold": "represented", "answer": "The missing term is **“represented”** — i.e., “In BM25, passages are **represented** as bag of words, and the ranking function is based on term and inverse document frequencies.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'represented' and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 13.253, "llm_ms": 1875.191, "top_contexts": [{"rank": 1, "score": 0.50992435, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 19, "text_snippet": "den test set (right), competitions.codalab.org/competitions/17208#results ). Retrieval. For the retrieval of support passages, we consider two methods: BM25 (Robertson et al., 1995) and DPR (Karpukhin et al., 2020). In BM25, passages are re"}, {"rank": 2, "score": 0.47964644, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 14, "text_snippet": "omponents: the retriever, the reader, and end-to-end system training. Retrieval methods have transitioned from traditional term-based techniques like TF-IDF (Sp¨arck Jones, 1972) and BM25 (Robertson et al., 1995; Roberts et al., 2020) to de"}, {"rank": 3, "score": 0.4763403, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 61, "text_snippet": "riever to a word overlap-based BM25 retriever [ 53]. Here, we replace RAG’s retriever with a ﬁxed BM25 system, and use BM25 retrieval scores as logits when calculating p(z|x). Table 6 shows the results. For FEVER, BM25 performs best, perhap"}, {"rank": 4, "score": 0.46602422, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 12, "text_snippet": "018b) described a technique to aggregate answers from different paragraphs, using conﬁdence and coverage scores. Passage retrieval is an important step in open domain question answering, and is an active area of research to improve QA syste"}, {"rank": 5, "score": 0.46572658, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 113, "text_snippet": "hods in Natural Language Processing (EMNLP) , pp. 5418–5426, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.437. URL https://aclanthology.org/2020.emnlp-main.437 . Stephen Robertson, Hugo "}]}
{"case_index": 155, "query": "According to the paper 'replug retrieval augmented black box language models', what exact term completes this statement: \"[BLANK] language models (Khandelwal et al., 2020; Borgeaud et al., 2022; Izacard et al., 2022b; Yasunaga et al., 2022), in con- trast, can retrieve knowledge from an external datastore when needed, potentially reducing hallucination and increas- ing coverage.\"?", "gold": "retrieval-augmented", "answer": "The missing term is **“Retrieval-augmented”**, i.e., the full phrase is:\n\n**“Retrieval-augmented language models (Khandelwal et al., 2020; Borgeaud et al., 2022; Izacard et al., 2022b; Yasunaga et al., 2022)…”**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'retrieval-augmented' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.975, "llm_ms": 1877.852, "top_contexts": [{"rank": 1, "score": 0.79247475, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 3, "text_snippet": "itly in their parameters. However, they are also prone to hallucination and cannot represent the full long tail of knowledge from the training corpus. Retrieval-augmented language models (Khandelwal et al., 2020; Borgeaud et al., 2022; Izac"}, {"rank": 2, "score": 0.7181793, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 5, "text_snippet": "ting the LM’s parameters, REPLUG treats the language model as a black box and augments it with a frozen or tunable retriever. This black-box assumption makes REPLUG applicable to large LMs (i.e., >100B parameters), which are often served vi"}, {"rank": 3, "score": 0.71552956, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 4, "text_snippet": "trieval-augmented language models (RALMs) (Khandelwal et al., 2019; Izacard et al., 2022; Lewis et al., 2020; Borgeaud et al., 2022) have shown impressive performance on knowledge-intensive tasks (Kwiatkowski et al., 2019; Petroni et al., 2"}, {"rank": 4, "score": 0.7148348, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 15, "text_snippet": "anguage models makes this approach infeasible. To ad- dress the challenges posed by large language models, we investigate retrieval-augmentation in the black-box setting , where users only have access to the model predictions and cannot acc"}, {"rank": 5, "score": 0.7141435, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 6, "text_snippet": "eval-augmented architecture. These models employ a non-parametric memory, e.g. a neural retriever over a large, external, potentially non-static knowledge source to enhance a parametric language model. In addition to their memorisation abil"}]}
{"case_index": 156, "query": "According to the paper 'refrag rethinking rag based decoding', what exact term completes this statement: \"During CPT, we input the first stokens x1:sinto the encoder and use its output to assist the decoder in [BLANK] the next otokens xs+1:s+o.\"?", "gold": "predicting", "answer": "The missing term is **“predicting”** — i.e., “…assist the decoder in **predicting** the next *o* tokens \\(x_{s+1:s+o}\\).”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'predicting' and is explicitly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.953, "llm_ms": 1533.514, "top_contexts": [{"rank": 1, "score": 0.6252727, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 27, "text_snippet": "repare the model for downstream tasks utilizing chunk embeddings. To further enhance performance, we introduce selective compression via RL. After aligning the encoder and decoder through CPT, we apply supervised fine-tuning (SFT) to adapt "}, {"rank": 2, "score": 0.599841, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 152, "text_snippet": "e downstream tasks (see section 5). REFRAG:Our approach is illustrated in figure 1. We use RoBERTa-large (Liu et al., 2019) as the encoder, feeding x1:stokens and evaluating the perplexity on the output tokens xs+1:s+o. We use REFRAG kto de"}, {"rank": 3, "score": 0.5934698, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 21, "text_snippet": "uery  Encoder Figure 1The main design ofREFRAG. The input context is chunked and processed by the light-weight encoder to produce chunk embeddings, which are precomputable for efficient reuse. A light-weight RL policy decide few chunks to e"}, {"rank": 4, "score": 0.5929645, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 15, "text_snippet": "igure 1) while preserving the autoregressive nature of the decoder, thereby supporting multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight reinforcement learning (RL) policy that sel"}, {"rank": 5, "score": 0.589416, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 14, "text_snippet": "ortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now "}]}
{"case_index": 157, "query": "According to the paper 'retrieval augmented generation for knowledge intensive nlp tasks', what exact term completes this statement: \"Here, we bring hybrid parametric and [BLANK] memory to the “workhorse of NLP,” i.e.\"?", "gold": "non-parametric", "answer": "The missing term is **“non-parametric”** — i.e., “hybrid parametric and **non-parametric** memory.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'non-parametric' and is directly supported by the provided context quoting parametric and non-parametric memory.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 9.611, "llm_ms": 1653.178, "top_contexts": [{"rank": 1, "score": 0.70762837, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 6, "text_snippet": "eval-augmented architecture. These models employ a non-parametric memory, e.g. a neural retriever over a large, external, potentially non-static knowledge source to enhance a parametric language model. In addition to their memorisation abil"}, {"rank": 2, "score": 0.70079255, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 12, "text_snippet": "ck- augmented networks [ 25] and memory layers [ 30]. In contrast, we explore a setting where both parametric and non-parametric memory components are pre-trained and pre-loaded with extensive knowledge. Crucially, by using pre-trained acce"}, {"rank": 3, "score": 0.6830661, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 9, "text_snippet": "y to the “workhorse of NLP,” i.e. sequence-to-sequence (seq2seq) models. We endow pre-trained, parametric-memory generation models with a non-parametric memory through a general-purpose ﬁne-tuning approach which we refer to as retrieval-aug"}, {"rank": 4, "score": 0.65135485, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 59, "text_snippet": " memory by leveraging approximate nearest neighbors search (Grave et al., 2017a). The related kNN-LM model (Khandelwal et al., 2020) replaced LSTMs by transformer networks, and scaled the memory to billions of tokens, leading to strong perf"}, {"rank": 5, "score": 0.6478014, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 1, "text_snippet": "ameters, and achieve state-of-the-art results when ﬁne-tuned on down- stream NLP tasks. However, their ability to access and precisely manipulate knowl- edge is still limited, and hence on knowledge-intensive tasks, their performance lags b"}]}
{"case_index": 158, "query": "According to the paper 'self rag learning to retrieve generate and critique through self reflection', what exact term completes this statement: \"[BLANK] Generation (RAG) augments the input space of LMs with retrieved text passages (Guu et al., 2020; Lewis et al., 2020), lea\"?", "gold": "retrieval-augmented", "answer": "The missing term is **“Retrieval-Augmented”**, i.e., **“Retrieval-Augmented Generation (RAG)”**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'retrieval-augmented' and is directly supported by context (e.g., 'retrieval-augmented generation (RAG)').", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 8.806, "llm_ms": 1237.403, "top_contexts": [{"rank": 1, "score": 0.7633307, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 2, "text_snippet": "ew framework called Self-Reflective Retrieval-Augmented Gen- eration ( SELF-RAG)that enhances an LM’s quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passage"}, {"rank": 2, "score": 0.7572136, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 6, "text_snippet": "y trained to leverage and follow facts from provided passages. This work introduces Self-Reflective Retrieval-augmented Generation ( SELF-RAG)to improve an LLM’s generation quality, including its factual accuracy without hurting its versati"}, {"rank": 3, "score": 0.7552206, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 96, "text_snippet": "ains an LM to learn to retrieve, generate, and critique text passages and its own generation by predicting the next tokens from its original vocabulary as well as newly added special tokens, called reflection tokens. SELF-RAGfurther enables"}, {"rank": 4, "score": 0.75080335, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 28, "text_snippet": "F-RAGis a framework that enhances the quality and factuality of an LLM through retrieval and self-reflection, without sacrificing LLM’s original creativity and versatility. Our end-to-end training lets an LM Mgenerate text informed by retri"}, {"rank": 5, "score": 0.7507894, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 7, "text_snippet": "ieval-augmented generation (RAG) (Lewis et al., 2020). In this framework, the input to models is augmented by prepending relevant documents that are retrieved from an external knowledge corpus (Guu et al., 2020). While RAG serves as a pract"}]}
{"case_index": 159, "query": "According to the paper 'corrective retrieval augmented generation', what exact term completes this statement: \"While Toolformer (Schick et al., 2023) is [BLANK] for calling APIs such as Wikipedia.\"?", "gold": "pre-trained", "answer": "The missing term is **“pre-trained”** — i.e., “While Toolformer (Schick et al., 2023) is **pre-trained** for calling APIs such as Wikipedia.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'pre-trained' and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.133, "llm_ms": 1507.579, "top_contexts": [{"rank": 1, "score": 0.61825466, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 22, "text_snippet": "rrelevant context and improve robustness. SAIL (Luo et al., 2023) is tuned on instructions to insert retrieved documents before instructions. While Toolformer (Schick et al., 2023) is pre-trained for calling APIs such as Wikipedia. In addit"}, {"rank": 2, "score": 0.5467687, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 164, "text_snippet": "intensive tasks for open-ended generation (e.g., instruction following). Recent or concurrent work studies instruction-tuning of retrieval systems (Asai et al., 2023b) or joint training of retrieval and LM components (Lin et al., 2023), whi"}, {"rank": 3, "score": 0.5438723, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 20, "text_snippet": " prior work often retrieves only once at the beginning, Jiang et al. (2023) propose to adaptively retrieve passages for generation on top of a proprietary LLM or Schick et al. (2023) train an LM to generate API calls for named entities. Yet"}, {"rank": 4, "score": 0.54178965, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 106, "text_snippet": ". , 21:140:1–140:67. Md. Rashad Al Hasan Rony, Ricardo Usbeck, and Jens Lehmann. 2022. Dialokg: Knowledge-structure aware task-oriented dialogue generation. In Findings of the Association for Computational Linguistics:NAACL 2022, Seattle, W"}, {"rank": 5, "score": 0.52369344, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 120, "text_snippet": "a, A., Leyton-Brown, K., and Shoham, Y . (2023). In-context retrieval-augmented language models. Transactions of the Association for Computational Linguistics , 11:1316–1331. Ranade, P. and Joshi, A. (2023). Fabula: Intelligence report gene"}]}
{"case_index": 160, "query": "According to the paper 'recomp improving retrieval augmented lms with compression and selective augmentation', what exact term completes this statement: \"3 Score (M,yi,[sj;xi]) = log pM(y|[sj;xi]), log [BLANK] assigned to target output accord\"?", "gold": "likelihood", "answer": "The missing term is **“likelihood”** — i.e., the statement reads: “log **likelihood** assigned to target output according to LM.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer correctly fills the blank with 'likelihood', which is explicitly present in the provided context snippet ('log likelihood assigned to target output').", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.698, "llm_ms": 1617.166, "top_contexts": [{"rank": 1, "score": 0.618371, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 29, "text_snippet": "g pM(y|[sj;xi]), log likelihood assigned to target output according to LM Mwhen candidate sentence is prepended to the input. We consider the sentence with the highest log likelihood as a positive example pi(line 3). To construct negative e"}, {"rank": 2, "score": 0.59606856, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 28, "text_snippet": " of input sequence xi and candidate sentences sj, we measure 3Recent work (Zhang et al., 2022) shows that extractive approach does not always preserve faithfulness, but such cases are still rare compared to abstractive approaches which can "}, {"rank": 3, "score": 0.5422289, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 33, "text_snippet": "istribution according to the language model, using a uniform prior: pk∝pLM(a|dk,q). Using the Softmax operator, we have that pk=exp(logpLM(a|dk,q))∑K i=1exp(logpLM(a|di,q)). Leave-one-out Perplexity Distillation (LOOP). Finally, we propose "}, {"rank": 4, "score": 0.5390213, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 32, "text_snippet": "e retrieval likelihood by only marginalizing over the retrieved documents D′. 4.2. Computing LM likelihood We use the LM as a scoring function to measure how much each document could improve the LM perplexity. Specifi- cally, we first compu"}, {"rank": 5, "score": 0.50750756, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 31, "text_snippet": "′⊂ D with the highest simi- larity scores from a corpus Dgiven an input context x, as described in §3.1. We then compute the retrieval likelihood of each retrieved document d: PR(d|x) =es(d,x)/γ P d∈D′es(d,x)/γ where γis a hyperparameter th"}]}
{"case_index": 161, "query": "According to the paper 'from local to global a graph rag approach to query focused summarization', what exact term completes this statement: \"[BLANK], GraphRAG recursively creates increasingly global summaries by using the LLM to create summaries spanni\"?", "gold": "specifically", "answer": "**In particular**, GraphRAG recursively creates increasingly global summaries by using the LLM to create summaries spanni", "answer_score": 0.3, "answer_correctness": 0.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Expected term is 'specifically' but model answered 'In particular'. Context supports 'In particular' phrasing, but it does not match the exact expected word.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 11.669, "llm_ms": 1335.496, "top_contexts": [{"rank": 1, "score": 0.7396387, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 9, "text_snippet": "tirety of a large text corpus. GraphRAG first uses an LLM to construct a knowledge graph, where nodes correspond to key entities in the corpus and edges represent relationships between those entities. Next, it partitions the graph into a hi"}, {"rank": 2, "score": 0.73091525, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 10, "text_snippet": "cted communities, with summaries at higher levels of the hierarchy recursively incorporating lower-level summaries. Together, these community summaries provide global descriptions and insights over the corpus. Finally, GraphRAG answers quer"}, {"rank": 3, "score": 0.72500247, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 17, "text_snippet": " are later used to answer queries as in Mao et al. 2020). These summaries are generated in parallel and iteratively aggregated into global summaries, similar to prior techniques (Feng et al., 2023; Gao et al., 2023; Khattab et al., 2022; Sh"}, {"rank": 4, "score": 0.7196363, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 8, "text_snippet": "nce analysis (Ranade and Joshi, 2023). Given a sensemaking query and a text with an implicit and interconnected set of concepts, an LLM can generate a summary that answers the query. The challenge, however, arises when the volume of data re"}, {"rank": 5, "score": 0.71802956, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 29, "text_snippet": "lements (nodes, edges, covariates) that the LLM can summarize in parallel at both indexing time and query time. The “global answer” to a given query is produced using a final round of query-focused summarization over all community summaries"}]}
{"case_index": 162, "query": "According to the paper 'self rag learning to retrieve generate and critique through self reflection', what exact term completes this statement: \"[BLANK] Generation (RAG)Ours: Self-reﬂective Retrieval-Augmented Genera\"?", "gold": "retrieval-augmented", "answer": "The missing term is **“Retrieval-Augmented”**, i.e., **“Retrieval-Augmented Generation (RAG)”**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'retrieval-augmented' and is directly supported by context phrases 'Retrieval-Augmented Generation (RAG)'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 11.374, "llm_ms": 2264.87, "top_contexts": [{"rank": 1, "score": 0.72836584, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 27, "text_snippet": "n (reasoning path consistency) and without retrieval. Recent work on LLM refinement (Dhuliawala et al., 2023; Madaan et al., 2023; Paul et al., 2023) prompts a model to generate task output, natural language feedback and refined task output"}, {"rank": 2, "score": 0.7108799, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 6, "text_snippet": "y trained to leverage and follow facts from provided passages. This work introduces Self-Reflective Retrieval-augmented Generation ( SELF-RAG)to improve an LLM’s generation quality, including its factual accuracy without hurting its versati"}, {"rank": 3, "score": 0.70510274, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 2, "text_snippet": "ew framework called Self-Reflective Retrieval-Augmented Gen- eration ( SELF-RAG)that enhances an LM’s quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passage"}, {"rank": 4, "score": 0.69578433, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 13, "text_snippet": "erifiability. consistently retrieves a fixed number of documents for generation regardless of the retrieval necessity (e.g., the bottom figure example does not require factual knowledge) and never second visits the generation quality. Moreo"}, {"rank": 5, "score": 0.69572604, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 96, "text_snippet": "ains an LM to learn to retrieve, generate, and critique text passages and its own generation by predicting the next tokens from its original vocabulary as well as newly added special tokens, called reflection tokens. SELF-RAGfurther enables"}]}
{"case_index": 163, "query": "According to the paper 'atlas few shot learning with retrieval augmented language models', what exact term completes this statement: \"•Thorough downstream experiments in few-shot settings, demonstrating [BLANK] results on few-shot NaturalQuestions (+2.8%), TriviaQA (+3.3%), FEVER (+5.1%), and results on par or stronger than models with 15 ×more parameters on MMLU.\"?", "gold": "state-of-the-art", "answer": "The missing term is **“state-of-the-art”**, i.e., “…demonstrating **state-of-the-art** results on few-shot NaturalQuestions (+2.8%), TriviaQA (+3.3%), FEVER (+5.1%), and results on par or stronger than models with 15× more parameters on MMLU.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer correctly fills blank with 'state-of-the-art' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.513, "llm_ms": 1788.957, "top_contexts": [{"rank": 1, "score": 0.74081707, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 13, "text_snippet": "ask at hand. •Thorough downstream experiments in few-shot settings, demonstrating state-of-the-art results on few-shot NaturalQuestions (+2.8%), TriviaQA (+3.3%), FEVER (+5.1%), and results on par or stronger than models with 15 ×more param"}, {"rank": 2, "score": 0.6979268, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 154, "text_snippet": "odel. By jointly pre-training the retriever module and the language model, we show that Atlashas strong few-shot learning capabilities on a wide range of knowledge intensive tasks, including NaturalQuestions, TriviaQA, FEVER, 8 KILT tasks a"}, {"rank": 3, "score": 0.6848089, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 110, "text_snippet": ", and using the full training set, and report results in Table 8. On these benchmarks, which require high-degree of memorisation, we clearly see the beneﬁts of retrieval-augmentation. Atlas-11B obtains state-of-the-art results on 64-shot qu"}, {"rank": 4, "score": 0.66930753, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 59, "text_snippet": "QA. It outperforms the previous best Figure 4. Ensembling random documents does not result in improved performance. BPB of Curie augmented with different methods (random, REPLUG andREPLUG LSR ) when varying the number of documents (i.e.; nu"}, {"rank": 5, "score": 0.6672854, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 1, "text_snippet": " Abstract Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter count"}]}
{"case_index": 164, "query": "According to the paper 'replug retrieval augmented black box language models', what exact term completes this statement: \"Internal [BLANK] of such models are not exposed and fine-tuning is not supported.\"?", "gold": "representations", "answer": "The missing term is **“representations”**:\n\n“Internal **representations** of such models are not exposed and fine-tuning is not supported.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'representations' and is directly supported by context stating 'Internal representations of such models are not exposed and fine-tuning is not supported.'", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.25, "hit_rank": 4, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.551, "llm_ms": 1731.919, "top_contexts": [{"rank": 1, "score": 0.7097976, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 5, "text_snippet": "ting the LM’s parameters, REPLUG treats the language model as a black box and augments it with a frozen or tunable retriever. This black-box assumption makes REPLUG applicable to large LMs (i.e., >100B parameters), which are often served vi"}, {"rank": 2, "score": 0.6577058, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 8, "text_snippet": "Black-Box Language Models Figure 1, REPLUG is extremely flexible and can be used with any existing black-box LM and retrieval model. We also introduce REPLUG LSR (REPLUG with LM- Supervised Retrieval), a training scheme that can further imp"}, {"rank": 3, "score": 0.6555822, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 71, "text_snippet": "a black box and augments it with a tuneable retrieval model. Our evaluation shows that REPLUG can be integrated with any existing language model to improve their performance  REPLUG: Retrieval-Augmented Black-Box Language Models on language"}, {"rank": 4, "score": 0.6541014, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 6, "text_snippet": "PIs. Internal representations of such models are not exposed and fine-tuning is not supported. In this work, we introduce REPLUG (Retrieve and Plug ), a new retrieval-augmented LM framework where the lan- guage model is viewed as a black bo"}, {"rank": 5, "score": 0.62085533, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 12, "text_snippet": "gnals, resulting in improved retrieval quality. •Evaluations on language modeling (§6), open-domain QA and MMLU demonstrate that REPLUG can im- prove the performance of various language models such as GPT, OPT and BLOOM, including very larg"}]}
{"case_index": 165, "query": "According to the paper 'retrieval augmented generation for knowledge intensive nlp tasks', what exact term completes this statement: \"Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x)via a top-K [BLANK].\"?", "gold": "approximation", "answer": "The missing term is **“approximation”** — i.e., “via a top-K **approximation**.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'approximation' and is directly supported by context stating 'via a top-K approximation'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.3333333333333333, "hit_rank": 3, "matched_by": "doc_id+chunk_id", "retrieval_ms": 13.71, "llm_ms": 1301.21, "top_contexts": [{"rank": 1, "score": 0.6367807, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 19, "text_snippet": "r produces the output sequence probability for each document, which are then marginalized, pRAG-Sequence (y|x)≈∑ z∈top-k(p(·|x))pη(z|x)pθ(y|x,z) =∑ z∈top-k(p(·|x))pη(z|x)N∏ ipθ(yi|x,z,y 1:i−1) RAG-Token Model In the RAG-Token model we can d"}, {"rank": 2, "score": 0.62802064, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 10, "text_snippet": "eural retriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The retriever (Dense Passage Retriever [ 26], henceforth DPR) provides latent documents conditioned on the input, and the seq2seq model (BART"}, {"rank": 3, "score": 0.60475296, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 18, "text_snippet": "hen describe the pηandpθcomponents, as well as the training and decoding procedure. 2.1 Models RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate the complete sequence . Technically, it treats the retriev"}, {"rank": 4, "score": 0.5979975, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 25, "text_snippet": "rator G. The retriever Raims to retrieve the top- Kdocuments D={dr1, ..., d rk}that are relevant to the input Xfrom the corpus C. Based on the input Xand the retrieved results D, the generator Gis responsible for generating the output Y. Th"}, {"rank": 5, "score": 0.5957152, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 29, "text_snippet": "triever inREPLUG by using the LM itself to provide supervision about which documents should be retrieved. Inspired by Sachan et al. (2022), our approach can be seen as adjusting the probabilities of the retrieved documents to match the prob"}]}
{"case_index": 166, "query": "According to the paper 'from local to global a graph rag approach to query focused summarization', what exact term completes this statement: \"For a class of global sensemaking questions over datasets in the 1 million token range, we show that GraphRAG leads to substantial [BLANK] over a conventional RAG baseline for\"?", "gold": "improvements", "answer": "The missing term is **“improvements”** — i.e., “GraphRAG leads to substantial **improvements** over a conventional RAG baseline …”.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'improvements' and is directly supported by the provided context quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.15, "llm_ms": 1973.13, "top_contexts": [{"rank": 1, "score": 0.7419976, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 96, "text_snippet": "tigate these downstream risks for questions of a global nature, which might otherwise be answered by samples of retrieved facts falsely presented as global summaries. 7 Conclusion We have presented GraphRAG, a RAG approach that combines kno"}, {"rank": 2, "score": 0.72079813, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 3, "text_snippet": "wo stages: first, to derive an entity knowledge graph from the source documents, then to pre- generate community summaries for all groups of closely related entities. Given a question, each community summary is used to generate a partial re"}, {"rank": 3, "score": 0.6927866, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 29, "text_snippet": "lements (nodes, edges, covariates) that the LLM can summarize in parallel at both indexing time and query time. The “global answer” to a given query is produced using a final round of query-focused summarization over all community summaries"}, {"rank": 4, "score": 0.6841268, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 0, "text_snippet": "From Local to Global: A GraphRAG Approach to Query-Focused Summarization Darren Edge1†Ha Trinh1†Newman Cheng2Joshua Bradley2Alex Chao3 Apurva Mody3Steven Truitt2Dasha Metropolitansky1Robert Osazuwa Ness1 Jonathan Larson1 1Microsoft Research"}, {"rank": 5, "score": 0.6837392, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 83, "text_snippet": "evel community summaries ( C0), it required over 97% fewer tokens. For a modest drop in performance compared with other global methods, root-level GraphRAG offers a highly efficient method for the iterative question answering that character"}]}
{"case_index": 167, "query": "According to the paper 'self rag learning to retrieve generate and critique through self reflection', what exact term completes this statement: \"Though our work also studies [BLANK] critique on retrieval and generation, we train our target LM on task examples augmented with reflection tokens from a critic model offline, with a far lower training cost compared to RLHF.\"?", "gold": "fine-grained", "answer": "The missing term is **“fine-grained”**, as in: “Though our work also studies **fine-grained** critique on retrieval and generation, …”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'fine-grained' and is directly supported by Context 2 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 14.499, "llm_ms": 1471.333, "top_contexts": [{"rank": 1, "score": 0.76528233, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 15, "text_snippet": "st a critic model during training, reducing overhead. The critic model, in part, is supervised on a dataset of input, output, and corresponding reflection tokens collected by prompting a propriety LM (i.e., GPT-4; OpenAI 2023). While we dra"}, {"rank": 2, "score": 0.739982, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 25, "text_snippet": " has proven effective in aligning LLMs with human preferences (Ouyang et al., 2022). Wu et al. (2023) introduce fine-grained RLHF with multiple reward models. Though our work also studies fine-grained critique on retrieval and generation, w"}, {"rank": 3, "score": 0.73811615, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 49, "text_snippet": "arning with critique. Recent work incorporates additional critique (feedback) during training, e.g., RLHF (Ouyang et al. 2022) via PPO. While PPO relies on 5  Preprint. separate reward models during training, we compute critique offline and"}, {"rank": 4, "score": 0.70037353, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 24, "text_snippet": "ering tasks and to generate with tree search, guided by LM-generated value scores. While their value function simply indicates an overall score of each generation, SELF-RAGtrains to an arbitrary LM to learn to generate fine-grained self-ref"}, {"rank": 5, "score": 0.67813575, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 26, "text_snippet": "able generation at inference, while RLHF focuses on human preference alignment during training. Other works use general control tokens to guide LM generation (Lu et al., 2022; Korbak et al., 2023), while SELF-RAGuses reflection tokens to de"}]}
{"case_index": 168, "query": "According to the paper 'corrective retrieval augmented generation', what exact term completes this statement: \"This refinement operation involves knowledge decom- position, filter, and [BLANK] (Section 4.4).\"?", "gold": "recomposition", "answer": "The missing term is **“reorganization”** — i.e., “This refinement operation involves knowledge decomposition, filter, and **reorganization** (Section 4.4).”", "answer_score": 0.0, "answer_correctness": 0.0, "answer_groundedness": 0.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 0.0, "abstained": false, "should_abstain": false, "notes": "Expected term is 'recomposition' per Section 4.4; model answered 'reorganization'. Context snippet indicates 'Knowledge Refinement' but does not support 'reorganization'.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 7.741, "llm_ms": 1567.511, "top_contexts": [{"rank": 1, "score": 0.5896417, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 44, "text_snippet": "ce. Discussion Preliminary experiments of employ- ing only the Correct andIncorrect actions show that the efficacy of CRAG was easily affected by the accuracy of the retrieval evaluator. The reason might be the distinct knowledge switch for"}, {"rank": 2, "score": 0.58593273, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 10, "text_snippet": " should not have been equally referred to and involved in RAG. On account of the above issues, this paper particularly studies the scenarios where the retriever returns inaccurate results. A method named Corrective Retrieval- Augmented Gene"}, {"rank": 3, "score": 0.58330095, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 1, "text_snippet": "e reliance on the paramet- ric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and inco"}, {"rank": 4, "score": 0.577377, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 6, "text_snippet": " The examples show that a low-quality retriever is prone to introducing a substantial amount of irrelevant information, impeding the generators from acquiring accurate knowledge and potentially misleading them. the parametric knowledge they"}, {"rank": 5, "score": 0.5771962, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 83, "text_snippet": " in Table 6. The findings indicate that the self-correction mecha- nism incurs only modest computational overheadwhile significantly enhancing performance, thereby validating its lightweight nature. 6 Conclusion & Limitation This paper stud"}]}
{"case_index": 169, "query": "According to the paper 'recomp improving retrieval augmented lms with compression and selective augmentation', what exact term completes this statement: \"We propose compressors: (1) Extractive compressor which selects relevant sentences from retrieved document set; (2) Abstractive compressor which generates a summary [BLANK] information from multiple retrieved documents.\"?", "gold": "synthesizing", "answer": "The missing term is **“synthesizing”**, as in:\n\n“…an Abstractive compressor which generates a summary **synthesizing** information from multiple retrieved documents.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'synthesizing' and is directly supported by the provided context quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.573, "llm_ms": 1464.515, "top_contexts": [{"rank": 1, "score": 0.79568034, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 11, "text_snippet": " prepending it as input to a language model at inference time. The compressed summary guides the LM to generate the correct answer, while significantly reducing the computation costs required to encode the documents. We propose compressors:"}, {"rank": 2, "score": 0.7748109, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 2, "text_snippet": "d an abstractive compressor which generates summaries by synthesizing information from multiple documents. Both compressors are trained to improve LMs’ performance on end tasks when the generated summaries are prepended to the LMs’ input, w"}, {"rank": 3, "score": 0.74209595, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 19, "text_snippet": "ining compressors for conciseness and effectiveness. We summarize the key ideas for our two compressors, extractive compressors and abstractive compressor here, and discuss their training schemes formally in Section 3. 2Improving retriever "}, {"rank": 4, "score": 0.7331537, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 12, "text_snippet": "ompressors implement multi-document query-focused summarization (Xu & Lapata, 2020), where we summarize retrieved evidence document set with respect to the input query. As we aim to enable RALM to generate correct output when summary is pre"}, {"rank": 5, "score": 0.72340393, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 1, "text_snippet": "ents, often spanning hundreds of words, make inference substantially more expensive. We propose compressing the retrieved documents into textual sum- maries prior to in-context integration. This not only reduces the computational costs but "}]}
{"case_index": 170, "query": "According to the paper 'replug retrieval augmented black box language models', what exact term completes this statement: \"As shown inarXiv:2301.12652v4 [cs.CL] 24 May 2023 REPLUG: [BLANK] Black-Box Language Models Figure 1, REPLUG is extremely flexible\"?", "gold": "retrieval-augmented", "answer": "The missing term is **“Retrieval-Augmented”**, completing the title as:\n\n**“REPLUG: Retrieval-Augmented Black-Box Language Models.”**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer correctly fills the blank with 'Retrieval-Augmented' (i.e., retrieval-augmented) and is directly supported by the provided context showing the paper title.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.3333333333333333, "hit_rank": 3, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.294, "llm_ms": 1482.027, "top_contexts": [{"rank": 1, "score": 0.791049, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 8, "text_snippet": "Black-Box Language Models Figure 1, REPLUG is extremely flexible and can be used with any existing black-box LM and retrieval model. We also introduce REPLUG LSR (REPLUG with LM- Supervised Retrieval), a training scheme that can further imp"}, {"rank": 2, "score": 0.76148075, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 71, "text_snippet": "a black box and augments it with a tuneable retrieval model. Our evaluation shows that REPLUG can be integrated with any existing language model to improve their performance  REPLUG: Retrieval-Augmented Black-Box Language Models on language"}, {"rank": 3, "score": 0.7282231, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 7, "text_snippet": " retrieved documents are prepended to the input context and fed into the black-box LM to make the final prediction. Because the LM context length limits the number of documents that can be prepended, we also introduce a new ensemble scheme "}, {"rank": 4, "score": 0.71681976, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 5, "text_snippet": "ting the LM’s parameters, REPLUG treats the language model as a black box and augments it with a frozen or tunable retriever. This black-box assumption makes REPLUG applicable to large LMs (i.e., >100B parameters), which are often served vi"}, {"rank": 5, "score": 0.71229935, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 0, "text_snippet": "REPLUG: Retrieval-Augmented Black-Box Language Models Weijia Shi,1 *Sewon Min,1Michihiro Yasunaga,2Minjoon Seo,3Rich James,4Mike Lewis,4 Luke Zettlemoyer1 4Wen-tau Yih4 Abstract We introduce REPLUG, a retrieval-augmented lan- guage modeling"}]}
{"case_index": 171, "query": "According to the paper 'corrective retrieval augmented generation', what exact term completes this statement: \"Ms) have attracted increasing attention and exhibited impressive abili- ties to understand [BLANK] and generate fluent language texts (Brown et al., 2020; Ouyang et al., 2022; Touvron et al., 2023a).\"?", "gold": "instructions", "answer": "The missing term is **“instructions”**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context 1 explicitly contains the phrase 'understand instructions', matching the expected answer and the model response.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.068, "llm_ms": 1079.038, "top_contexts": [{"rank": 1, "score": 0.6669978, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 4, "text_snippet": "Ms) have attracted increasing attention and exhibited impressive abili- ties to understand instructions and generate fluent language texts (Brown et al., 2020; Ouyang et al., 2022; Touvron et al., 2023a). Nevertheless, LLMs inevitably manif"}, {"rank": 2, "score": 0.65142053, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 4, "text_snippet": " for long-form generations relative to these models.1 1 I NTRODUCTION State-of-the-art LLMs continue to struggle with factual errors (Mallen et al., 2023; Min et al., 2023) despite their increased model and data scale (Ouyang et al., 2022)."}, {"rank": 3, "score": 0.62492174, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 53, "text_snippet": "rieved documents as the answer (Chen et al., 2017; Clark & Gardner, 2018; Wang et al., 2019; Karpukhin et al., 2020), a method inspired by reading comprehension (Richardson, 2013; Rajpurkar et al., 2016). Recently, generating the answer as "}, {"rank": 4, "score": 0.62119776, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 15, "text_snippet": " that answers do not correspond to spans in support documents, thus requiring ab- stractive models. Raffel et al. (2019) showed that generative models are competitive for reading com- prehension tasks such as SQuAD (Rajpurkar et al., 2016),"}, {"rank": 5, "score": 0.6163727, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 0, "text_snippet": "Corrective Retrieval Augmented Generation Shi-Qi Yan1*, Jia-Chen Gu2*, Yun Zhu3, Zhen-Hua Ling1 1National Engineering Research Center of Speech and Language Information Processing, University of Science and Technology of China, Hefei, China"}]}
{"case_index": 172, "query": "According to the paper 'corrective retrieval augmented generation', what exact term completes this statement: \"Furthermore, current methods mostly treat complete documents as reference knowledge both during retrieval and [BLANK].\"?", "gold": "utilization", "answer": "The missing term is **“generation”** — i.e., “…both during retrieval and **generation**.”", "answer_score": 0.0, "answer_correctness": 0.0, "answer_groundedness": 0.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 0.0, "abstained": false, "should_abstain": false, "notes": "Expected blank is 'utilization' (as in 'during retrieval and utilization'), but the model answered 'generation'. The provided context snippet mentions 'improve the utilization of documents', supporting the expected term; the model answer is not supported.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 12.531, "llm_ms": 1313.044, "top_contexts": [{"rank": 1, "score": 0.6516441, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 10, "text_snippet": " should not have been equally referred to and involved in RAG. On account of the above issues, this paper particularly studies the scenarios where the retriever returns inaccurate results. A method named Corrective Retrieval- Augmented Gene"}, {"rank": 2, "score": 0.62458956, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 1, "text_snippet": "evitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Al- though retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heav"}, {"rank": 3, "score": 0.62197626, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 18, "text_snippet": "pact the LLMs and exacerbate the problems. Thus, it is obvious that the lack of accurate and specific knowledge can lead to misleading or even inaccurate generation, which will severely hurt the experience of users in most practical applica"}, {"rank": 4, "score": 0.6139892, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 79, "text_snippet": "ges are provided as input (correct, incorrect and ambiguous content). All the data in the table only represents a rough estimate of the generation phase, the retrieval and data-processing stages are not included. anism, rather than solely f"}, {"rank": 5, "score": 0.6139723, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 20, "text_snippet": "model that specializes in response generation. Despite this, the methods above usually ignore a question, what if the retrieval goes wrong? Since the purpose of introducing a retrieval is to secure that generative LMs can obtain relevant an"}]}
{"case_index": 173, "query": "According to the paper 'recomp improving retrieval augmented lms with compression and selective augmentation', what exact term completes this statement: \"Early US models include X, S and PRO-4X, with a choice of 6-speed manual or 5-speed automatic [BLANK], a choice of [...] moved from Smyrna, Tennessee, to Nissan's facility in Canton, Mississippi.\"?", "gold": "transmissions", "answer": "The missing term is **“transmissions”** (i.e., “…with a choice of 6-speed manual or 5-speed automatic **transmissions**, …”).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'transmissions' and is directly supported by the provided context snippet.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 9.062, "llm_ms": 1156.537, "top_contexts": [{"rank": 1, "score": 0.7679409, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 9, "text_snippet": "acility in Canton, Mississippi. Early US models include X, S and PRO-4X, with a choice of 6-speed manual or 5-speed automatic transmissions, a choice of [...] moved from Smyrna, Tennessee, to Nissan's facility in Canton, Mississippi. Early "}, {"rank": 2, "score": 0.7438742, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 8, "text_snippet": " moved from Smyrna, Tennessee, to Nissan's facility in Canton, Mississippi. Early US models include X, S and PRO-4X, with a choice of 6-speed manual or 5-speed automatic transmissions, a choice of [...] moved from Smyrna, Tennessee, to Niss"}, {"rank": 3, "score": 0.589267, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 10, "text_snippet": "els include X, S and PRO-4X, with a choice of 6-speed manual…Retrieved documents DRECOMP (58 tokens)RetrieveCompressPrependNo retrieval (0 tokens)RALM (749 tokens)2010 ❌ ✅2015SummaryInput query xwhen did they stop making the nissan xterra?B"}, {"rank": 4, "score": 0.44867414, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 7, "text_snippet": "rieved evidence documents, and guide RALM to generate desired outputs when prepended to the input. To satisfy both efficiency and effectiveness constraints, our compressor strategically performs selective augmentation by generating an empty"}, {"rank": 5, "score": 0.31348905, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 24, "text_snippet": "TRACTIVE COMPRESSION As we formulate extractive compression as a ranking problem, training extractive compressor re- sembles training a reranker for the retrieved documents4with two differences. First, our compressor considers a different g"}]}
{"case_index": 174, "query": "According to the paper 'raptor recursive abstractive processing for tree organized retrieval', what exact term completes this statement: \"uestion answering systems (Chen et al., 2017; Yu et al., 2018), is to index large quantities of text, after splitting it into chunks (paragraphs), in a separate [BLANK] retrieval system.\"?", "gold": "information", "answer": "The missing term is **“information”**, completing the phrase as:\n\n“…in a separate **information retrieval system**.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'information' and is directly supported by Context 1 ('information retrieval system').", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.968, "llm_ms": 1383.941, "top_contexts": [{"rank": 1, "score": 0.73357385, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 5, "text_snippet": "uestion answering systems (Chen et al., 2017; Yu et al., 2018), is to index large quantities of text, after splitting it into chunks (paragraphs), in a separate information retrieval system. Retrieved information is then presented to the LL"}, {"rank": 2, "score": 0.69518006, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 8, "text_snippet": " indexing and retrieval system that uses a tree structure to capture both high-level and low-level details about a text. As shown in Figure 1, our system, RAPTOR, clusters chunks of text, generates text summaries of those clusters, and then"}, {"rank": 3, "score": 0.6453314, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 51, "text_snippet": "answering (Voorhees et al., 1999), documents are often retrieved from Wikipedia (Chen et al., 2017). Recently, dense retrievers based on neural networks have become popular. These usually follow a dual-encoder architecture (Yih et al., 2011"}, {"rank": 4, "score": 0.64088213, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 14, "text_snippet": "omponents: the retriever, the reader, and end-to-end system training. Retrieval methods have transitioned from traditional term-based techniques like TF-IDF (Sp¨arck Jones, 1972) and BM25 (Robertson et al., 1995; Roberts et al., 2020) to de"}, {"rank": 5, "score": 0.6340654, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 73, "text_snippet": "an effectively handle a wider range of questions, from higher-order thematic queries to detail-oriented questions. Detailed results for additional stories and an ablation study on layer contributions can be found in Appendix I. 5 C ONCLUSIO"}]}
{"case_index": 175, "query": "According to the paper 'corrective retrieval augmented generation', what exact term completes this statement: \"The relevance score is quantified into a total of three confidence degrees and then triggered the [BLANK] actions: { Correct ,Incorrect , Ambiguous } (Section 4.3).\"?", "gold": "corresponding", "answer": "The missing term is **“corresponding”**, i.e., the statement is:\n\n“The relevance score is quantified into a total of three confidence degrees and then triggered the **corresponding** actions: { Correct, Incorrect, Ambiguous } (Section 4.3).”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'corresponding' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.102, "llm_ms": 1888.528, "top_contexts": [{"rank": 1, "score": 0.6488342, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 27, "text_snippet": "or is constructed to estimate the relevance score of retrieved documents to the input query (Sec- tion 4.2). The relevance score is quantified into a total of three confidence degrees and then triggered the corresponding actions: { Correct "}, {"rank": 2, "score": 0.6468585, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 39, "text_snippet": "ngly where the upper and lower thresholds are set. If the confidence score is higher than the upper threshold, the retrieved document is identified as Correct , while identified as Incorrect if below the lower threshold. Otherwise, a more s"}, {"rank": 3, "score": 0.6170788, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 28, "text_snippet": "sition, filter, and recomposition (Section 4.4). If the action Incorrect is triggered, the retrieved documents will be discarded. Instead, web searches are resorted to and regarded as complementary knowledge sources for corrections (Section"}, {"rank": 4, "score": 0.6080613, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 42, "text_snippet": " fabricated facts. Therefore, we need to seek new sources of knowledge for correction. Here, web search is introduced to search from the Internet as elaborated in Section 4.5. This corrective action helps overcome the embarrassing challenge"}, {"rank": 5, "score": 0.594977, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 11, "text_snippet": "lity of retrieved documents for a query. This serves as a crucial component in RAG, contributing to informative generation by reviewing and evaluating the relevance and reliability of the retrieved documents. A confidence degree is quantifi"}]}
{"case_index": 176, "query": "According to the paper 'leveraging passage retrieval with generative models for open domain question answering', what exact term completes this statement: \"Born in Maida Vale, London…Where was Alan Turing born?[BLANK]2seq modelMaida Vale, LondonFigure 1: A simple approach to open domain question answering.\"?", "gold": "generativeseq", "answer": "The missing term is **“Generative”**, completing the phrase as:\n\n“…Where was Alan Turing born? **Generative** seq2seq model Maida Vale, London Figure 1: A simple approach to open domain question answering.”", "answer_score": 0.41000000000000003, "answer_correctness": 0.2, "answer_groundedness": 0.9, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.2, "groundedness": 0.9, "abstained": false, "should_abstain": false, "notes": "Expected exact term is 'generativeseq' (as in 'Generativeseq2seq model'). Model answered 'Generative', which is incomplete/incorrect. Context supports the exact concatenated term.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 11.007, "llm_ms": 1905.543, "top_contexts": [{"rank": 1, "score": 0.7082403, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 4, "text_snippet": "external source of knowledge, such as Wikipedia. Retrieval based approaches were previously con- sidered in the context of open domain question answering with extractive models (Chen et al., 2017). In that case, systems start by retrieving "}, {"rank": 2, "score": 0.44165558, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 20, "text_snippet": "es with SpaCy.2In DPR, passages and questions are represented as dense vector representations, computed using two BERT networks. The ranking function is the dot product between the query and passage represen- tations. Retrieval is performed"}, {"rank": 3, "score": 0.4382326, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 21, "text_snippet": "Raffel et al., 2019; Lewis et al., 2019). The model takes as input the question, as well as the support passages, and generates the answer. More precisely, each retrieved passage and its title are concatenated with the question, and process"}, {"rank": 4, "score": 0.43262833, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}, {"rank": 5, "score": 0.42740715, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 0, "text_snippet": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering Gautier Izacard1,2,3Edouard Grave1 1Facebook AI Research, Paris 2ENS, PSL University, Paris 3Inria, Paris gizacard|egrave@fb.com Abstract Generative mode"}]}
{"case_index": 177, "query": "According to the paper 'raptor recursive abstractive processing for tree organized retrieval', what exact term completes this statement: \"Retrieval methods have [BLANK] from traditional term-based techniques like TF-IDF (Sp¨arck Jones, 1972) and BM25 (Robertson et al., 1995; Roberts et al., 2020) to deep learning–based strategies (Karpukhin et al., 2020; Khattab & Zaharia, 2020; Sachan et al., 2023).\"?", "gold": "transitioned", "answer": "The missing term is **“transitioned”**:\n\n“Retrieval methods have **transitioned** from traditional term-based techniques like TF-IDF (Sp¨arck Jones, 1972) and BM25 (Robertson et al., 1995; Roberts et al., 2020) to deep learning–based strategies (Karpukhin et al., 2020; Khattab & Zaharia, 2020; Sachan et al., 2023).”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'transitioned' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 15.937, "llm_ms": 2172.298, "top_contexts": [{"rank": 1, "score": 0.70615596, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 14, "text_snippet": "omponents: the retriever, the reader, and end-to-end system training. Retrieval methods have transitioned from traditional term-based techniques like TF-IDF (Sp¨arck Jones, 1972) and BM25 (Robertson et al., 1995; Roberts et al., 2020) to de"}, {"rank": 2, "score": 0.6069161, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 4, "text_snippet": "trieval-augmented language models (RALMs) (Khandelwal et al., 2019; Izacard et al., 2022; Lewis et al., 2020; Borgeaud et al., 2022) have shown impressive performance on knowledge-intensive tasks (Kwiatkowski et al., 2019; Petroni et al., 2"}, {"rank": 3, "score": 0.6003126, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 50, "text_snippet": "trieval improves performance across a variety of tasks such as question answering (Voorhees et al., 1999; Chen et al., 2017; Kwiatkowski et al., 2019), fact checking (Thorne et al., 2018), dialogue (Dinan et al., 2019) or citation recommend"}, {"rank": 4, "score": 0.59111744, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 0, "text_snippet": "Published as a conference paper at ICLR 2024 RAPTOR: R ECURSIVE ABSTRACTIVE PROCESSING FOR TREE-ORGANIZED RETRIEVAL Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D. Manning Stanford University psarthi@cs."}, {"rank": 5, "score": 0.58308583, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 2, "text_snippet": "hat retrieval with recursive summaries offers significant improvements over tra- ditional retrieval-augmented LMs on several tasks. On question-answering tasks that involve complex, multi-step reasoning, we show state-of-the-art results; fo"}]}
{"case_index": 178, "query": "According to the paper 'leveraging passage retrieval with generative models for open domain question answering', what exact term completes this statement: \"While being a [BLANK] problem in natural lan- guage processing (V oorhees et al., 199\"?", "gold": "longstanding", "answer": "The missing term is **“longstanding”**, as in:\n\n“While being a **longstanding** problem in natural language processing (Voorhees et al., 1999) …”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'longstanding' and is directly supported by Context 3 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.3333333333333333, "hit_rank": 3, "matched_by": "doc_id+chunk_id", "retrieval_ms": 11.938, "llm_ms": 1895.27, "top_contexts": [{"rank": 1, "score": 0.6151655, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 10, "text_snippet": " problem in natural lan- guage processing (V oorhees et al., 1999), this task has recently regained interest following the work by Chen et al. (2017). In that version of the prob- lem, strong supervision is available to the learning system,"}, {"rank": 2, "score": 0.5934929, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 16, "text_snippet": "ous NLP tasks, including language mod- eling (Min et al., 2022; Borgeaud et al., 2022; Khandelwal et al., 2020) and open-domain question answering (Lewis et al., 2020; Izacard et al., 2022b; Hu et al., 2022). Specifi- cally, using the input"}, {"rank": 3, "score": 0.5921876, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 9, "text_snippet": "rticular, we show that the performance of our method signiﬁcantly improves when the number of retrieved passages increases. We believe that this is evidence that generative mod- els are good at combining evidence from multiple passages, com"}, {"rank": 4, "score": 0.5799062, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}, {"rank": 5, "score": 0.5780904, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 69, "text_snippet": "ance across a variety of NLP tasks when considered in isolation. Such tasks include open-domain question answering [ 5,29], fact checking [ 56], fact completion [ 48], long-form question answering [ 12], Wikipedia article generation [ 36], "}]}
{"case_index": 179, "query": "According to the paper 'replug retrieval augmented black box language models', what exact term completes this statement: \"This style of retrieval can be added to both encoder- decoder (Yu, 2022; Izacard et al., 2022b) and [BLANK] models (Khandelwal et al., 2020; Borgeaud et al., 2022; Shi et al., 2022; Rubin et al., 2022).\"?", "gold": "decoder-only", "answer": "The missing term is **“decoder-only”**, completing the sentence as: “…and **decoder-only** models (Khandelwal et al., 2020; Borgeaud et al., 2022; Shi et al., 2022; Rubin et al., 2022).”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'decoder-only' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 17.41, "llm_ms": 1842.043, "top_contexts": [{"rank": 1, "score": 0.7741655, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 17, "text_snippet": "inal predic- tion. This style of retrieval can be added to both encoder- decoder (Yu, 2022; Izacard et al., 2022b) and decoder-only models (Khandelwal et al., 2020; Borgeaud et al., 2022; Shi et al., 2022; Rubin et al., 2022). For example, "}, {"rank": 2, "score": 0.7101175, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 8, "text_snippet": "Black-Box Language Models Figure 1, REPLUG is extremely flexible and can be used with any existing black-box LM and retrieval model. We also introduce REPLUG LSR (REPLUG with LM- Supervised Retrieval), a training scheme that can further imp"}, {"rank": 3, "score": 0.70615965, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 0, "text_snippet": "REPLUG: Retrieval-Augmented Black-Box Language Models Weijia Shi,1 *Sewon Min,1Michihiro Yasunaga,2Minjoon Seo,3Rich James,4Mike Lewis,4 Luke Zettlemoyer1 4Wen-tau Yih4 Abstract We introduce REPLUG, a retrieval-augmented lan- guage modeling"}, {"rank": 4, "score": 0.70301324, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 71, "text_snippet": "a black box and augments it with a tuneable retrieval model. Our evaluation shows that REPLUG can be integrated with any existing language model to improve their performance  REPLUG: Retrieval-Augmented Black-Box Language Models on language"}, {"rank": 5, "score": 0.69310033, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 4, "text_snippet": "trieval-augmented language models require access to the internal LM repre- sentations (e.g., to train the model (Borgeaud et al., 2022; 1University of Washington2Stanford University3KAIST4Meta AI. *Work done while the first author was inter"}]}
{"case_index": 180, "query": "According to the paper 'raptor recursive abstractive processing for tree organized retrieval', what exact term completes this statement: \"(2023) highlights a potential [BLANK]: contiguous seg- mentation might not capture the complete semantic depth of the text.\"?", "gold": "shortcoming", "answer": "The missing term is **“shortcoming”**: “…Nair et al. (2023) highlights a potential **shortcoming**: contiguous segmentation might not capture the complete semantic depth of the text.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'shortcoming' and is directly supported by Context 5.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.2, "hit_rank": 5, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.577, "llm_ms": 1417.111, "top_contexts": [{"rank": 1, "score": 0.69513845, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 22, "text_snippet": " RAPTOR addresses the issue of semantic depth and connection in reading by building a recursive tree structure that balances broader thematic comprehension with granular details and which allows nodes to be grouped based on semantic sim- il"}, {"rank": 2, "score": 0.6527871, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 21, "text_snippet": "ntermediate nodes thus storing varying levels of detail, keeping granular details. However, both methods, due to their reliance on adjacency for grouping or summarizing adjacent nodes, may still overlook distant interdependencies within the"}, {"rank": 3, "score": 0.62922627, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 1, "text_snippet": "rom a retrieval corpus, limiting holistic under- standing of the overall document context. We introduce the novel approach of recursively embedding, clustering, and summarizing chunks of text, constructing a tree with differing levels of su"}, {"rank": 4, "score": 0.6143548, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 8, "text_snippet": " indexing and retrieval system that uses a tree structure to capture both high-level and low-level details about a text. As shown in Figure 1, our system, RAPTOR, clusters chunks of text, generates text summaries of those clusters, and then"}, {"rank": 5, "score": 0.61264795, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 18, "text_snippet": " ICLR 2024 Despite a diversity in methods, the retrieving components of models predominantly rely on stan- dard approaches, i.e., chunking corpora and encoding with BERT-based retrievers. Although this approach is widely adopted, Nair et al"}]}
{"case_index": 181, "query": "According to the paper 'self rag learning to retrieve generate and critique through self reflection', what exact term completes this statement: \"for long-form generations relative to these models.1 1 I NTRODUCTION [BLANK] LLMs continue to struggle with factual errors (Mallen et al., 2023; Min et al., 2023) despite their increased model and data scale (Ouyang et al., 2022).\"?", "gold": "state-of-the-art", "answer": "The missing term is:\n\n**“State-of-the-art”**\n\nSo the sentence reads: **“State-of-the-art LLMs continue to struggle with factual errors (Mallen et al., 2023; Min et al., 2023) despite their increased model and data scale (Ouyang et al., 2022).”**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'state-of-the-art' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 15.145, "llm_ms": 2049.453, "top_contexts": [{"rank": 1, "score": 0.7899615, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 4, "text_snippet": " for long-form generations relative to these models.1 1 I NTRODUCTION State-of-the-art LLMs continue to struggle with factual errors (Mallen et al., 2023; Min et al., 2023) despite their increased model and data scale (Ouyang et al., 2022)."}, {"rank": 2, "score": 0.72458094, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 6, "text_snippet": "y trained to leverage and follow facts from provided passages. This work introduces Self-Reflective Retrieval-augmented Generation ( SELF-RAG)to improve an LLM’s generation quality, including its factual accuracy without hurting its versati"}, {"rank": 3, "score": 0.70479035, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 0, "text_snippet": "Preprint. SELF-RAG: LEARNING TO RETRIEVE , GENERATE ,AND CRITIQUE THROUGH SELF-REFLECTION Akari Asai†, Zeqiu Wu†, Yizhong Wang†§, Avirup Sil‡, Hannaneh Hajishirzi†§ †University of Washington§Allen Institute for AI‡IBM Research AI {akari,zeq"}, {"rank": 4, "score": 0.690099, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 96, "text_snippet": "ains an LM to learn to retrieve, generate, and critique text passages and its own generation by predicting the next tokens from its original vocabulary as well as newly added special tokens, called reflection tokens. SELF-RAGfurther enables"}, {"rank": 5, "score": 0.68979406, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 27, "text_snippet": "n (reasoning path consistency) and without retrieval. Recent work on LLM refinement (Dhuliawala et al., 2023; Madaan et al., 2023; Paul et al., 2023) prompts a model to generate task output, natural language feedback and refined task output"}]}
{"case_index": 182, "query": "According to the paper 'from local to global a graph rag approach to query focused summarization', what exact term completes this statement: \"cted communities, with summaries at higher levels of the hierarchy recursively [BLANK] lower-level summaries.\"?", "gold": "incorporating", "answer": "The missing term is **“incorporating”** — i.e., “…with summaries at higher levels of the hierarchy recursively **incorporating** lower-level summaries.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'incorporating' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.511, "llm_ms": 1520.047, "top_contexts": [{"rank": 1, "score": 0.6856779, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 10, "text_snippet": "cted communities, with summaries at higher levels of the hierarchy recursively incorporating lower-level summaries. Together, these community summaries provide global descriptions and insights over the corpus. Finally, GraphRAG answers quer"}, {"rank": 2, "score": 0.66535157, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 44, "text_snippet": "ummaries at one level looking for general themes of interest, then read linked reports at a lower level that provide additional details for each subtopic. Here, however, we focus on their utility as part of a graph-based index used for answ"}, {"rank": 3, "score": 0.6319482, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 42, "text_snippet": "hical manner, recursively detecting sub-communities within each detected community until reaching leaf communities that can no longer be partitioned. 5  Each level of this hierarchy provides a community partition that covers the nodes of th"}, {"rank": 4, "score": 0.6311509, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 45, "text_snippet": "ommunities are used to generate summaries for higher-level communities as follows: •Leaf-level communities . The element summaries of a leaf-level community are prioritized and then iteratively added to the LLM context window until the toke"}, {"rank": 5, "score": 0.6224142, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 17, "text_snippet": " are later used to answer queries as in Mao et al. 2020). These summaries are generated in parallel and iteratively aggregated into global summaries, similar to prior techniques (Feng et al., 2023; Gao et al., 2023; Khattab et al., 2022; Sh"}]}
{"case_index": 183, "query": "According to the paper 'refrag rethinking rag based decoding', what exact term completes this statement: \"To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG [BLANK].\"?", "gold": "applications", "answer": "The missing term is **“applications”** — i.e., “…to improve latency in RAG **applications**.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'applications' and is directly supported by Context 1/2.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.273, "llm_ms": 1377.292, "top_contexts": [{"rank": 1, "score": 0.7893053, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 3, "text_snippet": " on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30 .85×the time-t"}, {"rank": 2, "score": 0.7503037, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 95, "text_snippet": "oducedREFRAG, a novel and efficient decoding framework tailored for RAG applications. By leveraging the inherent sparsity and block-diagonal attention patterns present in RAG contexts,REFRAG compresses, senses, and expands context represent"}, {"rank": 3, "score": 0.7380713, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 12, "text_snippet": "is information is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-a"}, {"rank": 4, "score": 0.70016736, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 94, "text_snippet": "Complementary approaches rank or prune context by estimated informativeness, e.g.,Selective Contextuses self-information to drop low-value tokens, and sentence-level methods learn context-aware encoders for question-specific compression and"}, {"rank": 5, "score": 0.6988789, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 15, "text_snippet": "igure 1) while preserving the autoregressive nature of the decoder, thereby supporting multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight reinforcement learning (RL) policy that sel"}]}
{"case_index": 184, "query": "According to the paper 'refrag rethinking rag based decoding', what exact term completes this statement: \"The objective is to align any encoder–decoder combination so that the generations produced [BLANK] contextclosely resemble those generated by the original decoder with access to the full context.\"?", "gold": "withcompressed", "answer": "The missing term is **“with compressed”**, i.e.:\n\n“…so that the generations produced **with compressed** context closely resemble those generated by the original decoder with access to the full context.”", "answer_score": 0.7, "answer_correctness": 1.0, "answer_groundedness": 0.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 0.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term ('withcompressed') in meaning; however the provided context snippets do not contain the quoted sentence or the term, so the answer is not supported by the given context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 12.48, "llm_ms": 1891.038, "top_contexts": [{"rank": 1, "score": 0.5916771, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 3, "text_snippet": " on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30 .85×the time-t"}, {"rank": 2, "score": 0.58585566, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 12, "text_snippet": "is information is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-a"}, {"rank": 3, "score": 0.58505225, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 15, "text_snippet": "igure 1) while preserving the autoregressive nature of the decoder, thereby supporting multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight reinforcement learning (RL) policy that sel"}, {"rank": 4, "score": 0.5777133, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 94, "text_snippet": "Complementary approaches rank or prune context by estimated informativeness, e.g.,Selective Contextuses self-information to drop low-value tokens, and sentence-level methods learn context-aware encoders for question-specific compression and"}, {"rank": 5, "score": 0.5631132, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 16, "text_snippet": "ated segment as an integral part of the generation output. SELF-RAGfurther enables a customizable decoding algorithm to satisfy hard or soft constraints, which are defined by reflection token predictions. In particular, our inference-time a"}]}
{"case_index": 185, "query": "According to the paper 'retrieval augmented generation for knowledge intensive nlp tasks', what exact term completes this statement: \"For language generation tasks, we ﬁnd that RAG models generate more speciﬁc, diverse and factual language than a [BLANK] parametric-only seq2seq baseline.\"?", "gold": "state-of-the-art", "answer": "The missing term is **“state-of-the-art”**, i.e., “…than a **state-of-the-art** parametric-only seq2seq baseline.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'state-of-the-art' and is directly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 12.163, "llm_ms": 1522.575, "top_contexts": [{"rank": 1, "score": 0.75205606, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 4, "text_snippet": "architectures. For language generation tasks, we ﬁnd that RAG models generate more speciﬁc, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline. 1 Introduction Pre-trained neural language models have been s"}, {"rank": 2, "score": 0.7261578, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 9, "text_snippet": "y to the “workhorse of NLP,” i.e. sequence-to-sequence (seq2seq) models. We endow pre-trained, parametric-memory generation models with a non-parametric memory through a general-purpose ﬁne-tuning approach which we refer to as retrieval-aug"}, {"rank": 3, "score": 0.72235286, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 1, "text_snippet": "e reliance on the paramet- ric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and inco"}, {"rank": 4, "score": 0.7205179, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 4, "text_snippet": " for long-form generations relative to these models.1 1 I NTRODUCTION State-of-the-art LLMs continue to struggle with factual errors (Mallen et al., 2023; Min et al., 2023) despite their increased model and data scale (Ouyang et al., 2022)."}, {"rank": 5, "score": 0.71692884, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 2, "text_snippet": "anism to explicit non-parametric memory have so far been only investigated for extractive downstream tasks. We explore a general-purpose ﬁne-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trained parametri"}]}
{"case_index": 186, "query": "According to the paper 'leveraging passage retrieval with generative models for open domain question answering', what exact term completes this statement: \"On the other hand, pro- cessing passages jointly in the decoder allows to better [BLANK] evidence from multiple passages.\"?", "gold": "aggregate", "answer": "The missing term is **“combine”** — as in: “processing passages jointly in the decoder allows to better **combine** evidence from multiple passages.”", "answer_score": 0.06, "answer_correctness": 0.0, "answer_groundedness": 0.2, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 0.2, "abstained": false, "should_abstain": false, "notes": "Expected term is 'aggregate', not 'combine'. Context explicitly mentions 'aggregate and combine evidence', supporting 'aggregate' for the blank. Model answered 'combine' incorrectly.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 6.358, "llm_ms": 1279.579, "top_contexts": [{"rank": 1, "score": 0.6992816, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 1, "text_snippet": "x- pensive to train and query. In this paper, we investigate how much these models can ben- eﬁt from retrieving text passages, potentially containing evidence. We obtain state-of-the- art results on the Natural Questions and Triv- iaQA open"}, {"rank": 2, "score": 0.6826261, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 2, "text_snippet": "s a ﬂexible framework to efﬁciently aggregate and com- bine evidence from multiple passages. 1 Introduction Recently, several works have shown that factual information can be extracted from large scale language models trained on vast quanti"}, {"rank": 3, "score": 0.6761409, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 31, "text_snippet": "tion answering. We observe that while conceptu- ally simple, this method outperforms existing work on the NaturalQuestion and TriviaQA benchmarks. In particular, generative models seem to perform well when evidence from multiple passages ne"}, {"rank": 4, "score": 0.6605023, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 9, "text_snippet": "rticular, we show that the performance of our method signiﬁcantly improves when the number of retrieved passages increases. We believe that this is evidence that generative mod- els are good at combining evidence from multiple passages, com"}, {"rank": 5, "score": 0.6431678, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}]}
{"case_index": 187, "query": "According to the paper 'retrieval augmented generation for knowledge intensive nlp tasks', what exact term completes this statement: \"We endow pre-trained, parametric-memory generation models with a non-parametric memory through a general-purpose ﬁne-tuning approach which we refer to as [BLANK] generation (RAG).\"?", "gold": "retrieval-augmented", "answer": "The missing term is **“retrieval-augmented”**, completing the statement as **“retrieval-augmented generation (RAG).”**", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'retrieval-augmented' and is directly supported by Context 1/2 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 14.073, "llm_ms": 1461.347, "top_contexts": [{"rank": 1, "score": 0.7653096, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 9, "text_snippet": "y to the “workhorse of NLP,” i.e. sequence-to-sequence (seq2seq) models. We endow pre-trained, parametric-memory generation models with a non-parametric memory through a general-purpose ﬁne-tuning approach which we refer to as retrieval-aug"}, {"rank": 2, "score": 0.75265706, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 2, "text_snippet": "anism to explicit non-parametric memory have so far been only investigated for extractive downstream tasks. We explore a general-purpose ﬁne-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trained parametri"}, {"rank": 3, "score": 0.68698037, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 1, "text_snippet": "e reliance on the paramet- ric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and inco"}, {"rank": 4, "score": 0.67636776, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 7, "text_snippet": "ieval-augmented generation (RAG) (Lewis et al., 2020). In this framework, the input to models is augmented by prepending relevant documents that are retrieved from an external knowledge corpus (Guu et al., 2020). While RAG serves as a pract"}, {"rank": 5, "score": 0.6506307, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 12, "text_snippet": "ck- augmented networks [ 25] and memory layers [ 30]. In contrast, we explore a setting where both parametric and non-parametric memory components are pre-trained and pre-loaded with extensive knowledge. Crucially, by using pre-trained acce"}]}
{"case_index": 188, "query": "According to the paper 'self rag learning to retrieve generate and critique through self reflection', what exact term completes this statement: \"If so, it outputs a retrieval token that calls a [BLANK] model on demand (Step 1).\"?", "gold": "retriever", "answer": "The missing term is **“retriever”** — i.e., “it outputs a retrieval token that calls a **retriever** model on demand (Step 1).”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'retriever' and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 13.301, "llm_ms": 1531.305, "top_contexts": [{"rank": 1, "score": 0.694212, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 7, "text_snippet": "rmittent special tokens (i.e., reflection tokens ). Reflection tokens are categorized into retrieval andcritique tokens to indicate the need for retrieval and its generation quality respectively (Figure 1 right). In particular, given an inp"}, {"rank": 2, "score": 0.67702687, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 34, "text_snippet": "l predicts the next output segment, as it does in a standard LM. If retrieval is needed, the model generates: a critique token to evaluate the retrieved passage’s relevance, the next response segment, and a critique token to evaluate if the"}, {"rank": 3, "score": 0.67396444, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 2, "text_snippet": "ew framework called Self-Reflective Retrieval-Augmented Gen- eration ( SELF-RAG)that enhances an LM’s quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passage"}, {"rank": 4, "score": 0.6730982, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 36, "text_snippet": "text with reflection tokens by unifying them as next token predictions from the expanded model vocabulary (i.e., the original vocabulary plus reflection tokens). Specifically, we train the generator model Mon a curated corpus with interleav"}, {"rank": 5, "score": 0.66940373, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 6, "text_snippet": "y trained to leverage and follow facts from provided passages. This work introduces Self-Reflective Retrieval-augmented Generation ( SELF-RAG)to improve an LLM’s generation quality, including its factual accuracy without hurting its versati"}]}
{"case_index": 189, "query": "According to the paper 'corrective retrieval augmented generation', what exact term completes this statement: \"It usually provides an extra knowledge source from a specific corpus, i.e., Wikipedia, which greatly improves the per- formance of LMs in a variety of tasks, especially in the [BLANK] ones.\"?", "gold": "knowledge-intensive", "answer": "The missing term is **“knowledge-intensive”**, completing the sentence as: “…especially in the **knowledge-intensive** ones.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'knowledge-intensive' and is explicitly supported by Context 1.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 9.541, "llm_ms": 1414.429, "top_contexts": [{"rank": 1, "score": 0.7020929, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 19, "text_snippet": " documents. It usually provides an extra knowledge source from a specific corpus, i.e., Wikipedia, which greatly improves the per- formance of LMs in a variety of tasks, especially in the knowledge-intensive ones. The proposed  methods gene"}, {"rank": 2, "score": 0.6339141, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 18, "text_snippet": "pact the LLMs and exacerbate the problems. Thus, it is obvious that the lack of accurate and specific knowledge can lead to misleading or even inaccurate generation, which will severely hurt the experience of users in most practical applica"}, {"rank": 3, "score": 0.6093771, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 4, "text_snippet": "trieval-augmented language models (RALMs) (Khandelwal et al., 2019; Izacard et al., 2022; Lewis et al., 2020; Borgeaud et al., 2022) have shown impressive performance on knowledge-intensive tasks (Kwiatkowski et al., 2019; Petroni et al., 2"}, {"rank": 4, "score": 0.60695803, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 7, "text_snippet": "ieval-augmented generation (RAG) (Lewis et al., 2020). In this framework, the input to models is augmented by prepending relevant documents that are retrieved from an external knowledge corpus (Guu et al., 2020). While RAG serves as a pract"}, {"rank": 5, "score": 0.604169, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 164, "text_snippet": "intensive tasks for open-ended generation (e.g., instruction following). Recent or concurrent work studies instruction-tuning of retrieval systems (Asai et al., 2023b) or joint training of retrieval and LM components (Lin et al., 2023), whi"}]}
{"case_index": 190, "query": "According to the paper 'raptor recursive abstractive processing for tree organized retrieval', what exact term completes this statement: \"3 M ETHODS Overview of RAPTOR Building on the idea that long texts often present subtopics and hierarchi- cal [BLANK] (Cao & Wang, 2022; Dong et al., 2023b), RAPTOR addresses the issue of semantic depth and connection in\"?", "gold": "structures", "answer": "The missing term is **“structures”**, completing the phrase as:\n\n“…long texts often present subtopics and hierarchical **structures** (Cao & Wang, 2022; Dong et al., 2023b)…”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'structures' and is directly supported by Context 1 which contains the exact phrase.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 6.389, "llm_ms": 1471.363, "top_contexts": [{"rank": 1, "score": 0.80884933, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 21, "text_snippet": "ntermediate nodes thus storing varying levels of detail, keeping granular details. However, both methods, due to their reliance on adjacency for grouping or summarizing adjacent nodes, may still overlook distant interdependencies within the"}, {"rank": 2, "score": 0.7504734, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 22, "text_snippet": " RAPTOR addresses the issue of semantic depth and connection in reading by building a recursive tree structure that balances broader thematic comprehension with granular details and which allows nodes to be grouped based on semantic sim- il"}, {"rank": 3, "score": 0.73177254, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 8, "text_snippet": " indexing and retrieval system that uses a tree structure to capture both high-level and low-level details about a text. As shown in Figure 1, our system, RAPTOR, clusters chunks of text, generates text summaries of those clusters, and then"}, {"rank": 4, "score": 0.691249, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 0, "text_snippet": "Published as a conference paper at ICLR 2024 RAPTOR: R ECURSIVE ABSTRACTIVE PROCESSING FOR TREE-ORGANIZED RETRIEVAL Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D. Manning Stanford University psarthi@cs."}, {"rank": 5, "score": 0.69101745, "doc_id": "2401.18059__raptor_recursive_abstractive_processing_for_tree_organized_retrieval.txt", "chunk_id": 9, "text_snippet": "uestions at different levels. 1arXiv:2401.18059v1 [cs.CL] 31 Jan 2024  Published as a conference paper at ICLR 2024 2 3 4 5  1 1  2  3  3  4 5  5 6  8  7 Index #8  Text: summary of  nodes 2 and 3  Child Nodes: 2, 3  Text Embedding  Text chu"}]}
{"case_index": 191, "query": "According to the paper 'refrag rethinking rag based decoding', what exact term completes this statement: \"Ablation studies in section 4 demonstrate thatthis [BLANK] achieving strong CPT performance.\"?", "gold": "recipeiscrucialfor", "answer": "The missing term is **“RL”** (i.e., the selective compression policy via reinforcement learning), as indicated by the ablation note: “Ablation study result for the advantage of RL… using our selective compression policy via RL…”.", "answer_score": 0.06, "answer_correctness": 0.0, "answer_groundedness": 0.2, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 0.2, "abstained": false, "should_abstain": false, "notes": "Expected completion is 'recipeiscrucialfor', not 'RL'. Context mentions RL ablation but does not support the exact quoted blank completion; answer is incorrect and weakly grounded.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 7.209, "llm_ms": 2291.237, "top_contexts": [{"rank": 1, "score": 0.63611937, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 12, "text_snippet": "is information is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-a"}, {"rank": 2, "score": 0.62962586, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 3, "text_snippet": " on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30 .85×the time-t"}, {"rank": 3, "score": 0.61404216, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 152, "text_snippet": "e downstream tasks (see section 5). REFRAG:Our approach is illustrated in figure 1. We use RoBERTa-large (Liu et al., 2019) as the encoder, feeding x1:stokens and evaluating the perplexity on the output tokens xs+1:s+o. We use REFRAG kto de"}, {"rank": 4, "score": 0.6115932, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 16, "text_snippet": "nsing most chunks for the query in RAG settings. We provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training and many real word long-context applications including RAG, multi-turn conversation with RA"}, {"rank": 5, "score": 0.6114584, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 176, "text_snippet": "Empirical verification of inference acceleration ofREFRAGwithk= 8. Ablation study result for the advantage of RL.Table 13 shows the advantage of using our selective compression policy via RL compared to using a lower compression rate. Ablat"}]}
{"case_index": 192, "query": "According to the paper 'retrieval augmented generation for knowledge intensive nlp tasks', what exact term completes this statement: \"It has obtained [BLANK] results on a diverse set of generation tasks and outperforms comparably-sized T5 models [32].\"?", "gold": "state-of-the-art", "answer": "The missing term is **“state-of-the-art”**, i.e.:\n\n“It has obtained **state-of-the-art** results on a diverse set of generation tasks and outperforms comparably-sized T5 models [32].”", "answer_score": 0.7, "answer_correctness": 1.0, "answer_groundedness": 0.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 0.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'state-of-the-art'. However, provided context snippets do not contain the quoted sentence or the term, so it is not supported by context.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 16.186, "llm_ms": 1827.478, "top_contexts": [{"rank": 1, "score": 0.63593495, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 74, "text_snippet": "nstrates a substantial performance advantage over supervised fine-tuned LLMs in all tasks and even outperforms ChatGPT in PubHealth, PopQA, biography generations, and ASQA (Rouge and MAUVE). Our approach also significantly outperforms a con"}, {"rank": 2, "score": 0.6206973, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 32, "text_snippet": "provement in accuracy. Second, we observe that using additional knowledge in generative models by using retrieval lead to important performance gains. On NaturalQuestions, the closed book T5 model obtains 36.6% accuracy with 11B parame- ter"}, {"rank": 3, "score": 0.6188413, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 70, "text_snippet": "re is capable of achieving strong performance across several tasks. 8  General-Purpose Architectures for NLP Prior work on general-purpose architectures for NLP tasks has shown great success without the use of retrieval. A single, pre-train"}, {"rank": 4, "score": 0.614925, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 71, "text_snippet": " pre-trained language model could achieve strong performance across both discriminative and generative tasks. For further improvement, BART [ 32] and T5 [ 51,52] propose a single, pre-trained encoder-decoder model that leverages bi-directio"}, {"rank": 5, "score": 0.6081332, "doc_id": "2005.11401__retrieval_augmented_generation_for_knowledge_intensive_nlp_tasks.txt", "chunk_id": 69, "text_snippet": "ance across a variety of NLP tasks when considered in isolation. Such tasks include open-domain question answering [ 5,29], fact checking [ 56], fact completion [ 48], long-form question answering [ 12], Wikipedia article generation [ 36], "}]}
{"case_index": 193, "query": "According to the paper 'from local to global a graph rag approach to query focused summarization', what exact term completes this statement: \"” Sensemaking tasks require reasoning over “ connections (which can be among people, places, and events) in order to anticipate their [BLANK] and act effectively ” (Klein et al., 2006).\"?", "gold": "trajectories", "answer": "The missing term is **“trajectories”**.", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'trajectories' and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.042, "llm_ms": 1500.984, "top_contexts": [{"rank": 1, "score": 0.6210576, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 7, "text_snippet": "plinary research over the past decade? ” Sensemaking tasks require reasoning over “ connections (which can be among people, places, and events) in order to anticipate their trajectories and act effectively ” (Klein et al., 2006). LLMs such "}, {"rank": 2, "score": 0.5656284, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 29, "text_snippet": "lements (nodes, edges, covariates) that the LLM can summarize in parallel at both indexing time and query time. The “global answer” to a given query is produced using a final round of query-focused summarization over all community summaries"}, {"rank": 3, "score": 0.5498299, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 8, "text_snippet": "nce analysis (Ranade and Joshi, 2023). Given a sensemaking query and a text with an implicit and interconnected set of concepts, an LLM can generate a summary that answers the query. The challenge, however, arises when the volume of data re"}, {"rank": 4, "score": 0.5278015, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 96, "text_snippet": "tigate these downstream risks for questions of a global nature, which might otherwise be answered by samples of retrieved facts falsely presented as global summaries. 7 Conclusion We have presented GraphRAG, a RAG approach that combines kno"}, {"rank": 5, "score": 0.51961774, "doc_id": "2404.16130__from_local_to_global_a_graph_rag_approach_to_query_focused_summarization.txt", "chunk_id": 6, "text_snippet": "2006; Laskar et al., 2020; Yao et al., 2017). This conventional approach, which we collectively call vector RAG , works well for queries that can be answered with information localized within a small set of records. However, vector RAG appr"}]}
{"case_index": 194, "query": "According to the paper 'recomp improving retrieval augmented lms with compression and selective augmentation', what exact term completes this statement: \"We achieve a compression rate of as low as 6% with minimal loss in performance for both tasks, significantly outperforming the off-the-shelf [BLANK] models.\"?", "gold": "summarization", "answer": "The missing term is **“summarization”**, i.e., “…outperforming the off-the-shelf **summarization** models.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'summarization' and is directly supported by Context 1 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 10.368, "llm_ms": 1293.623, "top_contexts": [{"rank": 1, "score": 0.6734888, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 3, "text_snippet": "n. We evaluate our approach on language modeling task and open domain question answering task. We achieve a compression rate of as low as 6% with minimal loss in performance for both tasks, significantly outperforming the off-the-shelf summ"}, {"rank": 2, "score": 0.654304, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 58, "text_snippet": "ning task. We observe a performance regression as the compression rate increases; however, even at a compression rate of32, our model remains competitive (as shown in table 1). In contrast, a compression rate of64appears to be overly aggres"}, {"rank": 3, "score": 0.63245744, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 15, "text_snippet": "6% and significantly outperforms prepending full documents. Our trained compressors also show promising results. For language modelling, both trained compressors achieve a compression ratio of 25% with minimal performance drop. When applied"}, {"rank": 4, "score": 0.62197554, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 82, "text_snippet": " and allowing the compression models to perform selective augmentation. Our experiments show that our compressors can improve the efficiency of retrieval augmented LMs significantly with minimal drop in performances. ACKNOWLEDGEMENT We than"}, {"rank": 5, "score": 0.6063085, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 13, "text_snippet": " outputs, and our abstractive compressor is distilled (West et al., 2022) from an extreme-scale LM (e.g. GPT-3), which achieves impressive summarization performance. Our experiments show that RECOMP can improve performance of frozen LMs on "}]}
{"case_index": 195, "query": "According to the paper 'corrective retrieval augmented generation', what exact term completes this statement: \"The retriever Raims to retrieve the top- [BLANK] D={dr1, ..., d rk}that are relevant to the input Xfrom the corpus C.\"?", "gold": "kdocuments", "answer": "The missing term is **“K documents”** — i.e., “the top- **K** documents.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Context explicitly states 'top- K documents'; model answer matches the required completion (kdocuments).", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 13.011, "llm_ms": 2315.855, "top_contexts": [{"rank": 1, "score": 0.6398859, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 25, "text_snippet": "rator G. The retriever Raims to retrieve the top- Kdocuments D={dr1, ..., d rk}that are relevant to the input Xfrom the corpus C. Based on the input Xand the retrieved results D, the generator Gis responsible for generating the output Y. Th"}, {"rank": 2, "score": 0.5580946, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 24, "text_snippet": " knowledge, this paper makes the first attempt to explore and design corrective strategies for RAG to improve its robustness of generation. 3 Task Formulation Following previous work (Lewis et al., 2020; Asai et al., 2024), given input Xand"}, {"rank": 3, "score": 0.5548996, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 10, "text_snippet": " should not have been equally referred to and involved in RAG. On account of the above issues, this paper particularly studies the scenarios where the retriever returns inaccurate results. A method named Corrective Retrieval- Augmented Gene"}, {"rank": 4, "score": 0.5401384, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 57, "text_snippet": "nted generators using a second “informed” retriever with access to the output, which the test-time retriever can be distilled from, and Hofstätter et al. (2022) recently proposed a training set ﬁltering/weighting approach to train stronger "}, {"rank": 5, "score": 0.539273, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 6, "text_snippet": " The examples show that a low-quality retriever is prone to introducing a substantial amount of irrelevant information, impeding the generators from acquiring accurate knowledge and potentially misleading them. the parametric knowledge they"}]}
{"case_index": 196, "query": "According to the paper 'recomp improving retrieval augmented lms with compression and selective augmentation', what exact term completes this statement: \"Although we do not have human annotations to train this model, prior work (Goyal et al., 2022; Chen et al., 2023; Potluri et al., 2023) suggests that the extreme- scale LMs can generate good [BLANK] summaries when prompted carefully.\"?", "gold": "query-focused", "answer": "The missing term is **“query-focused”**, i.e., “…can generate good **query-focused** summaries when prompted carefully.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Matches expected term 'query-focused' and is directly supported by Context 2.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.65, "llm_ms": 1187.71, "top_contexts": [{"rank": 1, "score": 0.6907402, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 77, "text_snippet": "by compressing retrieved documents into a concise summary or an empty sequence, facilitating selective retrieval augmentation. Prompt Compression Recent work (Wingate et al., 2022; Chevalier et al., 2023; Mu et al., 2023) proposes compressi"}, {"rank": 2, "score": 0.65719414, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 21, "text_snippet": "Compressor We train an encoder-decoder model encdec θto serve as an abstractive compressor, which takes the input sequence xand a concatenation of retrieved document set D [d1;d2;...dN]) and output a summary s. Although we do not have human"}, {"rank": 3, "score": 0.655403, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 32, "text_snippet": "each sentence from the retrieved documents, we prepend the Wikipedia page title to it to for decontextualization. 3.2 A BSTRACTIVE COMPRESSION To train an abstractive compressor, we distill the query-focused summarization ability of extreme"}, {"rank": 4, "score": 0.6517868, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 51, "text_snippet": "abstractive approach, we consider summaries generated from different prompts ( {sj}n 1in Figure 3) and empty summary, and choose the one that leads to the best end task performance. As oracle compression is model dependent, we also report m"}, {"rank": 5, "score": 0.65154046, "doc_id": "2310.04408__recomp_improving_retrieval_augmented_lms_with_compression_and_selective_augmentation.txt", "chunk_id": 63, "text_snippet": "performance under single document setting, they are not good at synthesizing information from multiple documents (Shaib et al. (2023) and hallucinate more often; See Section 6 for further analysis). 6 A NALYSIS AND DISCUSSIONS Transferring "}]}
{"case_index": 197, "query": "According to the paper 'replug retrieval augmented black box language models', what exact term completes this statement: \"Black-Box Language Models Figure 1, REPLUG is [BLANK] flexible and can be used with any existing black-box LM and retrieval model.\"?", "gold": "extremely", "answer": "The exact term is **“extremely”** (i.e., “REPLUG is **extremely** flexible and can be used with any existing black-box LM and retrieval model.”).", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'extremely' and is directly supported by the provided context quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id+chunk_id", "retrieval_ms": 7.682, "llm_ms": 1298.569, "top_contexts": [{"rank": 1, "score": 0.78774, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 8, "text_snippet": "Black-Box Language Models Figure 1, REPLUG is extremely flexible and can be used with any existing black-box LM and retrieval model. We also introduce REPLUG LSR (REPLUG with LM- Supervised Retrieval), a training scheme that can further imp"}, {"rank": 2, "score": 0.7632021, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 71, "text_snippet": "a black box and augments it with a tuneable retrieval model. Our evaluation shows that REPLUG can be integrated with any existing language model to improve their performance  REPLUG: Retrieval-Augmented Black-Box Language Models on language"}, {"rank": 3, "score": 0.7246446, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 5, "text_snippet": "ting the LM’s parameters, REPLUG treats the language model as a black box and augments it with a frozen or tunable retriever. This black-box assumption makes REPLUG applicable to large LMs (i.e., >100B parameters), which are often served vi"}, {"rank": 4, "score": 0.7133024, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 6, "text_snippet": "PIs. Internal representations of such models are not exposed and fine-tuning is not supported. In this work, we introduce REPLUG (Retrieve and Plug ), a new retrieval-augmented LM framework where the lan- guage model is viewed as a black bo"}, {"rank": 5, "score": 0.6949496, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 7, "text_snippet": " retrieved documents are prepended to the input context and fed into the black-box LM to make the final prediction. Because the LM context length limits the number of documents that can be prepended, we also introduce a new ensemble scheme "}]}
{"case_index": 198, "query": "According to the paper 'leveraging passage retrieval with generative models for open domain question answering', what exact term completes this statement: \"Building on that [BLANK] and the advances in pretrain- ing of natural language processing models, Roberts et al.\"?", "gold": "observation", "answer": "The missing term is **“observation”** — i.e., “Building on that **observation** and the advances in pretraining of natural language processing models, Roberts et al.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'observation' and is directly supported by Context 5 quote.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.2, "hit_rank": 5, "matched_by": "doc_id+chunk_id", "retrieval_ms": 13.343, "llm_ms": 1516.873, "top_contexts": [{"rank": 1, "score": 0.6812115, "doc_id": "2509.01092__refrag_rethinking_rag_based_decoding.txt", "chunk_id": 84, "text_snippet": " and pre-training paradigm for generative LLMs, leveraging cross-attention and end-to-end pre-training with 9  retrieval from a trillion-token data store, achieving strong performance. Subsequent work by Shi et al. (2024) and Lin et al. (20"}, {"rank": 2, "score": 0.6703295, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 0, "text_snippet": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering Gautier Izacard1,2,3Edouard Grave1 1Facebook AI Research, Paris 2ENS, PSL University, Paris 3Inria, Paris gizacard|egrave@fb.com Abstract Generative mode"}, {"rank": 3, "score": 0.658296, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 7, "text_snippet": "ve models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a). In this paper, we explore a simple approach hav- ing the best of both worlds, by building on the exciting develop"}, {"rank": 4, "score": 0.6564898, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 15, "text_snippet": " that answers do not correspond to spans in support documents, thus requiring ab- stractive models. Raffel et al. (2019) showed that generative models are competitive for reading com- prehension tasks such as SQuAD (Rajpurkar et al., 2016),"}, {"rank": 5, "score": 0.6543649, "doc_id": "2007.01282__leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering.txt", "chunk_id": 2, "text_snippet": "s a ﬂexible framework to efﬁciently aggregate and com- bine evidence from multiple passages. 1 Introduction Recently, several works have shown that factual information can be extracted from large scale language models trained on vast quanti"}]}
{"case_index": 199, "query": "According to the paper 'atlas few shot learning with retrieval augmented language models', what exact term completes this statement: \"Here, we propose an alternative which gives slightly stronger results, which relies on the following [BLANK].\"?", "gold": "observation", "answer": "The blank is **“prompting strategy”**.", "answer_score": 0.0, "answer_correctness": 0.0, "answer_groundedness": 0.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 0.0, "groundedness": 0.0, "abstained": false, "should_abstain": false, "notes": "Expected term is 'observation', but the answer says 'prompting strategy'. The provided context does not support 'prompting strategy' for the quoted sentence.", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": false, "retrieval_rr": 0.0, "hit_rank": null, "matched_by": "none", "retrieval_ms": 16.454, "llm_ms": 1619.689, "top_contexts": [{"rank": 1, "score": 0.64650947, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 12, "text_snippet": "ntributions: •A thorough study on how to design and train retrieval-augmented language models, with a focus on downstream few-shot learning and sample eﬃciency. •The ﬁndings of this study lead to a retrieval-augmented language model, called"}, {"rank": 2, "score": 0.61942136, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 1, "text_snippet": " Abstract Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter count"}, {"rank": 3, "score": 0.61375296, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 0, "text_snippet": "Atlas: Few-shot Learning with Retrieval Augmented Language Models Gautier Izacard∗ ∗,♦,♣,♥gizacard@fb.com Patrick Lewis∗,♦plewis@fb.com Maria Lomeli♦marialomeli@fb.com Lucas Hosseini♦hoss@fb.com Fabio Petroni♦fabiopetroni@fb.com Timo Schick"}, {"rank": 4, "score": 0.59712285, "doc_id": "2301.12652__replug_retrieval_augmented_black_box_language_models.txt", "chunk_id": 15, "text_snippet": "anguage models makes this approach infeasible. To ad- dress the challenges posed by large language models, we investigate retrieval-augmentation in the black-box setting , where users only have access to the model predictions and cannot acc"}, {"rank": 5, "score": 0.59668624, "doc_id": "2208.03299__atlas_few_shot_learning_with_retrieval_augmented_language_models.txt", "chunk_id": 2, "text_snippet": "ings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including"}]}
{"case_index": 200, "query": "According to the paper 'corrective retrieval augmented generation', what exact term completes this statement: \"To [BLANK] others to reproduce our results, we will publish all source code l\"?", "gold": "facilitate", "answer": "The missing term is **“facilitate”**, as in: “To **facilitate** others to reproduce our results, we will publish all source code later.”", "answer_score": 1.0, "answer_correctness": 1.0, "answer_groundedness": 1.0, "expected_unanswerable": false, "abstained": false, "judge": {"correctness": 1.0, "groundedness": 1.0, "abstained": false, "should_abstain": false, "notes": "Answer matches expected term 'facilitate' and is directly supported by Context 2 ('To facilitate others to reproduce our results...').", "judge_model": "gpt-5.2"}, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 0.5, "hit_rank": 2, "matched_by": "doc_id+chunk_id", "retrieval_ms": 8.849, "llm_ms": 1157.224, "top_contexts": [{"rank": 1, "score": 0.5098883, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 15, "text_snippet": "hers to reproduce our results, we will publish all source code later. In summary, our contributions in this paper are three-fold: 1) This paper studies the scenarios where the retriever returns inaccurate results and, to the best of our kno"}, {"rank": 2, "score": 0.46217817, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 14, "text_snippet": "l., 2024) for demonstrating its adaptability to RAG-based approaches. Results onfour datasets of PopQA (Mallen et al., 2023), Biog- raphy (Min et al., 2023), Pub Health (Zhang et al., 2023a), and Arc-Challenge (Bhakthavatsalam et al., 2021)"}, {"rank": 3, "score": 0.44973132, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 1, "text_snippet": "evitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Al- though retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heav"}, {"rank": 4, "score": 0.43826365, "doc_id": "2310.11511__self_rag_learning_to_retrieve_generate_and_critique_through_self_reflection.txt", "chunk_id": 97, "text_snippet": "meters or with conventional retrieval-augmented generation approaches. 10  Preprint. ETHICAL CONCERNS This work aims to improve the factuality of LLM outputs, the lack of which continues to cause nu- merous real-world problems (e.g., spread"}, {"rank": 5, "score": 0.43769178, "doc_id": "2401.15884__corrective_retrieval_augmented_generation.txt", "chunk_id": 23, "text_snippet": "t to retrieve in long-form generation. Compared with recent studies (Schick et al., 2023; Luo et al., 2023; Asai et al., 2024) that are the most relevant to our work, a main difference should be highlighted. These approaches target on explo"}]}
