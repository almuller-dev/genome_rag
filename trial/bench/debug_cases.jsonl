{"case_index": 1, "query": "What architecture replaces recurrence and convolution entirely?", "gold": "self-attention", "answer": "The **Transformer** architecture replaces **recurrence and convolution entirely**, relying instead **entirely on an attention mechanism**.", "answer_score": 0.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id", "retrieval_ms": 8.356, "llm_ms": 1385.245, "top_contexts": [{"rank": 1, "score": 0.4994183, "doc_id": "attention_is_all_you_need.txt", "chunk_id": 11, "text_snippet": " of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary in"}, {"rank": 2, "score": 0.47047108, "doc_id": "attention_is_all_you_need.txt", "chunk_id": 42, "text_snippet": "uous kernels, orO(logk(n))in the case of dilated convolutions [ 18], increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor o"}, {"rank": 3, "score": 0.4652304, "doc_id": "attention_is_all_you_need.txt", "chunk_id": 9, "text_snippet": ", remains. Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [ 2"}, {"rank": 4, "score": 0.46474266, "doc_id": "attention_is_all_you_need.txt", "chunk_id": 7, "text_snippet": " [ 35,2,5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15]. Recurrent models typically factor computation along the symbol positions of the input and "}, {"rank": 5, "score": 0.45765898, "doc_id": "attention_is_all_you_need.txt", "chunk_id": 8, "text_snippet": "y sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computatio"}]}
{"case_index": 2, "query": "How many attention heads are used in the base model?", "gold": "8", "answer": "The base model uses **8 attention heads** (“we employ *h = 8* parallel attention layers, or heads”).", "answer_score": 1.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id", "retrieval_ms": 7.808, "llm_ms": 1308.914, "top_contexts": [{"rank": 1, "score": 0.5687098, "doc_id": "attention_is_all_you_need.txt", "chunk_id": 26, "text_snippet": "ubspaces at different positions. With a single attention head, averaging inhibits this. MultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO where head i= Attention( QWQ i, KWK i, V WV i) Where the projections are parameter matrices WQ i∈Rdm"}, {"rank": 2, "score": 0.55278075, "doc_id": "attention_is_all_you_need.txt", "chunk_id": 27, "text_snippet": "ead, the total computational cost is similar to that of single-head attention with full dimensionality. 3.2.3 Applications of Attention in our Model The Transformer uses multi-head attention in three different ways: •In \"encoder-decoder att"}, {"rank": 3, "score": 0.5384912, "doc_id": "attention_is_all_you_need.txt", "chunk_id": 56, "text_snippet": " 4 5.19 25.3 50 8 4.88 25.5 80 256 32 32 5.75 24.5 28 1024 128 128 4.66 26.0 168 1024 5.12 25.4 53 4096 4.75 26.2 90 (D)0.0 5.77 24.6 0.2 4.95 25.5 0.0 4.67 25.3 0.2 5.47 25.7 (E) positional embedding instead of sinusoids 4.92 25.7 big 6 10"}, {"rank": 4, "score": 0.5272988, "doc_id": "attention_is_all_you_need.txt", "chunk_id": 57, "text_snippet": ", we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also"}, {"rank": 5, "score": 0.5110513, "doc_id": "attention_is_all_you_need.txt", "chunk_id": 43, "text_snippet": "tion layer and a point-wise feed-forward layer, the approach we take in our model. As side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in"}]}
{"case_index": 3, "query": "What optimizer and schedule are used for training?", "gold": "Adam", "answer": "They use the **Adam optimizer** with **β₁ = 0.9, β₂ = 0.98, ε = 10⁻⁹**.\n\nThe learning rate follows a **warmup + inverse square-root decay schedule**:\n\n\\[\n\\text{lrate} = d_{\\text{model}}^{-0.5}\\cdot \\min(\\text{step\\_num}^{-0.5},\\ \\text{step\\_num}\\cdot \\text{warmup\\_steps}^{-1.5})\n\\]\n\nThis means the learning rate **increases linearly for the first** `warmup_steps` **steps**, then **decreases proportional to** \\(1/\\sqrt{\\text{step\\_num}}\\). They use **warmup_steps = 4000**.", "answer_score": 1.0, "retrieval_labeled": true, "retrieval_hit": true, "retrieval_rr": 1.0, "hit_rank": 1, "matched_by": "doc_id", "retrieval_ms": 9.591, "llm_ms": 3346.159, "top_contexts": [{"rank": 1, "score": 0.57455736, "doc_id": "attention_is_all_you_need.txt", "chunk_id": 46, "text_snippet": "ined the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days). 5.3 Optimizer We used the Adam"}, {"rank": 2, "score": 0.439625, "doc_id": "attention_is_all_you_need.txt", "chunk_id": 45, "text_snippet": "to a 32000 word-piece vocabulary [ 38]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens. 5.2 Har"}, {"rank": 3, "score": 0.38883525, "doc_id": "attention_is_all_you_need.txt", "chunk_id": 47, "text_snippet": "corresponds to increasing the learning rate linearly for the first warmup _steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup _steps = 4000 . 5.4 Regularization We"}, {"rank": 4, "score": 0.38679403, "doc_id": "attention_is_all_you_need.txt", "chunk_id": 44, "text_snippet": "s section describes the training regime for our models. 5.1 Training Data and Batching We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encodin"}, {"rank": 5, "score": 0.37638423, "doc_id": "attention_is_all_you_need.txt", "chunk_id": 60, "text_snippet": "ut 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting a"}]}
