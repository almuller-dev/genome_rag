Improving language models by retrieving
from trillions of tokens
Sebastian Borgeaudy, Arthur Menschy, Jordan Hoï¬€manny, Trevor Cai, Eliza Rutherford, Katie Millican,
George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas,
Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saï¬€ron Huang, Loren Maggiore, Chris Jones,
Albin Cassirer, Andy Brock, Michela Paganini, Geoï¬€rey Irving, Oriol Vinyals, Simon Osindero,
Karen Simonyan, Jack W. Raez, Erich Elsenzand Laurent Sifrey,z
All authors from DeepMind,yEqual contributions,zEqual senior authorship
We enhance auto-regressive language models by conditioning on document chunks retrieved from a
large corpus, based on local similarity with preceding tokens. With a 2 trillion token database, our
Retrieval-Enhanced Transformer ( R/e.sc/t.sc/r.sc/o.sc) obtains comparable performance to GPT-3 and Jurassic-1
on the Pile, despite using 25 fewer parameters. After ï¬ne-tuning, R/e.sc/t.sc/r.sc/o.scperformance translates to
downstream knowledge-intensive tasks such as question answering. R/e.sc/t.sc/r.sc/o.sccombines a frozen B/e.sc/r.sc/t.sc
retriever,adiï¬€erentiableencoderandachunkedcross-attentionmechanismtopredicttokensbasedon
an order of magnitude more data than what is typically consumed during training. We typically train
R/e.sc/t.sc/r.sc/o.sc from scratch, yet can also rapidly R/e.sc/t.sc/r.sc/o.scï¬t pre-trained transformers with retrieval and still
achieve good performance. Our work opens up new avenues for improving language models through
explicit memory at unprecedented scale.
1. Introduction
Language modelling (LM) is an unsupervised task that consists of modelling the probability of text,
usually by factorising it into conditional next-token predictions ğ‘Â¹ğ‘¥1Â”Â“Â“Â“Â”ğ‘¥ğ‘›Âº=Ã
ğ‘–ğ‘Â¹ğ‘¥ğ‘–jğ‘¥Âğ‘–Âº. Neural
networks have proven to be powerful language models, ï¬rst in the form of recurrent architectures
(Graves, 2013; Jozefowicz et al., 2016; Mikolov et al., 2010) and more recently in the form of
Transformers (Vaswani et al., 2017), that use attention to contextualise the past. Large performance
improvementshavecomefromincreasingtheamountofdata,trainingcompute,ormodelparameters.
Transformers have been scaled from 100million parameter models in seminal work to over hundred
billion parameters (Brown et al., 2020; Radford et al., 2019) in the last two years which has led to
models that do very well on a wide array of tasks in a zero or few-shot formulation. Increasing model
size predictably improves performance on a wide range of downstream tasks (Kaplan et al., 2020).
The beneï¬ts of increasing the number of parameters come from two factors: additional computations
at training and inference time, and increased memorization of the training data.
Inthiswork,weendeavortodecouplethese,byexploringeï¬ƒcientmeansofaugmentinglanguage
models with a massive-scale memory without signiï¬cantly increasing computations. Speciï¬cally, we
suggest retrieval from a large text database as a complementary path to scaling language models.
Instead of increasing the size of the model and training on more data, we equip models with the
ability to directly access a large database to perform predictionsâ€”a semi-parametric approach. At
a high level, our Retrieval Transformer ( R/e.sc/t.sc/r.sc/o.sc) model splits the input sequence into chunks and
retrieves text similar to the previous chunk to improve the predictions in the current chunk. Existing
retrieval for language modelling work only considers small transformers ( 100millions parameters)
and databases of limited size (up to billions of tokens) (Guu et al., 2020; Khandelwal et al., 2020;
Lewisetal.,2020;Yogatamaetal.,2021). Toourknowledge,ourworkistheï¬rsttoshowthebeneï¬ts
of scaling the retrieval database to trillions of tokens for large parametric language models. Our main
Corresponding authors: {sborgeaud|amensch|jordanhoï¬€mann|sifre}@deepmind.comarXiv:2112.04426v3  [cs.CL]  7 Feb 2022

Improving language models by retrieving from trillions of tokens
200 400 8001600 7500
Number of Non-Embedding Params (M)0.70.80.91.0C4 Eval bits-per-byte
172M 425M 1.5B 7.5B Baseline RETRO [OFF] RETRO [ON]
01 10 100 1000 10000
Retrieval dataset (B Tokens)0.70.80.91.0
01 3510 3050100
Number of neighbors0.70.80.91.0
Figure 1jScaling of R/e.sc/t.sc/r.sc/o.sc.The performance gain of our retrieval models remains constant with
model scale (left), and is comparable to multiplying the parameteric model size by 10. The gain
increases with the size of the retrieval database (middle) and the number of retrieved neighbours
(right) on the C4 validation set, when using up to 40 neighbours. Past this, performance begins to
degrade, perhaps due to the reduced quality. At evaluation R/e.sc/t.sc/r.sc/o.sccan be used without retrieval
data ( R/e.sc/t.sc/r.sc/o.sc[OFF]), bringing limited performance degradation compared to baseline transformers.
contributions are the following.
â€¢We introduce R/e.sc/t.sc/r.sc/o.sc, a retrieval-enhanced autoregressive language model (Â§2.2). We use a
chunked cross-attention module to incorporate the retrieved text (Â§2.4), with time complexity
linear in the amount of retrieved data. We show that retrieving based on a pre-trained frozen
B/e.sc/r.sc/t.scmodel (Â§2.3) works at scale, removing the need for training and updating a retriever
network.
â€¢We show that our method scales well with model size and database size (Fig. 1): R/e.sc/t.sc/r.sc/o.sc
provides a constant gain for models ranging from 150M to 7B parameters, and R/e.sc/t.sc/r.sc/o.sccan be
improved at evaluation time by increasing the database size and the number of retrieved neigh-
bours. Our largest model obtains state-of-the-art results on a range of downstream evaluation
datasets including Wikitext103 (Merity et al., 2017) and the Pile (Gao et al., 2020) (Â§4). We
show that R/e.sc/t.sc/r.sc/o.sccan be ï¬ne-tuned to achieve competitive performance on downstream tasks
such as question answering (Â§4.3).
â€¢We propose an evaluation aware of proximity of test documents with the training set (Â§2.6),
addressing the problem of test set leakage (Lee et al., 2021). This is relevant for all language
models,andespeciallyforretrieval-enhancedmodelssincetheyhavedirectaccesstothetraining
dataset during evaluation. Using this methodology, we show that the performance of R/e.sc/t.sc/r.sc/o.sc
comes from both explicit neighbour copying and general knowledge extraction (Â§4.4).
2. Method
Wedesignourretrieval-enhancedarchitecturetobecapableofretrievingfromadatabasewithtrillions
of tokens. For this purpose, we retrieve at the level of contiguous token chunksinstead of individual
tokenswhichreducesstorageandcomputationrequirementsbyalargelinearfactor. Ourmethodï¬rst
constructs a key-value database, where values store raw chunks of text tokens and keys are frozen
B/e.sc/r.sc/t.scembedddings (Devlin et al., 2019). We use a frozen model to avoid having to periodically
re-compute embeddings over the entire database during training. Each training sequence is then split
into chunks, which are augmented with their ğ‘˜-nearest neighbour retrieved from the database. An
encoder-decoder architecture integrates retrieval chunks into the modelâ€™s predictions. We summarize
theR/e.sc/t.sc/r.sc/o.scarchitecture in Fig. 2, and detail it in this section. We end the section by introducing
2

Improving language models by retrieving from trillions of tokens
C C A F FW T r a ns f ormer 
Enc o der 
R etrie v al 
d ataset 
Frozen kNN Retriever 
KV
RETR O b lo c k ( x L ) N eig hb o ur s 
In p ut 
t o k ens Ch unk ed cr oss-att en tion ( C C A ) 
B ER T B ER T 
Condition 
A tt ending c h unk s Enc o ded neig hb o ur s 
C A 
C A 
A T T N Q 
EMB REA D A tt end Enc o ded neig hb o ur s 
C1 
C2 
C3 H1 
H2 
H3 
HH1+ 
H2+ E1E2E1
E2
CA (H1+, E1) 
CA (H2+, E2) 
CCA (H, E) 
X
Figure 2jR/e.sc/t.sc/r.sc/o.scarchitecture. Left:simpliï¬ed version where a sequence of length ğ‘›=12is split
intoğ‘™=3chunksofsize ğ‘š=4. Foreachchunk,weretrieve ğ‘˜=2neighboursof ğ‘Ÿ=5tokenseach. The
retrieval pathway is shown on top. Right:Details of the interactions in the C/c.sc/a.scoperator. Causality is
maintained as neighbours of the ï¬rst chunk only aï¬€ect the last token of the ï¬rst chunk and tokens
from the second chunk.
a new methodology to evaluate language models when an evaluation set is partially present in the
training set.
2.1. Training dataset
We use a multi-lingual version of MassiveText (Rae et al., 2021) for both training and retrieval data.
The dataset consists of text documents from multiple sources and multiple languages totalling over
5 trillion tokens (detailed in Table 1). Sequences are sampled from subsets of the training data,
with sampling weights given in the right-most column of Table 1. We tokenize the dataset using
SentencePiece (Kudo and Richardson, 2018) with a vocabulary of 128,000 tokens. During training
(unless otherwise speciï¬ed), we retrieve from 600B tokens from the training data. The training
retrieval database is made of the same subsets as the training data, in proportion that matches
the training sampling frequencies. During evaluation the retrieval database consists in the full
union of these datasets, with the exception of books for which we use a sub-sample of 4%. The
evaluation retrieval database thus contains 1.75T tokens. To limit test set leakage, we compute the
13-gram Jaccard similarity between train and test documents using the MinHash scheme and remove
all training documents with high similarity (0.8 or higher) to a validation or test set document.
Additionally, we remove all validation and test articles from Wikitext103 (Merity et al., 2017) from
our Wikipedia training data.
2.2. Retrieval-enhanced autoregressive token models
Our approach uses retrieval as a way to augment input examples at the granularity of small chunks
of tokens. Formally, we consider sequences of integer tokens in ğ•=Â»1Â”ğ‘£Â¼, obtained using a text
tokenizer1. Wespliteach ğ‘›-token-longexample ğ‘‹=Â¹ğ‘¥1Â”Â“Â“Â“Â”ğ‘¥ğ‘›Âºintoasequenceof ğ‘™chunksÂ¹ğ¶1Â”Â“Â“Â“Â”ğ¶ğ‘™Âº
of sizeğ‘š=ğ‘›
ğ‘™, i.e.ğ¶1,Â¹ğ‘¥1Â”Â“Â“Â“Â”ğ‘¥ğ‘šÂºÂ” Â“Â“Â“Â” ğ¶ğ‘™,Â¹ğ‘¥ğ‘› ğ‘šÂ¸1Â”Â“Â“Â“Â”ğ‘¥ğ‘›Âº2ğ•ğ‘š. We useğ‘›=2048andğ‘š=64.
We augment each chunk ğ¶ğ‘¢with a set R/e.sc/t.scDÂ¹ğ¶ğ‘¢Âºofğ‘˜neighbours from the database D.R/e.sc/t.scD(or
1We use the notation Â»1Â”ğ‘£Â¼,f1Â”Â“Â“Â“Â”ğ‘£gthroughout the text.
3

Improving language models by retrieving from trillions of tokens
R/e.sc/t.scfor brevity) is a non-trainable operator speciï¬ed in Â§2.3. Token likelihoods are provided by a
model, parameterized by ğœƒ, that takes as input both previous tokens and their retrieved neighbours.
This deï¬nes the following retrieval-enhanced sequence log-likelihood:
ğ¿Â¹ğ‘‹jğœƒÂ”DÂº,ğ‘™âˆ‘ï¸
ğ‘¢=1ğ‘šâˆ‘ï¸
ğ‘–=1ğœƒ 
ğ‘¥Â¹ğ‘¢ 1Âºğ‘šÂ¸ğ‘–jÂ¹ğ‘¥ğ‘—Âºğ‘—ÂÂ¹ğ‘¢ 1Âºğ‘šÂ¸ğ‘–Â”Â¹R/e.sc/t.scDÂ¹ğ¶ğ‘¢0ÂºÂºğ‘¢0Âğ‘¢
Â“ (1)
We set R/e.sc/t.scÂ¹ğ¶1Âº=;, namely the likelihood of tokens from the ï¬rst chunk does not depend on
any retrieval data. This likelihood deï¬nition preserves autoregressivity : the probability of the ğ‘–-th
token of the ğ‘¢-th chunk, ğ‘¥Â¹ğ‘¢ 1Âºğ‘šÂ¸ğ‘–, only depends on previously seen tokens Â¹ğ‘¥ğ‘—Âº16ğ‘—ÂÂ¹ğ‘¢ 1Âºğ‘šÂ¸ğ‘–and on the
data retrieved from the previous chunks Â¹R/e.sc/t.scÂ¹ğ¶ğ‘¢0ÂºÂºğ‘¢0Âğ‘¢. We can therefore directly samplewith log-
probability , where sampling within the chunk ğ¶ğ‘¢is conditioned on the neighbours Â¹R/e.sc/t.scÂ¹ğ¶ğ‘¢0ÂºÂºğ‘¢0Âğ‘¢.
This makes retrieval-enhanced models directly comparable with the largest language models that are
evaluated by sampling.
2.3. Nearest neighbour retrieval
Retrieval neighbours. Our database consists of a key-value memory. Each value consists of two
contiguous chunks of tokens which we denote Â»ğ‘Â”ğ¹Â¼whereğ‘is theneighbour chunk which is used
to compute the key, and ğ¹is itscontinuation in the original document. The corresponding key is
theB/e.sc/r.sc/t.scembedding of ğ‘, averaged over time, that we denote B/e.sc/r.sc/t.scÂ¹ğ‘Âº. For each chunk ğ¶, we
retrieve its approximate ğ‘˜-nearest neighbours from our key-value database using the ğ¿2distance
on BERT embeddings ğ‘‘Â¹ğ¶Â”ğ‘Âº=jjB/e.sc/r.sc/t.scÂ¹ğ¶Âº B/e.sc/r.sc/t.scÂ¹ğ‘Âºjj2
2. The model receives the corresponding
values R/e.sc/t.scÂ¹ğ¶Âº,Â¹Â»ğ‘1Â”ğ¹1Â¼Â”Â“Â“Â“Â”Â»ğ‘ğ‘˜Â”ğ¹ğ‘˜Â¼Âº. Both neighbour chunks and their continuations provide
meaningful improvements, as illustrated in our ablation study (Appendix D). We use a length 64for
bothğ‘ğ‘—andğ¹ğ‘—, thus R/e.sc/t.scÂ¹ğ¶Âºhas a shape of ğ‘˜ğ‘Ÿwithğ‘Ÿ=128. To avoid retrieving the chunk ğ¶ğ‘¢Â¸1
in the retrieval set R/e.sc/t.scÂ¹ğ¶ğ‘¢Âº, which would break causality during training, we ï¬lter out neighbours
originating from the same document as the training sequence ğ‘‹.
For a database of ğ‘‡elements, we can query the approximate nearest neighbours in OÂ¹logğ‘‡Âºtime.
We use the SCaNN library (Guo et al., 2020) to achieve this. This means that we can query our
2trillion token database in 10mswhilst evaluating or sampling from the model; this expense is
amortizedoverachunklength. Performingretrievalon-the-ï¬‚yistooslowtokeepupwiththetraining
calculationsâ€”we leverage the frozen aspect of the embedding operator B/e.sc/r.sc/t.scto precompute all
approximate nearest neighbours and save the results as part of the data. In Fig. 9 in the Appendix, we
show results where we only retrieve neighbours within Wikipedia. We ï¬nd that neighbours tend to
come from 2-3 links away from a given article whereas random articles are more than 5 links apart.
Table1jMassiveText . Thelastcolumnindicatesthesamplingweightduringtraining. Themultilingual
subsets include documents in 10 languages. The full breakdown is given in Â§A.1.
Source Token count (M) Documents (M) Multilingual Sampling frequency
Web 977,563 1,208 Yes 55%
Books 3,423,740 20 No 25%
News 236,918 398 No 10%
Wikipedia 13,288 23 Yes 5%
GitHub 374,952 143 No 5%
4

Improving language models by retrieving from trillions of tokens
2.4.R/e.sc/t.sc/r.sc/o.scmodel architecture
Our model relies on an encoder-decoder transformer architecture, integrating the retrieved data
through a cross-attention mechanism as introduced in Vaswani et al. (2017). First, the retrieved
tokens R/e.sc/t.scÂ¹ğ¶Âºare fed into an encoder Transformer, which computes the encoded neighbours set ğ¸.
Denoting the intermediate activations by ğ», our transformer decoder then interleaves R/e.sc/t.sc/r.sc/o.sc-blocks
R/e.sc/t.sc/r.sc/o.scÂ¹ğ»Â”ğ¸ÂºandstandardTransformerblocks LMÂ¹ğ»Âº(thehyperparameter ğ‘ƒÂ»1Â”ğ¿Â¼determinesat
which layers we use a R/e.sc/t.sc/r.sc/o.sc-block). These blocks are built from three diï¬€erent residual operators
with signature â„ğ‘›ğ‘‘!â„ğ‘›ğ‘‘: a fully-connected layer F/f.sc/w.sc, the standard sequence-level self-attention
layer A/t.sc/t.sc/n.sc, and a chunked cross-attention layer C/c.sc/a.scÂ¹Â”ğ¸Âºthat incorporates information from the
retrieval encoder:
R/e.sc/t.sc/r.sc/o.scÂ¹ğ»Â”ğ¸Âº,F/f.sc/w.scÂ¹C/c.sc/a.scÂ¹A/t.sc/t.sc/n.scÂ¹ğ»ÂºÂ”ğ¸ÂºÂºÂ”and L/m.scÂ¹ğ»Âº,F/f.sc/w.scÂ¹A/t.sc/t.sc/n.scÂ¹ğ»ÂºÂº(2)
Since F/f.sc/w.sc,A/t.sc/t.sc/n.scandC/c.sc/a.scare all autoregressive operators whose output at position ğ‘–only
depends onÂ¹â„ğ‘—Âºğ‘—6ğ‘–, any succession of R/e.sc/t.sc/r.sc/o.scand/l.sc/m.sclayers, followed by a token classiï¬cation
head deï¬nes an autoregressive log-likelihood (1). An overview of the model architecture is given in
Algorithm 1 and in Fig. 2. We next describe the retrieval encoder and the chunked cross-attention
layer in more detail, and explain how to sample from R/e.sc/t.sc/r.sc/o.sc.
Encodingretrievalneighbours. Foreachchunk ğ¶ğ‘¢,theğ‘˜retrievalneighbours R/e.sc/t.scÂ¹ğ¶ğ‘¢Âºarefedinto
a bi-directional transformer E/n.sc/c.sc/o.sc/d.sc/e.sc/r.sc , yielding the outputs ğ¸ğ‘—
ğ‘¢,E/n.sc/c.sc/o.sc/d.sc/e.sc/r.scÂ¹R/e.sc/t.scÂ¹ğ¶ğ‘¢Âºğ‘—Â”ğ»ğ‘¢Âº2â„ğ‘Ÿğ‘‘0,
whereğ‘—2 Â»1Â”ğ‘˜Â¼indexes each neighbour. The retrieval encoder is a non-causal transformer. It
is conditioned on ğ»ğ‘¢, the activations of chunk ğ¶ğ‘¢, through cross-attention layers; this allows the
representations of the retrieval encoder to be modulated by the retrieving chunk in a diï¬€erentiable
way. More precisely, the encoding of the ğ‘—thneighbour of the ğ‘¢thchunk, R/e.sc/t.scÂ¹ğ¶ğ‘¢Âºğ‘—, depends on the
attended activation ğ»ğ‘¢,Â¹â„Â¹ğ‘¢ 1Âºğ‘šÂ¸ğ‘–Âºğ‘–2Â»1Â”ğ‘šÂ¼2â„ğ‘šğ‘‘of chunkğ¶ğ‘¢at layer minÂ¹ğ‘ƒÂº. All neighbours for
all chunks are encoded in parallel, yielding a full encoded set ğ¸,Â¹ğ¸ğ‘—
ğ‘¢Âºğ‘¢2Â»1Â”ğ‘™Â¼Â”ğ‘—2Â»1Â”ğ‘˜Â¼2â„ğ‘™ğ‘˜ğ‘Ÿğ‘‘0. We
denoteğ¸ğ‘¢2â„ğ‘˜ğ‘Ÿğ‘‘0as the encoded neighbours for chunk ğ‘¢2Â»1Â”ğ‘™Â¼.
Chunked cross-attention. To perform the C/c.sc/a.scoperation, we ï¬rst split a given intermediate acti-
vationğ»2â„ğ‘›ğ‘‘intoğ‘™ 1attending chunks
ğ»Â¸
ğ‘¢,Â¹â„ğ‘¢ğ‘šÂ¸ğ‘– 1Âºğ‘–2Â»1Â”ğ‘šÂ¼2â„ğ‘šğ‘‘
ğ‘¢2Â»1Â”ğ‘™ 1Â¼, as depicted on the
right of Fig. 2. ğ»Â¸
ğ‘¢holds the intermediary embeddings of the last token in chunk ğ¶ğ‘¢and of the ï¬rst
ğ‘š 1tokens inğ¶ğ‘¢Â¸12. We compute the cross-attention between ğ»Â¸
ğ‘¢andğ¸ğ‘¢â€”the encoded retrieval
set obtained from chunk ğ¶ğ‘¢. Attention is computed across time and across neighbours simultaneously,
as we merge the neighbour and time dimensions of ğ¸ğ‘¢before applying cross-attention. Since there
is a notion of alignment between data chunks and retrieval neighbours, we use relative positional
encodings as described in Â§B.1.2.
We concatenate the ğ‘™ 1outputs of the per-chunk cross-attentions (each of shape ğ‘šğ‘‘) across
time, and properly pad the result; we thus form the output activation C/c.sc/a.scÂ¹ğ»Â”ğ¸Âº2â„ğ‘›ğ‘‘. Formally,
for each chunk ğ¶ğ‘¢and for each token ğ‘–2Â»1Â”ğ‘šÂ¼we set
C/c.sc/a.scÂ¹ğ»Â”ğ¸Âºğ‘¢ğ‘šÂ¸ğ‘– 1,C/a.scÂ¹â„ğ‘¢ğ‘šÂ¸ğ‘– 1Â”ğ¸ğ‘¢ÂºÂ” (3)
2The last token of chunk ğ¶ğ‘¢is the ï¬rst to be able to access the retrieved content ğ¸ğ‘¢while maintaining autoregressivity
in(1). Hence, there is a one token overlap between chunk ğ¶ğ‘¢=
ğ‘¥Â¹ğ‘¢ 1Âºğ‘šÂ¸ğ‘–
ğ‘–2Â»1Â”ğ‘šÂ¼and the corresponding attending chunk
ğ¶Â¸
ğ‘¢,Â¹ğ‘¥ğ‘¢ğ‘šÂ¸ğ‘– 1Âºğ‘–2Â»1Â”ğ‘šÂ¼.
5

Improving language models by retrieving from trillions of tokens
Algorithm 1: Overview of R/e.sc/t.sc/r.sc/o.scmodel architecture.
Hyperparam: ğ‘ƒandğ‘ƒenc, indices of layers with cross-attention in the decoder and encoder
respectively
Hyperparam: ğ¿andğ¿enc, number of decoder layers and number of encoder layers.
Input:ğ‘‹2ğ•ğ‘›: sequence of tokens. Â¹R/e.sc/t.scÂ¹ğ¶ğ‘¢ÂºÂº16ğ‘¢6ğ‘™: the retrieved neighbours
Output:ğ‘‚2â„ğ‘›jğ•j: the output logits
defE/n.sc/c.sc/o.sc/d.sc/e.sc/r.scÂ¹R/e.sc/t.scÂ¹ğ¶ğ‘¢Âº16ğ‘¢6ğ‘™Â”ğ»Âº:
Â¹ğ»ğ‘¢Âºğ‘¢2Â»1Â”ğ‘™Â¼ S/p.sc/l.sc/i.sc/t.scÂ¹ğ»Âº
forğ‘—2Â»1Â”ğ‘˜Â¼Â”ğ‘¢2Â»1Â”ğ‘™Â¼do// Encoder shared across neighbours and chunks
ğ¸ğ‘—
ğ‘¢=E/m.sc/b.scencÂ¹R/e.sc/t.scÂ¹ğ¶ğ‘¢Âºğ‘—Âº// May be shared with the decoder E M B
forğ‘02Â»1Â”ğ¿encÂ¼do
ğ¸ğ‘—
ğ‘¢ A/t.sc/t.sc/n.scencÂ¹ğ¸ğ‘—
ğ‘¢Âº// Bi-directional attention
ifğ‘02ğ‘ƒencthen
ğ¸ğ‘—
ğ‘¢ C/a.scencÂ¹ğ¸ğ‘—
ğ‘¢Â”ğ»ğ‘¢Âº
ğ¸ğ‘—
ğ‘¢ F/f.sc/w.scencÂ¹ğ¸ğ‘—
ğ‘¢Âº
returnğ¸
ğ» E/m.sc/b.scÂ¹ğ‘‹Âº
forğ‘2Â»1Â”ğ¿Â¼do
ğ» A/t.sc/t.sc/n.scÂ¹ğ»Âº// Causal attention
ifğ‘=minÂ¹ğ‘ƒÂºthen
// The neighbour E N C O D E R is conditioned with the decoder activations of
the last layer before the first cross-attention
ğ¸=E/n.sc/c.sc/o.sc/d.sc/e.sc/r.scÂ¹R/e.sc/t.scÂ¹ğ¶ğ‘¢Âº16ğ‘¢6ğ‘™Â”ğ»Âº
ifğ‘2ğ‘ƒthen
ğ» C/c.sc/a.scÂ¹ğ»Â”ğ¸Âº
ğ» F/f.sc/w.scÂ¹ğ»Âº
ğ‘‚ R/e.sc/a.sc/d.scÂ¹ğ»Âº
where C/a.scis the cross-attention residual operator over time-concatenated encoded neighbours. We
recall that this operator is deï¬ned in its simplest version by three parameter matrices ğ¾2â„ğ‘‘ğ‘Â”ğ‘„2
â„ğ‘‘ğ‘andğ‘‰2â„ğ‘‘ğ‘‘. For allâ„2â„ğ‘‘andğ‘Œ2â„ğ‘‡ğ‘‘, we deï¬ne
C/a.scÂ¹â„Â”ğ‘ŒÂº,softmaxÂ¹ğ‘Œğ¾ğ‘„ğ‘‡â„Âºğ‘Œğ‘‰Â” (4)
where the softmax is performed on the second dimension and all products are matrix products. We
use multi-head cross-attention, and add positional encodings to the softmax(see Â§B.1.2).
The ï¬rstğ‘š 1tokens cannot attend to any neighbour of a previous chunk; at these positions, we
deï¬ne C/c.sc/a.scas the identity, setting C/c.sc/a.scÂ¹ğ»Â”ğ¸Âºğ‘—,â„ğ‘—for all tokens ğ‘—2Â»1Â”ğ‘š 1Â¼. Finally, the last token
â„ğ‘™ğ‘šattends to the last retrieval set ğ¸ğ‘™and we set â„ğ‘™ğ‘š,C/a.scÂ¹â„ğ‘™ğ‘šÂ”ğ¸ğ‘™Âº(not shown in Fig. 2). Listing 1
contains a simpliï¬ed implementation of C/c.sc/a.sc. Note that chunked cross-attention is autoregressive:
the output of C/c.sc/a.scat position ğ‘–depends on the sequence from tokens from 0toğ‘–that is input to C/c.sc/a.sc.
With R/e.sc/t.sc/r.sc/o.scmodels, even though each C/c.sc/a.sccross-attention attends only to the neighbours of
the preceding chunk R/e.sc/t.scÂ¹ğ¶ğ‘¢ 1Âº, the dependencies over previous neighbours are propagated via the
self-attentionoperations. Theactivationsofthe ğ‘–thtokeninthe ğ‘¢thchunkthereforepotentiallydepend
upon the set of allprevious neighbours R/e.sc/t.scÂ¹ğ¶ğ‘¢0Âºğ‘¢0Âğ‘¢, without incurring the quadratic cost of cross
attending to that set.
6

Improving language models by retrieving from trillions of tokens
Sampling. Whensampling,attheendofachunk ğ¶ğ‘¢,weuseSCaNNtoretrieveneighbours R/e.sc/t.scÂ¹ğ¶ğ‘¢Âº,
based on the embedding B/e.sc/r.sc/t.scÂ¹ğ¶ğ‘¢Âº. The encoded neighbours ğ¸ğ‘¢=E/n.sc/c.sc/o.sc/d.sc/e.sc/r.scÂ¹R/e.sc/t.scÂ¹ğ¶ğ‘¢ÂºÂºare then
used to condition the generation of the next chunk ğ¶ğ‘¢Â¸1, which we do incrementally: overall the
cost of sampling is thus quadratic in the size of the sampled sequence, as when sampling from
regular Transformers; the added cost of retrieval is linear in the number of chunks ğ‘™, and is negligible
compared to the token sampling cost in practice.
2.5. Baseline Transformer architecture
We use a transformer (Vaswani et al., 2017) similar to the one described in (Radford et al., 2019),
with some minimal changes: we replace LayerNorm with RMSNorm (Zhang and Sennrich, 2019) and
use relative position encodings (Dai et al., 2019). As baselines, we train retrieval-free transformers
with 132M, 368M, 1.3B and 7.0B parameters (embedding matrices are excluded from parameter
counts). The hyperparameters we used are detailed in Table 2. All retrieval models use the same
size encoder for the retrieval data, with ğ‘‘0=896and 2 layers, which roughly adds 19ğ‘€parameters.
The encoder uses relative positional encodings. The retrieval models contain one R/e.sc/t.sc/r.sc/o.sc-block every
3 blocks, starting from layer 6. For our smallest model, C/c.sc/a.scis applied in layers 6, 9 and 12 of the
main pathway and also once for query conditioning in the encoder, which adds an additional 12ğ‘€
parameters. The relative number of extra parameters reduces as we increase the baseline model size.
All models are implemented using JAX (Bradbury et al., 2018) and Haiku (Hennigan et al., 2020).
2.6. Quantifying dataset leakage exploitation
R/e.sc/t.sc/r.sc/o.scmodels may arguably beneï¬t more easily from evaluation dataset leakage, i.e. the fact that
we evaluate on data that were also present in the training set. To better understand how retrieval
improves language modelling performance, we therefore quantify evaluation likelihood as a function
of the overlap between the evaluation and training datasets.
The following approach can be used with any language model, and depends only on the frozen
retriever system presented in Â§2.3. We split the evaluation sequences Â¹ğ‘‹ğ‘–Âºğ‘–into chunks of length
ğ‘š64, and we see the training data as a set of chunks C. For each evaluation chunk ğ¶2C, we
retrieve the 10 closest neighbours (of length up to 128) in the training data. We then compute the
longesttokensubstringcommontoboththeevaluationchunkanditsneighbours. Thisgivesanumber
ğ‘ 2Â»0Â”ğ‘šÂ¼. The value ğ‘ŸÂ¹ğ¶Âº=ğ‘ 
ğ‘š, ranging from 0(chunk never seen) to 1(chunk entirely seen), gives a
reliable indication of how much overlap there is between the evaluation chunk and the training data.
For a given model, we then obtain the log-likelihood Â¹ğ¶Âºof each chunk ğ¶, and the number of bytes
ğ‘Â¹ğ¶Âºit encodes. We then consider the ï¬ltered bits-per-bytes of the model:
8ğ›¼2Â»0Â”1Â¼Â”Cğ›¼,fğ¶2CÂ”ğ‘ŸÂ¹ğ¶Âº6ğ›¼gÂ”bpbÂ¹ğ›¼Âº,Ã
ğ¶2Cğ›¼Â¹ğ¶ÂºÃ
ğ¶2Cğ›¼ğ‘Â¹ğ¶ÂºÂ” (5)
Table 2jNumber of parameters for our baseline and R/e.sc/t.sc/r.sc/o.scmodels, excluding embeddings, along
with the corresponding hyperparameters.
Baseline parameters R/e.sc/t.sc/r.sc/o.sc ğ‘‘ ğ‘‘ ï¬€w# heads Head size # layers
132M 172M (+30%) 896 3,584 16 64 12
368M 425M (+15%) 1,536 6,144 12 128 12
1,309M 1,451M (+11%) 2,048 8,192 16 128 24
6,982M 7,532M (+8%) 4,096 16,384 32 128 32
7

Improving language models by retrieving from trillions of tokens
whichcorrespondtothebits-per-bytesonthesetofchunksthatoverlaplessthan ğ›¼%withthetraining
chunks. Note that the full evaluation bit-per-bytes performance is recovered by bpbÂ¹1Âº. The function
bpbÂ¹Âºallows us to evaluate the impact of evaluation leakage over predictive performance: for low ğ›¼,
bpbÂ¹ğ›¼Âºgives an indication on how the model performs on chunks that are entirely new; the slope of
bpbÂ¹Âºshows how much the model exploits evaluation leakage.
3. Related Work
Weï¬rstreviewexistingworkonusingretrievalforlanguagemodelling,andcompare R/e.sc/t.sc/r.sc/o.sctothese
works (see Table 3). As we train R/e.sc/t.sc/r.sc/o.scmodels on a large dataset containing a substantial section
of the internet, our work raises potential privacy, safety, and fairness issues that we then review.
3.1. Retrieval for language modelling
Brants et al. (2007) show that scaling the training data to trillions of tokens improves the machine
translationperformanceof ğ‘›-grammodels. Morerecently,GPT-2(Radfordetal.,2019),GPT-3(Brown
et al., 2020), and Jurassic-1 (Lieber et al., 2021) show that scaling up language models leads to
massiveimprovementsonmanydownstreamtasks. Atthesametime,Carlinietal.(2021)demonstrate
that large-scale language models can perfectly memorise parts of their training data, suggesting that
enhancing models with retrieval may lead to further improvements. However, signiï¬cant leakage
betweentrainandtestdatasets(Leeetal.,2021;Lewisetal.,2021)makescomparingandevaluating
large models trained on large datasets diï¬ƒcult, especially once retrieval capabilities over the training
dataset are added.
Historically, information retrieval for text relies on inverted index matching such as TF-IDF and
BM25 (Robertson and Zaragoza, 2009). Foundational work use latent topic modelling approaches
like LDA (Blei et al., 2003) to identify relevant neighbours (Wei and Croft, 2006). Work in machine
translation such as Zhang et al. (2018) and Gu et al. (2018) retrieve translation pairs based on edit
distance between source sentences and guide the translation output using the closest retrieved target
sentences. The retrieval database may also be structured â€” for example, Ahn et al. (2016) use a
symbolic knowledge graph to improve an RNN language model.
With the success of deep learning, retrieving systems have partly switched to dense learned
representations based on a neural networkâ€™s activations. Continuous cache (Grave et al., 2017)
adds probability mass to tokens for which previous activations resemble the current activation
vector, extending the modelâ€™s context to the local history. ğ‘˜NN-LM(Khandelwal et al., 2020) applies
this idea to transformers and extends the retrieval database to English Wikipedia, resulting in
Table 3jComparison of R/e.sc/t.sc/r.sc/o.scwith existing retrieval approaches.
# Retrieval tokens Granularity Retriever training Retrieval integration
Continuous Cache O 103Token Frozen ( LSTM) Add to probs
ğ‘˜NN-LM O 109Token Frozen (Transformer) Add to probs
S/p.sc/a.sc/l.sc/m.sc O 109Token Frozen (Transformer) Gated logits
D/p.sc/r.sc O 109Prompt Contrastive proxy Extractive QA
R/e.sc/a.sc/l.sc/m.sc O 109Prompt End-to-End Prepend to prompt
RAG O 109Prompt Fine-tuned D/p.sc/r.sc Cross-attention
F/i.scD O 109Prompt Frozen D/p.sc/r.sc Cross-attention
E/m.sc/d.sc/r.sc2O 109Prompt End-to-End (EM) Cross-attention
R/e.sc/t.sc/r.sc/o.sc(ours)O 1012Chunk Frozen ( B/e.sc/r.sc/t.sc) Chunked cross-attention
8

Improving language models by retrieving from trillions of tokens
substantial improvements on Wikitext103 evaluation. Continuous cache and ğ‘˜NN-LMdo not modify
the underlying neural-network models, but interpolate at inference between the language modelâ€™s
output and distributions computed from retrieved tokens. These methods can therefore be plugged
into any model without additional training, although this limits the modelâ€™s ability to reason about
the retrieved text. S/p.sc/a.sc/l.sc/m.sc(Yogatama et al., 2021) addresses this limitation by adding an extra gating
network to post-process the retrieved data; yet most of the network is unaï¬€ected by the retrieval
during inference.
The retrieval representations may be trained directly instead of relying on a pre-trained modelâ€”
retrieversystemshavebeendevelopedforthispurpose,primarilyonopen-domainquestionanswering.
Forexample, D/p.sc/r.sc(Karpukhinetal.,2020)trainstwo B/e.sc/r.sc/t.scmodels(forqueriesandkeysrespectively)
using a contrastive loss to align the representations of a question and of its answers. Lee et al. (2019)
useaninverseclozetasktoï¬ndsemanticrepresentationsofpassagesforretrieval. Theseworksdiï¬€ers
from continuous cache and ğ‘˜NN-LMin that they embeds passages (or chunks) of text together, as
opposed to each token individually. The retriever network is trained in isolation of the downstream
task that uses the retrieval data. This potential issue is speciï¬cally addressed by R/e.sc/a.sc/l.sc/m.sc(Guu et al.,
2020), which trains the retrieval system end-to-end to maximize the ï¬nal training cross-entropy. This
comes with the extra complexity of searching the database during training and periodically updating
the embedding table, severely limiting the scale at which it can operate. RAG(Lewis et al., 2020)
andF/i.scD(Izacard and Grave, 2021) build upon D/p.sc/r.scto set the state of the art on question answering
benchmarks by training encoder-decoder transformer models. More recently, E/m.sc/d.sc/r.sc2(Sachan et al.,
2021) extends F/i.scDby using an expectation-maximization algorithm to train the retriever end-to-end
and achieves state of the art results compared to similarly sized models.
In the open-domain dialogue setting, BlenderBot 2.0 (Komeili et al., 2021) learns to issue textual
internet queries, outperforming dense retrieval methods when evaluated on a task measuring how
close model responses are to those of humans. This involves collecting a dataset of human dialogues
with associated search queries, which limits the scalability of this approach. Hashemi et al. (2020)
introduce the Guided Transformer, a modiï¬ed Transformer similar to R/e.sc/t.sc/r.sc/o.sc, for document retrieval
and clarifying question selection. Although eï¬€ective on question answering and other tasks with
strongconditioning,noneofthesemethodsaredesignedtomodelarbitrarytextsequences,incontrast
with R/e.sc/t.sc/r.sc/o.sc.
R/e.sc/t.sc/r.sc/o.scsharescomponentswith ğ‘˜NN-LMandD/p.sc/r.scinthatitusesfrozenretrievalrepresentations.
R/e.sc/t.sc/r.sc/o.scmodels longer sequences than QA examples; this requires to reason at a sub-sequence level,
and to retrieve diï¬€erent documents for the diï¬€erent chunks of a sequence. Similar to F/i.scD,R/e.sc/t.sc/r.sc/o.sc
processes the retrieved neighbours separately in the encoder, and assemble them in the chunked
cross-attention. This diï¬€ers from e.g. R/e.sc/a.sc/l.sc/m.sc, that prepends retrieved documents to the prompt.
Using chunks allows for repeated retrieval whilst generating a sequence as opposed to retrieving
only once based on the prompt alone. Furthermore, retrieval is done during the whole pre-training
process in R/e.sc/t.sc/r.sc/o.sc, and is not simply plugged-in to solve a certain downstream task. Finally, previous
methods based on dense query vectors use small models and retrieval datasets with less than 3B
tokens (English Wikipedia). Table 3 summarizes the diï¬€erence of R/e.sc/t.sc/r.sc/o.scwith existing approaches.
3.2. Privacy, safety and fairness
Bender et al. (2021); Weidinger et al. (2021) highlight several dangers of large language models.
Those stem from their ability to memorise training data, their high training cost, the static nature
of their training data (Lazaridou et al., 2021), their tendency of amplifying inherent biases in the
training data, and their ability to generate toxic language (Gehman et al., 2020). In this section we
inspect these dangers, focusing on how retrieval augmented language models may exacerbate or
9

Improving language models by retrieving from trillions of tokens
mitigate them.
Large language models can perfectly memorise parts of their training data (Carlini et al., 2021).
When coupled with large training datasets gathered from the web or other sources, this has clear
privacyandsafetyimplications. Retrievalmodelssuchas R/e.sc/t.sc/r.sc/o.scthathaveaccesstotheentiretraining
dataset during inference exacerbate these privacy issues by being able to directly copy training data.
However, retrieval systems oï¬€er a path towards mitigating these concerns via obliteration of the
retrievable data at inference time. In addition, diï¬€erential privacy training (Abadi et al., 2016) of
retrieval models could guarantee that no private information is stored in the model weights, while
individualisation on private data could be made by updating the retrieval database at inference time.
Due to their high training cost, re-training large language model regularly to incorporate new
data, languages, and norms is prohibitively expensive. To keep retrieval models up-to-date, it may be
suï¬ƒcient to update the retrieval database, which is orders of magnitude cheaper than re-training
a model from scratch. In addition to the beneï¬ts of updating models in terms of fairness and bias,
simply training large language models has a signiï¬cant energy cost (Schwartz et al., 2020; Strubell
et al., 2019). Retrieval mechanisms oï¬€er a path to reducing the compute requirements needed to
train and update language models that reach a certain performance.
Large language models are prone to generating toxic outputs, as shown in Gehman et al. (2020).
Benderetal.(2021);JoandGebru(2020)advocatefortheimportanceofbettertrainingdatacuration
and documentation. Additionally, if portions of the training data are found to be eliciting biased or
toxic outputs after training, retrieval allows for some correction, as the oï¬€ending retrieval data can
be retroactively ï¬ltered. However, it is also the case that without careful analysis and intervention,
retrieval models may exacerbate biases that are present in the training data. Retrieval models can
also add a further source of bias through the selection mechanism for retrieval documents. Further
work in this area is required to better understand how retrieval aï¬€ects the bias and toxicity of the
model outputs.
Finally, samples from large models are diï¬ƒcult to interpret, making mitigating these issues all the
more challenging (Belinkov et al., 2020; Jain and Wallace, 2019). Retrieval provides more insights in
to the outputs of a model, as one can directly visualise or modify the neighbours that are being used.
The examples in Table 6, 7, 20 and 21 illustrate how retrieval makes language models more factual
and interpretable by providing more transparent outputs.
4. Results
We ï¬rst report results on language modelling benchmarks. Second, we show how to R/e.sc/t.sc/r.sc/o.scï¬t
pre-trained Transformer language models into retrieval models with few additional FLOPs. Next,
we report R/e.sc/t.sc/r.sc/o.scresults on question answering. Finally, we report evaluation metrics with leakage
ï¬ltering, to better understand the source of the gains with retrieval.
4.1. Language modelling
Datasets. We evaluate our models on C4 (Raï¬€el et al., 2020), Wikitext103 (Merity et al., 2017),
Curation Corpus (Curation, 2020), Lambada (Paperno et al., 2016) and the Pile (Gao et al., 2020).
We also evaluate on a set of manually selected Wikipedia articles that were added or heavily edited in
September 2021, months after our pre-training and retrieval dataset was collected (details are given
in Â§A.2). We construct the dataset with articles from the â€œfutureâ€ and manually remove new articles
that strongly overlap documents in our training data. This guarantees that the evaluation documents
are not leaked in our training data.
10

Improving language models by retrieving from trillions of tokens
200 400 8001600 7500
Non-Embedding Params (M)0.450.500.550.600.650.70
a) LAMBADA Accuracy172M 425M 1.5B 7.5B Baseline RETRO [OFF] RETRO [ON]
200 400 8001600 7500
Non-Embedding Params (M)0.500.550.600.650.70
b) Curation Corpus bpb
200 400 8001600 7500
Non-Embedding Params (M)2351020
c) Wikitext103 Perplexity
200 400 8001600 7500
Non-Embedding Params (M)0.600.650.700.750.800.85
d) Wikipedia Sept 21 bpb
Figure 3jScaling with respect to model size. (a) LAMBADA top-1 accuracy. (b) Evaluation loss on
curation corpus. (c) Perplexity on Wikitext103 valid. (d) Bits-per-byte on selected Wikipedia articles
from September 2021.
For C4, Wikitext103, the Pile, and our Wikipedia dataset we evaluate the language modelling
performance on entire documents and measure the bits-per-byte (bpb). We favour bits-per-byte over
loss as it is tokenizer agnostic. We evaluate with a sequence length of 2048 tokens but use a stride of
1024 within documents to mitigate boundary eï¬€ects. On Curation Corpus we concatenate the article,
the â€œTL;DR:â€ string, and the summary, but only evaluate the bpb on the summary. For Lambada we
evaluate the accuracy on the last word, using greedy generation.
Model scaling. In Fig. 1(left) and Fig. 3 we show the language modelling performance as we scale
models from 150 million to 7 billion (non-embedding) parameters. We see that on all datasets,
R/e.sc/t.sc/r.sc/o.scoutperforms the baseline at all model sizes. Furthermore, we observe that improvements do
not diminish as we scale the models. The performance is dataset dependent, with the largest gains on
Wikitext103 and C4. Wikipedia articles and other web pages are similar to Wikitext103 documents,
evenifnotexactcopies(Â§4.4),wethusobtaindramaticimprovementsonWikitext103asourretrieval
model is able to directly exploit these overlaps. The smallest gains are for Curation Corpus, where
R/e.sc/t.sc/r.sc/o.sconly slightly outperforms the baseline. This is expected as Curation Corpus summaries are
designed to only contain information from the source article and are not included in our retrieval
database. On our â€œfutureâ€ Wikipedia September 2021 dataset, we also observe consistent gains for
all model sizes.
Data scaling. Fig. 1 (middle) shows how scaling the retrieval database at evaluation improves the
language modelling performance. We observe dramatic gains as the retrieval data is increased from
Wikipedia(4billiontokens)toallofMassivetext(1.7Ttokens). Fig.1(right)showshowperformance
scales as we increase the number of retrieved chunks. Despite being only trained with 2 neighbours,
we see consistent improvements for all models when the number of neighbours is increased from 1 to
10. Furthermore, we observe that larger models are able to better utilise more neighbours: the 172M
model improves with up to 10 neighbours, whereas the 7B model improves with up to 40 neighbours.
The Pile. Weevaluateour7BmodelsonthePiletestsets3andcompareagainstthe178Bparameter
Jurrasic-1 (Lieber et al., 2021) model and the 280B parameter Gopher (Rae et al., 2021) model. We
do not compare against GPT-3 as it is outperformed by Jurassic-1 and Gopher on almost all subsets.
Fig. 4 shows the relative improvements in bits-per-byte over our 7B transformer baseline for our
3Due to legal and ethical concerns relating to their use, we exclude the Enron Emails and the Youtube Subtitles datasets.
11

Improving language models by retrieving from trillions of tokens
dm_mathematics
ubuntu_irc
nih_exporter
arxiv
uspto_backgrounds
opensubtitles
philpapers
hackernews
stackexchange
freelaw
pubmed_abstracts
books3
pile_cc
pubmed_central
gutenberg_pg_19
github20
020406080100% improvementRelative bits-per-byte improvement over our 7B baseline without retrieval
Jurassic-1 (178B)
Gopher (280B)
RETRO (7.5B)
Figure4jThePile: Comparisonofour7BbaselineagainstJurassic-1, Gopher, and R/e.sc/t.sc/r.sc/o.sc.We
observe that the retrieval model outperforms the baseline on all test sets and outperforms Jurassic-1
on a majority of them, despite being over an order of magnitude smaller.
7.5B R/e.sc/t.sc/r.sc/o.scmodel, Jurassic-1 and Gopher. Jurassic-1 outperforms the baseline on all datasets
except for books, likely due to the inclusion of books in our training data. Gopher and R/e.sc/t.sc/r.sc/o.sc
outperform the baseline on all test sets. Overall, R/e.sc/t.sc/r.sc/o.sc7.5B outperforms Jurassic-1 and Gopher on
a majority of the test sets. On the dm_mathematics andubuntu_irc subsets, our R/e.sc/t.sc/r.sc/o.scmodel
doesnotoutperformour7BbaselineandunderperformsJurassic-1. Wehypothesisethattheretrieved
neighbours on these datasets are not helpful, due to a combination of what is in our retrieval dataset
and the eï¬ƒcacy of the nearest-neighbour search.
Wikitext103. Tovalidateourapproachinacontrolledsetting,wecompareourmethodwith ğ‘˜NN-LM
(Khandelwal et al., 2020) on the Wikitext103 dataset in Table 4. We train a baseline transformer
on the training set of Wikitext103. This transformer has 24 layers, 1024 hidden units, 16 heads
and a key size of 64, as in Baevski and Auli (2019). Our baseline does not have adaptive input, and
our tokenizer has an open vocabulary, unlike Baevski and Auli (2019), which makes our baseline
Table 4jPerplexities on Wikitext103. When using the Wikpedia dataset for retrieval, R/e.sc/t.sc/r.sc/o.sc
performs similarly to our implementation of ğ‘˜NN-LM. As we scale the retrieval dataset, R/e.sc/t.sc/r.sc/o.sc
performs much better. The perplexities for retrieving from full MassiveText are quite low, which is
partly due to partial overlap with Wikitext103 not caught by our deduplication.
Model Retrieval Set #Database tokens #Database keys Valid Test
Adaptive Inputs (Baevski and Auli, 2019) - - - 17.96 18.65
S/p.sc/a.sc/l.sc/m.sc(Yogatama et al., 2021) Wikipedia 3B 3B 17.20 17.60
ğ‘˜NN-LM (Khandelwal et al., 2020) Wikipedia 3B 3B 16.06 16.12
Megatron (Shoeybi et al., 2019) - - - - 10.81
Baseline transformer (ours) - - - 21.53 22.96
ğ‘˜NN-LM (ours) Wikipedia 4B 4B 18.52 19.54
R/e.sc/t.sc/r.sc/o.sc Wikipedia 4B 0.06B 18.46 18.97
R/e.sc/t.sc/r.sc/o.sc C4 174B 2.9B 12.87 10.23
R/e.sc/t.sc/r.sc/o.sc MassiveText (1%) 18B 0.8B 18.92 20.33
R/e.sc/t.sc/r.sc/o.sc MassiveText (10%) 179B 4B 13.54 14.95
R/e.sc/t.sc/r.sc/o.sc MassiveText (100%) 1792B 28B 3.21 3.92
12

Improving language models by retrieving from trillions of tokens
perplexities a bit higher. The full experiment details and hyperparameters are given in Â§C.2 and
Table 11.
We re-implement ğ‘˜NN-LMwith our tokenizer and baseline transformer to produce embeddings of
size 1024 for every token in Wikitext103. ğ‘˜NN-LMhas probabilities ğ‘ğ‘˜NN-LM =ğœ†ğ‘ğ‘˜NNÂ¸Â¹1 ğœ†Âºğ‘L/m.sc
withğ‘ğ‘˜NNÂ¹ğ‘›ğ‘˜Âº/expÂ¹ ğ›¼ğ‘‘ğ‘˜Âº. We tune ğœ†=0Â“118andğ›¼=0Â“00785on the validation set (Fig. 7) and
report performance for these hyperparameters on both the validation and test set.
We ï¬ne-tune our baseline transformer into a R/e.sc/t.sc/r.sc/o.scmodel (Fig. 7), using the Wikitext103
training data and retrieving from Wikipedia with 2 neighbours. We only train the new weights, as
explained in Â§4.2, and share the embedding weights between the encoder and the main pathway.
This is necessary for Wikitext103 which is quite small, as training R/e.sc/t.sc/r.sc/o.scfrom scratch in this setting
leads to over-ï¬tting.
We evaluate the ï¬ne-tuned R/e.sc/t.sc/r.sc/o.scmodel with diï¬€erent retrieval sets. We use 10 neighbours at
evaluation for both R/e.sc/t.sc/r.sc/o.scandğ‘˜NN-LM. When retrieving from Wikipedia, we obtain results com-
parable to our ğ‘˜NN-LM implementation. Furthermore, scaling the retrieval database to MassiveText
yields dramatic improvements, though this is partly due to leakage (see Â§4.4). For reproducibility,
we also include results when retrieving from C4, which are close to previous state-of-the-art and
comparable to using 10 % of MassiveText.
It is worth noting that ğ‘˜NN-LMrequires 1024 ï¬‚oats for every token in the retrieval dataset,
totalling 15 terabytes (Tb) for the 4 billion tokens in Wikipedia. ğ‘˜NN-LMand other token-level
retrieval approaches therefore donâ€™t scale to retrieval databases with trillions of tokens such as
MassiveText. In comparison, R/e.sc/t.sc/r.sc/o.sconly requires 215Gb to index our Wikipedia dataset, and 93Tb
for MassiveText. Inspecting the number of retrieval database entries in Table 4 makes it clear why
retrieving at the chunk level is necessary when scaling to datasets with trillions of tokens.
4.2.R/e.sc/t.sc/r.sc/o.sc-ï¬tting baseline models
We extend baseline models into R/e.sc/t.sc/r.sc/o.scmodels by freezing the pre-trained weights and training
only chunked cross-attention and neighbour encoder parameters (less than 10% of weights for the
7B model) in Fig. 5. This oï¬€ers an eï¬ƒcient alternative path to enhance transformers with retrieval,
requiring only 6 million sequences (3% of the pre-training sequences that we used). Additionally,
by only training the new weights we ensure that when evaluated without retrieval, the original
modelperformanceisexactlymaintained. R/e.sc/t.sc/r.sc/o.scï¬ttingmodelsquicklysurpassestheperformanceof
baseline models and even achieves performance close to that of R/e.sc/t.sc/r.sc/o.scmodels trained from scratch.
The experiment hyperparameters are given in Â§C.3.
4.3. Question answering
We ï¬ne-tune our retrieval models on the Natural Questions (Kwiatkowski et al., 2019) dataset
to demonstrate that our retrieval pathway can be used to inject information from arbitrary data
sources. We use the version4provided by Izacard and Grave (2021) which is augmented with the
retrieved passages from D/p.sc/r.sc(Karpukhin et al., 2020). We ï¬ne-tune all the weights of our 7.5B
pre-trained R/e.sc/t.sc/r.sc/o.scmodel for 25,000 steps using the top 20 retrieved passages. We format the
data as â€œ question: {question} \n answer: {answer} â€ and left pad the data such that
â€œanswer: â€ coincides with the end of the ï¬rst chunk of 64 tokens and thus aligns with the ï¬rst
retrievingchunk. Themodelhasaccesstothequestionviatheprevioustokensinthesequenceaswell
as the top 20 DPR Wikipedia passages and their titles via the chunked cross-attention mechanism.
4https://github.com/facebookresearch/FiD
13

Improving language models by retrieving from trillions of tokens
Figure5jR/e.sc/t.sc/r.sc/o.sc-ï¬ttingabaselinetransformer. Anytransformercanbeï¬ne-tunedintoaretrieval-
enhanced transformer by randomly initializing and training only the chunked cross-attention and
retrieval encoder weights. Fine-tuning in this way quickly recovers and surpasses the non-retrieval
performance, and almost achieves the same performance as training a retrieval model from scratch
(shown by the arrow on the right hand side of each plot). We ï¬nd good performance R/e.sc/t.sc/r.sc/o.sc-ï¬tting
our models training on only 3% the number of tokens seen during pre-training.
The exact match scores are shown in Table 5 and the full ï¬ne-tuning details are given in Â§C.4. Our
method is competitive with previous approaches such as R/e.sc/a.sc/l.sc/m.sc,RAGandD/p.sc/r.sc, but underperforms
the more recent F/i.scD. In contrast with this work, we ï¬nd that increasing the number of neighbours
past20doesnotimprove R/e.sc/t.sc/r.sc/o.scperformanceonthistask. Wehypothesisethattheencoder-decoder
structure of T5â€”the base model in F/i.scDâ€” and the T5 pre-training objective leads to a model that
relies more on the encoder output than R/e.sc/t.sc/r.sc/o.sc, which is important in the QA setting. To compete
with T5-ï¬netuned models, future work should consider ways of forcing R/e.sc/t.sc/r.sc/o.scto rely further on the
retrieval encoder output when producing tokens.
4.4. Relating retrieval performance to dataset leakage.
Wereporttheï¬lteredevallossesasdetailedinÂ§2.6onC4, CurationCorpusandWikitext103inFig.6.
On C4 and Wikitext103, for which there is leakage into the training set, the slope is negative for both
baseline models and R/e.sc/t.sc/r.sc/o.scmodels. R/e.sc/t.sc/r.sc/o.scmodels exploit leakage more strongly than baseline
models,asindicatedbythemorenegativeslope. Thisisduetoitsexplicitabilitytocopy-pasteexisting
training chunks to predict leaked evaluation chunks (see a qualitative example of this model behavior
Table 5jQuestion answering results. Exact match accuracy on Natural Questions.
Model Test Accuracy
R/e.sc/a.sc/l.sc/m.sc(Guu et al., 2020) 40.4
D/p.sc/r.sc(Karpukhin et al., 2020) 41.5
RAG(Lewis et al., 2020) 44.5
E/m.sc/d.sc/r.sc2(Sachan et al., 2021) 52.5
F/i.scD(Izacard and Grave, 2021) 51.4
F/i.scD+ Distill. (Izacard et al., 2020) 54.7
Baseline 7B (closed book) 30.4
R/e.sc/t.sc/r.sc/o.sc7.5B (DPR retrieval) 45.5
14

Improving language models by retrieving from trillions of tokens
12.5% 50% 100%0.70.80.91.0Eval bpbC4172M 425M 1.5B 7.5B Baseline RETRO [ON]
12.5% 50% 100%
Max eval/train chunk overlap when filtering0.500.550.600.65Curation Corpus
12.5% 50% 100%0.20.40.60.8Wikitext103
12.5% 50% 100%0.600.650.700.750.800.85Wikipedia Sept 2021
Figure 6jPerformance vs. longest common retrieval substring. Evaluation loss as a function of
allowed longest common substring between evaluation data chunks and their nearest neighbours.
Retrieval still helps when considering chunks with no more than 8 contiguous tokens overlapping
with training dataset chunks.
on a Wikitext103 article in Table 19). On Curation Corpus, retrieval provides a constant oï¬€set, which
is expected as there is by design no leakage between Curation Corpus and the training dataset.
On the other hand, R/e.sc/t.sc/r.sc/o.scoutperforms baseline models at all leakage levels, down to ğ›¼=12Â“5%.
At this level, the loss is computed on chunks with less than 8contiguous tokens shared with the
closest matching chunk in the training datasetâ€”this is a reasonable level of overlap at which we
consider that there is no local leakage. Retrieval thus improves predictions on both chunks that are
syntactically similar to chunks in the training set, and on chunks that are syntactically diï¬€erent from
all training chunks. This points toward a non trivial R/e.sc/t.sc/r.sc/o.sccapacity of generalizing based on both
model parameters and retrieval database. Similar results are found on the Pile dataset (see Fig. 12,
Â§F.3).
4.5. Using R/e.sc/t.sc/r.sc/o.scfor sampling
We show examples of samples obtained using the 7.5B R/e.sc/t.sc/r.sc/o.scmodel in Table 6, Table 7 and
Appendix E. For each chunk (the ï¬rst one being the prompt), we juxtapose sampled chunks ğ¶ğ‘¢with
retrieved neighbours R/e.sc/t.scÂ¹ğ¶ğ‘¢Âº. To give an indication of local overlap, we colour each sampled token
in chunkğ¶ğ‘¢based on the length of the longest common preï¬x (LCP) found in the retrieved chunks
R/e.sc/t.scÂ¹ğ¶ğ‘¢ 1Âº. Similarly, we colour the retrieved chunks based on the LCP in the sampled chunk. For the
sample in Table 6, for which we chose the prompt, we observe that the retrieved chunks inï¬‚uence the
sample as there are overlaps between the sampled tokens and neighbour tokens. Overall, retrieval
reduces hallucinations (in line with the ï¬ndings of Shuster et al. (2021)) and makes the model more
knowledgeable, when comparing with samples produced with retrieval disabled. In the sample in
Table 7, the model recognises that the prompt is the beginning of the ï¬rst scene of Hamlet and
leverages retrieval data to continue it with only a few mistakes. We provide further examples in
Appendix E, including examples from the evaluation sets, as well as the detailed procedure used for
colouring the tables.
5. Conclusion
We present Retrieval-Enhanced Transformers ( R/e.sc/t.sc/r.sc/o.sc), a method for modelling arbitrary text se-
quences whilstretrievingfromdatabaseswithtrillions oftokensâ€”scalingthedataavailable to models
by an order of magnitude compared to what is typically consumed during training. R/e.sc/t.sc/r.sc/o.scmodels
15

Improving language models by retrieving from trillions of tokens
gains do not diminish for models with up to at least 7B parameters, and correspond to non-retrieval
models with 10more parameters on certain datasets. On Wikitext103 and the Pile, R/e.sc/t.sc/r.sc/o.scoutper-
forms previous models trained on large scale datasets. We also show that R/e.sc/t.sc/r.sc/o.scis competitive on
retrieval-intensive downstream tasks such as question answering.
R/e.sc/t.sc/r.sc/o.scmodels are ï¬‚exible and can be used without retrieval at evaluation and still achieve
comparable performance to baseline models. Conversely, baseline models can be rapidly ï¬ne-tuned
intoR/e.sc/t.sc/r.sc/o.scmodelstoobtainnearlythesameperformanceasiftrainedfromscratch. Carefulanalysis
shows that only a modest fraction of the gains obtained by R/e.sc/t.sc/r.sc/o.scare due to test set leakage. In
general, we caution for such leakage in large-scale language datasets and suggest further work in
better understanding the role of test set leakage in the performance of large-scale language models.
Overall, our work demonstrates at an unprecedented scale that semi-parametric approaches can
provide an orthogonal, more eï¬ƒcient approach than raw parameter scaling as we seek to build more
powerful language models.
Acknowledgements
We would like to thank Nikolai Grigorev, Marcâ€™aurelio Ranzato, Cyprien de Masson dâ€™Autume, Po-Sen
Huang,JohannesWelbl,LisaAnneHendricks,EthanPerez,Jeï¬€Stanway,EricNoland,GregoryWayne,
John Jumper, Julian Schrittwieser, Lorrayne Bennett, Devang Agrawal, Dani Yogatama, Susannah
Young, Nando de Freitas, Demis Hassabis, and Koray Kavukcuoglu for their help, advice and reviews.
Additionally, we would like to thank Zonglin Li, David Simcha, and the ScaNN developers for their
help.
16

Improving language models by retrieving from trillions of tokens
Table 6jSample - Beavers are interesting animals . The R/e.sc/t.sc/r.sc/o.sc[O/f.sc/f.sc] sample quickly diverges to other
animalswhilethe R/e.sc/t.sc/r.sc/o.sc[O/n.sc] sampletendstostayfocusedonthebeavertopicduetoneighbourconditioning.
Prompt and sample of R/e.sc/t.sc/r.sc/o.sc[O/f.sc/f.sc] Prompt and sample of R/e.sc/t.sc/r.sc/o.sc[O/n.sc] Â»ğ‘1ğ‘¢Â”ğ¹1ğ‘¢Â¼colored by LCP with ğ¶ğ‘¢Â¸1Â»ğ‘2ğ‘¢Â”ğ¹2ğ‘¢Â¼colored by LCP with ğ¶ğ‘¢Â¸1
colored by LCP with R/e.sc/t.scÂ¹ğ¶ğ‘¢ 1Âº
LCP=0,1,2,3,4,>5 LCP=0,1,2,3,4,>5 LCP=0,1,2,3,4,>5
Beavers are interesting animals that Beavers are interesting animals that .Beaversbuildtheirlodgesinpon naw them into smaller sections andd
live near rivers. They build live near rivers. They build ds they have created inwooded areas rag them into thewater.Engineers
.Like many things innature,there ofthePondBeaversare interesting
is a connection between creatures i animals because they change thehab
nthewild.Beaverponds cause tree itatinwhich they live.Beaversdo
s todrown,butthedeadtreesattra this by blocking upstreamstocreat
ctthegreat blue heron ,which often e ponds.Then they buildtheirhomes
return year after year .Over time , , calledlodges, inthese ponds .Bea
abeaverpond can attract more than versâ€™ bodies make them well-suited f
50 nestsina colony, calleda rooke or underwater building Special muscl
ry.An example of this can befound es close oï¬€ theirnoses,ears, and
inthelarge pond oï¬€ Bradford Road throatstokeepthewater out .Beave
at Carter Fields near theBoxford l rsâ€™ broadtailsact like rudders for
ine.Chris Leahy ,an expert with th steering.Their two very large ,ora
eMassachusetts Audubon Society who nge front teethare used tognawdow
wrote ntrees.Theybegin building theird
am
dams to create ponds. Frogs are am theirhousescalled beaverdamsin,thentheymeanthatyouare verybar-like tail , andtwo protruding tee
phibians, so they can live in both l theriverbeds .Theyalsoliveon lan usy.Beaversswim easily in streams ,th that are strong enoughtognawdo
and and water. They have great camou d.Beaversusetheirstrongteeth an picking up rocksandstickstobuil wntrees. Thebeaverusestrees,bra
ï¬‚age to hide from predators. The G dstrong jaws tocutdowntreesand dtheirdams.Theygnawattreeswit nches, andmudtobuilddams across
olden Retriever, or Golden, is a ver branches to buildtheirhomes.They htheirbig front teethtocutthem riversandstreams.These dams creat
y big dog. They are very strong, and also usetheir teeth and jawstoche down.Thenthey usepartsof the tre eadeep pool ofwater in which the
have a thick coat to help them live wupfood.Beaversusetheirbig,ï¬‚es tobuildtheirhouses.Beavers ar beaverbuilds its home .Abeaverhom
in attails toswim.Theyuse eclever builders .Theyknow exactly eis calledalodge.A babybeavero
whattheyneedtobuildtheir beave r â€œkitâ€ remains in thefamilylodge
rdams.Theyusemud from thestream untiltheageoftwo.Beaverfur,kn
tomaketheirdams stay together .Town as pelt ,was once highly popular
heyusetheirtailstopat down the asatrim for hats andcoats.How m
mud.Beaversputasnug room at the ightthepopularity of beaverfur co
topof theirdams for theirbabies. ntributed to thecolonization ofNew
Theystoretheirfood underwater .Be York?wwwWw
averseatthebark from the
cold areas. A kangaroo is an anima theirsharpteethto gnaw atrocks , thentheymean that you are very b w attreeswiththeirbig front teet
l that is very large. It can jump ve andcut trees. Theyusetheirï¬‚attusy.Beaversswimeasily in streams, hto cutthemdown.Thentheyusepa
ry high. They can run very fast. A ailstosteer when they areswimming pickinguprocksandstickstobuil rts of the trees to buildtheirhous
meerkat is a small animal that lives .Inthewinter, beavers havetobui dtheirdams.Theygnaw attreeswit es.Beavers are clever builders .The
in Africa. They live in colonies of ldahousecalledabeaverlodge. Th htheirbig front teethto cutthem yknow exactly what theyneedtobui
up to a hundred of them. They can c eybuild ahousethat is very strong down.Thentheyuseparts of the tre ldtheirbeaver dams .Theyusemud f
limb trees very easily. .Thewallsaremadeoftwigs.Theres tobuildtheirhouses.Beavers ar romthestreamtomaketheirdams st
oofismade e clever builders .Theyknow exactly ay together .Theyusetheirtailsto
whattheyneedtobuildtheirbeave pat down themud.Beavers put a snu
r dams.Theyusemud from thestream g room at thetopoftheirdams for
tomaketheirdams stay together .Ttheirbabies.Theystoretheirfood
heyusetheirtailstopat down the underwater .Beavers eat thebark fro
mud.Beavers put a snug room at the mthe trees thatthey cutdown!1. W
topoftheirdams for theirbabies. hat isthemain idea of theï¬rst pa
Theystoretheirfood underwater .Be ragraph?.2. What is themain idea o
avers eat thebark from the f thesecond paragraph?
A mouse is a small mammal that lives ofbranches andother tree parts.T
on land. It is a very good climber heyalsousetheirstrong jaws tocu
and it can run very fast. Penguins ttrees.Theybringthem to their ho
are birds that live on Antarctica. T use.Theyalsousetheirsharpteeth
hey have a thick coat to keep them w tochewup thetreeparts.Theyuse
arm. Rabbits are small animals that theirï¬‚attailstoswim to the top
live in the ground. They oftheirhouse.Thentheyusetheir
teeth and jawstochewup thetree
17

Improving language models by retrieving from trillions of tokens
Table 7jSample - Hamlet, Act 1, Scene 1. TheR/e.sc/t.sc/r.sc/o.sc[O/f.sc/f.sc] sample has correct syntax but is hallucinated,
and ends with repetition of one character ( FRANCISCO Approach me not ). The R/e.sc/t.sc/r.sc/o.sc[O/n.sc] sample is the
correct continuation of the original text, and is robust to formatting diï¬€erences between our prompt and the
retrieved data.
Prompt and sample of R/e.sc/t.sc/r.sc/o.sc[O/f.sc/f.sc] Prompt and sample of R/e.sc/t.sc/r.sc/o.sc[O/n.sc] Â»ğ‘1ğ‘¢Â”ğ¹1ğ‘¢Â¼colored by LCP with ğ¶ğ‘¢Â¸1Â»ğ‘2ğ‘¢Â”ğ¹2ğ‘¢Â¼colored by LCP with ğ¶ğ‘¢Â¸1
colored by LCP with R/e.sc/t.scÂ¹ğ¶ğ‘¢ 1Âº
LCP=0,1,2,3,4,>5 LCP=0,1,2,3,4,>5 LCP=0,1,2,3,4,>5
ACT I SCENE I. Elsinore. A pla ACT I SCENE I. Elsinore. A pla ACTISCENEI.Elsinore.A pla ><TEXT>ACT ISCENEI.Elsin
tform before the castle. FRANC tform before the castle. FRANC tform before thecastle.(FRAN ore.A platform before thecas
ISCO at his post. Enter to him ISCO at his post. Enter to him CISCO athis post.Entertohi tle.FRANCISCOathis post.E
BERNARDO BERNARDO Whoâ€™s there BERNARDO BERNARDO Whoâ€™s there mBERNARDO)BERNARDOWhoâ€™s the ntertohimBERNARDOBERNARDO
? FRANCISCO Nay, answer me: st ? FRANCISCO Nay, answer me: st re? FRANCISCONay,answer me : Whoâ€™s there ? FRANCISCONay,an
and, and unfold yourself. BERN and, and unfold yourself. BERN stand,and unfold yourself .BE swer me:stand,and unfold you
ARDO ARDO RNARDOLonglivetheking! FRA rself.BERNARDOLonglivethe
NCISCO Bernardo? BERNARDO He. king! FRANCISCO Bernardo? BERN
FRANCISCO You come most carefu ARDO He. FRANCISCO You come mo
lly upon your hour. BERNARDO â€™ st carefully upon your hour. B
Tis now struck twelve; get the ERNARDO â€™Tis now struck twelve
e to bed, Francisco. FRANCISCO ; get thee to bed, Francisco.
For this relief much thanks: FRANCISCO For this relief much
â€™tis bitter cold, And I am sic thanks: â€™tis bitter cold, And
k at heart. B ERNARDOHave you I am sick at heart.
Who calls ? FRANCISCO I am th Longlivetheking! FRANCISCO Long live the king ! FRANCISCO live the king ! FRANCISCOBern
e lord here; I, Francisco, tha Bernardo? BERNARDO He. FRANCI Bernardo ? BERNARDOHe.FRANCI ardo? BERNARDOHe.FRANCISCOY
t am sick of grief. [ Aside. B SCO You come most carefully up SCOYou come most carefully up ou come most carefully upon yo
ERNARDO The king ! FRANCISCO I on your hour. BERNARDO â€™Tis no on your hour .BERNARDOâ€™Tis no ur hour.BERNARDOâ€™Tis now str
am sick of that also. BERNARD w struck twelve; get thee to b w struck twelve; get thee to b uck twelve: get thee to bed ,F
O My lord ? FRANCISCO Do not a ed, Francisco. FRANCISCO For t ed,Francisco .FRANCISCOFor t rancisco.FRANCISCOFor this r
pproach me. BERNARDO his relief much thanks: â€™tis b his relief much thanks: â€™tis b elief much thanks: â€™tis bitter
itter cold, And I am sick at h itter cold ,AndIam sick at h cold,AndIam sick at heart .
eart. B eart.</TEXT></DOC><DOC><DO BERNARDOHaveyou had quiet g
CNO>romeo</DOCNO><TEXT>ACT Iuard? FRANCISCO Not a mouse st
PROLOGUE Two households ,bo irring. BERNARDO Well, good ni
th alike in dignity ,In fair V ght. IfyoudomeetHoratioand
erona,where we lay our scene , Marcellus, The rivals 2ofmy
From ancient grudge break to watch,bid them make haste. FR
new mutiny , ANCISCO I think I hear them .â€”
Stand,ho!who is there? EN
Francisco, I would speak with ERNARDOHaveyou had quiet gua had quiet guard ?FRANCISCONo ARDO Have youhad quiet guard ?
you. FRANCISCO Approach me not rd? FRANCISCO Not a mouse stir t a mouse stirring .BERNARDO W FRANCISCONot a mouse stirrin
, but speak. BERNARDO Your han ring. BERNARDO Well, good nigh ell, goodnight.Ifyoudo mee g.BERNARDO Well , goodnight.
d, your voice FRANCISCO I will t. Ifyou domeetHoratioand t Horatio andMarcellus ,The r Ifyou do meet Horatio andMarc
not hear thee speak. BERNARDO Marcellus, The rivals ofmywa ivals ofmywatch,bid them ma ellus,The rivals2 of mywatch
Francisco, your hand, I entre tch,bid them make haste. FRAN ke haste.FRANCISCOI think I ,bid them make haste .FRANCIS
at thee. FRANCISCO Approach me CISCO I think I hear them . Sta hear them .Stand,ho!Whoâ€™s th COI think I hear them.â€” Stand
not. BERNARDO Francisco FRANC nd,ho!who is there? Enter ere?(EnterHORATIOandMARCEL ,ho! who is there ?ENTERHORA
LUS)HORATIOFriendsto this g TIOANDMARCELLUS. HORATIOFri
round. MARCELLUS And liegemen endsto this ground. MARCELLUS
to the Dane. FRANCISCO Give yo And liegemen to the Dane .3FR
u good night. MARCELLUS O, far ANCISCOGiveyougood night. M
ewell, honest soldier: Who hat ARCELLUS O, farewell, honest s
h relieved you? FRANCISCO Bern oldier: Who hath relieved you?
ardohasmyplace.Giveyou go FRANCISCO Bernardo hath my pl
od night. (Exit ace. Give you good night
ISCO Approach me not. BERNARDO HORATIOandMARCELLUS HORATIO
I have a letter FRANCISCO App Friendsto this ground. MARCE
roach me not. BERNARDO For the LLUS And liegemen to the Dane.
king. FRANCISCO Approach me n FRANCISCO Give you good night
ot. BERNARDO Thereâ€™s no treaso . MARCELLUS O, farewell, hones
n inâ€™t. FRANCISCO Approach me t soldier: Who hath relieved y
not. BERNARDO I will ou? FRANCISCO Bernardo hath my
place. Give you good night.
18

Improving language models by retrieving from trillions of tokens
References
M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and L. Zhang. Deep learning
with diï¬€erential privacy. In ACM SIGSAC Conference on Computer and Communications Security ,
2016.
S. Ahn, H. Choi, T. PÃ¤rnamaa, and Y. Bengio. A neural knowledge language model. arXiv preprint
arXiv:1608.00318 , 2016.
A.BaevskiandM.Auli. Adaptiveinputrepresentationsforneurallanguagemodeling. In International
Conference on Learning Representations , 2019. URL https://openreview.net/forum?id=
ByxZX20qFQ .
Y. Belinkov, S. Gehrmann, and E. Pavlick. Interpretability and analysis in neural NLP. In Proceedings
of the 58th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts ,
pages 1â€“5, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.
acl-tutorials.1. URL https://aclanthology.org/2020.acl-tutorials.1 .
E. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell. On the dangers of stochastic parrots:
Can language models be too big? In ACM Conference on Fairness, Accountability, and Transparency ,
2021.
D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet Allocation. Journal of Machine Learn-
ing Research , 3(Jan):993â€“1022, 2003. URL https://jmlr.csail.mit.edu/papers/v3/
blei03a.html .
J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. V.
der Plas, S. Wanderman-Milne, and Q. Zhang. JAX: composable transformations of Python+NumPy
programs, 2018. URL http://github.com/google/jax .
T. Brants, A. C. Popat, P. Xu, F. J. Och, and J. Dean. Large Language models in machine translation.
InJoint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning , pages 858â€“867, 2007.
T.Brown,B.Mann,N.Ryder,M.Subbiah,J.D.Kaplan,P.Dhariwal,A.Neelakantan,P.Shyam,G.Sastry,
A.Askell,S.Agarwal,A.Herbert-Voss,G.Krueger,T.Henighan,R.Child,A.Ramesh,D.Ziegler,J.Wu,
C.Winter,C.Hesse,M.Chen,E.Sigler,M.Litwin,S.Gray,B.Chess,J.Clark,C.Berner,S.McCandlish,
A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners. In Advances
in Neural Information Processing Systems , 2020. URL https://proceedings.neurips.cc/
paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf .
N. Carlini, F. Tramer, E. Wallace, M. Jagielski, A. Herbert-Voss, K. Lee, A. Roberts, T. Brown, D. Song,
U. Erlingsson, A. Oprea, and C. Raï¬€el. Extracting training data from large language models.
Preprint, 2021.
C. Consonni, D. Laniado, and A. Montresor. Wikilinkgraphs: a complete, longitudinal and multi-
language dataset of the wikipedia link networks. In AAAI International Conference on Web and
Social Media , volume 13, 2019.
Curation. Curation corpus base, 2020.
Z.Dai,Z.Yang,Y.Yang,J.Carbonell,Q.Le,andR.Salakhutdinov. Transformer-XL:Attentivelanguage
models beyond a ï¬xed-length context. In Annual Meeting of the Association for Computational
Linguistics , July 2019. URL https://aclanthology.org/P19-1285 .
19

Improving language models by retrieving from trillions of tokens
J.Devlin,M.-W.Chang,K.Lee,andK.Toutanova.BERT:Pre-trainingofdeepbidirectionaltransformers
for language understanding. In Conference of the North American Chapter of the Association for
Computational Linguistics , June 2019. URL https://aclanthology.org/N19-1423 .
L.Gao,S.Biderman, S.Black, L.Golding, T.Hoppe, C.Foster,J.Phang, H.He, A.Thite, N.Nabeshima,
S. Presser, and C. Leahy. The Pile: An 800GB dataset of diverse text for language modeling. arXiv
preprint arXiv:2101.00027 , 2020.
S. Gehman, S. Gururangan, M. Sap, Y. Choi, and N. A. Smith. RealToxicityPrompts: Evaluating neural
toxic degeneration in language models. In Conference on Empirical Methods in Natural Language
Processing , Nov. 2020. URL https://aclanthology.org/2020.findings-emnlp.301 .
E. Grave, A. Joulin, and N. Usunier. Improving neural language models with a continuous cache. In
International Conference on Learning Representations , 2017. URL https://openreview.net/
forum?id=B184E5qee .
A. Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850 ,
2013.
J. Gu, Y. Wang, K. Cho, and V. O. Li. Search engine guided neural machine translation. In AAAI
Conference on Artiï¬cial Intelligence , 2018.
R. Guo, P. Sun, E. Lindgren, Q. Geng, D. Simcha, F. Chern, and S. Kumar. Accelerating large-scale
inference with anisotropic vector quantization. In International Conference on Machine Learning ,
2020. URL https://arxiv.org/abs/1908.10396 .
K. Guu, K. Lee, Z. Tung, P. Pasupat, and M. Chang. Retrieval augmented language model pre-training.
InInternational Conference on Machine Learning , 2020.
H. Hashemi, H. Zamani, and W. B. Croft. Guided transformer: Leveraging multiple external sources
for representation learning in conversational search. In Proceedings of the 43rd International ACM
SIGIR Conference on Research and Development in Information Retrieval , pages 1131â€“1140, 2020.
T. Hennigan, T. Cai, T. Norman, and I. Babuschkin. Haiku: Sonnet for JAX, 2020. URL http:
//github.com/deepmind/dm-haiku .
G. Izacard and E. Grave. Leveraging passage retrieval with generative models for open domain
question answering. In Conference of the European Chapter of the Association for Computational
Linguistics , Apr. 2021. URL https://aclanthology.org/2021.eacl-main.74 .
G. Izacard, F. Petroni, L. Hosseini, N. De Cao, S. Riedel, and E. Grave. A memory eï¬ƒcient baseline for
open domain question answering. arXiv preprint arXiv:2012.15156 , 2020.
S. Jain and B. C. Wallace. Attention is not Explanation. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers) , pages 3543â€“3556, Minneapolis, Minnesota, June
2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1357. URL https:
//aclanthology.org/N19-1357 .
E. S. Jo and T. Gebru. Lessons from archives: Strategies for collecting sociocultural data in machine
learning. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency , pages
306â€“316, 2020.
R. Jozefowicz, O. Vinyals, M. Schuster, N. Shazeer, and Y. Wu. Exploring the limits of language
modeling. arXiv preprint arXiv:1602.02410 , 2016.
20

Improving language models by retrieving from trillions of tokens
J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu,
and D. Amodei. Scaling laws for neural language models. CoRR, 2020. URL https://arxiv.
org/abs/2001.08361 .
V. Karpukhin, B. Oguz, S. Min, P. Lewis, L. Wu, S. Edunov, D. Chen, and W.-t. Yih. Dense passage re-
trievalforopen-domainquestionanswering. In ConferenceonEmpiricalMethodsinNaturalLanguage
Processing , Nov. 2020. URL https://aclanthology.org/2020.emnlp-main.550 .
U. Khandelwal, O. Levy, D. Jurafsky, L. Zettlemoyer, and M. Lewis. Generalization through memoriza-
tion: Nearest neighbor language models. In International Conference on Learning Representations ,
2020. URL https://openreview.net/forum?id=HklBjCEKvH .
M. Komeili, K. Shuster, and J. Weston. Internet-augmented dialogue generation. arXiv preprint
arXiv:2107.07566 , 2021.
T. Kudo and J. Richardson. Sentencepiece: A simple and language independent subword tokenizer
and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226 , 2018.
T. Kwiatkowski, J. Palomaki, O. Redï¬eld, M. Collins, A. Parikh, C. Alberti, D. Epstein, I. Polosukhin,
M. Kelcey, J. Devlin, K. Lee, K. N. Toutanova, L. Jones, M.-W. Chang, A. Dai, J. Uszkoreit, Q. Le, and
S. Petrov. Natural Questions: a benchmark for question answering research. Transactions of the
Association of Computational Linguistics , 7:452â€“466, Mar. 2019. URL https://aclanthology.
org/Q19-1026 .
A. Lazaridou, A. Kuncoro, E. Gribovskaya, D. Agrawal, A. Liska, T. Terzi, M. Gimenez, C. de Mas-
son dâ€™Autume, S. Ruder, D. Yogatama, K. Cao, T. KociskÃ½, S. Young, and P. Blunsom. Pitfalls of static
language modelling. CoRR, 2021. URL https://arxiv.org/abs/2102.01951 .
K. Lee, M.-W. Chang, and K. Toutanova. Latent Retrieval for Weakly Supervised Open Domain
Question Answering. In Annual Meeting of the Association for Computational Linguistic , June 2019.
URLhttp://arxiv.org/abs/1906.00300 .
K. Lee, D. Ippolito, A. Nystrom, C. Zhang, D. Eck, C. Callison-Burch, and N. Carlini. Deduplicating
training data makes language models better. arXiv preprint arXiv:2107.06499 , 2021.
P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. KÃ¼ttler, M. Lewis, W.-t. Yih,
T.RocktÃ¤schel,S.Riedel,andD.Kiela. Retrieval-augmentedgenerationforknowledge-intensiveNLP
tasks. In Advances in Neural Information Processing Systems , 2020. URL https://proceedings.
neurips.cc/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf .
P. Lewis, P. Stenetorp, and S. Riedel. Question and answer test-train overlap in open-domain question
answering datasets. In Conference of the European Chapter of the Association for Computational
Linguistics , Apr. 2021. URL https://aclanthology.org/2021.eacl-main.86 .
O. Lieber, O. Sharir, B. Lenz, and Y. Shoham. Jurassic-1: Technical details and evaluation. White Paper.
AI21 Labs , 2021.
I. Loshchilov and F. Hutter. Decoupled weight decay regularization. In International Conference on
Learning Representations , 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7 .
S. Merity, C. Xiong, J. Bradbury, and R. Socher. Pointer sentinel mixture models. In International
Conference on Learning Representations , 2017. URL https://openreview.net/forum?id=
Byj72udxe .
21

Improving language models by retrieving from trillions of tokens
T. Mikolov, M. Karaï¬Ã¡t, L. Burget, J. Cernock `y, and S. Khudanpur. Recurrent neural network based
language model. Interspeech , 2(3):1045â€“1048, 2010.
D. Paperno, G. Kruszewski, A. Lazaridou, N. Q. Pham, R. Bernardi, S. Pezzelle, M. Baroni, G. Boleda,
and R. FernÃ¡ndez. The LAMBADA dataset: Word prediction requiring a broad discourse context.
InAnnual Meeting of the Association for Computational Linguistics , Aug. 2016. URL https://
aclanthology.org/P16-1144 .
A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are unsupervised
multitask learners. Preprint, 2019.
J. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoï¬€mann, F. Song, J. Aslanides, S. Henderson, R. Ring,
S. Young, E. Rutherford, T. Hennigan, J. Menick, A. Cassirer, R. Powell, G. van den Driessche, L. A.
Hendricks, M. Rauh, P.-S. Huang, A. Glaese, J. Welbl, S. Dathathri, S. Huang, J. Uesato, J. Mellor,
I. Higgins, A. Creswell, N. McAleese, A. Wu, E. Elsen, S. Jayakumar, E. Buchatskaya, D. Budden,
E. Sutherland, K. Simonyan, M. Paganini, L. Sifre, L. Martens, X. L. Li, A. Kuncoro, A. Nematzadeh,
E. Gribovskaya, D. Donato, A. Lazaridou, A. Mensch, J.-B. Lespiau, M. Tsimpoukelli, N. Grigorev,
D. Fritz, T. Sottiaux, M. Pajarskas, T. Pohlen, Z. Gong, D. Toyama, C. de Masson dâ€™Autume, Y. Li,
T. Terzi, V. Mikulik, I. Babuschkin, A. Clark, D. de Las Casas, A. Guy, J. Bradbury, M. Johnson,
B. Hechtman, L. Weidinger, I. Gabriel, W. Isaac, E. Lockhart, S. Osindero, L. Rimell, C. Dyer,
O. Vinyals, K. Ayoub, J. Stanway, L. Bennett, D. Hassabis, K. Kavukcuoglu, and G. Irving. Scaling
language models: Methods, analysis & insights from training Gopher. arXiv submission , 2021.
C.Raï¬€el,N.Shazeer,A.Roberts,K.Lee,S.Narang,M.Matena,Y.Zhou,W.Li,andP.J.Liu. Exploring
the limits of transfer learning with a uniï¬ed text-to-text transformer. Journal of Machine Learning
Research, 21(140):1â€“67, 2020. URL http://jmlr.org/papers/v21/20-074.html .
S. Rajbhandari, J. Rasley, O. Ruwase, and Y. He. Zero: Memory optimizations toward training trillion
parameter models. In IEEE International Conference for High Performance Computing, Networking,
Storage and Analysis , 2020.
S.RobertsonandH.Zaragoza. Theprobabilisticrelevanceframework: BM25andbeyond. Foundations
and Trends in Information Retrieval , 3:333â€“389, Jan 2009.
D.S.Sachan,S.Reddy,W.Hamilton,C.Dyer,andD.Yogatama. End-to-endtrainingofmulti-document
reader and retriever for open-domain question answering. arXiv preprint arXiv:2106.05346 , 2021.
R. Schwartz, J. Dodge, N. A. Smith, and O. Etzioni. Green AI. Communications of the Association for
Computing Machinery , 63(12):54â€“63, Nov. 2020.
M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro. Megatron-LM: Training
multi-billion parameter language models using model parallelism. CoRR, 2019. URL http:
//arxiv.org/abs/1909.08053 .
K. Shuster, S. Poï¬€, M. Chen, D. Kiela, and J. Weston. Retrieval augmentation reduces hallucination in
conversation. arXiv:2104.07567 [cs] , Apr. 2021. URL http://arxiv.org/abs/2104.07567 .
E. Strubell, A. Ganesh, and A. McCallum. Energy and policy considerations for deep learning in NLP.
InAssociation for Computational Linguistics , July 2019. URL https://aclanthology.org/
P19-1355 .
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser,
and I. Polosukhin. Attention is all you need. In Advances in Neural Information Pro-
cessing Systems , 2017. URL https://proceedings.neurips.cc/paper/2017/file/
3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf .
22

Improving language models by retrieving from trillions of tokens
X. Wei and W. B. Croft. LDA-based document models for ad-hoc retrieval. In ACM SIGIR International
Conference on Research and Development in Information Retrieval , 2006. URL http://portal.
acm.org/citation.cfm?doid=1148170.1148204 .
L.Weidinger,I.Gabriel,C.Griï¬ƒn,M.Rauh,J.Uesato,J.Mellor,W.Isaac,P.-S.Huang,L.A.Hendricks,
M. Cheng, B. Balle, J. Haas, C. Biles, L. Rimell, W. Hawkins, M. Glaese, A. Kasirzadeh, Z. Kenton,
S. Brown, A. Birhane, T. Stepleton, G. Irving, and S. Legassick. Ethical and social risks of harm
from language models. arXiv submission , 2021.
D. Yogatama, C. de Masson dâ€™Autume, and L. Kong. Adaptive semiparametric language models.
Transactions of the Association for Computational Linguistics , 9:362â€“373, 2021.
B. Zhang and R. Sennrich. Root mean square layer normalization. In Advances in Neural Information
Processing Systems , 2019. URL https://proceedings.neurips.cc/paper/2019/file/
1e8a19426224ca89e83cef47f1e7f53b-Paper.pdf .
J. Zhang, M. Utiyama, E. Sumita, G. Neubig, and S. Nakamura. Guiding neural machine translation
with retrieved translation pieces. In Conference of the North American Chapter of the Association for
Computational Linguistics , 2018.
23

Improving language models by retrieving from trillions of tokens
A. Datasets
We provide a full description of MassiveText and of our extract of recent Wikipedia articles.
A.1. Full description of MassiveText
The full break down of MassiveText by source and languages is given in Table 8. For a full description
and analysis of MassiveText, see Rae et al. (2021).
Source Language Token count (M) Documents Sampling weight
WebEn 483,002 604,938,816 0.314
Ru 103,954 93,004,882 0.033
Es 95,762 126,893,286 0.033
Zh 95,152 121,813,451 0.033
Fr 59,450 76,612,205 0.033
De 57,546 77,242,640 0.033
Pt 44,561 62,524,362 0.033
It 35,255 42,565,093 0.033
Sw 2,246 1,971,234 0.0044
Ur 631 455,429 0.0011
Books En 3,423,740 20,472,632 0.25
News En 236,918 397,852,713 0.1
WikipediaEn 3,977 6,267,214 0.0285
De 2,155 3,307,818 0.003
Fr 1,783 2,310,040 0.003
Ru 1,411 2,767,039 0.003
Es 1,270 2,885,013 0.003
It 1,071 2,014,291 0.003
Zh 927 1,654,772 0.003
Pt 614 1,423,335 0.003
Ur 61 344,811 0.0001
Sw 15 58,090 0.0004
Github - 374,952 142,881,832 0.05
Total - 5,026,463 1,792,260,998 1
Table 8jMassiveText dataset. The ï¬nal column indicates the sampling weight for each dataset
during training. For the retrieval database, the entire dataset is used, with the exception of books for
which we use a sub-sample of 4%.
A.2. Wikipedia September 2021
We create an evaluation dataset consisting of 23 Wikipedia articles that were added or heavily edited
in September 2021, after we collected our training dataset. In addition, we ï¬lter out articles that rely
too heavily on templated content, using the method detailed in Â§2.6 to identify articles with chunks
that have a high overlap with their neighbours. Fig. 10 show that little overlap remains between our
test dataset and the retrieved neighbours from the training dataset. The full list of included articles is
given in Table 9.
24

Improving language models by retrieving from trillions of tokens
Table 9jFull set of articles included in our Wikipedia Sept. 2021 evaluation dataset.
Megan Rohrer Aakashavaani
Emma Raducanu Junior Eurovision Song Contest 2021
Ambra Sabatini Pavilion Bukit Jalil
WhyDonate Blake Desjarlais
The Juggernaut (company) 2021 All-Ireland Senior Football Championship Final
Angela Diaz Drift-barrier hypothesis
2020 Summer Paralympics Venomics
2021 Afghan protests Great Circle (novel)
Rexh Xhakli Hurricane Ida
Julia Laskin 2021 Montenegrin episcopal enthronement protests
Cuijk At War With the Silverï¬sh
Ghoubet Wind Power Station
We ï¬rst parse articles using mwparserfromhell5. We then remove sections with the following
titles: â€œreferencesâ€,â€œexternallinksâ€,â€œsourcesâ€,â€œfurtherreadingâ€,â€œseealsoâ€,â€œcitationsâ€,andâ€œnoteâ€. In
the remaining sections, we remove Wikilinks and remove the following templates: â€œreï¬‚istâ€, â€œnotelistâ€,
â€œnotelist-uaâ€, â€œnotelist-lrâ€, â€œnotelist-urâ€, and â€œnotelist-lgâ€. We also exclude objects with the â€œrefâ€ or
â€œtableâ€ tag and clean the remaining text with the strip_code function. Finally, we concatenate the
title and all the sections and use \n\nto delimitate them.
B. Details on the retrieval architecture
Wegivedetailsonthe R/e.sc/t.sc/r.sc/o.scarchitecture,andontheï¬ne-tuningprocedureweusefor R/e.sc/t.sc/r.sc/o.scï¬tting
existing language models.
B.1. R/e.sc/t.sc/r.sc/o.scarchitecture and implementation
B.1.1. Feed-forward architecture
Asmentionedinthemaintext,theoverallencoder-decoderarchitectureisfullyfeed-forward. Westart
with a sequence ğ‘‹2ğ•ğ‘›=Â¹ğ¶ğ‘¢Âº16ğ‘¢6ğ‘™, and its pre-computed neighbours Â¹R/e.sc/t.scÂ¹ğ¶ğ‘¢ÂºÂº16ğ‘¢6ğ‘™and returns
logits in â„ğ‘›jğ•j. Along with A/t.sc/t.sc/n.sc,F/f.sc/w.sc,C/c.sc/a.scandC/a.scoperators introduced in the main text, we
deï¬ne the decoder embedding layer E/m.sc/b.sc :ğ•ğ‘›!â„ğ‘›ğ‘‘, the S/p.sc/l.sc/i.sc/t.scoperator that extracts chunked
intermediary embeddings S/p.sc/l.sc/i.sc/t.scÂ¹ğ»Âº,Â¹ğ»ğ‘¢Âº16ğ‘¢6ğ‘™2â„ğ‘™ğ‘šğ‘‘and the read-out layer R/e.sc/a.sc/d.sc :â„ğ‘›ğ‘‘!
â„ğ‘›jğ•j. We then describe the forward pass in Algorithm 1. In addition to the usual Transformer ones,
R/e.sc/t.sc/r.sc/o.scarchitecture hyperparameters involves the layer indices ğ‘ƒencandğ‘ƒ, at which the encoder and
the decoder perform cross-attention.
B.1.2. Relative positional encoding in the chunked cross-attention layer
TheC/a.scoperator uses relative positional logits, that are computed from a speciï¬c relative distance
separatingdatatokensfromretrievaltokens. Indeed,weexpectanyretrievalneighbour R/e.sc/t.scÂ¹ğ¶ğ‘¢Âºğ‘—and
the chunk ğ¶ğ‘¢to be relatively well aligned, and assume that they start at the same position. Therefore,
when computing C/a.scÂ¹ğ»Â¸
ğ‘¢Â”ğ¸ğ‘¢Âº, we set the distance between the data token ğ‘–2Â»1Â”ğ‘™Â¼of chunkğ¶Â¸
ğ‘¢and
5https://github.com/earwig/mwparserfromhell
25

Improving language models by retrieving from trillions of tokens
the retrieval token ğ‘–02Â»1Â”2ğ‘™Â¼ofR/e.sc/t.scÂ¹ğ¶ğ‘¢Âºğ‘—to be
ğ‘‘Â¹ğ‘–Â”ğ‘–0Âº,ğ‘– ğ‘–0Â¸ğ‘™ 1Â“ (6)
When computing the encoder cross-attentions C/a.scÂ¹R/e.sc/t.scÂ¹ğ¶ğ‘¢Âºğ‘—Â”ğ»ğ‘¢Âº, we set the distance between the
retrieval token ğ‘–02Â»1Â”2ğ‘™Â¼and the data token ğ‘–2Â»1Â”ğ‘™Â¼to be
ğ‘‘encÂ¹ğ‘–0Â”ğ‘–Âº,ğ‘–0 ğ‘–Â“ (7)
Positional logits are obtained as a linear transform of a cosine vector computed from Â¹ğ‘‘Â¹ğ‘–Â”ğ‘–0ÂºÂºğ‘–Â”ğ‘–0, and
are added to content logits, as in a regular self-attention block.
B.1.3. Chunked cross-attention implementation
Our implementation of the C/c.sc/a.scoperator, shown in Listing 1, is based on a vectorized application of
a cross-attention layer. For simplicity, we omit the multi-head attention logic and use the simplest
Q,K,V attention. We omit relative positional logits computation, described above.
B.1.4. Optional sharing of embedding matrices
Weusedisjointembeddingsfortheencoderanddecoderbydefault, whichallowsustouseadiï¬€erent
dimensionality for the encoder (typically kept at ğ‘‘E/n.sc/c.sc=896Âºand for the decoder (that we scale up
toğ‘‘=8192). It is possible to share the embeddings, with little diï¬€erence in training, as we show in
the ablation section.
B.2. Baseline to R/e.sc/t.sc/r.sc/o.scmodel ï¬ne-tuning
As shown in Fig. 5, we found that we were able to take a pre-trained baseline transformer and add
R/e.sc/t.sc/r.sc/o.scthroughï¬ne-tuning. Inallcases, wefrozeallweightsfrompre-trainingandfreshlyinitialised
the retrieval encoder and cross-attention weights. In all cases, the cross-attention is added every third
layer starting at layer six. The learning rate for the three smaller models was set to 210 4and
half that for the larger model. We experimented with allowing the entire model to resume training
during ï¬ne-tuning but consistently found that the best approach was to freeze the pre-trained model.
This kept the retrieval-oï¬€ performance frozen whereas when all weights were tuned the retrieval oï¬€
performance would degrade.
C. Training details and hyperparameters
We provide the hyperparameters used in the various experiments of Â§4.
C.1. Language model pre-training
In Table 10, we show the hyperparameters of the diï¬€erent models we train. In all cases, we train for
419,430,400,000 training tokens. The three smaller models are trained with a batch size of 256 and
the largest model is trained with a batch size of 1024. The minimum learning rate is set to 0.1 times
the maximum learning rate, which is shown in Table 10. The learning rate is decayed using a cosine
cycle length that matches the total number of training tokens. All models are trained using AdamW
(Loshchilov and Hutter, 2019) with a weight decay parameter of 0.1. The learning rate linearly
increases from 10 7to the maximum learning rate over the ï¬rst 750 steps of training. All models use
ZeRO to shard the optimiser state (Rajbhandari et al., 2020). Additional infrastructure details can be
found in Rae et al. (2021).
26

Improving language models by retrieving from trillions of tokens
Listing 1jJax implementation of the chunked cross attention , simpliï¬ed.
n = 128 # Sequence length
m = 16 # Chunk length
r = 32 # Retrieval length
k = 4 # Number of neighbours
d = 16 # Embedding size
l = n // m # Number of chunks
# Parameters
Q = jnp.zeros((d, d))
K = jnp.zeros((d, d))
V = jnp.zeros((d, d))
def relative_positional_encodings(attending_length, attended_length):
# Classical relative positional encodings
...
def cross_attention(chunk, neighbour):
m, d = chunk.shape
r, d = neighbour.shape
queries = chunk @ Q
keys = neighbour @ K
logits = queries @ keys.T
values = neighbour @ V
return logits, values
def multi_neighbour_cross_attention(chunk, neighbours):
m, d = chunk.shape
k, r, d = neighbours.shape
logits, values = jnp.vectorize(cross_attention,
signature=â€™(m,d),(r,d)->(m,r),(r,d)â€™)(
chunk, neighbours)
assert logits.shape == (k, m, r)
assert values.shape == (k, r, d)
logits += relative_positional_encodings(m, r)[None, :, :]
logits = jnp.moveaxis(logits, 0, -1).reshape((m, r *k))
values = jnp.moveaxis(values, 0, 1).reshape((r *k, d))
return jax.nn.softmax(logits) @ values
def multi_chunk_cross_attention(observation, neighbours):
attending_chunks = jnp.pad(observation[m-1:],
((0, m - 1), (0, 0)),
mode=â€™constantâ€™).reshape(l, m, d)
chunked_output = jnp.vectorize(multi_neighbour_cross_attention,
signature=â€™(m,d),(k,r,d)->(m,d)â€™)(
attending_chunks, neighbours)
assert chunked_output.shape == (l, m, d)
output = jnp.pad(chunked_output.reshape(n, d),
((m - 1, 0), (0, 0)),
mode=â€™constantâ€™)[:n]
return output
observation = jnp.zeros((n, d)) # Input
neighbours = jnp.zeros((l, k, r, d))
h = multi_chunk_cross_attention(observation, neighbours)
assert h.shape == (n, d) # Output
27

Improving language models by retrieving from trillions of tokens
Table 10jR/e.sc/t.sc/r.sc/o.scmodel hyperparameters , along with the size of the decoder.
Baseline ğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™ğ‘‘ğ‘“ğ‘“ğ‘¤ #heads Head size #layers ğ‘ƒ ğ‘ƒ E/n.sc/c.scMax LR
247M 896 3584 16 64 12 Â»6Â”9Â”12Â¼ Â» 1Â¼210 4
564M 1536 6144 12 128 12 Â»6Â”9Â”12Â¼ Â» 1Â¼210 4
1,574M 2048 8192 16 128 24 Â»9Â”12Â”Â“Â“Â“Â” 24Â¼ Â» 1Â¼210 4
7,505M 4096 16384 32 128 32 Â»9Â”12Â”Â“Â“Â“Â” 32Â¼ Â» 1Â¼110 4
Table 11jHyperparameters for the Wikitext103 experiments presented in Table 4. We use the same
learning rate schedule for the baseline and the R/e.sc/t.sc/r.sc/o.sc-ï¬tting. For R/e.sc/t.sc/r.sc/o.sc-ï¬tting, we reset the
schedule i.e. the schedule starts from step 0, not from step 35,000.
Model Number of layers 18
ğ‘‘ 1024
ğ‘‘F/f.sc/w.sc 4096
Key size 64
Value size 64
Number of heads 16
Training data Dataset Wikitext103train
Sequence length 3072
Batch size 128
Tokenizer vocabulary size 128,000
Optimisation optimiser Adam
Adamâ€™sğ›½1 0.9
Adamâ€™sğ›½2 0.95
Adamâ€™sğœ€ 1e-8
Dropout rate 0.25
Schedule Learning rate start 1e-7
Learning rate max 2.5e-4
Learning rate min 2e-5
Warmup steps 4,000
Cosine cycle steps 100,000
Evaluation Overlapping proportion 87.5 %
C.2. Wikitext103 comparison
WeprovidemoredetailsonourWikitext103resultspresentedinÂ§4.1andTable4. Wetrainabaseline
transformer on the Wikitext103 training set with the hyperparameters presented in Table 11. The
learning rate ramps linearly from 110 7to2Â“510 4in the ï¬rst 4,000 steps, then decays to
210 5at 100,000 steps using a cosine schedule. The baseline checkpoint at step 35,000 has the
lowest perplexity on Wikitext103 valid, of 21Â“58, for overlapping proportion of 75% (sliding window
evaluation that only uses probabilities for tokens that have at least 75% of the sequence length of
context, when available). We use this checkpoint for all our baseline and ğ‘˜NN-LMnumbers reported
in Table 4, except that Table 4 reports for an overlapping proportion of 87.5 %, which slightly lowers
the perplexity of our baseline to 21.53 on Wikitext103 valid.
We also use the 35,000 step baseline checkpoint as initialization for a R/e.sc/t.sc/r.sc/o.scï¬t, which otherwise
uses the same optimiser and schedule hyperparameters but only trains the new retrieval weights, as
explained in Â§4.2. Our best R/e.sc/t.sc/r.sc/o.scï¬t checkpoint has a Wikitext103 valid perplexity 18Â“46, when
retrieving from Wikipedia. We use this R/e.sc/t.sc/r.sc/o.sccheckpoint in Table 4 for all other retrieval sets. The
evaluation curves for our baseline and R/e.sc/t.sc/r.sc/o.scï¬t is shown if Fig. 7 (left). In this particular case,
28

Improving language models by retrieving from trillions of tokens
because Wikitext103 is quite small, training a R/e.sc/t.sc/r.sc/o.scmodel from scratch led to weaker results than
the baseline, at least when retrieving from Wikipedia, as we couldnâ€™t ï¬nd an eï¬€ective way to mitigate
the increased over-ï¬tting due to the additional weights of R/e.sc/t.sc/r.sc/o.sc.
We also re-implement ğ‘˜NN-LMusing the same tokenizer and dataset that we use for our base-
line and R/e.sc/t.sc/r.sc/o.scï¬tting experiments. ğ‘˜NN-LMhas probabilities ğ‘ğ‘˜NN-LM =ğœ†ğ‘ğ¿ğ‘€Â¸Â¹1 ğœ†Âºğ‘ğ‘˜ğ‘ğ‘with
ğ‘ğ‘˜ğ‘ğ‘Â¹ğ‘›ğ‘˜Âº/expÂ¹ ğ›¼ğ‘‘ğ‘˜Âº. To tuneğœ†andğ›¼, we begin with ğ›¼=0Â“0012, which corresponds to the inverse
of the standard deviation of the norm of the embeddings that we use as keys and queries for ğ‘˜NN-LM.
We ï¬nd the best ğœ†=0Â“118. We then ï¬nd the best ğ›¼=0Â“00785for that value of ğœ†. Fig. 7 center and
right respectively show the perplexity of ğ‘˜NN-LM as a function of ğœ†andğ›¼.
0 20 40 60 80
1,000 steps18202224Wikitext103Valid perplexity 104
103
102
101
alpha18202224
0.0 0.2 0.4
lambda18202224
Baseline RETROfit kNN-LM
Figure 7jWikitext103valid perplexities. Left:Baseline and R/e.sc/t.sc/r.sc/o.scï¬t (initialized from baselineâ€™s
checkpoint at 35,000 steps) perplexities as a function of training steps. Center and right: ğ‘˜NN-LM
perplexity as a function of ğœ†(forğ›¼=0Â“0012) andğ›¼(forğœ†=0Â“12) respectively.
C.3. R/e.sc/t.sc/r.sc/o.scï¬tting baseline models experiments
In Table 12, we give the hyperparameters used for R/e.sc/t.sc/r.sc/o.scï¬tting the models on Massive Text.
Table 12jHyperparameters for the R/e.sc/t.sc/r.sc/o.scï¬tting experiments
Model Layers with R/e.sc/t.sc/r.sc/o.sc-block (ğ‘ƒ) Learning rate Batch size
172M Every 3rdfrom 6 210 4!210 5256
425M Every 3rdfrom 6 210 4!210 5256
1.5B Every 3rdfrom 6 210 4!210 5256
7.5B Every 3rdfrom 6 110 4!110 5256
C.4. Question answering experiments
We ï¬ne-tune our 7.5B R/e.sc/t.sc/r.sc/o.scmodel for 25,000 steps, using a batch size of 128, a learning rate
cosine scheduled from 10 6to10 7, with a linear ramp of 750 steps. We use dropout in the decoder
only, as it performs better than using dropout in both the encoder and the decoder. Each neighbour
is formatted as title: {title}, source: {source} . We use the top 20 neighbours from
D/p.sc/r.scwhen training and evaluating.
29

Improving language models by retrieving from trillions of tokens
Table 13jPerformance of R/e.sc/t.sc/r.sc/o.scfor diï¬€erent variants. Model performance on C4 evaluation set,
measured in bytes-per-bits, for a 247M parameter model trained with a 157 billion token schedule.
Ablation group Ablation C4 eval bpb
Model R/e.sc/t.sc/r.sc/o.sc 0.822
No query conditioning 0.829
No CA positional encodings 0.826
Shared embeddings 0.823
6-layer encoder 0.821
Retrieval values Neighbours N 0.950
Continuations F 0.895
No retrieval 0.987
Training neighbours 1 training neighbours 0.858
4 training neighbours 0.847
Cross attention position CA top layer (1/12) 0.827
CA mid layer (6/12) 0.823
CA top layer (12/12) 0.831
CA all layers 0.860
CA every 3 from 1 0.823
D. Model ablations
We validate important design choices by evaluating what happens when we do not include them. We
use the 247M parameter model for all experiments and we train on a compressed 157 billion token
schedule for all ablation experiments. We describe results relative to the default settings presented in
the main text and recalled here. We report C4 evaluation loss at the end of the training process, and
also compares how the evaluation loss decrease versus the training time, measured relatively to the
baseline training time. Results are reported in Fig. 8 and Table 13.
Using relative encodings in cross-attention. Using relative encodings in cross-attention, as de-
scribed in Â§B.1.2, provides a pure improvement both in the number of steps to reach a given perfor-
mance and computational eï¬ƒciency.
Conditioning the encoder on the previous chunk. Conditioning the encoder on the previous
chunkâ€™s intermediate embeddings, as described in Â§B.1.1, provides a pure improvement both in term
of number of steps and computational eï¬ƒciency.
Sharing embeddings. Sharing embeddings across the encoder and the decoder does not aï¬€ect
performance. This motivates us using separate embeddings, as it allows to have a narrower encoder
than decoder as we scale up the decoder size.
Attending neighbours and their continuation. R/e.sc/t.sc/r.sc/o.scmodels are trained by attending, for a
given chunk, to both the neighbours of the preceding chunk and their continuation in time. We
measure how training and evaluating R/e.sc/t.sc/r.sc/o.scmodels on neighbours only and their continuation
only aï¬€ects performance. Overall, attending to neighbours only provides 22%of the performance
improvement due to retrieval in R/e.sc/t.sc/r.sc/o.sc, while attending the future of the neighbours gives 56%of
30

Improving language models by retrieving from trillions of tokens
0.820.840.860.88C4 eval bits-per-bytesRETRO
No CA positional encodings
0.820.840.860.88
RETRO
No query conditioning
0.820.840.860.88
RETRO: distinct embeddings
Shared embeddings
0.0 0.2 0.4 0.6 0.8 1.0 1.2
Training time (relative to baseline)0.820.840.860.88C4 eval bits-per-bytesRETRO: 2 layer encoder
6 layer encoder
0.820.840.860.88
RETRO: 2 training nei.
1 training nei.
4 training nei.
0.0 0.2 0.4 0.6 0.8 1.0 1.2
Training time (relative to baseline)0.80.91.01.11.2RETRO: retrieve [N,F]
Neighbours N
Continuations F
No retrieval
0.0 0.2 0.4 0.6 0.8 1.0 1.2
Training time (relative to baseline)0.8200.8250.8300.8350.840
RETRO: CA every 3 from 6
CA top layer (1/12)
CA mid layer (6/12)
CA top layer (12/12)
CA all layers
CA every 3 from 1
Figure 8jComputational eï¬ƒciency for diï¬€erent variants. We report the training curves plotting
C4 evaluation bytes per bits against time, relative to the time taken to train the baseline R/e.sc/t.sc/r.sc/o.sc
model. Overall, our design choices are optimal in term of computational eï¬ƒciency.
the performance. Attending to both neighbours and their continuation is the most eï¬ƒcient choice
both in term of ï¬nal performance and training eï¬ƒciency.
Training a deeper encoder. All models in the text use a relatively small R/e.sc/t.sc/r.sc/o.scencoder. We
experimented with a 3deeper encoder. We found that this resulted in a tiny decrease in lossâ€“ 0.15%
at the cost of a larger training time ( Â¸20%). Overall, using a shallow encoder is the best choice in
term of training eï¬ƒciency.
Training with multiple neighbours. We measure the eï¬€ect of training on a single retrieved neigh-
bour, as well as training on 4 neighbours ( R/e.sc/t.sc/r.sc/o.scuses 2 neighbours in training). Training on a
single neighbour results in a large decrease in performance, while training on 4 neighbours does not
give substantial performance improvement at the end of training, but induces a large computational
overhead. Overall, we ï¬nd that using 2 neighbours is the best choice in term of training eï¬ƒciency.
Furthermore, evaluation can be done with additional neighbours.
Frequency of cross-attention. We measure how the frequency of cross-attention in the decoder
aï¬€ects performance. Overall, attending only once at the top or the bottom layer is a bad choice, while
attending once on a mid-depth layer is relatively sound. We choose to have cross-attention every 3
layer as this provides a good trade-oï¬€ between performance and run-time.
31

Improving language models by retrieving from trillions of tokens
E. Qualitative experiments
We illustrate the usage of R/e.sc/t.sc/r.sc/o.scmodels by looking at the perplexity of evaluation samples and by
producing samples autoregressively.
E.1. Inspecting neighbours and perplexities on evaluation data
To build an intuition of what kind of information is leveraged by R/e.sc/t.sc/r.sc/o.scmodels, we suggest to
have a closer look at a few evaluation documents and the corresponding retrieved data in Tables
16, 17, 18 and 19. In these tables, the 4 rows corresponds to the ï¬rst 4 chunks of the documents.
The left-most column shows the chunk ğ¶ğ‘¢from the document being evaluated, where each token is
colouredbythenegativecrossentropylossdiï¬€erence ğ¿R/e.sc/t.sc/r.sc/o.sc[O/f.sc/f.sc] ğ¿R/e.sc/t.sc/r.sc/o.sc,apositivevalue,coloured
in yellow, indicates that R/e.sc/t.sc/r.sc/o.scperforms better when it has access to neighbours data. The second
columns also shows the evaluated chunk ğ¶ğ‘¢but where each token ğ‘–is coloured by the length of the
longest common preï¬x (LCP) with the preceding neighbours, i.e. the largest integer ğ‘—such that
the preï¬xÂ¹ğ‘¥ğ‘– ğ‘— 1Â”Â“Â“Â“Â”ğ‘¥ğ‘–Âºalso appears in R/e.sc/t.scÂ¹ğ¶ğ‘¢ 1Âº. Conversely, columns three and four show the
ï¬rst two neighbours and their continuation, respectively Â»ğ‘1
ğ‘¢Â”ğ¹1
ğ‘¢Â¼andÂ»ğ‘2
ğ‘¢Â”ğ¹2
ğ‘¢Â¼coloured by LCP with
subsequent chunk ğ¶ğ‘¢Â¸1. LCP colouring helps to visually identify where the evaluated document
overlaps the retrieved data. Note that the ï¬rst chunk, ğ¶1, in the second column is not coloured as
it does not have any preceding neighbours to compute LCP with. Similarly, we do not show the
neighbours of the fourth chunk, as these are not used to condition any of the ï¬rst four chunks.
Our qualitative analysis exhibits two major behaviors.
Firstly, we observe that sometimes, speciï¬c facts in ğ¶ğ‘¢can be extracted from the preceding
neighbours R/e.sc/t.scÂ¹ğ¶ğ‘¢ 1Âºand that this can correspond to signiï¬cant reduction in loss from the R/e.sc/t.sc/r.sc/o.sc
model for the corresponding tokens. Some examples of such behavior include the journal name
Publishers Weekly in Table 16, the football team name Tyronein Table 17 or the event dates 25 August
to 6 September 2020 in Table 18. In these three examples, the evaluated data consists of recent
Wikipedia articles written in September 2021, after we built our retrieval dataset (see section Â§A.2).
Yet, relevant information to predict this new data was available in the pre-existing retrieval data and
theR/e.sc/t.sc/r.sc/o.scmodel seems to be able to correctly leverage it.
On the other hand, we also observe that some of the evaluation data can partially leak in our
training and retrieval data, despite the use of deduplication. R/e.sc/t.sc/r.sc/o.sccan dramatically exploit such
leakage. Table 19 illustrates this behavior, where the chunks ğ¶2andğ¶3largely overlaps R/e.sc/t.scÂ¹ğ¶1Âºand
R/e.sc/t.scÂ¹ğ¶2Âºrespectively, up to small formatting diï¬€erences, which leads to much lower R/e.sc/t.sc/r.sc/o.scloss for
all the corresponding tokens. Fig. 6 shows that it is possible to quantify how much of the R/e.sc/t.sc/r.sc/o.scloss
reduction is due to each of these two behaviors, by ï¬ltering out evaluation chunks that overlaps with
the retrieval set.
E.2. Inspecting samples
We can follow the same procedure as above on samples generated using R/e.sc/t.sc/r.sc/o.scmodels, in order to
better understand where retrieval data had an inï¬‚uence on sampling. We show examples of samples
obtained using the 7.5B R/e.sc/t.sc/r.sc/o.scmodel in Table 6, 7, 20 and 21.
E.3. Neighbour quantiï¬cation
To quantify a notion of distance between the source document and the retrieved chunks, we can ask
the distance between source articles when retrieving only from Wikipedia. Consonni et al. (2019)
32

Improving language models by retrieving from trillions of tokens
Figure9jWikipedialink-distancebetweenretrievedarticles. Foreachsequences,chunkcombina-
tion we compute the link distance between the target and the top-5 neighbours using only Wikipedia.
The rank shows the relative neighbour distance, where rank-1 is the ï¬rst neighbour and rank 5 is
the ï¬fth. The diï¬€erent colours represent link distance. Because we do not retrieve from the same
document, 1 is the smallest value. We ï¬nd, on average, the distance between random articles with a
path between them is over 5.0
provides a Wikipedia link dataset which, for each article, contains a list of neighbouring articles.
Using this, we construct a directed graph and compute the distance from one page to another. In
Fig. 9 we compute the link-distance between training sequences and the retrieved neighbours. We
ï¬nd that retrieved documents tend to be from articles that are quite close to the article containing
the target. Furthermore, we ï¬nd that on average the distance increases with rank, suggesting that
our neighbours are both useful and that the order is reasonable. This provides conï¬dence for our
larger-scale experiments where document distance is less well deï¬ned.
F. Complementary quantitative results
We report tables corresponding to quantitative ï¬gures of the main text, as well as further ï¬ltered
language model results on the Pile.
F.1. Main text datasets
We report the performance of R/e.sc/t.sc/r.sc/o.scand baseline models, measured in bits-per-bytes on evaluation
set, in Table 14.
F.2. The Pile
In Fig. 4, we compare R/e.sc/t.sc/r.sc/o.scagainst Jurassic-1 (Lieber et al., 2021). The full bits-per-bytes results
are reported in Table 15.
F.3. Filtered results
Distribution of leaked chunks in our main evaluation sets. We evaluate leakage between the
evaluation sets and the training set by measuring the proportion of evaluation chunks with a certain
33

Improving language models by retrieving from trillions of tokens
Table 14jFull results for the main language modelling datasets. First three sets of rows correspond
to Fig. 1, last set of rows to Fig. 3.
Baseline R/e.sc/t.sc/r.sc/o.sc[Oï¬€] R/e.sc/t.sc/r.sc/o.sc[On]
172M 425M 1.5B 7.5B 172M 425M 1.5B 7.5B 172M 425M 1.5B 7.5B
C4 Eval bpb 0.98 0.92 0.84 0.78 0.98 0.92 0.84 0.78 0.82 0.77 0.71 0.66
C4 Eval bpb (900B) - - - - - - - - 0.88 0.83 0.76 0.71
C4 Eval bpb (360B) - - - - - - - - 0.92 0.87 0.80 0.74
C4 Eval bpb (180B) - - - - - - - - 0.94 0.89 0.81 0.75
C4 Eval bpb (90B) - - - - - - - - 0.95 0.89 0.82 0.76
C4 Eval bpb (36B) - - - - - - - - 0.96 0.90 0.83 0.77
C4 Eval bpb (18B) - - - - - - - - 0.96 0.91 0.83 0.77
C4 Eval bpb (9B) - - - - - - - - 0.96 0.91 0.83 0.77
C4 Eval bpb (4B) - - - - - - - - 0.97 0.91 0.84 0.78
C4 Eval bpb (2B) - - - - - - - - 0.97 0.91 0.84 0.78
C4 Eval bpb ( ğ‘˜=1) - - - - - - - - 0.84 0.79 0.73 0.67
C4 Eval bpb ( ğ‘˜=2) - - - - - - - - 0.83 0.78 0.72 0.67
C4 Eval bpb ( ğ‘˜=3) - - - - - - - - 0.82 0.78 0.71 0.66
C4 Eval bpb ( ğ‘˜=4) - - - - - - - - 0.82 0.77 0.71 0.66
C4 Eval bpb ( ğ‘˜=5) - - - - - - - - 0.82 0.77 0.71 0.66
C4 Eval bpb ( ğ‘˜=10) - - - - - - - - 0.82 0.77 0.71 0.66
C4 Eval bpb ( ğ‘˜=20) - - - - - - - - 0.82 0.77 0.71 0.66
C4 Eval bpb ( ğ‘˜=30) - - - - - - - - 0.82 0.77 0.71 0.65
C4 Eval bpb ( ğ‘˜=40) - - - - - - - - 0.83 0.77 0.71 0.65
C4 Eval bpb ( ğ‘˜=50) - - - - - - - - 0.83 0.78 0.71 0.66
C4 Eval bpb ( ğ‘˜=60) - - - - - - - - 0.84 0.78 0.72 0.66
C4 Eval bpb ( ğ‘˜=70) - - - - - - - - 0.84 0.79 0.72 0.66
C4 Eval bpb ( ğ‘˜=80) - - - - - - - - 0.85 0.79 0.73 0.66
C4 Eval bpb ( ğ‘˜=90) - - - - - - - - 0.85 0.79 0.73 0.66
C4 Eval bpb ( ğ‘˜=100) - - - - - - - - 0.85 0.79 - 0.67
Lambada Accuracy 0.42 0.51 0.61 0.69 0.47 0.54 0.63 0.70 0.52 0.60 0.67 0.73
Curation Corpus bpb 0.69 0.63 0.56 0.52 0.68 0.64 0.57 0.51 0.66 0.61 0.55 0.50
Wikitext103 Perplexity 25.62 19.29 13.98 10.65 25.88 19.78 13.89 10.40 3.32 2.96 2.53 2.22
Wikipedia Sept. 2021 bpb 0.85 0.78 0.71 0.65 0.86 0.79 0.71 0.65 0.79 0.73 0.66 0.61
overlapğ‘ŸÂ¹ğ¶Âº. We show histograms in Fig. 10. We can see that ğ¶4has some slight overlaps between
train and evaluation. Similarly, chunks of Wikitext103 appear in the training set despite having
removed the actual Wikitext103 evaluation documents from the training set. On the other hand, our
Wikipedia September 21 dataset shows almost no leakage (data being original documents that did
not exist at training data creation), and neither does Curation Corpus.
Filtered results on the Pile. We report chunk overlap distribution and ï¬ltered performance curves
on the Pile in Fig. 12 and Fig. 11, respectively. The qualitative interpretation of the ï¬ltered curves
is the same: R/e.sc/t.sc/r.sc/o.scmodels exploit leakage more, but the performance improvement they provide
remains signiï¬cant even on original chunks that havenâ€™t been observed in the training set.
34

Improving language models by retrieving from trillions of tokens
Table 15jFull results on The Pile, measured in bits-per-bytes. Jurassic-1 and GPT-3 numbers are
taken from Lieber et al. (2021). Gopher numbers are taken from Rae et al. (2021).
Subset 7B Baseline (Ours) GPT-3 Jurassic-1 Gopher 7.5B R/e.sc/t.sc/r.sc/o.sc
arxiv 0.742 0.838 0.680 0.641 0.714
books3 0.792 0.802 0.835 0.706 0.653
dm_mathematics 1.177 1.371 1.037 1.135 1.164
freelaw 0.576 0.612 0.514 0.506 0.499
github 0.420 0.645 0.358 0.367 0.199
gutenberg_pg_19 0.803 1.163 0.890 0.652 0.400
hackernews 0.971 0.975 0.869 0.888 0.860
nih_exporter 0.650 0.612 0.590 0.590 0.635
opensubtitles 0.974 0.932 0.879 0.894 0.930
philpapers 0.760 0.723 0.742 0.682 0.699
pile_cc 0.771 0.698 0.669 0.688 0.626
pubmed_abstracts 0.639 0.625 0.587 0.578 0.542
pubmed_central 0.588 0.690 0.579 0.512 0.419
stackexchange 0.714 0.773 0.655 0.638 0.624
ubuntu_irc 1.200 0.946 0.857 1.081 1.178
uspto_backgrounds 0.603 0.566 0.537 0.545 0.583
0% 50% 100%
Eval/train chunk overlapChunk densityC4
0% 50% 100%
Eval/train chunk overlapCuration Corpus
0% 50% 100%
Eval/train chunk overlapWikitext103
0% 50% 100%
Eval/train chunk overlapWikipedia Sept 2021
Figure 10jDistribution of the overlap between evaluation and train chunks for C4, Curation
Corpus, Wikitext103 and Wikipedia Sept. 2021.
35

Improving language models by retrieving from trillions of tokens
0.40.50.60.70.80.91.0Eval bpbarxiv172M 425M 1.5B 7.5B Baseline RETRO [ON]
0.40.50.60.70.80.91.0bookcorpus2
0.30.40.50.60.70.80.91.0books3
0.91.01.11.21.31.4dm_mathematics
0.60.81.01.21.4Eval bpbeuroparl
0.40.50.60.70.8freelaw
0.20.40.60.81.0github
0.20.40.60.81.0gutenberg_pg_19
0.70.80.91.01.11.2Eval bpbhackernews
0.650.700.750.80nih_exporter
0.60.70.80.91.01.11.2opensubtitles
0.40.50.60.70.80.91.0openwebtext2
0.40.50.60.70.80.91.0Eval bpbphilpapers
0.50.60.70.80.91.0pile_cc
0.550.600.650.700.750.800.85pubmed_abstracts
12.5% 50% 100%
Max allowed eval/train overlap0.30.40.50.60.70.8pubmed_central
12.5% 50% 100%
Max allowed eval/train overlap0.60.70.80.91.01.1Eval bpbstackexchange
12.5% 50% 100%
Max allowed eval/train overlap0.60.81.01.21.41.6ubuntu_irc
12.5% 50% 100%
Max allowed eval/train overlap0.550.600.650.700.75uspto_backgrounds
Figure 11jFiltered evaluation losses on the Pile , with baseline Transformers and R/e.sc/t.sc/r.sc/o.sc.
36

Improving language models by retrieving from trillions of tokens
Chunk densityarxiv bookcorpus2 books3 dm_mathematicsChunk densityeuroparl freelaw github gutenberg_pg_19Chunk densityhackernews nih_exporter opensubtitles openwebtext2Chunk densityphilpapers pile_cc pubmed_abstracts pubmed_central
0% 50% 100%
Eval/train chunk overlapChunk densitystackexchange
0% 50% 100%
Eval/train chunk overlapubuntu_irc
0% 50% 100%
Eval/train chunk overlapuspto_backgrounds
Figure12jDistributionoftheoverlapbetweenevaluationandtrainchunks forthePileevaluation
sets.
37

Improving language models by retrieving from trillions of tokens
Table 16jGreat Circle (novel) , from Wikipedia September 21. The article is about a recent novel and chunks
ğ¶3andğ¶4are speciï¬cally about its reception. The name Publishers Weekly of the journal that reviewed the
novel appears both in the neighbours Â»ğ‘1
3Â”ğ¹1
3Â¼Â”Â»ğ‘2
3Â”ğ¹2
3Â¼of chunkğ¶3and in the subsequent chunk ğ¶4, where the
loss for those tokens is signiï¬cantly reduced by R/e.sc/t.sc/r.sc/o.sc.
ğ¶ğ‘¢colored by loss diï¬€erence ğ¶ğ‘¢colored by LCP with R/e.sc/t.scÂ¹ğ¶ğ‘¢ 1Âº Â»ğ‘1ğ‘¢Â”ğ¹1ğ‘¢Â¼colored by LCP with ğ¶ğ‘¢Â¸1Â»ğ‘2ğ‘¢Â”ğ¹2ğ‘¢Â¼colored by LCP with ğ¶ğ‘¢Â¸1
ğ¿R/e.sc/t.sc/r.sc/o.sc[O/f.sc/f.sc] ğ¿R/e.sc/t.sc/r.sc/o.sc 6 0Â“5Â”=0Â”>0Â“5LCP=0,1,2,3,4,>5 LCP=0,1,2,3,4,>5 LCP=0,1,2,3,4,>5
Great Circle (novel)Great Circle i Great Circle (novel) Great Circle i The Dutch House (novel)The Dutch H The Dutch House (novel)The Dutch H
s a 2021 novel by Maggie Shipstead, s a 2021 novel by Maggie Shipstead, ouseis a2019 novel by Ann Patchett ouseis a2019 novel by Ann Patchett
published on May 4, 2021, by Alfred published on May 4, 2021, by Alfred .It was published by Harper on Sept .It was published by Harper on Sept
A. Knopf.The novel has been shortl A. Knopf. The novel has been shortl ember 24, 2019. It tells thestory o ember 24, 2019. It tells thestory o
isted for the 2021 Booker Prize.Sy isted for the 2021 Booker Prize. Sy fabrother and sister over thecour fabrother and sister over thecour
nopsis The novel consists of two pa nopsis The novel consists of two pa se of ï¬ve decades .The novel was a se of ï¬ve decades.[2]The novel wa
rallel narratives about two ï¬ctiona rallel narratives about two ï¬ctiona ï¬nalist for the2020 Pulitzer Priz saï¬nalist for the2020 Pulitzer P
l women. One is l women. One is e for Fiction .PlotThe Dutch House rize for Fiction.[3]Plot[edit]Th
is amansion located inElkins Park e Dutch House is amansion located i
,Pennsylvania , asuburb of Philadel nElkins Park ,Pennsylvania , asubur
phia.It was built in1922 bytheVa b of Philadelphia .It was built in1
nHoebeek family , ahusband and wife 922 bytheVanHoebeek family , ahusb
originally from theNetherlands who and and wife originally from theNet
made their fortune inthetobacco in herlands whomade their fortune int
dustry.Cyril Conroy , aself-made re hetobacco industry .Cyril Conroy , a
al estate mogul self-
aboutthe disappeared 20th-century aboutthedisappeared 20th -century onbecoming aï¬lmmaker .She has fo basedcloselyonher own youthful e
aviatorMarianGraves,while the oth aviator Marian Graves ,whiletheoth undasubjectforher ï¬lm project , xperiences .(She plans theï¬lm to b
erisaboutthestruggling 21st-cent erisaboutthestruggling 21st -cent an obscure African American actress etheï¬rst of two parts , thesecond
uryHollywood actressHadleyBaxter, ury Hollywood actress Hadley Baxter ,credited only as â€œthe watermelon wom dealingwith theaftermath of thef
who isattempting tomakeaï¬lmab who isattempting to make aï¬lm ab anâ€ in old Hollywood ï¬lms ,andthe irstâ€™s events.) Byrne plays ayoung
outMarian.Hadleyâ€™snarrative isto out Marian .Hadleyâ€™s narrative isto subsequent ï¬lm recounts her search ï¬lm student named Julie (Hoggâ€™s ava
ldinthe ï¬rst-person,whileMarian ldintheï¬rst-person,while Marian forthis woman even as it covers ,in tar), who starts her artistic educat
â€™ssectionsaretoldin thethird-pe â€™s sections are told inthethird-pe themanner of theearlier Dunyement ionwithhigh hopes of making amovi
rson rson aries,Dunyeâ€™s friendships and her l e aboutaboy named Tony ,living in
ove life.InThe Watermelon Woman ,Dworking-class Sunderland ,who adores
unye makes theï¬lm she set out to m his mother â€” â€œis almost obsessed wi
ake in 1990 about African American w thher,â€ as eager Julie tells her ad
omen artists , aï¬lm that both inven visers.Her idealism is evident from
ts an artistic predecessor withwhom thestart.The advisers are skepti
she can identify and also â€œï¬ndsâ€ C cal,and no wonder; Julieâ€™s family i
heryl herself as theartist that she s posh,withacomfortable country e
seeks.As Dunye identiï¬es herself state and
.ReceptionGreatCirclereceived .Reception Great Circle received ï¬rst edition hardcover Reception The book also debuted at number tw
veryfavorable reviews,withacumul very favorable reviews ,withacumul Thenoveldebuted at number one on T o on The New York Times Hardcover No
ative"Rave"ratingatthereviewag ative "Rave" rating at thereview ag he New York Times ï¬ction best-selle nï¬ction best- sellers list on July 2
gregatorwebsiteBookMarks,basedo gregator website Book Marks , based o r list.As oftheweek ending Februa 8, 2019.[5] It spent eleven weeks on
n22bookreviewsfrommainstream li n22 book reviews frommainstream li ry 20, 2021, thenovelhas spent 38 thelist.[6]Reception[edit] Att
terarycritics.Thenoveldebutedat terary critics .The novel debuted at weeks on thelist.Atthe review ag he review aggregator website Book Ma
numberfourteen onTheNewYorkTim number fourteen onThe New York Tim gregator website Book Marks ,which a rks,which assign sindividual rating
esHardcoverï¬ctionbest-sellerlis es Hardcover ï¬ction best -seller lis ssignsindividual ratings tobook re stobook reviews from mainstream li
tfortheweekendingMay tfor theweek ending May views from mainstream literary criti terary critics , thebook received a
cs, thenovelreceived a cumulative cumulative "Positive" rating based o
"Rave" rating based on 38 reviews ,wn 29 reviews: 12 "Rave" reviews ,6"
ith only one "mixed"review.Publish Positive" reviews ,9"Mixed" reviews
ersWeeklywrote,"Bennett renders h , and2"Pan" reviews.[7] Publisher
er characters andtheir struggles wi sWeeklygavethebook a mixed revie
th great compassion , andexplores thw,writing,"Unfortunately ,all thre
ecomplicated state of mind that Ste e
lla ï¬nds herself in while passing a
s white." In its
8,2021.Criticspraisedthenovel 8, 2021. Critics praised thenovel
forsustaining itslengthandforSh for sustaining itslengthandfor Sh
ipsteadâ€™sresearch andintricatenov ipsteadâ€™sresearch andintricatenov
elstructure forperfectly interweav elstructure for perfectly interweav
ingtheparallelnarratives ,despite ingtheparallel narratives ,despite
thetimeandcircumstances separati thetimeandcircumstances separati
ngthem.Inits starred review,Pub ng them.Initsstarredreview,Pub
lishersWeeklywrote,"Shipsteadman lishersWeeklywrote,"Shipstead man
agestoportray both Marianâ€™sand Ha agestoportray both Marianâ€™ s andHa
dleyâ€™s dleyâ€™s
38

Improving language models by retrieving from trillions of tokens
Table 17jAll-Ireland Senior Football Championship Final , from Wikipedia September 21. The name of
the team Tyroneappears both in the second neighbours Â»ğ‘2
1Â”ğ¹2
1Â¼of chunkğ¶1and in the subsequent chunk ğ¶2,
where the loss for those tokens is signiï¬cantly reduced by R/e.sc/t.sc/r.sc/o.sc.
ğ¶ğ‘¢colored by loss diï¬€erence ğ¶ğ‘¢colored by LCP with R/e.sc/t.scÂ¹ğ¶ğ‘¢ 1Âº Â»ğ‘1ğ‘¢Â”ğ¹1ğ‘¢Â¼colored by LCP with ğ¶ğ‘¢Â¸1Â»ğ‘2ğ‘¢Â”ğ¹2ğ‘¢Â¼colored by LCP with ğ¶ğ‘¢Â¸1
ğ¿R/e.sc/t.sc/r.sc/o.sc[O/f.sc/f.sc] ğ¿R/e.sc/t.sc/r.sc/o.sc 6 0Â“5Â”=0Â”>0Â“5LCP=0,1,2,3,4,>5 LCP=0,1,2,3,4,>5 LCP=0,1,2,3,4,>5
2021 All-Ireland Senior Football Cha 2021 All-Ireland Senior Football Cha 2018 All-Ireland Senior Football Cha 2018 All-Ireland Senior Football Cha
mpionship FinalThe 2021 All-Irelan mpionship Final The 2021 All-Irelan mpionship FinalThe 2018 All -Irelan mpionship FinalThe 2018 All -Irelan
d Senior Football Championship Final d Senior Football Championship Final d Senior Football Championship Final d Senior Football Championship Final
was the 134th ï¬nal of the All-Irel was the 134th ï¬nal of the All-Irel was the131stï¬naloftheAll-Irel was the131stï¬naloftheAll-Irel
and Senior Football Championship and and Senior Football Championship and and Senior Football Championship and and Senior Football Championship and
the culmination of the 2021 All-Ire the culmination of the 2021 All-Ire theculmination of the2018 All-Ire theculmination of the2018 All-Ire
land Senior Football Championship. T land Senior Football Championship. T land Senior Football Championship inland Senior Football Championship in
he match was played at Croke Park in he match was played at Croke Park in Gaelic football .The match wasplay Gaelic football .The match wasplay
Dublin on 11 September 2021. It was Dublin on 11 September 2021. It was ed at Croke Park inDublinon 2Sept ed at Croke Park inDublinon 2Sept
originally scheduled originally scheduled ember 2018.[3]It was thesecond ti ember 2018.It was thesecond time
metheteamshadmetin the ï¬nal ; D theteamshadmetin the ï¬nal ; Dubl
ublin won the ï¬rstencounter in199 in wonthe ï¬rstencounter in1995.
5.Theï¬nal was shown live inIrel Itwas thethird consecutive year th
andonRTÃ‰ Two as part of The Sunday atateam qualiï¬ed under thesystem
Game live programme ,presented byMof second chances introduced in200
ichael Lyster from Croke Park ,with 1;Tyronequaliï¬ed despite defeat i
studio analysis from Joe Brolly , nits provincial championship .Dubl
in wonthe ï¬nal by a margin of six
points
for28Augustbuthadtobe postpon for 28 August but hadto be postpon game 23â€“23 after extra time ,howeve witha last-ditch plan of action â€“
edbytwoweekswhentheâ€“semi-ï¬na edbytwo weeks when theâ€“ semi-ï¬na r Ulster progressed under the compet play the Munster/Ulster Semi -Finalo
lwaspostponed dueto aCOVID-19ou lwaspostponed due to aCOVID-19 ou ition rules as theyscored three tir nMarch 16 th,withthe winners topl
tbreak.Ulsterchampions Tyronetook tbreak.Ulster champions Tyronetook esinthe match against Leinsterâ€™s t ay Connacht inthe following dayâ€™s F
onConnachtchampions Mayo,inwhat onConnacht champions Mayo , inwhat wo.The semi -ï¬nals took place inmi inal.On March 16 th thenMunsterha
wastheirï¬rstevermeetinginaf wastheirï¬rstever meeting in a f d November andsaw both the away tea d aneasywinover Ulster (9-07 to0
inal,winningtheir4thtitleafter inal,winning their 4th title after mswin,as Ulster beat Glasgow andE-00) but thankfully for the Munster
a2â€“14to0â€“15win.Mayolost a 2â€“14 to 0â€“15 win .Mayo lost dinburgh beat Connacht .Theï¬nalwa players,the pitch cut up so badly d
s heldonSaturday December 20 at Mu uring the game ,it was decided topo
rrayï¬eld Stadium andsaw Ulster bea stpone the following dayâ€™s hurling F
t Edinburgh 21â€“27 towinthe Celtic inal (until Easter Sunday) withthe
Cup.2004â€“05 season The format of football Final going ahead onits ow
the competition was changed for the nonSt.Patrickâ€™s Day .Less than a
second edition of the competition .T week later , onMarch 23rd ,seven
he competition was moved toAprilan
dMaytorun after the conclusion of
the Celtic League competition ,with
only eight
their11thconsecutive ï¬nalsince their 11thconsecutive ï¬nalsince 1-16to0-15 winners toqualify for which Dublin won by0-12to0-9.D
1989,losing6ï¬nalsin9years,wi 1989, losing 6 ï¬nals in9 years,wi their10th league ï¬nal in the past ublin are going for an unprecedented
ththislatestdefeatonanidentica ththis latest defeat on anidentica 13 years. They have won seven of t fourth successive Championship win
lscorelineto2020,whenMayolost l scoreline to2020, when Mayo lost heirprevious league ï¬nalsunder Co over Kerry. Prior to theircurrent r
toDublin.Background wereaiming toDublin.Background were aiming dy since 2002, losing theothertwo un,which started with the2011 All-
towintheirfourthtitleandï¬rst towintheir fourth title andï¬rst toWaterford (2007 ) andDublin (201 Irelandï¬nal,they had only managed
All-Irelandsince1951.Sincethen, All-Ireland since 1951. Since then, 1 ).Despitethedefeat there were two consecutive victories over them
theyhadlosttenï¬nals(1989,1996 they had lost ten ï¬nals (1989, 1996 some distinct positives froma Galwa ontwo separate occasions - 1909an
,1997,2004,2006, , 1997, 2004, 2006, y perspective- most notably thesoli dâ€™24, 1976 andâ€™77.The longest wi
d displays of DaithÃ­ Burke at centre nningsequence in therivalrywasse
-back,Joseph Cooney at wing-back antbyKerry between 1941 and1975, wh
dRonanBurke at full-back. Colm Cal en they won each of thesix Champion
lanan continued his excellent form iship meetings. Kerry went nine games
ngoalandalso hit a stunning free unbeaten between 1978 and2009, wit
fromdistance.Indeed it wasnotth h four victories either side of a dr
eGalway defence that wastheproble amatic draw at thequarter-ï¬nal sta
m geinThurlesin2001.Sunday will
marktheir11th
2012,2013,2016,2017,2020).app 2012, 2013, 2016, 2017, 2020). app
earedintheirseventhï¬nal,winnin earedin theirseventhï¬nal,winnin
gonthreeoccasions in2003,2005a g onthreeoccasions in 2003, 2005 a
nd2008.Thisï¬nalwastheï¬fthto nd2008.Thisï¬nal was theï¬fthto
becontested bycountyteamsfromC be contested bycounty teams fromC
onnachtandUlster,theotherï¬nals onnachtandUlster, theotherï¬nals
were1925(GalwaybeatCavan),1943 were1925 (Galway beat Cav an), 1943
(Roscommon beatCavan), 1948(Cavan (Roscommon beat Cav an), 1948 (Cavan
beat beat
39

Improving language models by retrieving from trillions of tokens
Table 18j2020 Summer Paralympics , from Wikipedia September 21. The original dates of the event,
25 August to 6 September 2020 , appears both in the neighbors Â»ğ‘1
1Â”ğ¹1
1Â¼Â”Â»ğ‘2
1Â”ğ¹2
1Â¼of chunkğ¶1and in the
subsequent chunk ğ¶2, where the loss for those tokens is signiï¬cantly reduced by R/e.sc/t.sc/r.sc/o.sc. Interestingly, in this
case, the neighbors were written at a time when the event hadnâ€™t yet been postponed.
ğ¶ğ‘¢colored by loss diï¬€erence ğ¶ğ‘¢colored by LCP with R/e.sc/t.scÂ¹ğ¶ğ‘¢ 1Âº Â» ğ‘1ğ‘¢Â”ğ¹1ğ‘¢Â¼colored by LCP with ğ¶ğ‘¢Â¸1Â»ğ‘2ğ‘¢Â”ğ¹2ğ‘¢Â¼colored by LCP with ğ¶ğ‘¢Â¸1
ğ¿R/e.sc/t.sc/r.sc/o.sc[O/f.sc/f.sc] ğ¿R/e.sc/t.sc/r.sc/o.sc 6 0Â“5Â”=0Â”>0Â“5LCP=0,1,2,3,4,>5 LCP=0,1,2,3,4,>5 LCP=0,1,2,3,4,>5
2020 Summer ParalympicsThe , brand 2020 Summer Paralympics The , brand picsGames.* The2020Summer Paraly 2020Summer Paralym picsTheare an
ed as the Tokyo 2020 Paralympic Game ed as the Tokyo 2020 Paralympic Game mpicsare an upcoming major internat upcoming major international multi-
s, was an international multi-sport s, was an international multi-sport ional multi-sport event forathletes sport event forathleteswithdisabi
parasports event held from 24 August parasports event held from 24 August withdisabilities governed by theI lities governed by theInternational
to 5 September 2021 in Tokyo, Japan to 5 September 2021 in Tokyo, Japan nternational Paralympic Committee .S Paralympic Committee .Scheduled as
. They were the 16th Summer Paralymp . They were the 16th Summer Paralymp cheduled as the16thSummer Paralym pthe16thSummer Paralym picGames,th
ic Games as organized by the Interna ic Games as organized by the Interna icGames,it is planned tobeheld i ey arescheduled tobeheld in Tokyo
tional Paralympic Committee (IPC). tional Paralympic Committee (IPC). n Tokyo, Japanfrom25Augustto6 S ,Japan between 24 August and 5Sept
eptember 2020.3. 2019 BWF Para-Bad ember2021. Originally dueto takep
minton World Championships- The 20 lacebetween 25Augustand 6Septemb
19 BWF Para-Badminton World Champion er2020. On 24March 2020, the IOCa
ships was held from 20to 25August nd the Tokyo Organizing Committee of
2019inBasel,Switzerland .- Menâ€™s ï¬cially announced that the2020Sum
event: Gold Medal: Pramod Bhagat in merOlympics and2020Summer Paralym
Singles SL3 Event andPramod Bhagat picswould be postponed to 2021,due
andManoj totheCOVID-19 pandemic, markingt
heï¬rst time that the Paralym picsh
as beenpostponed. They will stillb
e publicly marketed as
Originally scheduled totakeplacef Originally scheduled to takeplacef once submitted .This process was u Olympiad, have now been postponed a
rom25 August to6September 2020,i rom25Augustto6 September 2020, indertaken following thepostponement ndrescheduled for 23 July to8 Augu
nMarch2020 both the2020Summer Ol n March 2020 boththe2020Summer Olof the Tokyo 2020Games due tothe st 2021in Tokyo,Japan.TheGames
ympicsandParalympicswerepostpone ympicsandParalympicswerepostpone COVID-19 pandemic, withboththeOly werepostponed inMarch 2020 as are
dbyoneyearduetotheCOVID-19pa d byone year duetotheCOVID-19 pa mpicsand Paralym picspushed back a sultof theworldwide Covid-19 pande
ndemic,withtherescheduled Gamess ndemic,with therescheduled Games s year.Now,the Tokyo 2020 Olympics mic, although they will still keep t
till referred toasTokyo2020form tillreferredto as Tokyo 2020 for m are scheduled for July 23 toAugust henameTokyo2020 for marketing and
arketingandbranding purposes.As arketingandbranding purposes .As 8 whilethe Paralym picsareduetof branding purposes .This will be th
withtheOlympics , theGameswerela with the Olympics, the Games were la ollow from August 24 toSeptember 5. eï¬rst time theOlympicGameshave
rgelyheldbehind rgelyheldbehind Therefund process is separate for been postponed rather than cancelled
ticketholders outside of Japan , who .
purchased tickets through authorise
d ticket resellers (ATR). Each ATR
has its own individual refund proced
ure.Early ï¬gures from therefund
process for the Tokyo 2020 Olympics
stated that around 18 per cent
closeddoorswith nooutsidespecta closed doors withnooutsidespecta has been rescheduled toMay 1-4 bec Olympic Games ,when Tokyo became th
torsduetoastateofemergency in torsduetoastateofemergency in ause of travel restrictions under th eï¬rst cityinAsiatohosttheOly
theGreater Tokyo Area andotherpre theGreaterTokyoAreaandother pre ecurrent state of emergency inToky mpicand Paralym picGames,but unfor
fectures.TheGameswerethesecond fectures. TheGameswerethesecond oandother 10 prefectures across Ja tunately strong winds made it an imp
Summer Paralympicshosted by Tokyos Summer Paralympicshosted by Tokyospan.The Tokyo 2020 organizing comm ossible task this timearound.Memb
ince1964,andthethirdParalympics ince 1964, and thethirdParalympics ittee announced that theï¬rst of 18 ers oftheTokyo Organising Committe
heldinJapanoverallsincethe199 heldin Japan overall since the199 testevents for theOlympicand Par e oftheOlympicand Paralym picGame
8WinterParalympicsinNagano.Th 8 Winter ParalympicsinNagano.Th alympicGames will involve wheelchai s (Tokyo 2020), Tokyo Metropolitan G
eGamesfeatured eGamesfeatured r rugby,which will be held inYoyog overnment oï¬ƒcials ,Tokyo 2020 Torc
i National Stadium from April 3 to4 h Relay Oï¬ƒcial Ambassadors andrep
.The FINA Diving World Cup will fo resentatives from Miyagi Prefecture
llow from April 18 to23 attheToky joinedthearrival ceremony .FLAME
o Aquatics Centre ,which will also s OF RECOVERYThe Olympic ï¬‚ame will
erve as an Olympic qualifying event . now be put on display at various loc
The spread of theCOVID-19 pandemi ationsin theTohoku region , tohigh
c has slowed down inTokyo three wee lightthemessage of hope in theare
ks aftertheJapanese capital entere as worst aï¬€ected by the2011Great
d a state of emergency on East Japan Earthqu
539medaleventsin22sports,with 539 medal events in 22 sports ,with
badmintonandtaekwondo bothmaking badminton andtaekwondo both making
their Paralym picdebuttoreplacef theirParalympicdebuttoreplace f
ootball7-a-sideand sailing .China ootball 7-a-side andsailing.China
toppedthe medal table fortheï¬fth toppedthemedal table fortheï¬fth
consecutive Paralympics,with96go consecutive Paralympics,with 96 go
ldsand207totalmedals.GreatBrit ldsand207 total medals . GreatBrit
ainï¬nishedsecondforthenintht ain ï¬nished second forthenintht
ime, ime,
40

Improving language models by retrieving from trillions of tokens
Table 19jDaniel Radcliï¬€e , from Wikitext103Valid, retrieval data from c4. The chunks ğ¶2andğ¶3are almost
entirely retrieved from neighbours Â»ğ‘1Â”ğ¹1Â¼andÂ»ğ‘2Â”ğ¹2Â¼respectively, up to formatting diï¬€erences, which
dramatically reduces the loss for these tokens. This example illustrates that when training data leaks into
evaluation sets despite deduplication, our R/e.sc/t.sc/r.sc/o.scmodel can directly exploit this leakage.
ğ¶ğ‘¢colored by loss diï¬€erence ğ¶ğ‘¢colored by LCP with R/e.sc/t.scÂ¹ğ¶ğ‘¢ 1Âº Â»ğ‘1ğ‘¢Â”ğ¹1ğ‘¢Â¼colored by LCP with ğ¶ğ‘¢Â¸1Â»ğ‘2ğ‘¢Â”ğ¹2ğ‘¢Â¼colored by LCP with ğ¶ğ‘¢Â¸1
ğ¿R/e.sc/t.sc/r.sc/o.sc[O/f.sc/f.sc] ğ¿R/e.sc/t.sc/r.sc/o.sc 6 0Â“5Â”=0Â”>0Â“5LCP=0,1,2,3,4,>5 LCP=0,1,2,3,4,>5 LCP=0,1,2,3,4,>5
= Daniel Radcliï¬€e =Daniel Jacob R = Daniel Radcliï¬€e = Daniel Jacob R Daniel Jacob Rad cliï¬€e(born 23 July Daniel Jacob Rad cliï¬€e(born 23 July
adcliï¬€e ( born 23 July 1989 ) is an adcliï¬€e ( born 23 July 1989 ) is an 1989) is an English actor who rose 1989) is an English actor who rose
English actor who rose to prominenc English actor who rose to prominenc toprominence as thetitle character toprominence as thetitle character
e as the title character in the Harr e as the title character in the Harr intheHarryPotterï¬lmseries.He intheHarryPotterï¬lmseries.He
y Potter ï¬lm series. He made his ac y Potter ï¬lm series. He made his ac made his actingdebut at10yearsomade his actingdebut at10yearso
ting debut at 10 years of age in BBC ting debut at 10 years of age in BBC f age inBBC One â€™s1999 television ff age inBBC One â€™s1999 television m
Oneâ€™s 1999 television ï¬lm David Co Oneâ€™s 1999 television ï¬lm David Co ilmDavid Copperï¬eld ,followed by h ovie David Copperï¬eld ,followed by
pperï¬eld, followed by his cinematic pperï¬eld, followed by his cinematic is cinematic debut in2001â€™sThe Tai hisï¬lmdebutin2001â€™sThe Tailor
debut debut lor of Panama. At age 11, he was cas of Panama. At age 11, he was cast as
t as Harry Potter in the ï¬rst Harry Harry Potter in the ï¬rst Harry Pot
Potter ï¬lm, and starred in the ser ter ï¬lm, and starred in the series
ies for 10 years until the release o for 10 years until the release of th
f the eighth and ï¬nal ï¬lm in 2011. e eighth and ï¬nal ï¬lm in 2011. Rad
Radcliï¬€e began to branch out to s cliï¬€ebegantobranchout to stage
tage acting in 2007, starring in the acting in 2007, starring in the Lond
London and New York productions ofon and New York productions ofEquus
Equus,and ,andinthe
in2001â€™sThe TailorofPanama. At in2001â€™sThe Tailor of Panama. At in2001â€™sTheTailorofPanama.At ofPanama.At age 11, he was cast a
age 11,hewascastas Harry Potter age 11, he was cast as Harry Potter age 11, he was cast as Harry Potter s Harry Potter intheï¬rst Harry Po
intheï¬rst Harry Potterï¬lm, and in the ï¬rst Harry Potter ï¬lm, and intheï¬rst Harry Potter ï¬lm,and tterï¬lm,andstarredintheseries
starred in the series for 10 yearsu starred in the series for 10 years u starredintheseries for 10 years u for 10 years until thereleaseoft
ntil the release oftheeighth and f ntil the release of the eighth and f ntilthereleaseoftheeighthandf heeighthandï¬nalï¬lm in2011.R
inalï¬lm in 2011. Radcliï¬€ebegan inal ï¬lm in 2011.Radcliï¬€e began inalï¬lm in2011.Radcliï¬€e began adcliï¬€e began tobranch out tostag
to branch outto stage acting in 200 to branch out to stage acting in 200 tobranch out tostage acting in200 e actingin2007, starring intheLo
7, starring inthe London and New 7, starring in the London and New 7, starring intheLondonandNewYondonandNewYorkproductions ofEqu
rkproductions ofEquus, and in the us, and in the 2011 Broadway revival
2011 Broadway revival of the musical of the musical How to Succeed in Bu
How to Succeed in Business Without siness Without Really Trying. He sta
Really Trying. He starred in the 201 rred in the 2012 horror ï¬lm The Wom
2 horror ï¬lm The Woman in Black, an an in Black, and played beat poet Al
d played beat poet Allen Ginsberg in len Ginsberg in the 2013 independent
the 2013 independent ï¬lm Kill Your ï¬lm Kill Your Darlings.Hehascon
Darlings.Hehascontributed to ma tributedtomanycharities, includin
ny charities g Demelza House Childrenâ€™s
Yorkproductions ofEquus, and in t Yorkproductions ofEquus, and in t York productions ofEquus,andin t in the2011 Broadway revival of the
he 2011 Broadway revival ofthe musi he 2011 Broadway revival of the musi he2011 Broadway revival of themusi musical How to Succeed inBusiness
cal How to SucceedinBusiness Witho cal How to Succeed in Business Witho cal How to Succeed inBusiness Witho Without Really Trying .Hestarredin
utReallyTrying.He starred inthe ut Really Trying. He starred in the ut Really Trying .Hestarredin the the2012 horror ï¬lm TheWomaninB
2012 horror ï¬lmThe Woman inBlack, 2012 horror ï¬lm The Woman in Black, 2012 horror ï¬lm TheWomaninBlack,lack,andplayed beat poet Allen Gin
and played beat poet Allen Gins berg and played beat poet Allen Ginsberg andplayed beat poet Allen Ginsberg sbergin the2013 independent ï¬lm K
inthe2013 independent ï¬lmKill Y in the 2013 independent ï¬lm Kill Y in the2013 independent ï¬lm Kill Y ill Your Darlings .Hehas contribute
our<unk>.He has contributed to ma our<unk>.Hehascontributed to ma our Darlings .He has contributed to d to many charities , including Demel
ny charities, ny charities, many charities , including Demelza H za House Childrenâ€™sHospice and The
ouseChildrenâ€™sHospice and TheTrev TrevorProject.Healso made public
orProject.Healso made public serv service announcements for thelatter
ice announcements for thelatter.In .In 2011, he wasawarded the Trevor
2011, he wasawarded the Trevor Pro Projectâ€™s "HeroAward."
jectâ€™s "HeroAward."Sources disagr
ee about Radcliï¬€eâ€™s personal wealth
; hewasreported to have earned Â£1
millionfor theï¬rst Harry Potter
including <unk >HospiceCareforCh including <unk>HospiceCarefor Ch
ildren,and The Trevor Projectfors ildren,and TheTrevorProjectfors
uicideprevention among LGBTQyouth, uicide prevention among LGBTQ youth ,
whichgavehimits Hero Award in20 which gave him its Hero Award in 20
11.= =Earlylife==Radcliï¬€ew 11.= = Early life = = Radcliï¬€e w
asborninWestLondon, England .He asborninWest London ,England.He
istheonlychildofAlanGeorgeRad istheonly child ofAlan George Rad
cliï¬€e,aliteraryagent,and cliï¬€e,a literary agent ,and
41

Improving language models by retrieving from trillions of tokens
Table 20jSample - DÃ©claration des droits de lâ€™homme: Article premier. TheR/e.sc/t.sc/r.sc/o.sc[O/f.sc/f.sc] sample has
correct syntax and is almost plausible but is hallucinated. The R/e.sc/t.sc/r.sc/o.sc[O/n.sc] sample is correctly copied from
neighbour data, and robustly re-formated according to our prompt.
Prompt and sample of R/e.sc/t.sc/r.sc/o.sc[O/f.sc/f.sc] Prompt and sample of R/e.sc/t.sc/r.sc/o.sc[O/n.sc]Â»ğ‘1ğ‘¢Â”ğ¹1ğ‘¢Â¼colored by LCP with ğ¶ğ‘¢Â¸1Â»ğ‘2ğ‘¢Â”ğ¹2ğ‘¢Â¼colored by LCP with ğ¶ğ‘¢Â¸1
colored by LCP with R/e.sc/t.scÂ¹ğ¶ğ‘¢ 1Âº
LCP=0,1,2,3,4,>5 LCP=0,1,2,3,4,>5 LCP=0,1,2,3,4,>5
Article premier - Les hommes Article premier - Les hommes delâ€™homme et du citoyen .Ar Les hommes naissent etdemeur
naissent et demeurent libres e naissent et demeurent libres e ticle1erLes hommes naissent ent libres etÃ©gaux en droits.
t Ã©gaux en droits. Les distinc t Ã©gaux en droits. Les distinc etdemeurent libres etÃ©gaux Les distinctions sociales ne
tions sociales ne peuvent Ãªtre tions sociales ne peuvent Ãªtre endroits.Les distinctions so peuvent Ãªtre fondÃ©es que sur l
fondÃ©es que sur lâ€™utilitÃ© com fondÃ©es que sur lâ€™utilitÃ© com cialesnepeuvent Ãªtre fondÃ©es â€™utilitÃ© commune .Art.2. -
mune. Article 2. - Le but de mune. Article 2. - Le but de que surlâ€™utilitÃ© commune .A Le butdetouteassociation po
toute association politique e toute association politique e rticle2Le butdetouteasso litique est laconservation de
st la conservation des droits st la conservation des droits ciation politique est laconse sdroitsnaturelsetimprescri
naturels et naturels et rvation des droitsnaturelset ptiblesdelâ€™Homme.Cesdroits
imprescriptiblesdelâ€™homme. sontla libertÃ©, la propriÃ©tÃ©
Cesdroitssont la libertÃ©, la , la sÃ»retÃ© , etlarÃ©sistance
propriÃ©tÃ©, la sÃ»retÃ© , etlar Ã lâ€™oppression. Art. 3.-Le
Ã©sistance Ã lâ€™oppression.Art principedetouteSouverain et
icle3Leprincipedetoutes Ã©rÃ©sideessentiellement dans
ouverainetÃ© rÃ©side essentielle laNation.Nulcorps, nul indi
ment dans la nation. Nul corps vidu ne peut exercer dâ€™autorit
, nul individu ne peut exercer Ã© qui nâ€™en Ã©mane expressÃ©ment .
dâ€™autoritÃ©quinâ€™en Art
imprescriptibles de lâ€™homme, imprescriptiblesdelâ€™homme. criptibles delâ€™homme.Cesdro et imprescriptibles de lâ€™homm
et par consÃ©quent la garantie Cesdroitssont la libertÃ©, la itssontla libertÃ©, la propri e.Cesdroitssontla libertÃ©,
Ã  chacun des droits suivants propriÃ©tÃ©, la sÃ»retÃ© etlarÃ© Ã©tÃ©, lasÃ»retÃ©etlarÃ©sistanc lapropriÃ©tÃ© , lasÃ»retÃ© et la
: Article 3. - La propriÃ©tÃ© sistanceÃ lâ€™oppression.Arti eÃ  lâ€™oppression .Article3 - rÃ©sistance Ã  lâ€™oppression .A
est un droit inviolable et sa cle3.- Leprincipedetoute Le principe detoute souverai rticle3 - Le principe detout
crÃ©. Toute personne a le droit souverain etÃ© rÃ©side essentiel netÃ© rÃ©side essentiellement da e souverainetÃ© rÃ©side essentie
de procÃ©der Ã  sa propre cons lement dans la nation. Nul cor nslaNation.Nul corps ,nul i llement dans laNation.Nul co
ervation. Article 4. - Le ps, nul individu ne peut exerc ndividunepeut exercer d â€™auto rps,nul individu nepeut exer
er dâ€™autoritÃ© qui n ritÃ©qui nâ€™enÃ©maneexpressÃ©me cer dâ€™autoritÃ©qui nâ€™enÃ©mane
nt.Article 4 -LalibertÃ©co expressÃ©ment.Article 4 -La
nsisteÃ pouvoir faire tout ce libertÃ©consisteÃ pouvoir fai
quinenuitpasÃ autrui : ai re tout ce qui ne nuit pas Ã  a
nsi, lâ€™exercice des droits nat utrui : ainsi, lâ€™exercice des
urelsdechaquehommenâ€™a de b droits naturels de chaque homm
ornes que celles qui assurent e nâ€™a de bornes que celles qui
auxautresmembres delasociÃ© assurent aux autres membres d
tÃ©la jouissance de e la sociÃ©tÃ© la jouissance de
ces mÃªmes droits.Cesbornes
but de toute association est â€™enÃ©maneexpressÃ©ment.Artic mane expressÃ©ment .Article4 mane expressÃ©ment .Article4
la dÃ©fense des droits de lâ€™hom le4.-LalibertÃ©consisteÃ  -LalibertÃ© consiste Ã pouvoi -LalibertÃ© consiste Ã pouvoi
me et du citoyen. Tout citoye pouvoir faire tout ce qui ne r faire tout cequinenuitpa r faire tout cequinenuitpa
n a le droit de participer Ã  l nuit pas Ã  autrui : ainsi, lâ€™e s Ã autrui : ainsi ,lâ€™exercice s Ã autrui : ainsi ,lâ€™exercice
a direction des aï¬€aires publi xercice des droits naturels de desdroitsnaturelsdechaque desdroitsnaturelsdechaque
ques. Article 5. - Lâ€™impuni chaque homme nâ€™a de bornes qu hommenâ€™ade bornesquecelle hommenâ€™ade bornesquecelle
tÃ© nâ€™a jamais Ã©tÃ© et ne sera j e celles qui assurent aux autr squiassurent aux autres memb squiassurent aux autres memb
amais une ï¬n en elle-mÃªme. Lâ€™ es membres de la sociÃ©tÃ© la jo resde lasociÃ©tÃ©lajouissanc resde lasociÃ©tÃ©lajouissanc
imp uissance de ces mÃªmes edeces mÃªmes droits.Cesbor edeces mÃªmes droits.Cesbor
nes ne peuvent Ãªtre dÃ©terminÃ©e nes ne peuvent Ãªtre dÃ©terminÃ©e
s que par la loi.Article 5 - s que par la loi.Article 5 -
Laloinâ€™a le droit de dÃ©fend Laloinâ€™a le droit de dÃ©fend
re que les actions nuisibles Ã  re que les actions nuisibles Ã 
la sociÃ©tÃ©. Tout ce qui nâ€™est la sociÃ©tÃ©. Tout ce qui nâ€™est
pas dÃ©fendu par la loi ne peu pas dÃ©fendu par la loi ne peu
t Ãªtre empÃªchÃ©, et nul ne peut t Ãªtre empÃªchÃ©, et nul ne peut
Ãªtrecontraint Ã fairecequâ€™ Ãªtrecontraint Ã fairecequâ€™
ellen ellen
unitÃ©, comme le despotisme, s droits.Cesbornes ne peuvent
â€™est toujours rÃ©vÃ©lÃ©e Ãªtre un Ãªtre dÃ©terminÃ©es que par la l
instrument dâ€™oppression. La ty oi.Article 5.-Laloinâ€™a
rannie qui sâ€™est Ã©lue juge su le droit de dÃ©fendre que les a
prÃªme de la conscience des aut ctions nuisibles Ã  la sociÃ©tÃ©.
res ne peut Ãªtre jugÃ©e. La jus Tout ce qui nâ€™est pas dÃ©fendu
tice se trouve dans la consci par la loi ne peut Ãªtre empÃªc
ence de chaque citoyen, hÃ©, et nul ne peut Ãªtre
42

Improving language models by retrieving from trillions of tokens
Table 21jSample - Decimals of ğœ‹.TheR/e.sc/t.sc/r.sc/o.sc[O/f.sc/f.sc] sample quickly diverges two digits after the end
of the prompt whereas R/e.sc/t.sc/r.sc/o.sc[O/n.sc] correctly outputs a large number of ğœ‹digits, directly copied from the
neighbours data.
Prompt and sample of R/e.sc/t.sc/r.sc/o.sc[O/f.sc/f.sc] Prompt and sample of R/e.sc/t.sc/r.sc/o.sc[O/n.sc] Â»ğ‘1ğ‘¢Â”ğ¹1ğ‘¢Â¼colored by LCP with ğ¶ğ‘¢Â¸1Â»ğ‘2ğ‘¢Â”ğ¹2ğ‘¢Â¼colored by LCP with ğ¶ğ‘¢Â¸1
colored by LCP with R/e.sc/t.scÂ¹ğ¶ğ‘¢ 1Âº
LCP=0,1,2,3,4,>5 LCP=0,1,2,3,4,>5 LCP=0,1,2,3,4,>5
Pi = 3. 1415926535 8979323846 Pi = 3. 1415926535 8979323846 â€œ14159265358979323846 264 33 46 2643383279 5028841971 69399
2643383279 5028841971 69399375 2643383279 5028841971 69399375 83279 5028841971 693993751058 375105820974944 592307816406
10 5820974944 5923078164 06286 10 5820974944 5923078164 06286 20974944 5923078164 0628 620899 28620899 8628034 825 3421170679
20899 8628034825 3421170679 20899 8628034825 3421170679 8628034825 34211706798214808 82148086513282306647 0938446
651 3282306647 0938446095 5058 095 5058223172 53594081284811
223172 53594081284811174502 8 1745028410270193 8521105559 6
410270193 8521105559 644622948 446229489 5493038196 442881097
9 54930381964428810975 665933 56659334461 2847564823 378678
4461 284 7564823 3786783 3165 2712019091 45 64856692 346
0
8294049602 8988496069 9858349 8214808651 3282306647 0938446 6513282306647 093844 6095 5058 47 093844 6095 5058223172 5359 4
065 9873246379 9644789435 8628 095 5058223172 53594081284811 223172 5359 40812848111745020812848111745028410270193 85
730709 6540159079 5944069810 5 174502 8410270193 8521105559 6 8410270193 8521 105559 644 6229421105559 644 6229489 5493038196
992965913 7095378412 69378359 446229489 5493038196442881097 89 5493038196 44288109 7566593 44288109 756659334461 284 7564
5 6659334461 284 34461 284 75648233786783165 27 8233786783165 27120190914564
12019091 4564856692 346034861 856692 3460348610 4543266482 1
0 4543266482 1339360726 024914 339360726 0249141273724587006
12737245870066 0631558817 488 6 0631558817 4881520920 962829
1520920 9628292540 91715 364 2540 91715 364367892590360
10 6940372045 7088679512 85612 75648233786783165 2712019091 23 3786783165 2712019091 4564 165 27120190914564856692 3460
30857 9046461290 9276642155 56 4564856692 3460348610 45432664 856692 346034 8610 4543266 4821348610 4543266 4821339360726 0
54603269 5656128798 6366475705 82 1339360726 024914127372458 339360726 0249141273 724587006 249141273 7245870066 063155881
6294954741 5886335339 57657 70066 0631558817 4881520920 96 60631558817 4881520920962829 748815209209628292540 91715 3
28292540 91715 2540 91715 364367892590360 01 64367892590360 0113305305 488
13305305 4882046652 1384146951 2046652 1384146951 9415116094
94151160943305727036 5759591 3305727036 5759591953 09218611
953 0921861173 8193261179 3105 73 8193261179 310511854807446
1185480744623799 6274 95 23799 6274 956735 1885 752724 89
1227
76345 5770886953 7988876910 79 364367892590360 0113305305 48
66169745 6493974637 6345801550 82046652 1384146951 9415116094
6663542854 6333764630 6356284 3305727036 5759591953 0921861
271 7885339804 5672434 173 8193261179 31051185480744
623799 6274
43