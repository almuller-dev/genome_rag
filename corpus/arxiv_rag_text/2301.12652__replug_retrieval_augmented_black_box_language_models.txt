REPLUG: Retrieval-Augmented Black-Box Language Models
Weijia Shi,1 *Sewon Min,1Michihiro Yasunaga,2Minjoon Seo,3Rich James,4Mike Lewis,4
Luke Zettlemoyer1 4Wen-tau Yih4
Abstract
We introduce REPLUG, a retrieval-augmented lan-
guage modeling framework that treats the lan-
guage model (LM) as a black box and augments
it with a tuneable retrieval model. Unlike prior
retrieval-augmented LMs that train language mod-
els with special cross attention mechanisms to en-
code the retrieved text, REPLUG simply prepends
retrieved documents to the input for the frozen
black-box LM. This simple design can be eas-
ily applied to any existing retrieval and language
models. Furthermore, we show that the LM can
be used to supervise the retrieval model, which
can then find documents that help the LM make
better predictions. Our experiments demonstrate
thatREPLUG with the tuned retriever significantly
improves the performance of GPT-3 (175B) on
language modeling by 6.3%, as well as the perfor-
mance of Codex on five-shot MMLU by 5.1%.
1. Introduction
Large language models (LLMs) such as GPT-3 (Brown et al.,
2020a) and Codex (Chen et al., 2021a), have demonstrated
impressive performance on a wide range of language tasks.
These models are typically trained on very large datasets and
store a substantial amount of world or domain knowledge
implicitly in their parameters. However, they are also prone
to hallucination and cannot represent the full long tail of
knowledge from the training corpus. Retrieval-augmented
language models (Khandelwal et al., 2020; Borgeaud et al.,
2022; Izacard et al., 2022b; Yasunaga et al., 2022), in con-
trast, can retrieve knowledge from an external datastore
when needed, potentially reducing hallucination and increas-
ing coverage. Previous approaches of retrieval-augmented
language models require access to the internal LM repre-
sentations (e.g., to train the model (Borgeaud et al., 2022;
1University of Washington2Stanford University3KAIST4Meta
AI.
*Work done while the first author was interning at Meta AI.
Correspondence to: Weijia Shi <swj0419@uw.edu>.
Figure 1. Different from previous retrieval-augmented ap-
proaches (Borgeaud et al., 2022) that enhance a language model
with retrieval by updating the LM’s parameters, REPLUG treats
the language model as a black box and augments it with a frozen
or tunable retriever. This black-box assumption makes REPLUG
applicable to large LMs (i.e., >100B parameters), which are often
served via APIs.
Izacard et al., 2022b) or to index the datastore (Khandelwal
et al., 2020)), and are thus difficult to be applied to very
large LMs. In addition, many best-in-class LLMs can only
be accessed through APIs. Internal representations of such
models are not exposed and fine-tuning is not supported.
In this work, we introduce REPLUG (Retrieve and Plug ),
a new retrieval-augmented LM framework where the lan-
guage model is viewed as a black box and the retrieval
component is added as a tuneable plug-and-play module.
Given an input context, REPLUG first retrieves relevant
documents from an external corpus using an off-the-shelf
retrieval model. The retrieved documents are prepended to
the input context and fed into the black-box LM to make
the final prediction. Because the LM context length limits
the number of documents that can be prepended, we also
introduce a new ensemble scheme that encodes the retrieved
documents in parallel with the same black-box LM, allow-
ing us to easily trade compute for accuracy. As shown inarXiv:2301.12652v4  [cs.CL]  24 May 2023

REPLUG: Retrieval-Augmented Black-Box Language Models
Figure 1, REPLUG is extremely flexible and can be used
with any existing black-box LM and retrieval model.
We also introduce REPLUG LSR (REPLUG with LM-
Supervised Retrieval), a training scheme that can further
improve the initial retrieval model in REPLUG with super-
vision signals from a black-box language model. The key
idea is to adapt the retriever to the LM, which is in contrast
to prior work (Borgeaud et al., 2022) that adapts language
models to the retriever. We use a training objective which
prefers retrieving documents that improve language model
perplexity, while treating the LM as a frozen, black-box
scoring function.
Our experiments show that REPLUG can improve the perfor-
mance of diverse black-box LMs on both language modeling
and downstream tasks, including MMLU (Hendrycks et al.,
2021) and open-domain QA (Kwiatkowski et al., 2019; Joshi
et al., 2017). For instance, REPLUG can improve Codex
(175B) performance on MMLU by 4.5%, achieving compa-
rable results to the 540B, instruction-finetuned Flan-PaLM.
Furthermore, tuning the retriever with our training scheme
(i.e., REPLUG LSR ) leads to additional improvements, in-
cluding up to 6.3% increase in GPT-3 175B language mod-
eling. To the best of our knowledge, our work is the first to
show the benefits of retrieval to large LMs (>100B model
parameters), for both reducing LM perplexity and and im-
proving in-context learning performance. We summarize
our contributions as follows:
•We introduce REPLUG (§3), the first retrieval-
augmented language modeling framework for enhanc-
ing large black-box language models with retrieval.
•We propose a training scheme (§4) to further adapt an
off-the-shelf retrieval model to the LM, using the lan-
guage modeling scores as supervision signals, resulting
in improved retrieval quality.
•Evaluations on language modeling (§6), open-domain
QA and MMLU demonstrate that REPLUG can im-
prove the performance of various language models
such as GPT, OPT and BLOOM, including very large
models with up to 175B parameters.
2. Background and Related Work
Black-box Language Models Large language models
(i.e., >100B), such as GPT-3 (Brown et al., 2020a),
Codex (Chen et al., 2021a), and Yuan 1.0 (Wu et al., 2021),
are not open-sourced due to commercial considerations
and are only available as black-box APIs, through which
users can send queries and receive responses. On the
other hand, even open sourced language models such as
OPT-175B (Zhang et al., 2022a) and BLOOM-176B (Scao
et al., 2022) require significant computational resourcesto run and finetune locally. For example, finetuning
BLOOM-176B requires 72 A100 GPUs (80GB memory,
$15k each (Younes Belkda, 2022)), making them inac-
cessible to researchers and developers with limited re-
sources. Traditionally, retrieval-augmented model frame-
works (Khandelwal et al., 2020; Borgeaud et al., 2022; Yu,
2022; Izacard et al., 2022b; Goyal et al., 2022) have fo-
cused on the white-box setting, where language models
are fine-tuned to incorporate retrieved documents. How-
ever, the increasing scale and black-box nature of large
language models makes this approach infeasible. To ad-
dress the challenges posed by large language models, we
investigate retrieval-augmentation in the black-box setting ,
where users only have access to the model predictions and
cannot access or modify its parameters.
Retrieval-augmented Models Augmenting language
models with relevant information retrieved from various
knowledge stores has shown to be effective in improving
performance on various NLP tasks, including language mod-
eling (Min et al., 2022; Borgeaud et al., 2022; Khandelwal
et al., 2020) and open-domain question answering (Lewis
et al., 2020; Izacard et al., 2022b; Hu et al., 2022). Specifi-
cally, using the input as query, (1) a retriever first retrieves a
set of documents (i.e., sequences of tokens) from a corpus
and then (2) a language model incorporates the retrieved
documents as additional information to make a final predic-
tion. This style of retrieval can be added to both encoder-
decoder (Yu, 2022; Izacard et al., 2022b) and decoder-only
models (Khandelwal et al., 2020; Borgeaud et al., 2022; Shi
et al., 2022; Rubin et al., 2022). For example, Atlas (Izacard
et al., 2022b) finetunes an encoder-decoder model jointly
with the retriever by modeling documents as latent variables,
while RETRO (Borgeaud et al., 2022) changes the decoder-
only architecture to incorporate retrieved texts and pretrains
the language model from scratch. Both methods require
updating the model parameters through gradient descent,
which cannot be applied to black-box LMs. Another line
of retrieval-augmented LMs such as kNN-LM (Khandelwal
et al., 2020; Zhong et al., 2022) retrieves a set of tokens and
interpolates between the LM’s next token distribution and
kNN distributions computed from the retrieved tokens at
inference. Although kNN-LM does not require additional
training, it requires access to internal LM representations
to compute the kNN distribution, which are not always
available for large LMs such as GPT-3. In this work, we in-
vestigate ways to improve large black-box language models
with retrieval. While concurrent work (Mallen et al., 2022;
Si et al., 2023; Yu et al., 2023; Khattab et al., 2022) has
demonstrated that using a frozen retriever can improve GPT-
3 performance on open-domain question answering, we
approach the problem in a more general setting, including
language modeling and understanding tasks. We also pro-
pose an ensemble method to incorporate more documents

REPLUG: Retrieval-Augmented Black-Box Language Models
Figure 2. REPLUG at inference (§3). Given an input context, REPLUG first retrieves a small set of relevant documents from an external
corpus using a retriever (§3.1 Document Retrieval ). Then it prepends each document separately to the input context and ensembles output
probabilities from different passes (§3.2 Input Reformulation ).
and a training scheme to further adapt the retriever to large
LMs.
3. R EPLUG
We introduce REPLUG (Retrieve and Plug ), a new retrieval-
augmented LM paradigm where the language model is
treated as black box and the retrieval component is added as
a potentially tuneable module.
As shown in Figure 2, given an input context, REPLUG first
retrieves a small set of relevant documents from an external
corpus using a retriever (§3.1). Then we pass the concate-
nation of each retrieved document with the input context
through the LM in parallel, and ensemble the predicted
probabilities (§3.2).
3.1. Document Retrieval
Given an input context x, the retriever aims to retrieve a
small set of documents from a corpus D={d1...dm}that
are relevant to x. Following prior work (Qu et al., 2021;
Izacard & Grave, 2021b; Ni et al., 2021), we use a dense
retriever based on the dual encoder architecture, where an
encoder is used to encode both the input context xand the
document d. Specifically, the encoder maps each document
d∈Dto an embedding E(d)by taking the mean pooling of
the last hidden representation over the tokens in d. At query
time, the same encoder is applied to the input context xto
obtain a query embedding E(x). The similarity between the
query embedding and the document embedding is computed
by their cosine similarity:
s(d, x) = cos( E(d),E(x)) (1)
The top- kdocuments that have the highest similarity scores
when compared with the input xare retrieved in this step.For efficient retrieval, we precompute the embedding of
each document d∈Dand construct FAISS index (Johnson
et al., 2019) over these embeddings.
3.2. Input Reformulation
The retrieved top- kdocuments provide rich information
about the original input context xand can potentially help
the LM to make a better prediction. One simple way to
incorporate the retrieved documents as part of the input to
the LM is to prepend xwith all kdocuments. However, this
simple scheme is fundamentally restricted by the number
of documents (i.e., k) we can include, given the language
model’s context window size. To address this limitation, we
adopt an ensemble strategy described as follows. Assume
D′⊂ D consists of kmost relevant documents to x, ac-
cording to the scoring function in Eq. (1). We prepend each
document d∈ D′tox, pass this concatenation to the LM
separately, and then ensemble output probabilities from all
kpasses. Formally, given the input context xand its top- k
relevant documents D′, the output probability of the next
token yis computed as a weighted average ensemble:
p(y|x,D′) =X
d∈D′p(y|d◦x)·λ(d, x),
where◦denotes the concatenation of two sequences and the
weight λ(d, x)is based on the similarity score between the
document dand the input context x:
λ(d, x) =es(d,x)
P
d∈D′es(d,x)
Although our ensemble method requires running the LM
ktimes, the cross attention is performed between each re-
trieved document and the input context. Therefore, com-
pared with the method of prepending all the retrieved docu-

REPLUG: Retrieval-Augmented Black-Box Language Models
ments, our ensemble methods do not incur additional com-
putational cost overhead.
4. R EPLUG LSR: Training the Dense
Retriever
Instead of relying only on existing neural dense retrieval
models (Karpukhin et al., 2020a; Izacard et al., 2022a; Su
et al., 2022), we further propose REPLUG LSR (REPLUG
with LM-Supervised Retrieval), which adapts the retriever
inREPLUG by using the LM itself to provide supervision
about which documents should be retrieved.
Inspired by Sachan et al. (2022), our approach can be seen
as adjusting the probabilities of the retrieved documents
to match the probabilities of the output sequence perplexi-
ties of the language model. In other words, we would like
the retriever to find documents that result in lower perplex-
ity scores. As shown in Figure 3, our training algorithm
consists of the four steps: (1) retrieving documents and
computing the retrieval likelihood (§4.1), (2) scoring the
retrieved documents by the language model (§4.2), (3) up-
dating the retrieval model parameters by minimizing the KL
divergence between the retrieval likelihood and the LM’s
score distribution (§4.3), and (4) asynchronous update of
the datastore index (§4.4).
4.1. Computing Retrieval Likelihood
We retrieve kdocuments D′⊂ D with the highest simi-
larity scores from a corpus Dgiven an input context x, as
described in §3.1. We then compute the retrieval likelihood
of each retrieved document d:
PR(d|x) =es(d,x)/γ
P
d∈D′es(d,x)/γ
where γis a hyperparameter that controls the temerature of
the softmax. Ideally, the retrieval likelihood is computed by
marginalizing over all the documents in the corpus D, which
is intractable in practice. Therefore, we approximate the
retrieval likelihood by only marginalizing over the retrieved
documents D′.
4.2. Computing LM likelihood
We use the LM as a scoring function to measure how much
each document could improve the LM perplexity. Specifi-
cally, we first compute PLM(y|d, x), the LM probability
of the ground truth output ygiven the input context xand
a document d. The higher the probability, the better the
document diis at improving the LM’s perplexity. We then
compute the LM likelihood of each document das follows:
Q(d|x, y) =ePLM(y|d,x)/β
P
d∈D′ePLM(y|d,x)/βwhere βis another hyperparameter.
4.3. Loss Function
Given the input context xand the corresponding ground
truth continuation y, we compute the retrieval likelihood
and the language model likelihood. The dense retriever is
trained by minimizing the KL divergence between these two
distributions:
L=1
|B|X
x∈BKL
PR(d|x)∥QLM(d|x, y)
,
where Bis a set of input contexts. When minimizing the
loss, we can only update the retrieval model parameters. The
LM parameters are fixed due to our black-box assumption.
4.4. Asynchronous Update of the Datastore Index
Because the parameters in the retriever are updated during
the training process, the previously computed document em-
beddings are no longer up to date. Therefore, following Guu
et al. (2020), we recompute the document embeddings and
rebuild the efficient search index using the new embeddings
every Ttraining steps. Then we use the new document
embeddings and index for retrieval, and repeat the training
procedure.
5. Training Setup
In this section, we describe the details of our training proce-
dure. We first describe the model setting in REPLUG (§5.1)
and then describe the procedure for training the retriever in
REPLUG LSR (§5.2).
5.1. R EPLUG
In theory, any type of retriever, either dense (Karpukhin
et al., 2020b; Ni et al., 2021) or sparse (Robertson et al.,
2009), could be used for REPLUG. Following prior
work (Izacard et al., 2022b), we use the Contriever (Izacard
et al., 2022a) as the retrieval model for REPLUG, as it has
demonstrated strong performance.
5.2. R EPLUG LSR
ForREPLUG LSR , we initialize the retriever with the
Contriever model (Izacard et al., 2022a). We use GPT-3
Curie (Brown et al., 2020b) as the supervision LM to com-
pute the LM likelihood.
Training data We use 800K sequences of 256 tokens
each, sampled from the Pile training data (Gao et al., 2020),
as our training queries. Each query is split into two parts:
the first 128 tokens are used as the input context x, and the
last 128 tokens are used as the ground truth continuation
y. For the external corpus D, we sample 36M documents

REPLUG: Retrieval-Augmented Black-Box Language Models
Figure 3. REPLUG LSR training process (§4). The retriever is trained using the output of a frozen language model as supervision
signals.
of 128 tokens from the Pile training data. To avoid trivial
retrieval, we ensure that the external corpus documents do
not overlap with the documents from which the training
queries are sampled.
Training details To make the training process more ef-
ficient, we pre-compute the document embeddings of the
external corpus Dand create a FAISS index (Johnson et al.,
2019) for fast similarity search. Given a query x, we retrieve
the top 20 documents from the FAISS index and compute
the retrieval likelihood and the LM likelihood with a tem-
perature of 0.1. We train the retriever using the Adam opti-
mizer (Kingma & Ba, 2015) with a learning rate of 2e-5, a
batch size of 64, and a warmup ratio of 0.1. We re-compute
the document embeddings every 3k steps and fine-tune the
retriever for a total of 25k steps.
6. Experiments
We perform evaluations on both language modeling (§6.1)
and downstream tasks such as MMLU (§6.2) and open-
domain QA (§6.3). In all settings, REPLUG ˜improve the
performance of various black-box language models, show-
ing the effectiveness and generality of our approach.
6.1. Language Modeling
Datasets The Pile (Gao et al., 2020) is a language mod-
eling benchmark that consists of text sources from diverse
domains such as web pages, code and academic papers. Fol-
lowing prior work, we report bits per UTF-8 encoded byte
(BPB) as the metric on each subset domain.
Baselines We consider GPT-3 and GPT-2 family language
model as the baselines. The four models from GPT-3
(Davinci, Curie, Baddage and Ada) are black-box modelsthat are only accessible through API
Our model We add REPLUG andREPLUG LSR to the
baselines. We randomly subsampled Pile training data
(367M documents of 128 tokens) and use them as the re-
trieval corpus for all models. As the Pile dataset has made
efforts to deduplicate documents across train, validation and
test splits (Gao et al., 2020), we did not do additional filter-
ing. For both REPLUG andREPLUG LSR , we use a length
of 128-token context to do retrieval and adopt the ensem-
ble method (Section 3.2) to incorporate top 10 retrieved
documents during inference.
Results Table 1 reports the results of the original base-
lines, baselines augmented with the REPLUG, and baselines
augmented with the REPLUG LSR . We observe that both
REPLUG andREPLUG LSR significantly outperform the
baselines. This demonstrates that simply adding a retrieval
module to a frozen language model (i.e., the black-box set-
ting) is effective at improving the performance of different
sized language models on language modeling tasks. Fur-
thermore, REPLUG LSR consistently performs better than
REPLUG by a large margin. Specifically, REPLUG LSR
results in 7.7% improvement over baselines compared to
4.7% improvement of REPLUG averaged over the 8 models.
This indicates that further adapting the retriever to the target
LM is beneficial.
6.2. MMLU
Datasets Massive Multi-task Language Understanding
(MMLU (Hendrycks et al., 2021)) is a multiple choice QA
dataset that covers exam questions from 57 tasks includ-
ing mathematics, computer science, law, US history and
etc. The 57 tasks are grouped into 4 categories: humani-
ties, STEM, social sciences and other. Following Chung

REPLUG: Retrieval-Augmented Black-Box Language Models
Model # Parameters Original + R EPLUG Gain % + R EPLUG LSR Gain %
GPT-2 Small 117M 1.33 1.26 5.3 1.21 9.0
Medium 345M 1.20 1.14 5.0 1.11 7.5
Large 774M 1.19 1.15 3.4 1.09 8.4
XL 1.5B 1.16 1.09 6.0 1.07 7.8
GPT-3 Ada 350M 1.05 0.98 6.7 0.96 8.6
(black-box) Babbage 1.3B 0.95 0.90 5.3 0.88 7.4
Curie 6.7B 0.88 0.85 3.4 0.82 6.8
Davinci 175B 0.80 0.77 3.8 0.75 6.3
Table 1. Both REPLUG and REPLUG LSR consistently enhanced the performance of different language models. Bits per byte
(BPB) of the Pile using GPT-3 and GPT-2 family models (Original) and their retrieval-augmented versions (+ REPLUG and + REPLUG
LSR. The gain % shows the relative improvement of our models compared to the original language model.
Model # Parameters Humanities Social. STEM Other All
Codex 175B 74.2 76.9 57.8 70.1 68.3
PaLM 540B 77.0 81.0 55.6 69.6 69.3
Flan-PaLM 540B - - - - 72.2
Atlas 11B 46.1 54.6 38.8 52.8 47.9
Codex + R EPLUG 175B 76.0 79.7 58.8 72.1 71.4
Codex + R EPLUG LSR 175B 76.5 79.9 58.9 73.2 71.8
Table 2. REPLUG and REPLUG LSR improves Codex by 4.5% and 5.1% respectively. Performance on MMLU broken down into 4
categories. The last column averages the performance over these categories. All models are evaluated based on 5-shot in-context learning
with direct prompting.
et al. (2022a), we evaluate REPLUG in the 5-shot in-context
learning setting.
Baselines We consider two groups of strong previous mod-
els as baselines for comparisons. The first group of base-
lines is the state-of-the-art LLMs including Codex1(Chen
et al., 2021b), PaLM (Chowdhery et al., 2022), and Flan-
PaLM (Chung et al., 2022b). According to Chung et al.
(2022b), these three models rank top-3 in the leaderboard
of MMLU. The second group of baselines consists of
retrieval-augmented language models. We only include At-
las (Izacard et al., 2022b) in this group, as no other retrieval-
augmented LMs have been evaluated on the MMLU dataset.
Atlas trains both the retriever and the language model, which
we consider a white-box retrieval LM setting.
Our model We add REPLUG andREPLUG LSR only to
Codex because other models such as PaLM and Flan-PaLM
are not accessible to the public. We use the test question as
the query to retrieve 10 relevant documents from Wikipedia
(2018, December) and prepend each retrieved document
to the test question, resulting in 10 separate inputs. These
inputs are then separately fed into the language models, and
the output probabilities are ensemble together.
1Code-Davinci-002Results Table 2 presents the results from the baselines,
REPLUG, and REPLUG LSR on the MMLU dataset. We
observe that both the REPLUG andREPLUG LSR improve
the original Codex model by 4.5% and 5.1%, respectively.
In addition, REPLUG LSR largely outperforms the previous
retrieval-augmented language model, Atlas, demonstrating
the effectiveness of our black-box retrieval language model
setting. Although our models slightly underperform Flan-
PaLM, this is still a strong result because Flan-PaLM has
three times more parameters. We would expect that the
REPLUG LSR could further improve Flan-PaLM, if we had
access to the model.
Another interesting observation is that the REPLUG LSR
outperforms the original model by 1.9% even in the STEM
category. This suggests that retrieval may improve a lan-
guage model’s problem-solving abilities.
6.3. Open Domain QA
Lastly, we conduct evaluation on two open-domain QA
datasets: Natural Questions (NQ) (Kwiatkowski et al., 2019)
and TriviaQA (Joshi et al., 2017).
Datasets NQ and TriviaQA are two open-domain QA
datasets consisting of questions, answers collected from
2Si et al. (2022) augment Codex with concatenation of 10
documents retrieved by contriever.

REPLUG: Retrieval-Augmented Black-Box Language Models
NQ TQA
Model Few-shot Full Few-shot Full
Chinchilla 35.5 - 64.6 -
PaLM 39.6 - - -
Codex 40.6 - 73.6 -
RETRO†- 45.5 - -
R2-D2†- 55.9 - 69.9
Atlas†42.4 60.4 74.5 79.8
Codex + Contriever cc244.2 - 76.0 -
Codex + R EPLUG 44.7 - 76.8 -
Codex + R EPLUG LSR 45.5 - 77.3 -
Table 3. Performance on NQ and TQA. We report results for both
few-shot (64 shots for Chinchilla, PaLM, and Atlas; 16 shots for
Codex-based models) and full training data settings. REPLUG
LSR improves Codex by 12.0% on NQ and 5.0% on TQA, making
it the best-performing model in the few-shot setting. Note that
models with †are finetuned using training examples, while other
models use in-context learning.
Wikipedia and the Web. Following prior work (Izacard
& Grave, 2021a; Si et al., 2022), we report results for the
filtered set of TriviaQA. For evaluation, we consider the few-
shot setting where the model is only given a few training
examples and full data where the model is given all the
training examples.
Baselines We compare our model with several state-of-
the-art baselines, both in a few-shot setting and with full
training data. The first group of models consists of power-
ful large language models, including Chinchilla (Hoffmann
et al., 2022), PaLM (Chowdhery et al., 2022), and Codex.
These models are all evaluated using in-context learning
under the few-shot setting, with Chinchilla and PaLM eval-
uated using 64 shots, and Codex using 16 shots. The sec-
ond group of models for comparison includes retrieval-
augmented language models such as RETRO (Borgeaud
et al., 2021), R2-D2 (Fajcik et al., 2021), and Atlas (Izacard
et al., 2022b). All of these retrieval-augmented models are
finetuned on the training data, either in a few-shot setting or
with full training data. Specifically, Atlas is finetuned on 64
examples in the few-shot setting.
Our model We add REPLUG andREPLUG LSR to Codex
with Wikipedia (2018, December) as the retrieval corpus to
evaluate the model in a 16-shot in context learning. Sim-
ilar to the setting in language modeling and MMLU, we
incorporate top-10 retrieved documents using our proposed
ensemble method.
Results As shown in Table 3, REPLUG LSR significantly
improves the performance of the original Codex by 12.0%
on NQ and 5.0% on TQA. It outperforms the previous best
Figure 4. Ensembling random documents does not result in
improved performance. BPB of Curie augmented with different
methods (random, REPLUG andREPLUG LSR ) when varying the
number of documents (i.e.; number of ensemble times.)
model, Atlas, which was fine-tuned with 64 training exam-
ples, achieving a new state-of-the-art in the few-shot setting.
However, this result still lags behind the performance of
retrieval-augmented language models fine-tuned on the full
training data. This is likely due to the presence of near-
duplicate test questions in the training set (e.g., Lewis et al.
(2021) found that 32.5% of test questions overlap with the
training sets in NQ).
7. Analysis
7.1. R EPLUG performance gain does not simply come
from the ensembling effect
The core of our method design is the use of an ensem-
ble method that combines output probabilities of different
passes, in which each retrieved document is prepended sep-
arately to the input and fed into a language model. To study
whether the gains come solely from the ensemble method,
we compare our method to ensembling random documents.
For this, we randomly sample several documents, concate-
nated each random document with the input, and ensemble
the outputs of different runs (referred to as "random"). As
shown in Figure 6, we evaluated the performance of GPT-3
Curie on Pile when augmented with random documents,
documents retrieved by REPLUG, and documents retrieved
byREPLUG LSR . We observed that ensembling random
documents leads to worse performance, indicating that the
performance gains of REPLUG do not solely come from the
ensembling effect. Instead, ensembling the relevant docu-
ments is crucial for the success of REPLUG. Additionally,
as more documents were ensembled, the performance of
REPLUG andREPLUG LSR improved monotonically. How-
ever, a small number of documents (e.g., 10) was sufficient
to achieve large performance gains.

REPLUG: Retrieval-Augmented Black-Box Language Models
Perplexity
14.0016.6019.2021.8024.4027.00
Parameters (Million)10047585012251600
Original
+ RE-PLUG
GPT-2
(a)
Perplexity
11.0013.6016.2018.8021.4024.00
Parameters (Million)1002075405060258000
Original
+ RE-PLUG
BLOOM (b)
Perplexity
9.0013.0017.0021.0025.0029.00
Parameters (Million)1001000100001000001000000
Original
+ RE-PLUG
OPT (c)
Figure 5. GPT-2, BLOOM and OPT models of varying sizes consistently benefit from REPLUG.The x-axis indicates the size of the
language model and the y-axis is its perplexity on Wikitext-103.
7.2. R EPLUG is applicable to diverse language models
Here we further study whether REPLUG could enhance di-
verse language model families that have been pre-trained
using different data and methods. Specifically, we focus on
three groups of language models with varying sizes: GPT-
2 (117M, 345M, 774M, 1.5B parameters) (Brown et al.,
2020a), OPT (125M, 350M, 1.3B, 2.7B, 6.7B, 13B, 30B,
66B) (Zhang et al., 2022b) and BLOOM (560M, 1.1B, 1.7B,
3B and 7B) (Scao et al., 2022). We evaluate each model on
Wikitext-103 (Stephen et al., 2017) test data and report its
perplexity. For comparison, we augment each model with
REPLUG that adopts the ensemble method to incorporate
top 10 retrieved documents. Following prior work (Khan-
delwal et al., 2020), we use Wikitext-103 training data as
the retrieval corpus.
Figure 5 shows the performance of different-sized language
models with and without REPLUG. We observe that the
performance gain brought by REPLUG stays consistent with
model size. For example, OPT with 125M parameters
achieves 6.9% perplexity improvement, while OPT with
66B parameters achieves 5.6% perplexity improvement. Ad-
ditionally, REPLUG improves the perplexity of all the model
families. This indicates that REPLUG is applicable to di-
verse language models with different sizes.
7.3. Qualitative Analysis: rare entities benefit from
retrieval
To understand why the REPLUG improves language model-
ing performance, we conducted manual analysis of exam-
ples in which the REPLUG results in a decrease in perplexity.
We find that REPLUG is more helpful when texts contain
rare entities. Figure 6 shows a test context and its contin-
uation from the Wikitext-103 test set. For REPLUG, we
use the test context as a query to retrieve a relevant docu-ment from Wikitext-103 training data. We then compute
the perplexity of the continuation using the original GPT-2
1.5B and its REPLUG enhanced version. After incorporating
the retrieved document, the perplexity of the continuation
improves by 11%. Among all tokens in the continuation, we
found that REPLUG is most helpful for the rare entity name
"Li Bai". This is likely because the original LM does not
have sufficient information about this rare entity name. How-
ever, by incorporating the retrieved document, REPLUG was
able to match the name with the relevant information in the
retrieved document, resulting in better performance.
Figure 6. Rare entities benefit from retrieval . After incorporat-
ing the retrieved document during inference, the entity " Li Bai "
and the token " greatest " in the continuation show the most im-
provement in perplexity (15% for " Li Bai " and 5% for " greatest ").
Other tokens’ perplexity changes are within 5%.
8. Conclusion
We introduce REPLUG, a retrieval-augmented language
modeling paradigm that treats the language model as a
black box and augments it with a tuneable retrieval model.
Our evaluation shows that REPLUG can be integrated with
any existing language model to improve their performance

REPLUG: Retrieval-Augmented Black-Box Language Models
on language modeling or downstream tasks. This work
opens up new possibilities for integrating retrieval into large-
scale black-box language models and demonstrates even the
state-of-the-art large-scale LMs could benefit from retrieval.
However, REPLUG lacks interpretability as it is unclear
when the model relies on retrieved knowledge or parametric
knowledge. Future research could focus on developing more
interpretable retrieval-augmented language models.
References
Borgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Ruther-
ford, E., Millican, K., Driessche, G. v. d., Lespiau, J.-B.,
Damoc, B., Clark, A., et al. Improving language mod-
els by retrieving from trillions of tokens. arXiv preprint
arXiv:2112.04426 , 2021.
Borgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Ruther-
ford, E., Millican, K., Van Den Driessche, G. B., Lespiau,
J.-B., Damoc, B., Clark, A., et al. Improving language
models by retrieving from trillions of tokens. In Interna-
tional Conference on Machine Learning , pp. 2206–2240.
PMLR, 2022.
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,
Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
Askell, A., Agarwal, S., Herbert-V oss, A., Krueger, G.,
Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J.,
Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,
Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S.,
Radford, A., Sutskever, I., and Amodei, D. Language
models are few-shot learners. In Larochelle, H., Ranzato,
M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances
in Neural Information Processing Systems , volume 33, pp.
1877–1901. Curran Associates, Inc., 2020a. URL https:
//proceedings.neurips.cc/paper/2020/file/
1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf .
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan,
J. D., Dhariwal, P., Neelakantan, A., Shyam, P.,
Sastry, G., Askell, A., Agarwal, S., Herbert-V oss, A.,
Krueger, G., Henighan, T., Child, R., Ramesh, A.,
Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M.,
Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J.,
Berner, C., McCandlish, S., Radford, A., Sutskever,
I., and Amodei, D. Language models are few-shot
learners. In Proc. of NeurIPS , 2020b. URL https:
//proceedings.neurips.cc/paper/2020/file/
1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf .
Chen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto,
H. P., Kaplan, J., Edwards, H., Burda, Y ., Joseph, N.,
Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov,
M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray,
S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavar-
ian, M., Winter, C., Tillet, P., Such, F. P., Cummings,D., Plappert, M., Chantzis, F., Barnes, E., Herbert-
V oss, A., Guss, W. H., Nichol, A., Paino, A., Tezak,
N., Tang, J., Babuschkin, I., Balaji, S., Jain, S., Saun-
ders, W., Hesse, C., Carr, A. N., Leike, J., Achiam,
J., Misra, V ., Morikawa, E., Radford, A., Knight, M.,
Brundage, M., Murati, M., Mayer, K., Welinder, P., Mc-
Grew, B., Amodei, D., McCandlish, S., Sutskever, I.,
and Zaremba, W. Evaluating large language models
trained on code. CoRR , abs/2107.03374, 2021a. URL
https://arxiv.org/abs/2107.03374 .
Chen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto,
H. P., Kaplan, J., Edwards, H., Burda, Y ., Joseph, N.,
Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov,
M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray,
S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavar-
ian, M., Winter, C., Tillet, P., Such, F. P., Cummings,
D., Plappert, M., Chantzis, F., Barnes, E., Herbert-
V oss, A., Guss, W. H., Nichol, A., Paino, A., Tezak,
N., Tang, J., Babuschkin, I., Balaji, S., Jain, S., Saun-
ders, W., Hesse, C., Carr, A. N., Leike, J., Achiam,
J., Misra, V ., Morikawa, E., Radford, A., Knight, M.,
Brundage, M., Murati, M., Mayer, K., Welinder, P., Mc-
Grew, B., Amodei, D., McCandlish, S., Sutskever, I.,
and Zaremba, W. Evaluating large language models
trained on code. CoRR , abs/2107.03374, 2021b. URL
https://arxiv.org/abs/2107.03374 .
Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,
G., Roberts, A., Barham, P., Chung, H. W., Sutton, C.,
Gehrmann, S., et al. Palm: Scaling language modeling
with pathways. arXiv preprint arXiv:2204.02311 , 2022.
Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y .,
Fedus, W., Li, E., Wang, X., Dehghani, M., Brahma,
S., et al. Scaling instruction-finetuned language models.
arXiv preprint arXiv:2210.11416 , 2022a.
Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y .,
Fedus, W., Li, E., Wang, X., Dehghani, M., Brahma,
S., et al. Scaling instruction-finetuned language models.
arXiv preprint arXiv:2210.11416 , 2022b.
Fajcik, M., Docekal, M., Ondrej, K., and Smrz, P. R2-
D2: A modular baseline for open-domain question an-
swering. In Findings of the Association for Computa-
tional Linguistics: EMNLP 2021 , pp. 854–870, Punta
Cana, Dominican Republic, November 2021. Association
for Computational Linguistics. doi: 10.18653/v1/2021.
findings-emnlp.73. URL https://aclanthology.org/
2021.findings-emnlp.73 .
Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T.,
Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N.,
Presser, S., and Leahy, C. The Pile: An 800gb dataset
of diverse text for language modeling. arXiv preprint
arXiv:2101.00027 , 2020.

REPLUG: Retrieval-Augmented Black-Box Language Models
Goyal, A., Friesen, A., Banino, A., Weber, T., Ke, N. R.,
Badia, A. P., Guez, A., Mirza, M., Humphreys, P. C.,
Konyushova, K., et al. Retrieval-augmented reinforce-
ment learning. In International Conference on Machine
Learning , pp. 7740–7765. PMLR, 2022.
Guu, K., Lee, K., Tung, Z., Pasupat, P., and Chang, M.
Retrieval augmented language model pre-training. In
International Conference on Machine Learning , pp. 3929–
3938. PMLR, 2020.
Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika,
M., Song, D., and Steinhardt, J. Measuring massive
multitask language understanding. In International Con-
ference on Learning Representations , 2021. URL https:
//openreview.net/forum?id=d7KBjmI3GmQ .
Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E.,
Cai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A.,
Welbl, J., Clark, A., et al. Training compute-optimal
large language models. arXiv preprint arXiv:2203.15556 ,
2022.
Hu, Y ., Hua, H., Yang, Z., Shi, W., Smith, N. A., and Luo, J.
Promptcap: Prompt-guided task-aware image captioning.
arXiv preprint arXiv:2211.09699 , 2022.
Izacard, G. and Grave, E. Leveraging passage retrieval
with generative models for open domain question an-
swering. In Proceedings of the 16th Conference of
the European Chapter of the Association for Compu-
tational Linguistics: Main Volume , pp. 874–880, On-
line, April 2021a. Association for Computational Lin-
guistics. doi: 10.18653/v1/2021.eacl-main.74. URL
https://aclanthology.org/2021.eacl-main.74 .
Izacard, G. and Grave, E. Leveraging passage retrieval with
generative models for open domain question answering.
InProc. of EACL , 2021b. URL https://arxiv.org/
abs/2007.01282 .
Izacard, G., Caron, M., Hosseini, L., Riedel, S., Bojanowski,
P., Joulin, A., and Grave, E. Unsupervised dense in-
formation retrieval with contrastive learning. Trans-
actions on Machine Learning Research , 2022a. URL
https://openreview.net/forum?id=jKN1pXi7b0 .
Izacard, G., Lewis, P., Lomeli, M., Hosseini, L., Petroni,
F., Schick, T., Dwivedi-Yu, J., Joulin, A., Riedel, S., and
Grave, E. Few-shot learning with retrieval augmented lan-
guage models. arXiv preprint arXiv:2208.03299 , 2022b.
Johnson, J., Douze, M., and Jégou, H. Billion-scale similar-
ity search with gpus. IEEE Transactions on Big Data , 7
(3):535–547, 2019.Joshi, M., Choi, E., Weld, D., and Zettlemoyer, L. Trivi-
aQA: A large scale distantly supervised challenge dataset
for reading comprehension. In Proceedings of the 55th
Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pp. 1601–1611,
Vancouver, Canada, July 2017. Association for Compu-
tational Linguistics. doi: 10.18653/v1/P17-1147. URL
https://aclanthology.org/P17-1147 .
Karpukhin, V ., Oguz, B., Min, S., Lewis, P., Wu, L., Edunov,
S., Chen, D., and Yih, W.-t. Dense passage retrieval
for open-domain question answering. In Proceedings of
the 2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP) , pp. 6769–6781, Online,
November 2020a. Association for Computational Lin-
guistics. doi: 10.18653/v1/2020.emnlp-main.550. URL
https://aclanthology.org/2020.emnlp-main.550 .
Karpukhin, V ., Oguz, B., Min, S., Lewis, P., Wu, L., Edunov,
S., Chen, D., and Yih, W.-t. Dense passage retrieval
for open-domain question answering. In Proceedings of
the 2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP) , pp. 6769–6781, 2020b.
Khandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer, L.,
and Lewis, M. Generalization through memorization:
Nearest neighbor language models. In International
Conference on Learning Representations , 2020. URL
https://openreview.net/forum?id=HklBjCEKvH .
Khattab, O., Santhanam, K., Li, X. L., Hall, D., Liang, P.,
Potts, C., and Zaharia, M. Demonstrate-search-predict:
Composing retrieval and language models for knowledge-
intensive nlp. arXiv preprint arXiv:2212.14024 , 2022.
Kingma, D. P. and Ba, J. Adam: A method for stochastic
optimization. In ICLR (Poster) , 2015.
Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M.,
Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Devlin,
J., Lee, K., Toutanova, K., Jones, L., Kelcey, M., Chang,
M.-W., Dai, A. M., Uszkoreit, J., Le, Q., and Petrov, S.
Natural questions: A benchmark for question answering
research. Transactions of the Association for Computa-
tional Linguistics , 7:452–466, 2019. doi: 10.1162/tacl_a_
00276. URL https://aclanthology.org/Q19-1026 .
Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V .,
Goyal, N., Küttler, H., Lewis, M., Yih, W.-t., Rocktäschel,
T., Riedel, S., and Kiela, D. Retrieval-augmented gen-
eration for knowledge-intensive nlp tasks, 2020. URL
https://arxiv.org/abs/2005.11401 .
Lewis, P., Stenetorp, P., and Riedel, S. Question and an-
swer test-train overlap in open-domain question answer-
ing datasets. In Proceedings of the 16th Conference of the
European Chapter of the Association for Computational
Linguistics: Main Volume , pp. 1000–1008, 2021.

REPLUG: Retrieval-Augmented Black-Box Language Models
Mallen, A., Asai, A., Zhong, V ., Das, R., Hajishirzi, H., and
Khashabi, D. When not to trust language models: Investi-
gating effectiveness and limitations of parametric and non-
parametric memories. arXiv preprint arXiv:2212.10511 ,
2022.
Min, S., Shi, W., Lewis, M., Chen, X., Yih, W.-t., Hajishirzi,
H., and Zettlemoyer, L. Nonparametric masked language
modeling. arXiv preprint arXiv:2212.01349 , 2022.
Ni, J., Qu, C., Lu, J., Dai, Z., Ábrego, G. H., Ma, J., Zhao,
V . Y ., Luan, Y ., Hall, K. B., Chang, M., and Yang, Y .
Large dual encoders are generalizable retrievers, 2021.
URL https://arxiv.org/abs/2112.07899 .
Qu, Y ., Ding, Y ., Liu, J., Liu, K., Ren, R., Zhao, W. X.,
Dong, D., Wu, H., and Wang, H. RocketQA: An opti-
mized training approach to dense passage retrieval for
open-domain question answering. In Proceedings of
the 2021 Conference of the North American Chapter
of the Association for Computational Linguistics: Hu-
man Language Technologies , pp. 5835–5847, Online,
June 2021. Association for Computational Linguistics.
doi: 10.18653/v1/2021.naacl-main.466. URL https:
//aclanthology.org/2021.naacl-main.466 .
Robertson, S., Zaragoza, H., et al. The probabilistic rele-
vance framework: Bm25 and beyond. Foundations and
Trends® in Information Retrieval , 3(4):333–389, 2009.
Rubin, O., Herzig, J., and Berant, J. Learning to retrieve
prompts for in-context learning. In Proceedings of the
2022 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Lan-
guage Technologies , pp. 2655–2671, 2022.
Sachan, D. S., Lewis, M., Yogatama, D., Zettlemoyer,
L., Pineau, J., and Zaheer, M. Questions are all you
need to train a dense passage retriever. arXiv preprint
arXiv:2206.10658 , 2022.
Scao, T. L., Fan, A., Akiki, C., Pavlick, E., Ili ´c, S., Hesslow,
D., Castagné, R., Luccioni, A. S., Yvon, F., Gallé, M.,
et al. Bloom: A 176b-parameter open-access multilingual
language model. arXiv preprint arXiv:2211.05100 , 2022.
Shi, W., Michael, J., Gururangan, S., and Zettlemoyer, L.
Nearest neighbor zero-shot inference. 2022.
Si, C., Gan, Z., Yang, Z., Wang, S., Wang, J., Boyd-Graber,
J., and Wang, L. Prompting gpt-3 to be reliable. arXiv
preprint arXiv:2210.09150 , 2022.
Si, C., Gan, Z., Yang, Z., Wang, S., Wang, J., Boyd-Graber,
J., and Wang, L. Prompting gpt-3 to be reliable. In
Proc. of ICLR , 2023. URL https://openreview.net/
forum?id=98p5x51L5af .Stephen, M., Caiming, X., James, B., and Socher, R. Pointer
sentinel mixture models. In 5th International Conference
on Learning Representations, ICLR 2017, Toulon, France,
April 24-26, 2017, Conference Track Proceedings , 2017.
Su, H., Kasai, J., Wang, Y ., Hu, Y ., Ostendorf, M., Yih, W.-t.,
Smith, N. A., Zettlemoyer, L., Yu, T., et al. One embedder,
any task: Instruction-finetuned text embeddings. arXiv
preprint arXiv:2212.09741 , 2022.
Wu, S., Zhao, X., Yu, T., Zhang, R., Shen, C., Liu, H., Li,
F., Zhu, H., Luo, J., Xu, L., et al. Yuan 1.0: Large-scale
pre-trained language model in zero-shot and few-shot
learning. arXiv preprint arXiv:2110.04725 , 2021.
Yasunaga, M., Aghajanyan, A., Shi, W., James, R.,
Leskovec, J., Liang, P., Lewis, M., Zettlemoyer, L., and
Yih, W.-t. Retrieval-augmented multimodal language
modeling. arXiv preprint arXiv:2211.12561 , 2022.
Younes Belkda, T. D. A gentle introduction to 8-bit matrix
multiplication, 2022. URL https://huggingface.co/
blog/hf-bitsandbytes-integration .
Yu, W. Retrieval-augmented generation across heteroge-
neous knowledge. In Proceedings of the 2022 Confer-
ence of the North American Chapter of the Association for
Computational Linguistics: Human Language Technolo-
gies: Student Research Workshop , pp. 52–58, Hybrid:
Seattle, Washington + Online, July 2022. Association
for Computational Linguistics. doi: 10.18653/v1/2022.
naacl-srw.7. URL https://aclanthology.org/2022.
naacl-srw.7 .
Yu, W., Iter, D., Wang, S., Xu, Y ., Ju, M., Sanyal, S., Zhu,
C., Zeng, M., and Jiang, M. Generate rather than retrieve:
Large language models are strong context generators.
2023.
Zhang, S., Diab, M., and Zettlemoyer, L. Democratizing ac-
cess to large-scale language models with opt-175b. Meta
AI, 2022a.
Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,
Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V .,
et al. Opt: Open pre-trained transformer language models.
arXiv preprint arXiv:2205.01068 , 2022b.
Zhong, Z., Lei, T., and Chen, D. Training language models
with memory augmentation. In Empirical Methods in
Natural Language Processing (EMNLP) , 2022.

REPLUG: Retrieval-Augmented Black-Box Language Models
Knowledge : Arctic Ocean. Although over half of Europe’s original forests disappeared through the centuries of deforestation, Europe
still has over one quarter of its land area as forest, such as the broadleaf and mixed forests, taiga of Scandinavia and Russia, mixed
rainforests of the Caucasus and the Cork oak forests in the western Mediterranean. During recent times, deforestation has been slowed
and many trees have been planted. However, in many cases monoculture plantations of conifers have replaced the original mixed natural
forest, because these grow quicker. The plantations now cover vast areas of land, but offer poorer habitats for many European
Question : As of 2015, since 1990 forests have in Europe and have in Africa and the Americas.
A. "increased, increased" B. "increased, decreased" C. "decreased, increased" D. "decreased, decreased"
Answer : B
Knowledge : Over the past decades, the political outlook of Americans has become more progressive, with those below the age of thirty
being considerably more liberal than the overall population. According to recent polls, 56% of those age 18 to 29 favor gay marriage,
68% state environmental protection to be as important as job creation, 52% "think immigrants ´strengthen the country with their hard
work and talents, ´" 62% favor a "tax financed, government-administrated universal health care" program and 74% "say ´people ´s will ´should
have more influence on U.S. laws than the Bible, compared to 37%, 49%, 38%, 47% and 58% among the
Question : As of 2019, about what percentage of Americans agree that the state is run for the benefit of all the people?
A. 31% B. 46% C. 61% D. 76%
Answer : B
...
Knowledge : last week at a United Nations climate meeting in Germany, China and India should easily exceed the targets they set for
themselves in the 2015 Paris Agreement... India is now expected to obtain 40 percent of its electricity from non-fossil fuel sources by
2022, eight years ahead of schedule." Solar power in Japan has been expanding since the late 1990s. By the end of 2017, cumulative
installed PV capacity reached over 50 GW with nearly 8 GW installed in the year 2017. The country is a leading manufacturer of solar
panels and is in the top 4 ranking for countries
Question : Which of the following countries generated the most total energy from solar sources in 2019?
A. China B. United States C. Germany D. Japan
Table 4. Prompt for MMLU
Knowledge : received 122,000 buys (excluding WWE Network views), down from the previous year ´s 199,000 buys. The event is named
after the Money In The Bank ladder match, in which multiple wrestlers use ladders to retrieve a briefcase hanging above the ring. The
winner is guaranteed a match for the WWE World Heavyweight Championship at a time of their choosing within the next year. On the
June 2 episode of "Raw", Alberto Del Rio qualified for the match by defeating Dolph Ziggler. The following week, following Daniel
Bryan being stripped of his WWE World Championship due to injury, Stephanie McMahon changed the
Question : Who won the mens money in the bank match?
Answer : Braun Strowman
Knowledge : in 3D on March 17, 2017. The first official presentation of the film took place at Disney ´s three-day D23 Expo in August
2015. The world premiere of "Beauty and the Beast" took place at Spencer House in London, England on February 23, 2017; and the
film later premiered at the El Capitan Theatre in Hollywood, California, on March 2, 2017. The stream was broadcast onto YouTube. A
sing along version of the film released in over 1,200 US theaters nationwide on April 7, 2017. The United Kingdom received the same
version on April 21, 2017. The film was re-released in
Question : When does beaty and the beast take place
Answer : Rococo-era
...
Knowledge : Love Yourself "Love Yourself" is a song recorded by Canadian singer Justin Bieber for his fourth studio album "Purpose"
(2015). The song was released first as a promotional single on November 8, 2015, and later was released as the album ´s third single. It
was written by Ed Sheeran, Benny Blanco and Bieber, and produced by Blanco. An acoustic pop song, "Love Yourself" features an
electric guitar and a brief flurry of trumpets as its main instrumentation. During the song, Bieber uses a husky tone in the lower registers.
Lyrically, the song is a kiss-off to a narcissistic ex-lover who did
Question : love yourself by justin bieber is about who
Table 5. Prompt for open-domain QA